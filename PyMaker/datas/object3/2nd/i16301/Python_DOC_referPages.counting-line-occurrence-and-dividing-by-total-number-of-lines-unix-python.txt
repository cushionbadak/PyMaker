Natural Text
I have a textfile  as such:The desired output should be:The new column is the count of the line divided by the total number of lines.Currently i'm doing this:But that only gives me the count of the lines not the probability. Also, my file is really huge, like 1,000,000,000 lines with at least 20 chars per column.How could I get the desired output correctly and quickly? Is there a pythonic solution that is as fast?
Note that the uniq only counts repeated lines, and it must be preceded by sort for it to consider all lines in a file. For , the following code using collections.Counter is much more effective as it does not need to sort anything at all:This script outputsfor the input given in your description.However if you need to coalesce only successive lines, like , note that any solution using  gives the output given in your question, but your  approach will not. The output of :notIf this is the behavior you want, you can use :The difference is that given a  having the contents as you prescribe, the uniq pipe would not produce the output you gave in the example, instead you would get: As this is not what your input example said, it can be that you can't use  without  to solve your problem - then you need to resort to my first example and Python will be most certainly faster than your Unix command line.By the way, these work identically in all Pythons > 2.6.
Here is a pure AWK solution:It uses AWK's arrays and the special variable , which keeps track of the number of lines.Let's dissect the code.  The first blockis executed once for each line in the input.  Here  represents every single line, and it is used as an index over the array , which therefore just counts the number of occurrences of each line.The second blockis executed at the end of the input.  At this point,  contains the number of occurrences for each line in the input and is indexed by the lines themselves: hence by cycling over it we are able to print a table of lines and relative occurrences (we divide by the total number of lines, ).
This has several advantages. It iterates through the lines in the file rather than loading the whole file, it takes advantage of existing  functionality, it can sort, and it's clear what's going on.
A solution in Python, but I'm not sure about performance on 1,000,000,000 lines.Output : Edit : this solution can be improved using the Counter object in Python : https://docs.python.org/2/library/collections.html#collections.Counter
Maybe by using dictionaries in python that automatically can only have one value    
Another solution in Python:Output:
This does not sort on probability. The performance is O(3n) because of the three loops and this could be reduced to O(2n) by using a subclass of TexIOBase which keeps track of lines or a subclass of Counter that keeps track of total lines processed.
If it's too expensive to do all the processing in RAM, you could consider a simple database. sqlite comes with all installations of python. This example could easily be optimized, but I felt simplicity favored speed when demonstrating the approach:


Answer URL
https://docs.python.org/3/library/collections.html#collections.Counter
https://docs.python.org/3/library/itertools.html#itertools.groupby
