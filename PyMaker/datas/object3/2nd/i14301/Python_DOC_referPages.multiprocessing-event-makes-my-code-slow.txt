Natural Text
I have a main process that is eating data from many different markets. It does some preliminary processing on the message and then passes it into a multiprocessing Queue (each unique market has its own dedicated process, call it , on the other end of the Queue). Then the main process calls  for the particular market involved. Within  is a call to , which pauses the process unless there is data being fed into the queue. At the end of  it calls  I do this because I am using a  loop to catch data the moment it comes through the queue. If I don't pause  it will use 100% of the CPU and I don't have enough cores for that (not to mention it's massively wasteful).This evening I realized that  is taking WAY too long to run, from .3 to 18 seconds. Market data messages can come in every 12 milliseconds so clearly this is unworkable. Every aspect of  is very fast, except for . This call accounts for almost 100% of the run time. I am storing all the  objects in a dictionary, defined in a config file. I fear that one of two things is happening:Each instance of setting and clearing the Event blocks all the other ones, in a way similar to how  works with shared objects.  is just slow, and takes a long time for its state to propagate across processes...I am thinking of solving this by piping the data with  (ZeroMQ) rather than a , but before I set that up I thought to ask the smart people. Am I doing something obviously wrong here? Is there any way to speed up the  flagging?  EDITIn response to the comment, here is an example: In the  file, I define the dictionary like so:Then in the main process which reads the data, I call , which looks something like this: Then finally in , which is started upon program startup:EDIT2Procs are spawned and started like this: 
I think your problem is that your  code is broken.Imagine this scenario:main process calls  for . calls  and . wakes up, does , and starts processing.main process calls  again for the same . calls  and . finishes processing the first message, calls .Now  is just waiting around for the  to be set again. Which may not happen for quite a while. And, even if it happens quickly, it's still not going to catch up; it only does one  for each .So, what you end up with is  appearing to fall farther and farther behind. And when you try to profile it to figure out why, you see that it's spending all its time waiting on . But this isn't because  is slow, it's just because the event trigger got lost.This isn't the only race condition here, it's just the most obvious one.The general problem is that you can't use event objects this way. Normally, you solve it by using a  instead, or one-shot triggering and self-resetting s, plus looping over the  after each .But really, there is no need to do this in the first place. If you just remove the  entirely, when  calls , that blocks until there's something there. So, when  calls , that wakes up , and there's nothing else needed.In fact, the whole point of  is that it's inherently self-synchronized. If you don't want that, use a , and then you can use a  for signaling. But in the simple case, that's just a less efficient version of a .


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.get
