Natural Text
I'm using a python script as a driver for a hydrodynamics code.  When it comes time to run the simulation, I use  to run the code, collect the output from stdout and stderr into a  --- then I can print (and save to a log-file) the output information, and check for any errors.  The problem is, I have no idea how the code is progressing.  If I run it directly from the command line, it gives me output about what iteration its at, what time, what the next time-step is, etc.Is there a way to both store the output (for logging and error checking), and also produce a live-streaming output?The relevant section of my code:Originally I was piping the  through  so that a copy went directly to the log-file, and the stream still output directly to the terminal -- but that way I can't store any errors (to my knowlege).Edit:Temporary solution:then, in another terminal, run  (s.t. ).
You have two ways of doing this, either by creating an iterator from the  or  functions and do:orOr you can create a  and a  file. Pass the  to the  and read from the This way you will have the data written in the  as well as on the standard output. The only advantage of the file approach is that your code doesn't block. So you can do whatever you want in the meantime and read whenever you want from the  in a non-blocking way. When you use ,  and  functions will block until either one character is written to the pipe or a line is written to the pipe respectively.
Executive Summary (or "tl;dr" version): it's easy when there's at most one , otherwise it's hard.It may be time to explain a bit about how  does its thing.(Caveat: this is for Python 2.x, although 3.x is similar; and I'm quite fuzzy on the Windows variant.  I understand the POSIX stuff much better.)The  function needs to deal with zero-to-three I/O streams, somewhat simultaneously.  These are denoted , , and  as usual.You can provide:, indicating that you don't want to redirect the stream.  It will inherit these as usual instead.  Note that on POSIX systems, at least, this does not mean it will use Python's , just Python's actual stdout; see demo at end.An  value.  This is a "raw" file descriptor (in POSIX at least).  (Side note:  and  are actually s internally, but are "impossible" descriptors, -1 and -2.)A stream—really, any object with a  method.   will find the descriptor for that stream, using , and then proceed as for an  value., indicating that Python should create a pipe. (for  only): tell Python to use the same descriptor as for .  This only makes sense if you provided a (non-) value for , and even then, it is only needed if you set .  (Otherwise you can just provide the same argument you provided for , e.g., .)The easiest cases (no pipes)If you redirect nothing (leave all three as the default  value or supply explicit ),  has it quite easy.  It just needs to spin off the subprocess and let it run.  Or, if you redirect to a non-—an  or a stream's —it's still easy, as the OS does all the work.  Python just needs to spin off the subprocess, connecting its stdin, stdout, and/or stderr to the provided file descriptors.The still-easy case: one pipeIf you redirect only one stream,  still has things pretty easy.  Let's pick one stream at a time and watch.Suppose you want to supply some , but let  and  go un-redirected, or go to a file descriptor.  As the parent process, your Python program simply needs to use  to send data down the pipe.  You can do this yourself, e.g.:or you can pass the stdin data to , which then does the  shown above.  There is no output coming back so  has only one other real job: it also closes the pipe for you.  (If you don't call  you must call  to close the pipe, so that the subprocess knows there is no more data coming through.)Suppose you want to capture  but leave  and  alone.  Again, it's easy: just call  (or equivalent) until there is no more output.  Since  is a normal Python I/O stream you can use all the normal constructs on it, like:or, again, you can use , which simply does the  for you.If you want to capture only , it works the same as with .There's one more trick before things get hard.  Suppose you want to capture , and also capture  but on the same pipe as stdout:In this case,  "cheats"!  Well, it has to do this, so it's not really cheating: it starts the subprocess with both its stdout and its stderr directed into the (single) pipe-descriptor that feeds back to its parent (Python) process.  On the parent side, there's again only a single pipe-descriptor for reading the output.  All the "stderr" output shows up in , and if you call , the stderr result (second value in the tuple) will be , not a string.The hard cases: two or more pipesThe problems all come about when you want to use at least two pipes.  In fact, the  code itself has this bit:But, alas, here we've made at least two, and maybe three, different pipes, so the  returns either 1 or 0.  We must do things the hard way.On Windows, this uses  to accumulate results for  and , and has the parent thread deliver  input data (and then close the pipe).On POSIX, this uses  if available, otherwise , to accumulate output and deliver stdin input.  All this runs in the (single) parent process/thread.Threads or poll/select are needed here to avoid deadlock.  Suppose, for instance, that we've redirected all three streams to three separate pipes.  Suppose further that there's a small limit on how much data can be stuffed into to a pipe before the writing process is suspended, waiting for the reading process to "clean out" the pipe from the other end.  Let's set that small limit to a single byte, just for illustration.  (This is in fact how things work, except that the limit is much bigger than one byte.)If the parent (Python) process tries to write several bytes—say, to , the first byte goes in and then the second causes the Python process to suspend, waiting for the subprocess to read the first byte, emptying the pipe.Meanwhile, suppose the subprocess decides to print a friendly "Hello! Don't Panic!" greeting.  The  goes into its stdout pipe, but the  causes it to suspend, waiting for its parent to read that , emptying the stdout pipe.Now we're stuck: the Python process is asleep, waiting to finish saying "go", and the subprocess is also asleep, waiting to finish saying "Hello! Don't Panic!".The  code avoids this problem with threading-or-select/poll.  When bytes can go over the pipes, they go.  When they can't, only a thread (not the whole process) has to sleep—or, in the case of select/poll, the Python process waits simultaneously for "can write" or "data available", writes to the process's stdin only when there is room, and reads its stdout and/or stderr only when data are ready.  The  code (actually  where the hairy cases are handled) returns once all stdin data (if any) have been sent and all stdout and/or stderr data have been accumulated.If you want to read both  and  on two different pipes (regardless of any  redirection), you will need to avoid deadlock too.  The deadlock scenario here is different—it occurs when the subprocess writes something long to  while you're pulling data from , or vice versa—but it's still there.The DemoI promised to demonstrate that, un-redirected, Python es write to the underlying stdout, not .  So, here is some code:When run:Note that the first routine will fail if you add , as a  object has no .  The second will omit the  if you add  since  has been redirected to .(If you redirect Python's file-descriptor-1, the subprocess will follow that redirection.  The  call produces a stream whose  is greater than 2.)
We can also use the default file iterator for reading stdout instead of using iter construct with readline(). 
If you're able to use third-party libraries, You might be able to use something like  (disclosure: I'm its maintainer). This library allows non-blocking access to output streams from subprocesses - it's layered over the  module.
A good but "heavyweight" solution is to use Twisted - see the bottom.If you're willing to live with only stdout something along those lines should work:(If you use read() it tries to read the entire "file" which isn't useful, what we really could use here is something that reads all the data that's in the pipe right now)One might also try to approach this with threading, e.g.:Now we could potentially add stderr as well by having two threads.Note however the subprocess docs discourage using these files directly and recommends to use  (mostly concerned with deadlocks which I think isn't an issue above) and the solutions are a little klunky so it really seems like the subprocess module isn't quite up to the job (also see: http://www.python.org/dev/peps/pep-3145/ ) and we need to look at something else.A more involved solution is to use Twisted as shown here: https://twistedmatrix.com/documents/11.1.0/core/howto/process.htmlThe way you do this with Twisted is to create your process using  and providing a  that then processes output asynchronously.  The Twisted sample Python code is here: https://twistedmatrix.com/documents/11.1.0/core/howto/listings/process/process.py
It looks like line-buffered output will work for you, in which case something like the following might suit. (Caveat: it's untested.)  This will only give the subprocess's stdout in real time.  If you want to have both stderr and stdout in real time, you'll have to do something more complex with .
Why not set  directly to ? And if you need to output to a log as well, then you can simply override the write method of f.
Here is a class which I'm using in one of my projects. It redirects output of a subprocess to the log. At first I tried simply overwriting the write-method but that doesn't work as the subprocess will never call it (redirection happens on filedescriptor level). So I'm using my own pipe, similar to how it's done in the subprocess-module. This has the advantage of encapsulating all logging/printing logic in the adapter and you can simply pass instances of the logger to : If you don't need logging but simply want to use  you can obviously remove large portions of the code and keep the class shorter. You could also expand it by an  and  method and call  in  so that you could easily use it as context.
All of the above solutions I tried failed either to separate stderr and stdout output, (multiple pipes) or blocked forever when the OS pipe buffer was full which happens when the command you are running outputs too fast (there is a warning for this on python poll() manual of subprocess). The only reliable way I found was through select, but this is a posix-only solution:
In addition to all these answer, one simple approach could also be as follows:Loop through the readable stream as long as it's readable and if it gets an empty result, stop it.The key here is that  returns a line (with  at the end) as long as there's an output and empty if it's really at the end.Hope this helps someone.
Similar to previous answers but the following solution worked for for me on windows using Python3 to provide a common method to print and log in realtime (getting-realtime-output-using-python):
Based on all the above I suggest a slightly modified version (python3):while loop calling readline (The iter solution suggested seemed to block forever for me - Python 3, Windows 7)structered so handling of read data does not need to be duplicated after poll returns not-stderr piped into stdout so both output outputs are readAdded code to get exit value of cmd.Code:
I think that the  method is a bit misleading: it actually fills the stdout and stderr that you specify in the .Yet, reading from the  that you can provide to the 's stdout and stderr parameters will eventually fill up OS pipe buffers and deadlock your app (especially if you've multiple processes/threads that must use ).My proposed solution is to provide the stdout and stderr with files - and read the files' content instead of reading from the deadlocking . These files can be  - which can also be accessed for reading while they're being written into by . Below is a sample usage:And this is the source code which is ready to be used with as many comments as I could provide to explain what it does:If you're using python 2, please make sure to first install the latest version of the subprocess32 package from pypi.
None of the Pythonic solutions worked for me.It turned out that  or similar may block forever.Therefore, I use  like this:This solution is convenient if you are already using . captures the success status of the entire command chain (only available in Bash).If I omitted the , then this would always return zero since  never fails. might be necessary for printing each line immediately into the terminal, instead of waiting way too long until the "pipe buffer" gets filled.However, unbuffer swallows the exit status of assert (SIG Abort)... also logs stderror to the file.


Answer URL
https://docs.python.org/3/library/functions.html#iter
