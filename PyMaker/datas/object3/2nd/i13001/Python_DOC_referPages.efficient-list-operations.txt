Natural Text
I have a large matrix (1,017,209 rows) from which I need to read out elements, make operations on them, and collecting the results into lists. When I do it on 10,000 rows or even 100,000 it finishes in a reasonable time, however 1,000,000 does not. Here is my code:What makes it slow at that scale (what is the bottleneck)? Are there ways to optimize it?
A couple of things:Don't read in the entire file at once, you don't appear to be doing anything that requires multiple lines.Look at using  for loading your data.Really stop indexing in the giant  list.


Answer URL
https://docs.python.org/3/library/csv.html#csv.reader
