Natural Text
Is there any global parameter (preferably env variable) one can set to limit the maximal cores used by Python? I know there are parameters to set for specific packages such as multithreading or numpy, but I would like to control Python itself so that I'm sure it won't use more than N cores.The reason I would like a global parameter is so that I can set it to users in my network so that we would be able to work together on the same machine; currently whenever one runs a script, Python just uses the most cores it can, jamming the others.Python version 2.6-7, Ubuntu version 14.04.Thanks.
using the affinity package you can limit the each process on a os level to single core. affinity just calls the underlying linux sched_setaffinity function, you can either set it to a specific core number or to a range.see the pthread_setaffinity_np  man page for more details on how to set ranges of cores, notice that if you want to set the affinity of other processes you should have permissions to manage that process.furthermore notice that children processes inherit the father's process affinity.


Answer URL
https://docs.python.org/3/glossary.html#term-global-interpreter-lock
