Natural Text
I want to use spacy as for NLP for an online service.Each time a user makes a request I call the script "my_script.py"which starts with:The problem I'm having is that those two lines take over 10 seconds, is it possible to keep English() in the ram or some other option to reduce this load time to less than a second?
You said that you want to launch a freestanding script () whenever a request comes in. This will use capabilites from  without the overhead of loading . With this approach, the operating system will always create a new process when you launch your script. So there is only one way to avoid loading  each time: have a separate process that is already running, with  loaded, and have your script communicate with that process. The code below shows a way to do that. However, as others have said, you will probably benefit by changing your server architecture so  is loaded within your web server (e.g., using a Python-based web server).The most common form of inter-process communication is via TCP/IP sockets. The code below implements a small server which keeps  loaded and processes  requests from the client. It also has a client which transmits requests to that server and receives results back. It's up to you to decide what to put into those transmissions.There is also a third script. Since both client and server need send and receive functions, those functions are in a shared script called . (Note that the client and server each load a separate copy of ; they do not communicate through a single module loaded into shared memory.)I assume both scripts are run on the same machine. If not, you will need to put a copy of  on both machines and change  to the machine name or IP address for the server.Run  as a background process (or just in a different terminal window for testing). This waits for requests, processes them and sends the results back:Load  as a quick-running script to request a result from the nlp server (e.g., ): contains code that is used by both the client and server:Note: you should make sure the result sent from the server only uses native data structures (dicts, lists, strings, etc.). If the result includes an object defined in , then the client will automatically import  when it unpacks the result, in order to provide the object's methods.This setup is very similar to the HTTP protocol (server waits for connections, client connects, client sends a request, server sends a response, both sides disconnect). So you might do better to use a standard HTTP server and client instead of this custom code. That would be a "RESTful API", which is a popular term these days (with good reason). Using standard HTTP packages would save you the trouble of managing your own client/server code, and you might even be able to call your data-processing server directly from your existing web server instead of launching . However, you will have to translate your request into something compatible with HTTP, e.g., a GET or POST request, or maybe just a specially formatted URL.Another option would be to use a standard interprocess communication package such as PyZMQ, redis, mpi4py or maybe zmq_object_exchanger. See this question for some ideas: Efficient Python IPCOr you may be able to save a copy of the  object on disk using the  package (https://pypi.python.org/pypi/dill) and then restore it at the start of . That may be faster than importing/reconstructing it each time and simpler than using interprocess communication.  
Your target should be to initialize the spacy models only once.Use a class , and make spacy a class attribute. Whenever you would use it, it would be the same instance of the attribute.
So here is a hack to do this ( I personally would refactor my code and not do this but since your requirement does not elaborate much i am going to suggest this-)You must have a daemon which runs the online service. Import spacy in the daemon and pass it as a parameter to the file that does the nlp stuff. I would refactor my code to use a class as mentioned in the solution by @dhruv which is much cleaner. The following example is a rough sketch of how to go about things. (Very bad programming principle though.) File1.pyFile2.pyThe above method will have a load time for the very first time the daemon is started , after that it's just a function call.Hope this helps!
Your fundamental problem here is launching a new script for every request. Instead of running a script for every request, run a function from within the script on every request.There are a variety of ways to handle user requests. The simplest is to periodically poll for requests and add them to a queue. The async framework is also useful for this kind of work.This talk by raymond hettinger is an excellent introduction to concurrency in Python.
Since you are using Python you can program some sort of workers (I think at some point you will need to scale you application also) where these initialisation are only done once! We have tried Gearman for similar usecase and it works well.Cheers


Answer URL
