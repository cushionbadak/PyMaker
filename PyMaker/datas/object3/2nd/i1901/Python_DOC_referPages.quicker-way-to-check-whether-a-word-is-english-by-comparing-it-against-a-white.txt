Natural Text
I am trying to eliminate all non-english words from many (100k) preprocssed text files (porter stemmed and lowercased, dropped all non a-z characters). I already parallelized the process to speed things up, but it is still painfully slow. Is there any more efficient way to do this in python?   
You are checking for  - and your  is a list. Lists are good for storing stuff, but lookup of "does x is in" your list takes . Use a  instead, that drops the lookup to  and it eleminate any dupes so your base -size of things to look up in drops as well if you got duplicates.If your set does not change afterwards, go and use a  - which is immutable.Read: Documentation of setsIf you use follow @DeepSpace 's suggestion, and leverage set operations you get even better performance:Output:See set operationsO(n): worstcase: your word is the last of 200k words in your list and you check the whole list - which takes 200k checks.O(1): lookup time is constant, no matter how many items are in your datastructure, it takes the same amount of time to check if its in. To get this benefit, a  has a more complex storage-solution that takes slightly more memory (then a list) to perform so well on lookups.Edit: worst case scenario for not finding a word inside a set/list:


Answer URL
https://docs.python.org/3/tutorial/datastructures.html#sets
https://docs.python.org/3/library/stdtypes.html#frozenset
