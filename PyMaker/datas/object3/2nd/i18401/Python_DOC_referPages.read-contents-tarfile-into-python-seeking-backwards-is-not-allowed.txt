Natural Text
I am new to python. I am having trouble reading the contents of a tarfile into python. The data are the contents of a journal article (hosted at pubmed central). See info below. And link to tarfile which I want to read into Python.http://www.pubmedcentral.nih.gov/utils/oa/oa.fcgi?id=PMC13901ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/b0/ac/Breast_Cancer_Res_2001_Nov_9_3(1)_61-65.tar.gzI have a list of similar .tar.gz file I will eventually want to read in as well. I think (know) all of the tarfiles have a .nxml file associated with them. It is the content of the .nxml files I am actually interested in extracting/reading. Open to any suggestions on the best way to do this...Here is what I have if I save the tarfile to my PC. All runs as expected.I learned today that to in order to access the tarfile directly from the pubmed centrals FTP site I have to set up a network request using . Below is the revised code (and link to stackoverflow answer I received):Read contents of .tar.gz file from website into a python 3.x objectHowever, when I run the remaining piece of the code (below) I get an error message ("seeking backwards is not allowed"). How come?The code fails on the last line, where I try to read the .nxml content associated with my tarfile. Below is the actual error message I receive. What does it mean? What is my best workaround for reading/accessing the content of these .nxml files which are all embedded in tarfiles?Thanks in advance for your help. Chris
What's going wrong: Tar files are stored interleaved. They come in the order header, data, header, data, header, data, etc. When you enumerated the files with , you've already read through the entire file to get the headers. Then when you asked the tarfile object to read the data, it tried to seek backward from the last header to the first data. But you can't seek backward in a network stream without closing and reopening the urllib request.How to work around it: You'll need to download the file, save a temporary copy to disk or to a StringIO, enumerate the files in this temporary copy, and then extract the files you want.
I had the same error when trying to  the file, so I extracted all to a tmp directory instead of using , or :


Answer URL
