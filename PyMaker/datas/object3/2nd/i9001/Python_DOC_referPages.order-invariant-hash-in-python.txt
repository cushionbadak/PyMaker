Natural Text
In Python, I would like to quickly compute an order-invariant hash for the lines of a file as a way to identify "uniquely" its content.  These files are for example the output of a  and thus the order of the lines is random.Here is an example that achieves what I want (using one of the hashers in hashlib), but at the expense of having to sort the lines.  Note that sorting the lines is just a way to achieve the goal, i.e. to get a hash that doesn't depend on the ordering of the lines in the file. But clearly, I'd like to avoid the O(n*log(n)) cost, esp. when the files are much longer.So, for e.g. 1MB, 50K rows file:But:What is a better/faster way to do that?As noted in this answer, Scala has an order-invariant hash based on Murmurhash, but I assume it is the 32-bit version of mmh3 (too collision-prone for my usage), and also I would rather use some standard library available in Python rather than implementing something in C or in Cython. Murmurhash3 has a 128bit version, but its output is different on x64 vs x86. I would like to have machine independent results.So, in summary, I would like:consistent results across machine architectureslow collision rate, i.e. at least 128 bits with good dispersion (but I don't need the hash to be cryptographic)reasonably fast, i.e. at least under 5ms for 1MB, 50K lines file.readily available, if possible, as a library on PyPi or Conda.amenable to files with repeated lines (so just XORing per-line hashes is a non-starter, as any pair of identical lines would cancel each other).Edits and notes:Thanks to several comments, the code above is updated to sort lines in memory.  The original version for  was:The associated wall time (for the file used above) was then 238 ms.  This is now reduced to 77 ms, but still way slower than not sorting the lines. Sorting will add a n*log(n) cost for n lines.The encoding (to UTF-8) and reading in mode  nor  is necessary when reading lines, as then we get strings not bytes.  I don't want to rely on assuming that the files contain only ASCII data; reading in  could lead to lines not properly split. I don't have the same concern when  is False, because then I don't have to split the file, and thus the fastest way is to slurp chunks of binary data to update the hasher.
I think you should sort the file before () or come up with another solution for your actual problem.Anyways, a possible approach in Python using a frozenset:OutputI doubt it will match your performance constrains. I don't know the time complexity (Big O) of the frozenset(). It also assumes lines are unique. Again, I highly suggest to tackle the underlying problem differently.
How about this merkle-style map-reduce (hash concatenated mapped hashes, optional sort for invariant after hash map step):
Thank you all for the interesting comments and answers so far.At this time, the best answer for large files (>350K lines) is (a) below. It is based on Murmurhash3, adding the  of each line. For smaller files, it is (b) below: a variant of the frozenset approach proposed by Rolf, which I adapted to produce 128 bits hash (although I wouldn't vouch for the quality of those 128 bits).a)  for each line and addIn my setting: constant 0.4 second per million lines.b) two frozenset hashIn my setting: between 0.2 and 0.6 second per million lines.NotesAfter consideration, I decided it was ok to read the lines of the file in binary mode, even if they potentially contain UTF-8 text.  The reason is, if some Unicode character contains a , the line would be accidentally split at that point. The file would then get the same digest as another, where the two parts of that line were arranged differently (or even split apart and put at some other place through the file), but the probability of this is extremely slow and I can live with it.Adding all the 128-bit hashes in (a) is done using Python's arbitrary precision ints. At first, I tried to keep the sum in 128 bits (by repeatingly and-ing with the  constant).  But it turns out to be slower than letting Python use arbitrary precision and do the masking once at the end.I am trying to get 128 bits from the regular hash of a frozenset by taking two hashes: that of the frozenset, and another one from the frozenset augmented with a line that is unlikely to appear in any file (kind of the same as using different seed for the hash, I guess).Complete resultsA full notebook is available here.  It creates pseudo-random files of arbitrary sizes, and tries several digest approaches while measuring the time taken by each of them.  This is run on an EC2 instance (r3.4xlarge, using an EBS volume for the storage of the pseudo-random file) and Jupyter iPython notebook, and Python 3.6.For 46341 lines, we get: These are order-dependent, just here for comparison. isn't really suitable as it gives only 64 bits. is a cythonized version of the function given above in (a), but there is little difference. is extremely fast, but it is order dependent.  My attempts (not listed here) to derive an order-invariant version all gave some pretty slow results. The reason, I think, is the apparently high cost of initializing and finalizing the hash.For larger files, the  wins. Here is for 11.8M lines:Focusing on the two leading contenders (order invariant, not the others), here is how much time they take in function of size (number of lines). The y-axis is microseconds/line and the x-axis is number of lines of the file. Note how the  spends a constant time (0.4 us) per line.Next stepsSorry for the long-winded answer. This is only an interim answer, as I might (time permitting) try later further experimentation with either numba or Cython (or C++) of a direct implementation of Murmurhash3.


Answer URL
https://docs.python.org/3/library/stdtypes.html#frozenset
