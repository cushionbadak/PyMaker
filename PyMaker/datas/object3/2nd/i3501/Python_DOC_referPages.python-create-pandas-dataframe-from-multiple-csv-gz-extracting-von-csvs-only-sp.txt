Natural Text
I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:I guess I need some help within the for loop???
If you have same columns in all your  files then you can try the code below.I have added  so that after reading  first row can be assigned as the column names.
An alternative to darindaCoder's answer:

Edit: I googled my way into https://stackoverflow.com/a/21232849/186078.However of late I am finding it faster to do any manipulation using numpy and then assigning it once to dataframe rather than manipulating the dataframe itself on an iterative basis and it seems to work in this solution too.I do sincerely want anyone hitting this page to consider this approach, but don't want to attach this huge piece of code as a comment and making it less readable.  You can leverage numpy to really speed up the dataframe concatenation. Timing stats:
The Dask library can read a dataframe from multiple files: (Source: http://dask.pydata.org/en/latest/examples/dataframe-csv.html)The Dask dataframes implement a subset of the Pandas dataframe API. If all the data fits into memory, you can call  to convert the dataframe into a Pandas dataframe.
Almost all of the answers here are either unnecessarily complex (glob pattern matching) or rely on additional 3rd party libraries. You can do this in 2 lines using everything Pandas and python (all versions) already have built in.For a few files - 1 liner:For many files:This pandas line which sets the df utilizes 3 things:Python's map (function, iterable) sends to the function (the) the iterable (our list) which is every csv elementin filepaths).Panda's read_csv() function reads in each CSV file as normal.Panda's concat() brings all these under one df variable.
If you want to search recursively (Python 3.5 or above), you can do the following:Note that the three last lines can be expressed in one single line:You can find the documentation of  here. Also, I used instead of , as it returns an iterator instead of a list.EDIT: Multiplatform recursive function:You can wrap the above into a multiplatform function (Linux, Windows, Mac), so you can do:Here is the function:
If the multiple csv files are zipped, you may use zipfile to read all and concatenate as below: 
I found this method pretty elegant.
one liner using , but if you'd like to specify additional args, you could do:Note:  be itself does not let you supply additional args.
Another on-liner with list comprehension which allows to use arguments with read_csv.
Easy and FastImport 2 or more 's without having to make a list of names. 


Answer URL
https://docs.python.org/3/library/glob.html#glob.glob
https://docs.python.org/3/library/glob.html#glob.iglob
