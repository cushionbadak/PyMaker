Natural Text
Problem : Iam working on a data analysis project which require me to compare the substrings of an unknown word against a corpus of good and bad words.I initially generated 4 lists and stored them in pickle format in disk.So in my code whenever a new string arises i will take these 4 lists and compare the sub-strings into these 4 lists and calculate the score.Due to all these 4 lists stored in memory,program is slowAlso each list has millions of words and if i want to do a search iam taking much longer time as it is taking O(n) time.Solution Required:Any effiecient way to store the 4 lists so that they wont bulk up my memory.Any better way to search the string in the 4 lists.How to access large lists in python.Code Part:Example Explaination clean_x_list = ['ap','pp','pl','le','bo','xl','ap'] clean_y_list = ['apa','ppa','fpl','lef','bfo','xdl','mpd'] bad_x_list = ['ti','qw','zx','qa','qa','qa','uy'] bad_y_list =  ['zzx','zxx','qww','qww','qww','uyx','uyx']Here suppose these are my 4 Lists:My new string came -- suppose apple - Now i will calculate x words for apple => ['ap','pp','pl','le'] - Now i will calculate y words for apple => ['app','ppl','ple','lea']Now i will search each x-word of apple i.e ['ap','pp','pl','le'] in both clean_x_list and bad_x_listthen i will calculate frequency and occurency countoccurence of ap in clean_x_list = 2frequence of ap in clean_x_list = 2 /7occurence of ap in bad_x_list   = 0occurence of ap in bad_x_list   = 0 /7similarly i compute the other words occurence and frequency and finally sum it
Consider sorting your lists, and using  for searching your lists. Worst case lookup time is O(log n) in this case.
There are basically three options: A linear scan of the list in O(n), ...... a binary search in the sorted list in O(logn), using the  module, ...... and a lookup in a hash  in O(1): Obviously, the  is by far the fastest, but it also takes up a bit more memory than the  based approaches. The  approach might be a good compromise, being already 5000 times faster then the linear scan in this example and only requiring to sort the list.However, unless your computer is very limited on memory, this should not be a problem. In particular, it will not make the code slower. Either it all fits into memory, and all is well, or it does not, and your program comes to a grinding halt.If your good/bad word lists can contain duplicates, then  is not an option, and  will not work well, either. In this case, create a  for each of those lists. Then you can get the occurrence count and frequencies for each of the substrings in your text. Being a kind of hash map / dictionary, lookup in the  will also be O(1).And analogously for , , and so on.
One option to save space is to store the word files in a compressed way, but also not read the entire word file into memory.  For this an easy option is gzip.GzipFile which allows you to operate on a gzip archive like a regular file:This way you can treat each line in the file as an item in a list, and process them accordingly.Note the  (or ) open method, which causes it to be processed as text, not binary - this will depend on if you're just storing plain text/json, or using a binary format for your data (like pickle).


Answer URL
https://docs.python.org/3/library/bisect.html
https://docs.python.org/3/library/collections.html#collections.Counter
https://docs.python.org/3/library/gzip.html#gzip.GzipFile
