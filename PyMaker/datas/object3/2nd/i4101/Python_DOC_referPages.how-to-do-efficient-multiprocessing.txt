Natural Text
I am using multiprocessing.Pool.map to run my code in parallel in my workstation which has got 10 physical cores (20 cores if I include the logical ones also).To summarize my code, I have to do some calculations with 2080 matrices. So,I divide 2080 matrices to 130 groups each containing 16 matrices.The calculation of these 16 matrices are then distributed over 16 cores (Should I use only 10 since I have only 10 physical cores?) using multiprocessing.Pool.map.My questions are:(1) When I monitor the usage of CPU in 'system monitor' in Ubuntu, I find many a times only 1 CPU usage is showing 100% instead of 16 CPU's showing 100% of usage. 16 CPU's show 100% usage only for short duration. Why does this happen? How to improve it?(2) Will I be able to improve the calculation time by dividing 2080 matrices into 104 groups each having 20 matrices and then distribute the calculation of these 20 matrices over 10 or 16 cores?My code snippet is as below:Any help will be really useful as I am first time parallelizing a python code.Thank you.EDIT:I tried the following chnages in the code and it is working fine now:   pool.imap_unordered(f,range(2080),chunksize=260)
Your problem is here:You're creating a new Pool at every loop and submitting only a few jobs to it. In order for the loop to continue, the few jobs have to complete before more jobs can be executed. In other words, you're not using parallelism at its full potential.What you should do is create a single Pool, submit all the jobs, and then, out of the loop, join:Note the use of  instead of : this is, again, to avoid waiting for a small portion of jobs to complete before submitting new jobs.An even better approach is to call / only once, constructing a single range object and avoiding the for-loop:As for your question about the number of CPUs to use, first of all note that Pool will use all the CPUs available (as returned by  by default if you don't specify the  argument -- give it a try.It's not clear to me what you mean by having 10 physical cores and 20 logical ones. If you're talking about hyperthreading, then it's fine: use them all. If instead you're saying that you're using a virtual machine with more virtual CPUs than the host CPUs, then using 20 instead of 10 won't make much difference.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map_async
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map
https://docs.python.org/3/library/os.html#os.cpu_count
