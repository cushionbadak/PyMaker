Natural Text
I hope this isn't a duplicate, but I couldn't find a fully satisfying answer for this specific problem. Given a function with multiple list arguments and one iterable, e.g. here with two listsEach list get accesed at different entries therefore the operations are seperated and can be parallized. What is the best way to do this with python's multiprocessing?One easy way of parallelization would be by using the map-function:The problem is if one does so one produces for every process a copy of the lists (if I understand the map-function right) and work then in parallel at different position in those copies. At the end (after the two iterables [0,1] has been touched) the result of pool.map isbut I wantHow to achieve this? Should one split the list's by the iterable before, run the specific operations in parallel and then merge them again?Thanks in advance and excuse please if I mix something up, I just started to use the multiprocessing-library.EDIT: Operations on different parts on a list can be parallized without synchronization, operations on the whole list can not be parallized (without synchronization). Therefore a solution to my specific problem is to split the lists and the function into the operations and into parts of the lists. After that one merges the parts of the lists to get the whole list back.
You cannot share memory between processes (technically, you can on fork-based systems provided you don't change objects/affect ref count which would rarely ever happen in a real-world usage) - your options are to either use a shared structure (most of them available under the ) which will do the synchronization/updates for you, or to pass only the data needed for processing and then stitch back together the result.Your example is simple enough for both approaches to work without serious penalties so I'd just go with a manager:Or if your use case is more favorable to stitching the data after processing:That being said, the output is not what you've listed/expected but it is what should happen based on your function.Also, if your case is this simple you might want to steer clear from multiprocessing altogether - the overhead multiprocessing adds (plus the manager synchronization) is not worth it unless  does some really CPU-intensitve task.
Here a solution to the problem. I dunno if this is the best way but it works:


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing-managers
