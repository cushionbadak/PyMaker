Natural Text
I am creating a bag of words from a text corpus and am trying to limit the size of my vocabulary because the program freezes when I try to convert my list to a pandas dataframe. I am using Counter to count the number occurrences of each word:My input would be a list of tokens of length num_samples where each text sample is a list of tokens. For my output I want a pandas DataFrame with shape (num_samples, 10000) where 10000 is the size of my vocabulary. Before, my  vocabulary size () would get very large (greater than 50,000.How can I choose the 10,000 most frequently occurring words from my  list of Counter objects and place then in a DataFrame while preserving number of text samples?
To find the overall top 10000 words, the easiest way would be  a global :At this point, you could just useIf you would like to find the count of the words for the specific entries, add now the following code (after the previous one).Now just use
 returns you the most common n elements.Here : https://docs.python.org/3/library/collections.html#collections.Counter.most_common
you can most frequently occuring words by using counter most_comman helping function:


Answer URL
https://docs.python.org/3/library/collections.html#collections.Counter.most_common
