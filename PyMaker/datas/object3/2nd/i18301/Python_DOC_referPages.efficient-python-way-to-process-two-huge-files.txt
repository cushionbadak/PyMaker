Natural Text
I am working on a problem where I have to find if a number falls within a certain range. However, the problem is complicated due to the fact that the files I am dealing with have hundreds of thousands of lines. Below I try to explain the problem in as simple a language as possible. Here is a brief description of my input files :File Ranges.txt has some ranges whose min and max are tab separated.This can have about 10,000,000 such lines with ranges. NOTE: The ranges never overlap. File Numbers.txt has a list of numbers and some values associated with each number. And so on. Again there are hundreds of thousands of such lines with numbers and their associated values.What I wish to do is take every number from Numbers.txt and check if it falls within any of the ranges in Ranges.txt. For all such numbers that fall within a range, I have to get a mean of their associated values (ie a mean per range). For eg. in the example above in Numbers.txt, there are two numbers 34 and 37 that fall within the range 30-40 in Ranges.txt, so for the range 30-40 I have to calculate the mean of the associated values of 34 and 37. (i.e mean of 0.79 and 0.87), which is 0.82My final output file should be the Ranges.txt but with the mean of the associated values of all numbers falling within each range. Something like :Output.txtand so on.Would appreciate any help and ideas on how this can be written efficiently in Python. 
Obviously you need to run each line from Numbers.txt against each line from Ranges.txt.You could just iterate over Numbers.txt, and, for each line, iterate over Ranges.txt. But this will take forever, reading the whole Ranges.txt file millions of times.You could read both of them into memory, but that will take a lot of storage, and it means you won't be able to do any processing until you've finished reading and preprocessing both files.So, what you want to do is read Ranges.txt into memory once and store it as, say, a list of pairs of ints instead, but read Numbers.txt lazily, iterating over the list for each number.This kind of thing comes up all the time. In general, you want to make the bigger collection into the outer loop, and make it as lazy as possible, while the smaller collection goes into the inner loop, and is pre-processed to make it as fast as possible. But if the bigger collection can be preprocessed more efficiently (and you have enough memory to store it!), reverse that.And speaking of preprocessing, you can do a lot better than just reading into a list of pairs of ints. If you sorted Ranges.txt, you could find the closest range without going over by bisecting then just check that (18 steps), instead of checking each range exhaustively (100000 steps).This is a bit of a pain with the stdlib, because it's easy to make off-by-one errors when using , but there are plenty of ActiveState recipes to make it easier (including one linked from the official docs), not to mention third-party modules like  or  that give you a sorted collection in a simple OO interface.So, something like this pseudocode:By the way, if the parsing turns out to be any more complicated than just calling  on each line, I'd suggest using the  module… but it looks like that won't be a problem here.What if you can't fit Ranges.txt into memory, but can fit Numbers.txt? Well, you can sort that, then iterate over Ranges.txt, find all of the matches in the sorted numbers, and write the results out for that range.This is a bit more complicated, because it you have to bisect_left and bisect_right and iterate everything in between. But that's the only way in which it's any harder. (And here, a third-party class will help even more. For example, with a  as your sorted collection, it's just .)If the ranges can overlap, you need to be a bit smarter—you have to find the closest range without going over the start, and the closest range without going under the end, and check everything in between. But the main trick there is the exact same one used for the last version. The only other trick is to keep two copies of ranges, one sorted by the start value and one by the end, and you'll need to have one of them be a map to indices in the other instead of just a plain list.
The naive approach would be to read Numbers.txt into some structure in number order, then read each line of Ranges, us a binary search to find the lowest number in the range, and the read through the numbers higher than that to find all those within the range, so that you can produce the corresponding line of output.I assume the problem is that you can't have all of Numbers in memory.So you could do the problem in phases, where each phase reads a portion of Numbers in, then goes through the process outlined above, but using an annotated version of Ranges, where each line includes the COUNT of the values so far that has produced that mean, and will write a similarly annotated version.Obviously, the initial pass will not have an annotated version of Ranges, and the final pass will not produce one.
It looks like your data in both the files are already sorted.  If not, first sort them by an external tool or using Python.Then, you can go through the two files in parallel.  You read a number from the  file, and see if it is in a range in  file, reading as many lines from that file as needed to answer that question.  Then read the next number from , and repeat.  The idea is similar to merging two sorted arrays, and should run in  time,  and  are the sizes of the files.  If you need to sort the files, the run time is .  Here is a quick program I wrote to implement this:On files with 10,000,000 numbers in each of the files, the above runs in about 1.5 minute on my computer without the output part.


Answer URL
