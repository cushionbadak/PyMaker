Natural Text
In Python >= 3.3, in order to help troubleshoot Unicode encoding and decoding issues, I would like to be able to examine, from Python code, the actual internal data stored in strings. How do I do that?There is a str.encode() method, which returns a bytes representation, but in general this is a byte sequence as translated by a particular codec (selected by the "encoding" argument), not the actual raw bytes stored in the str object. There is a "unicode_internal" encoding option, but it's deprecated, and it's not clear whether, in 3.3, it returns the real internal data (organized how?), or some translation of it.PEP 393 describes the internal structure of Unicode data, from which it appears that access to this from Python would need to report string kind (1/2/4 byte), representation (ASCII/compact) and also an array of bytes containing the string contents, (whose format is ASCII, UCS1, 2, or 4, I think).I've not found methods on the str type that offer this access within Python. Is there some other approach? Perhaps a clever way to use struct? Or a C library that exposes these string internals? Update 2014-03-13:Thanks to all who have responded with advice about why one should not want to access a string's internal structure. This is certainly valid advice for a normal Python program. Nonetheless, my question is: how to do it? To expand on the rationale: it is in order to troubleshoot encoding-decoding problems, where one function (in some library perhaps) creates and returns a str, and another function (perhaps in some other library) is supposed to do something with that str. I want to inspect the exact contents of that intermediate str,  (ie: I want to split the problem space in half), and to do so without introducing the further variable of having one or another python function transform that data into some other form (like ASCII with escape sequences). Amongst other reasons, I want to know the exact internal data in case one of another of the libraries is actually sensitive to the internal data format. Said libraries might well be written in C, have access to that data, and handling it incorrectly.Also, it is indeed supposed to be the case that a str should be treatable as a sequence of code points with internal internal representation of no concern. But if there is actually a bug in string handling, I don't want to be misled by it, and if there isn't, I'd like the confidence that there isn't. Given the complexity of the string library, zero bugs would be quite an achievement.So: How might I inspect the string's internal structure?
A Unicode string in Python should be considered as a sequence of Unicode code points.  How this is represented internally is completely immaterial to encoding and decoding issues.You can access the numerical values of the Unicode code points by using the  function on the individual characters of the string:I don't think this is particular helpful for debugging encoding issues (or for anything else), but it might clarify what a Unicode string is conceptually.
The internal switch to a more space-efficient storage for unicode values introduced by PEP-393 were made for performance reasons only.As such they have zero impact on how encoding from and decoding to unicode  values works in Python code. There is absolutely no point in accessing the internal representation from Python. The character  is either stored as ,  or , depending on how much space the highest codepoint in the string demands, but it'll still be encoded to  in ASCII, Latin-1 or UTF-8.Unless you are writing a C extension that has to deal with this internal representation, there is absolutely no need to worry about how Python actually stored the data.To debug encoding or decoding issues, I'd use the  function to represent a string using only ASCII codepoints and Python string literal escapes, or you can use the  function for turning individual characters into an integer for each codepoint.For bytes values, the  function also comes in handy to quickly turn a series of bytes into their hex representations.


Answer URL
