Natural Text
What is the fastest way to process each line of a csv and write to a new csv ? Is there a way to use the least memory as well as be the fastest? Please see the following code. It requests a csv from an API but it takes very long to go through the for loop I commented. Also I think it is using all the memory on my server.When I run the python memory profiler , why is the line on the for loop having memory increment?  Is the actual for loop saving something in memory, or is my utf-8 convertor messing something up?When I put the "@profile" symbol on the utf_8-encoder function as well, I see the memory on the above for loop disappeared:But now there is memory on the convertor's for loop (i didn't let it run as long as last time so it only got to 56MB before I did ctrl+C):
I found it to be much faster and not using so much memory my server crashes to use dataframes to read the csv:Then I am able to process it using apply, for example:I suspect the major issue with the previous attempt had something to do with the UTF8 encoder
For starters, for should use izip from itertools. See below.in izip is a generator version of zip, which it has a lower memory impact. Though you probably won't have much of a gain since it looks like you're zipping one item at a time.  I would take a look at your cleanDict() function. It has tons of if statements to evaluate and as such it takes time. Lastly, if you are really pressed for more speed and can't figure out where to get it from, check using the or in other words take a look at parallel processing. https://docs.python.org/3/library/concurrent.futures.htmlAlso please take a look at the PEP 8 guidelines for python. https://www.python.org/dev/peps/pep-0008/ Your indentations are wrong.  All indentations should be 4 spaces.  If nothing else it helps with readability.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html
