Natural Text
Consider a function  which takes a lot of arguments . Based on these arguments (objects), the function  composes a rather complex object  and returns it.  implements , so  itself serves as a function. Since the composition of  is pretty time consuming and in my scenario there is no point in having multiple instances of  based on the same arguments , they are to be cached.The question is now: How to efficiently compute a hash based on multiple arguments ? Currently I am using a python dictionary, and i concatenate the  representations of each  to build each key. It works in my scenario, but it feels rather awkward. I need to call the resulting objects  in a very high frequency, so I suspect the repeated call of  and the string concatenations waste a lot of computation time.
You can use the  built-in function, combining the hashes of the items in  together. The typical way to do this (see e.g. the documentation) would be an  across all the hashes of the individual objects:it is advised to somehow mix together (e.g. using exclusive or) the hash values for the components of the object that also play a part in comparison of objectsTo implement that in a functional way, using  and :See also this question.
Since version 3.2, Python already contains an implementation of an LRU cache that you can useto cache your functions' results based on their arguments: Example:Output:(Notice how  only got called once)As suggested in the comments, it's probably best to simply use  the es of your arguments to build the cache-key for your arguments - that's what  already does for you.If you're still on Python 2.7, Raymond Hettinger has posted some recipes with LRU caches that you could use in your own code.


Answer URL
https://docs.python.org/3/library/functions.html#hash
https://docs.python.org/3/reference/datamodel.html#object.__hash__
https://docs.python.org/3/library/operator.html
https://docs.python.org/3/library/functools.html#functools.reduce
