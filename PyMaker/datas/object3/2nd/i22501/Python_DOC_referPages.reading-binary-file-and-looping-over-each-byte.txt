Natural Text
In Python, how do I read in a binary file and loop over each byte of that file?
Python 2.4 and EarlierPython 2.5-2.7Note that the with statement is not available in versions of Python below 2.5. To use it in v 2.5 you'll need to import it:In 2.6 this is not needed.Python 3In Python 3, it's a bit different. We will no longer get raw characters from the stream in byte mode but byte objects, thus we need to alter the condition:Or as benhoyt says, skip the not equal and take advantage of the fact that  evaluates to false. This makes the code compatible between 2.6 and 3.x without any changes. It would also save you from changing the condition if you go from byte mode to text or the reverse.
This generator yields bytes from a file, reading the file in chunks:See the Python documentation for information on iterators and generators.
If the file is not too big that holding it in memory is a problem:where process_byte represents some operation you want to perform on the passed-in byte.If you want to process a chunk at a time:
To read a file — one byte at a time (ignoring the buffering)  — you could use the two-argument  built-in function:It calls  until it returns nothing  (empty bytestring).  The memory doesn't grow unlimited for large files. You could pass   to , to disable the buffering — it guarantees that only one byte is read per iteration (slow).-statement closes the file automatically — including the case when the code underneath raises an exception.Despite the presence of internal buffering by default, it is still inefficient to process one byte at a time. For example, here's the  utility that eats everything it is given:Example:It processes ~1.5 GB/s when  on my machine and only ~7.5 MB/s when . That is, it is 200 times slower to read one byte at a time. Take it into account if you can rewrite your processing to use more than one byte at a time and if you need performance. allows you to treat a file as a  and a file object simultaneously. It can serve as an alternative to loading the whole file in memory if you need access both interfaces. In particular, you can iterate one byte at a time over a memory-mapped file just using a plain -loop: supports the slice notation. For example,  returns  bytes from the file starting at position . The context manager protocol is not supported before Python 3.2; you need to call  explicitly in this case. Iterating over each byte using  consumes more memory than , but  is an order of magnitude faster.
To sum up all the brilliant points of chrispy, Skurmedel, Ben Hoyt and Peter Hansen, this would be the optimal solution for processing a binary file one byte at a time:For python versions 2.6 and above, because:python buffers internally - no need to read chunks DRY principle - do not repeat the read linewith statement ensures a clean file close'byte' evaluates to false when there are no more bytes (not when a byte is zero)Or use J. F. Sebastians solution for improved speedOr if you want it as a generator function like demonstrated by codeape:
Reading binary file in Python and looping over each byteNew in Python 3.5 is the  module, which has a convenience method specifically to read in a file as bytes, allowing us to iterate over the bytes. I consider this a decent (if quick and dirty) answer:Interesting that this is the only answer to mention .In Python 2, you probably would do this (as Vinay Sajip also suggests):In the case that the file may be too large to iterate over in-memory, you would chunk it, idiomatically, using the  function with the  signature - the Python 2 version:(Several other answers mention this, but few offer a sensible read size.)Best practice for large files or buffered/interactive readingLet's create a function to do this, including idiomatic uses of the standard library for Python 3.5+:Note that we use .  blocks until it gets all the bytes requested of it or .  allows us to avoid blocking, and it can return more quickly because of this. No other answers mention this as well.Demonstration of best practice usage:Let's make a file with a megabyte (actually mebibyte) of pseudorandom data:Now let's iterate over it and materialize it in memory: We can inspect any part of the data, for example, the last 100 and first 100 bytes:Don't iterate by lines for binary filesDon't do the following - this pulls a chunk of arbitrary size until it gets to a newline character - too slow when the chunks are too small, and possibly too large as well:The above is only good for what are semantically human readable text files (like plain text, code, markup, markdown etc... essentially anything ascii, utf, latin, etc... encoded).
Python 3, read all of the file at once:You can iterate whatever you want using  variable.
If you have a lot of binary data to read, you might want to consider the struct module.  It is documented as converting "between C and Python types", but of course, bytes are bytes, and whether those were created as C types does not matter.  For example, if your binary data contains two 2-byte integers and one 4-byte integer, you can read them as follows (example taken from  documentation):You might find this more convenient, faster, or both, than explicitly looping over the content of a file.
if you are looking for something speedy, here's a method I've been using that's worked for years:if you want to iterate chars instead of ints, you can simply use , which should be a bytes() object in py3.
After trying all the above and using the answer from @Aaron Hall, I was getting memory errors for a ~90 Mb file on a computer running Window 10, 8 Gb RAM and Python 3.5 32-bit. I was recommended by a colleague to use  instead and it works wonders.By far, the fastest to read an entire binary file (that I have tested) is:ReferenceMultitudes faster than any other methods so far. Hope it helps someone!


Answer URL
https://docs.python.org/3/library/struct.html
https://docs.python.org/3/reference/expressions.html#yield-expressions
