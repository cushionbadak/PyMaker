Natural Text
I am working with a big dataset now. My input will be 4 different datasets and i have to apply a particular function to each dataset. So what i have done is to read all the four dataset and apply the function in parallel to each dataset using pool.map. So now i have a parent and 4 child process. Everything is fine till this.Q1. Now what happens inside each process. In the function which i am applying on each dataset, i am comparing each tuple with other tuples, so its a kind of recursion. Is there a way to make it parallel, because this comparison may take long time since the dataset will be big. How to make it because its already a child process? Is it possible to parallelize it again within child process, because i have more processors, so i want to utilize it.Q2. What i have in mind for parallelization of this recursive task is, if i am comparing tuple x with tuple y( every tuple with all other tuple), i can make chunks for x and each chunk does the comparison with y. This i guess can be done with two 'for loops'.  Any suggestions how to do this?
Re: Q1, If you're creating your child processes using a , then no, the worker processes cannot have children. Attempting to create one will raise an exception:The reason is stated pretty clearly - the processes in a  are daemonic, and daemonic processes can't have children. The reason for this is that terminating the parent process will terminate its daemonic children, but the daemonic children will not be able to terminate their children, which will leave behind orphaned processes. This is stated in the documentation:Note that a daemonic process is not allowed to create child processes.  Otherwise a daemonic process would leave its children orphaned if it  gets terminated when its parent process exits.You can get around this by your parent processes creating a set of non-daemonic  objects, rather than using a . Then, each child can create its own :Output:This approach seems like it will work ok for you, since your initial dataset just contains four items. You can just create one  per item in the dataset, and still have some free CPUs to spare for each sub-process to use in a small .Re: Q2, it sounds like you could use  to create one large iterable of each pair of tuples you want to compare. You can then use  to parallelize comparing each pair. Here's an example showing how that works:Output:


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.daemon
https://docs.python.org/3/library/itertools.html#itertools.product
