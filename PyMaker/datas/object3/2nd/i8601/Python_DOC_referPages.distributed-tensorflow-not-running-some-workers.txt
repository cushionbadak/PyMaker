Natural Text
I'm trying to get a very simple example of distributed TensorFlow working. However, I'm having a bug that appears non-deterministically between runs. On some runs, it works perfectly. Outputting something along the lines of:However, every once in a while, one or more of the workers fails to run, resulting in output like this:If I run the loop indefinitely, it seems that the missing workers always startup at some point, but only minutes later, which isn't practical.I've found that two things make the issue go away (but make the program useless): 1. Not declaring any tf Variables inside the  scope. If I even declare one variable (e.g.  below), the issue starts cropping up. and 2. setting the  param in  to  for all workers. This causes the bug to go away even if variables are declared, but it seems wrong to make all of the workers the chief. The way I'm currently setting it below -  - is taken directly from a TensorFlow tutorial.Here's the simplest code I can get to replicate the issue. (You may have to run multiple times to see the bug, but it almost always shows up within 5 runs
I'm guessing the cause here is the implicit coordination protocol in how a  starts, which is implemented here:If this session is the chief:Run the variable initializer op.Else (if this session is not the chief):Run an op to check if the variables has been initialized.While any of the variables has not yet been initialized.Wait 30 seconds.Try creating a new session, and checking to see if the variables have been initialized.(I discuss the rationale behind this protocol in a video about Distributed TensorFlow.)When every session is the chief, or there are no variables to initialize, the  will always start immediately. However, once there is a single variable, and you only have a single chief, you will see that the non-chief workers have to wait for the chief to act.The reason for using this protocol is that it is robust to various processes failing, and the delay—while very noticeable when running everything on a single process—is short compared to the expected running time of a typical distributed training job.Looking at the implementation again, it does seem that this 30-second timeout should be configurable (as the  argument to ), but there is currently no way to set this timeout when you create a , because it uses a hardcoded set of arguments for creating a session manager.This seems like an oversight in the API, so please feel free to open a feature request on the GitHub issues page!
As mrry said, the problem exists because:Non-chief relies on chief to initialize the model.If it isn't initialized, then it waits for 30 secs.Performance-wise, there's no difference to wait for the chief and kicks in at the next 30s. However, I was doing a research recently which required me to enforce strictly synchronized update, and this problem needed to be taken care of.The key here is to use a barrier, depending on your distributed setting. Assume you are using thread-1 to run ps, and thread-2~5 to run workers, then you only need to:Instead of using a MonitoredTrainingSession, use a tf.train.Supervisor, which enables you to set recovery_wait_secs, with default=30s. Change it to 1s to reduce your wait time.sv = tf.train.Supervisor(is_chief=is_chief,                                   logdir=...                                   init_op=...                                   ...                                   recovery_wait_secs=1s)sess = sv.prepare_or_wait_for_session(server.target,  config=sess_config)Use a barrier. Assume you are using threads:In main:In actual training function:Then just proceeds happily. The barrier will make sure that all models reaches this step, and there's for sure no race conditions.


Answer URL
https://docs.python.org/3/library/threading.html#barrier-objects
