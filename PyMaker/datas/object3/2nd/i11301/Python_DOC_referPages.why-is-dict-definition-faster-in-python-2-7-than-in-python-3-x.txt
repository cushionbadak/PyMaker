Natural Text
I have encountered a (not very unusual) situation in which I had to either use a  or a list comprehension expression. And then I wondered which one is faster.This StackOverflow answer provided me the solution, but then I started to test it myself. Basically the results were the same, but I found an unexpected behavior when switching to Python 3 that I got curious about, and namely:I had the assumption that Python 3 is faster than Python 2, but it turned out in several posts (1, 2) that it's not the case. Then I thought that maybe Python 3.5 will perform better at such a simple task, as they state in their :The language is mostly the same, but many details, especially how  built-in objects like dictionaries and strings work, have changed  considerably, and a lot of deprecated features have finally been  removed.But nope, it performed even worse:I've tried to dive into the Python 3.5 source code for , but my knowledge of C language is not sufficient to find the answer myself (or, maybe I even don't search in the right place).So, my question is:What makes the newer version of Python slower comparing to an older version of Python on a relatively simple task such as a  definition, as by the common sense it should be vice-versa? I'm aware of the fact that these differences are so small that in most cases they can be neglected. It was just an observation that made me curious about why the time increased and not remained the same at least?
Let's disassemble :Python 2.7 implementation of BUILD_MAPPython 3.5 implementation of BUILD_MAPIt's little bit more code.EDIT:Python 3.4 implementation of BUILD_MAP id exactly the same as 2.7 (thanks @user2357112). I dig deeper and it's looks like Python 3 min size of dict is 8 PyDict_MINSIZE_COMBINED constPyDict_MINSIZE_COMBINED is the starting size for any new, non-split dict. 8 allows dicts with no more than 5 active entries; experiments suggested this suffices for the majority of dicts (consisting mostly of usually-small dicts created to pass keyword arguments). Making this 8, rather than 4 reduces the number of resizes for most dictionaries, without any significant extra memory use.Look at _PyDict_NewPresized in Python 3.4and in 2.7In both cases  has value 1.Python 2.7 create a empty dict and Python 3.4 create a 7-element dict.
Because nobody caresThe differences you are citing are on the order of tens or hundreds of nanoseconds.  A slight difference in how the C compiler optimizes register use could easily cause such changes (as could any number of other C-level optimization differences).  That, in turn, could be caused by any number of things, such as changes in the number and usage of local variables in the C implementation of Python (CPython), or even just switching C compilers.The fact is, nobody is actively optimizing for these small differences, so nobody is going to be able to give you a specific answer.  CPython is not designed to be fast in an absolute sense.  It is designed to be scalable.  So, for example, you can shove hundreds or thousands of items into a dictionary and it will continue to perform well.  But the absolute speed of creating a dictionary simply isn't a primary concern of the Python implementors, at least when the differences are this small.
As @Kevin already stated:CPython is not designed to be fast in an absolute sense. It is  designed to be scalableTry this instead:And again:That pretty shows that you can't benchmark Python3 as losing against Python2 on such an insignificant difference. From the look of things, Python3 should scale better.


Answer URL
https://docs.python.org/3/library/dis.html#dis.dis
