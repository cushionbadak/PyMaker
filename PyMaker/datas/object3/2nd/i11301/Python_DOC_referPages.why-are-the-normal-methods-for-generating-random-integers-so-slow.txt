Natural Text
Not a maths major or a cs major, I just fool around with python (usually making scripts for simulations/theorycrafting on video games) and I discovered just how bad random.randint is performance wise. It's got me wondering why random.randint or random.randrange are used/made the way they are. I made a function that produces (for all intents and actual purposes) identical results to random.randint:There is a massive 180% speed boost using that to generate an integer in the range (inclusive) 0-65 compared to random.randrange(0, 66), the next fastest method.Furthermore, the adaptation of this function as an alternative to random.choice is 75% faster, and I'm sure adding larger-than-one stepped ranges would be faster (although I didn't test that). For almost double the speed boost as using the fastrandint function you can simply write it inline:So in summary, why am I wrong that my function is a better, why is it faster if it is better, and is there a yet even faster way to do what I'm doing?
 calls  which does a bunch of range/type checks and conversions and then uses  to generate a random int.  again does some range checks and finally uses .So if you remove all the checks for edge cases and some function call overhead, it's no surprise your  is quicker.
 and others are calling into  which may be less efficient that direct calls to , but for good reason. It is actually more correct to use a  that calls into , as it can be done in an unbiased manner.You can see that using random.random to generate values in a range ends up being biased since there are only M floating point values between 0 and 1 (for M pretty large). Take an N that doesn't divide into M, then if we write M = k N + r for . At best, using we'll get  numbers coming out with probability (k+1)/M and  numbers coming out with probability . (This is at best, using the pigeon hole principle - in practice I'd expect the bias to be even worse).Note that this bias is only noticeable forA large number of samplingwhere N is a large fraction of M the number of floats in (0,1]So it probably won't matter to you, unless you know you need unbiased values - such as for scientific computing etc.In contrast, a value from  can be unbiased by using rejection sampling from repeated calls to . Of course managing this can introduce additional overhead.AsideIf you end up using a custom random implementation then From the python 3 docsAlmost all module functions depend on the basic function random(), which   generates a random float uniformly in the semi-open range [0.0, 1.0).This suggests that  and others may be implemented using . If this is the case I would expect them to be slower,incurring at least one addition function call overhead per call.Looking at the code referenced in https://stackoverflow.com/a/37540577/221955 you can see that this will happen if the random implementation doesn't provide a  function.
This is probably rarely a problem but  works while  crashes. The slower time is probably the price you need to pay to have a function that works for all possible cases...


Answer URL
https://docs.python.org/3/library/random.html
