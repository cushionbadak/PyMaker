Natural Text
I'd like to create a set of processes with the following structure:, which dequeues requests from an external source.  generates a variable number of worker processes. which does some preliminary processing on job requests, then sends data to . , which accepts job requests from  processes. When it has received enough requests, it sends the batch to a process that runs on the GPU. After getting the results back, it has to then send back the completed batch of requests back to the  processes such that the worker that requested it receives it back One could envision doing this with a number of queues. Since the number of  processes is variable, it would be ideal if  had a single input queue into which s put their job request and their specific return queue as a tuple. However, this isn't possible--you can only share vanilla queues in python via inheritance, and  fail with:Is there a pythonic way to do this without invoking some external library?
 is implemented with a pipe, a deque and a thread.When you call queue.put() the objects ends up in the deque and the thread takes care of pushing it into the pipe.You cannot share threads within processes for obvious reasons. Therefore you need to use something else.Regular pipes and sockets can be easily shared.Nevertheless I'd rather use a different architecture for your program.The  process would act as an orchestrator routing the tasks to two different Pools of processes, one for CPU bound jobs and the other to GPU bound ones. This would imply you need to share more information within the workers but it's way more robust and scalable.Here you get a draft:


Answer URL
https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled
https://docs.python.org/3/library/multiprocessing.html#exchanging-objects-between-processes
