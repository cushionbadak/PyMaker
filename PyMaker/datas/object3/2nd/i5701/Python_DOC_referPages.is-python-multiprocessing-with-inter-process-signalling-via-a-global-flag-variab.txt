Natural Text
I'm running many subprocesses (more than I have cores) and, if one meets a certain condition, I set a value of a global variable .Then if  was set, all subsequent subprocesses exit as quickly as possible.See e.g. this simple example, where I multiply the results of my 20 calls to a  function, but I "bail out" if any of those calls returns zero:It works just fine (or to be precise, it has worked every time I've tried it). i.e. my desktop has 4 cores, and if one of the first 4 subprocesses sets bailout to 1 (as almost always happens in this example), then all subsequent runs exit on the  condition.But is it safe?I mean, all a subprocess can ever do is set  to 1. But what if two subprocesses both want to set bailout to 1? Is it possible for them to attempt it at the same time, causing bailout to become undefined? Or is it guaranteed that this will never happen, (perhaps because the top level process always handles the completed subprocesses serially?)
Globals are not shared between processes. If you add some logging to , you can see what's really going on:This will produce output like:What's happening is that  is starting 4 processes, which are being re-used as they become available. So while processing the 20 items in , eventually each individual process has its  set to 1. When that process is reused, it triggers the bailout clause.Since you're randomly deciding when to set , it's possible for it to never happen while processing the 20 items, or it may happen in some processes but not others, so you may not get the same results I pasted above, but at least some of the processes are likely to go into bailout mode.If you're looking for a reliable way to share state between processes, check out https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes.
Is it possible?Is it safe?Is it guaranteed?While GIL-stepping indeed makes all the threads-based ( not the subprocesses-based ) multiprocessing efforts to still appear in a pure- processing-flow, the question is more about a principal approach and whether all the above raised issues are safely satisfied.Rather do not try to go against documented pieces of advice:Best let's mention the explicit statements from the documentation:16.6.3. Programming guidelines...Explicitly pass resources to child processesOn Unix a child process can make use of a shared resource created in a parent process using a global resource. However, it is better to pass the object as an argument to the constructor for the child process.Apart from making the code (potentially) compatible with Windows this also ensures that as long as the child process is still alive the object will not be garbage collected in the parent process. This might be important if some resource is freed when the object is garbage collected in the parent process.and16.6.3.2 Windows...Global variablesBear in mind that if code run in a child process tries to access a global variable, then the value it sees (if any) may not be the same as the value in the parent process at the time that  was called.However, global variables which are just module level constants cause no problems.Besides the native pythonic tools to help communicate or "share" state ( which not only I advocate, where possible, to better never share ), there are smart tools for designing indeed distributed-systems, using multi-agent concepts, where each thread may use other lightweight communication tools, being performance less penalised, than the native GIL-stepped operations allow ( ref. ZeroMQ, nanomsg et al ).


Answer URL
https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes
