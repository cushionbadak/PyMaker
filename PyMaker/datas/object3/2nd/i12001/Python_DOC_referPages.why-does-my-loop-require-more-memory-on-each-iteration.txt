Natural Text
I am trying to reduce the memory requirements of my python 3 code. Right now each iteration of the for loop requires more memory than the last one. I wrote a small piece of code that has the same behaviour as my project:For each iteration of the for loop the threads in simulation() each have a memory usage equal to the total memory used by my code.Does Python clone my entire environment each time the parallel processes are run, including the variables not required by f()? How can I prevent this behaviour?Ideally I would want my code to only copy the memory it requires to execute f() while I can save the results in memory.
Though the script does use quite a bit of memory even with the "smaller" example values, the answer toDoes Python clone my entire environment each time the parallel  processes are run, including the variables not required by f()? How  can I prevent this behaviour?is that it does in a way clone the environment with forking a new process, but if copy-on-write semantics are available, no actual physical memory needs to be copied until it is written to. For example on this system seems to be available and in use, but this may not be the case on other systems. On Windows this is strictly different as a new Python interpreter is executed from  instead of forking. Since you mention using , you're using some flavour of UNIX or UNIX like system, and you get  semantics.For each iteration of the for loop the processes in simulation() each  have a memory usage equal to the total memory used by my code.The spawned processes will display almost identical values of , but this can be misleading, because mostly they occupy the same actual physical memory mapped to multiple processes, if writes do not occur. With  the story is a bit more complicated, since it "chops the iterable into a number of chunks which it submits to the process pool as separate tasks". This submitting happens over  and submitted data will be copied. In your example the  and 2**20 function calls also dominate the CPU usage. Replacing the mapping with a single vectorized multiplication in  took the script's runtime from around 150s to 0.66s on this machine.We can observe  with a (somewhat) simplified example that allocates a large array and passes it to a spawned process for read-only processing:Output on this machine:Now if we replace the function  with a function that modifies the array:the results are quite different:The for-loop does indeed require more memory after each iteration, but that's obvious: it stacks the  from the mapping, so it has to allocate space for a new array to hold both the old results and the new and free the now unused array of old results.
Maybe you should know the difference between  and  in . see this What is the difference between a process and a thread.In the for loop, there are , not . Threads share the address space of the process that created it; processes have their own address space.You can print the process id, type .


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map
