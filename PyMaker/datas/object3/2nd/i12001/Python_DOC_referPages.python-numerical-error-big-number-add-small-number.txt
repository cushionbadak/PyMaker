Natural Text
This question already has an answer here:Is floating point math broken?                    28 answers                This a exercise from Udacity's Deep learning course. Can anyone explain why the final answer is not 1.0?
Let's check what  sees of  in the floating point addition:What happens is that all but the leading  get shifted out of the binary mantissa of  while equalizing the exponent with . This means that the final value is , which gives exactly the observed value
Because  cannot be represented exactly as floating point value:If you use a number that can be represented exactly such as  and change iteration count to  you will get . However as @user2357112 pointed out even this property holds only if all of the intermediate results can be represented exactly using floating point value.Check Python tutorial for more details: https://docs.python.org/3/tutorial/floatingpoint.html


Answer URL
https://docs.python.org/3/tutorial/floatingpoint.html
