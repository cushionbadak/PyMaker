Natural Text
I am working with large files holding several million records each (approx 2GB unpacked, several hundred MBs gzip). I iterate over the records with , which allows me to get either a small portion (for debug and development) or the whole thing when I want to test the code. I have noticed an absurdly large memory usage for my code and thus I am trying to find the memory leak in my code. Below is the output from memory_profiler on a paired read (where I open two files and zip the records), for ONLY 10**5 values (the default value get overwritten). The code, in short, does some filtering on the records and keeps track of how many times the strings occur in the file (zipped pair of strings in this particular case).100 000 strings of 150 chars with integer values in a Counter should land around 100 MBs tops, which I checked using following function by @AaronHall. Given the memory_profiler output I suspect that islice doesn't let go of the previous entities over the course of the iteration. A google search landed me at this bug report however it's marked as solved for Python 2.7 which is what I am running at the moment.Any opinions?EDIT: I have tried to skip  as per comment below and use a for loop like which makes no significant difference. It is in the case of a single file, in order to avoid  which also comes from .A secondary troubleshooting idea i had was to check if  reads and expands the file to memory, and thus cause the issue here. However running the script on decompressed files doesn't make a difference.  
Note that memory_profiler only reports the maximum memory consumption for each line. For long loops this can be misleading as the first line of the loop always seem to report a disproportionate amount of memory. That is because it compares the first line of the loop with respect to memory consumption of the line before, which would be out of the loop. It doesn't mean that the first line of the loop consumes 2985Mb but rather that the difference between the peak in memory within the loop is 2985Mb higher that out of the loop.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.islice
