Natural Text
I have a list with 12K asian surnames from a census and a list with 200K names. I'd like to classify those 200K people as asians or non-asians based on wether their surname appears on my 12K list.Is there a fast way to verify if one of the elemenst in the list contains one of the surnames in the 12K list?
The best way to do this is to convert your 12K list into a set data structure. Then you can iterate over the census data and check if each is in the set.This is almost certainly the fastest way to accomplish what you need in Python or any language, and should be reasonably fast on a 200K sized list.Wai Leong Yeow suggests a binary search, which is faster than just checking the list directly, but that will still be a O(log n) operation on 200K different names, where N is 12,000, meaning it will likely be more than 10x slower just for the iterative part (This is a simplification - in reality there are some constant factors masked by the big O notation, but the constant time solution is certainly still faster). Sorting it will take O(n log n) time, where as turning it into a set takes O(n) time, meaning that this method has faster preprocessing as well.
It depend to your real problem. do you want machine learning(as you tag: classification) to predict asian/non-asian name?If yes: Try some semi supervised methods. To do this, first randomly select(near 10%) of your 200k data, then search for it in 12k, if it exist, label it to 1, else label it to 0. then use some classification algorithm like, Random Forest,SVM or KNN. You can also model your names something like Bag Of word(In your problem Bag Of Letter! or something like that): https://en.wikipedia.org/wiki/Bag-of-words_modelfor classification task, take a look at scikit-learn lib: http://scikit-learn.org/If NO(you don't want to use machine learning solutions):There exist some fast string search algorithm that search a string in a corpus of other string with some Technics. there are many algorithm, like Boyer Moore: https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithmFor more details this can be good: https://softwareengineering.stackexchange.com/questions/183725/which-string-search-algorithm-is-actually-the-fastest
I would recommend to use local sensitive hashing in the first step before training any machine learning models. That probably will help as you don't have many features. If you want something stronger you can use Naive Bayes and some feature engineering. 
Depends on what you mean by "fast".James suggested using Python's built-in  to test for membership. Python's  implementation uses hash tables. Average time complexity is O(1) but the worst case can be O(n) where n is the cardinality of the set of asian surnames. So in the worst case scenario, you might just end up with O(mn) instead of O(m) where m is the cardinality of the set of names to classify.For reference, see: https://wiki.python.org/moin/TimeComplexityIf you want to have a guarantee on the worst case, you can achieve it with sorting the set  and doing binary search. This will end up with O(m lg n) time complexity.Binary search: https://docs.python.org/3.1/library/bisect.htmlIt really depends on how well the hashing function works for your data.


Answer URL
https://docs.python.org/3/reference/expressions.html#membership-test-operations
