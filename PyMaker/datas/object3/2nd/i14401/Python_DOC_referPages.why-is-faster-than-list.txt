Natural Text
I recently compared the processing speeds of  and  and was surprised to discover that  runs more than three times faster than . I ran the same test with  and  and the results were practically identical:  and  both took around 0.128sec / million cycles, while  and  took roughly 0.428sec / million cycles each.Why is this? Do  and  (and probably  and , too) immediately pass back a copies of some empty stock literal while their explicitly-named counterparts (, , , ) fully go about creating an object, whether or not they actually have elements?I have no idea how these two methods differ but I'd love to find out.I couldn't find an answer in the docs or on SO, and searching for empty brackets turned out to be more problematic than I'd expected.I got my timing results by calling  and , and  and , to compare lists and dictionaries, respectively. I'm running Python 2.7.9.I recently discovered "Why is if True slower than if 1?" that compares the performance of  to  and seems to touch on a similar literal-versus-global scenario; perhaps it's worth considering as well.
Because  and  are literal syntax. Python can create bytecode just to create the list or dictionary objects: and  are separate objects. Their names need to be resolved, the stack has to be involved to push the arguments, the frame has to be stored to retrieve later, and a call has to be made. That all takes more time.For the empty case, that means you have at the very least a  (which has to search through the global namespace as well as the  module) followed by a , which has to preserve the current frame:You can time the name lookup separately with :The time discrepancy there is probably a dictionary hash collision. Subtract those times from the times for calling those objects, and compare the result against the times for using literals:So having to call the object takes an additional  seconds per 10 million calls.You can avoid the global lookup cost by aliasing the global names as locals (using a  setup, everything you bind to a name is a local):but you never can overcome that  cost.
 requires a global lookup and a function call but  compiles to a single instruction. See:
Because  is a function to convert say a string to a list object, while  is used to create a list off the bat. Try this (might make more sense to you):WhileGives you a actual list containing whatever you put in it.
The answers here are great, to the point and fully cover this question. I'll drop a further step down from byte-code for those interested. I'm using the most recent repo of CPython; older versions behave similar in this regard but slight changes might be in place.Here's a break down of the execution for each of these,  for  and  for .The  instruction:You should just view the horror:Terribly convoluted, I know. This is how simple it is:Create a new list with  (this mainly allocates the memory for a new list object),  signalling the number of arguments on the stack. Straight to the point.Check that nothing went wrong with . Add any arguments (in our case this isn't executed) located on the stack with  (a macro).No wonder it is fast! It's custom-made for creating new lists, nothing else :-)The  instruction:Here's the first thing you see when you peek at the code handling :Looks pretty harmless, right? Well, no, unfortunately not,  is not a straightforward guy that will call the function immediately, it can't. Instead, it grabs the object from the stack, grabs all arguments of the stack and then switches based on the type of the object; is it a:? Nope, it is ,  isn't of type ? Nope, see previous.? Nopee, see previous.We're calling the  type, the argument passed in to  is . CPython now has to call a generic function to handle any callable objects named , yay more function calls.This function again makes some checks for certain function types (which I cannot understand why) and then, after creating a dict for kwargs if required, goes on to call . finally gets us somewhere! After performing even more checks  it grabs the  slot from the  of the  we've passed in, that is, it grabs . It then proceeds to create a tuple out of of the arguments passed in with  and, finally, a call can finally be made!, which matches  takes over and finally creates the list object. It calls the lists  which corresponds to  and allocates memory for it with : This is actually the part where it catches up with , finally. All the previous are necessary to handle objects in a generic fashion.In the end,  calls  and initializes the list with any available arguments, then we go on a returning back the way we came. :-)Finally, remmeber the , that's another guy that contributes here.It's easy to see that, when dealing with our input, Python generally has to jump through hoops in order to actually find out the appropriate  function to do the job. It doesn't have the curtesy of immediately calling it because it's dynamic, someone might mask  (and boy do many people do) and another path must be taken. This is where  loses much: The exploring Python needs to do to find out what the heck it should do.Literal syntax, on the other hand, means exactly one thing; it cannot be changed and always behaves in a pre-determined way.Footnote: All function names are subject to change from one release to the other. The point still stands and most likely will stand in any future versions, it's the dynamic look-up that slows things down.
Why is  faster than ?The biggest reason is that Python treats  just like a user-defined function, which means you can intercept it by aliasing something else to  and do something different (like use your own subclassed list or perhaps a deque). It immediately creates a new instance of a builtin list with .My explanation seeks to give you the intuition for this.Explanation is commonly known as literal syntax. In the grammar, this is referred to as a "list display". From the docs:A list display is a possibly empty series of expressions enclosed in  square brackets:A list display yields a new list object, the contents being specified  by either a list of expressions or a comprehension. When a  comma-separated list of expressions is supplied, its elements are  evaluated from left to right and placed into the list object in that  order. When a comprehension is supplied, the list is constructed from  the elements resulting from the comprehension.In short, this means that a builtin object of type  is created. There is no circumventing this - which means Python can do it as quickly as it may.On the other hand,  can be intercepted from creating a builtin  using the builtin list constructor.For example, say we want our lists to be created noisily:We could then intercept the name  on the module level global scope, and then when we create a , we actually create our subtyped list:Similarly we could remove it from the global namespace and put it in the builtin namespace:And now:And note that the list display creates a list unconditionally:We probably only do this temporarily, so lets undo our changes - first remove the new  object from the builtins:Oh, no, we lost track of the original. Not to worry, we can still get  - it's the type of a list literal:So...Why is  faster than ?As we've seen - we can overwrite  - but we can't intercept the creation of the literal type. When we use  we have to do the lookups to see if anything is there.Then we have to call whatever callable we have looked up. From the grammar:A call calls a callable object (e.g., a function) with a possibly  empty series of arguments:We can see that it does the same thing for any name, not just list:For  there is no function call at the Python bytecode level:It simply goes straight to building the list without any lookups or calls at the bytecode level.ConclusionWe have demonstrated that  can be intercepted with user code using the scoping rules, and that  looks for a callable and then calls it.Whereas  is a list display, or a literal, and thus avoids the name lookup and function call.


Answer URL
https://docs.python.org/3/reference/expressions.html#list-displays
