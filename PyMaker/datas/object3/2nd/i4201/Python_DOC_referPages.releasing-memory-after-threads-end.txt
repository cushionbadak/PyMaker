Natural Text
I'm working on an application that is primarily an API, but also has a multithreaded background job processing system, used to perform scheduled jobs as well as ad hoc jobs that take too long for an instant API response.This will be forked 10 times through gunicorn. Any single forked process is capable of picking up a job to run, so job processing is balanced between the processes alongside the servicing of API requests.My challenge is with how each process will continue to claim the peak amount of memory it ever needed for job processing. Some jobs require 1.5GB-2GB worth of memory.Given enough time, eventually all 10 processes will have had to work those kinds of jobs, and each will be clinging to upwards of 2GB of memory. Even if the average memory use of the process rarely exceeds 100MB.These intensive jobs are only run through dedicated threads within the process.Is there ANY mechanism to compel Python to release the memory claimed specifically for a thread at the thread's closure? Or any general mechanism to force a Python process to reset memory to only what's actively needed at that moment?Side note: I'm also exploring forking instead of threading but thus far that's introducing other problems I'm not sure I can work around.
Without a concrete example of what your API and worker processes/threads are doing its hard to provide a specific answer.Python is a reference counted language: when an object is not referenced by any other it is free to be garbage collected. One can force the garbage collector to run (see https://docs.python.org/3/library/gc.html), but almost always its best to just let it do its thing.When your worker threads exit any objects created within the thread are likely to be garbage collected; and exception to this would be objects placed in some global data structure (but your use-case doesn't sound like that it something you would be doing).
Just to prove that threads get destroyed after their job finished, you can run this code:The output will be something like this:As you can see, whenever a thread ends, total thread count decreases.UPDATE:Ok. I hope this script will satisfy you.And this is the output:I really don't know why there is a 408 KB difference buy it may be overhead for using 15 MB memory.


Answer URL
https://docs.python.org/3/library/gc.html
