Natural Text
Imagine we have an iterator, say . And we have two functions, each accepting an iterator as the only parameter, say  and . In SQL world we would call them aggregate functions. Is there any way to obtain results of both without buffering the iterator output?To do it, we would need to pause and resume aggregate function execution, in order to feed them both with the same values without storing them. Maybe is there a way to express it using async things without sleeps?
Let's consider how to apply two aggregate functions to the same iterator, which we can only exhaust once. The initial attempt (which hardcodes  and  for brevity, but is trivially generalizable to an arbitrary number of aggregate functions) might look like this:This implementation has the downside that it stores all the generated elements in memory at once, despite both functions being perfectly capable of stream processing. The question anticipates this cop-out and explicitly requests the result to be produced without buffering the iterator output. Is it possible to do this?Serial execution: itertools.teeIt certainly seems possible. After all, Python iterators are external, so every iterator is already capable of suspending itself. How hard can it be to provide an adapter that splits an iterator into two new iterators that provide the same content? Indeed, this is exactly the description of , which appears perfectly suited to parallel iteration:The above produces the correct result, but doesn't work the way we'd like it to. The trouble is that we're not iterating in parallel. Aggregate functions like  and  never suspend - each insists on consuming all of the iterator content before producing the result. So  will exhaust  before  has had a chance to run at all. Exhausting elements of  while leaving  alone will cause those elements to be accumulated inside an internal FIFO shared between the two iterators. That's unavoidable here - since  must see the same elements,  has no choice but to accumulate them. (For more interesting details on , refer to this post.)In other words, there is no difference between this implementation and the first one, except that the first one at least makes the buffering explicit. To eliminate buffering,  and  must run in parallel, not one after the other.Threads: concurrent.futuresLet's see what happens if we run the aggregate functions in separate threads, still using  to duplicate the original iterator:Now  and  actually run in parallel (as much as the GIL permits), threads being managed by the excellent  module. It has a fatal flaw, however: for  not to buffer the data,  and  must process their items at exactly the same rate. If one is even a little bit faster than the other, they will drift apart, and  will buffer all intermediate elements. Since there is no way to predict how fast each will run, the amount of buffering is both unpredictable and has the nasty worst case of buffering everything.To ensure that no buffering occurs,  must be replaced with a custom generator that buffers nothing and blocks until all the consumers have observed the previous value before proceeding to the next one. As before, each consumer runs in its own thread, but now the calling thread is busy running a producer, a loop that actually iterates over the source iterator and signals that a new value is available. Here is an implementation:This is quite some amount of code for something so simple conceptually, but it is necessary for correct operation. loops over the outside iterator and sends the items to the consumers, one value at a time. It uses a barrier, a convenient synchronization primitive added in Python 3.2, to wait until all consumers are done with the old value before overwriting it with the new one in . Once the new value is actually ready, a condition is broadcast.  is a generator that transmits the produced values as they arrive, until detecting . The code can be generalized run any number of aggregate functions in parallel by creating consumers in a loop, and adjusting their number when creating the barrier.The downside of this implementation is that it requires creation of threads (possibly alleviated by making the thread pool global) and a lot of very careful synchronization at each iteration pass. This synchronization destroys performance - this version is almost 2000 times slower than the single-threaded , and 475 times slower than the simple but non-deterministic threaded version.Still, as long as threads are used, there is no avoiding synchronization in some form. To completely eliminate synchronization, we must abandon threads and switch to cooperative multi-tasking. The question is is it possible to suspend execution of ordinary synchronous functions like  and  in order to switch between them?Fibers: greenletIt turns out that the  third-party extension module enables exactly that. Greenlets are an implementation of fibers, lightweight micro-threads that switch between each other explicitly. This is sort of like Python generators, which use  to suspend, except greenlets offer a much more flexible suspension mechanism, allowing one to choose who to suspend to.This makes it fairly easy to port the threaded version of  to greenlets:The logic is the same, but with less code. As before,  produces values retrieved from the source iterator, but its  doesn't bother with synchronization, as it doesn't need to when everything is single-threaded. Instead, it explicitly switches to every consumer in turn to do its thing, with the consumer dutifully switching right back. After going through all consumers, the producer is ready for the next iteration pass.Results are retrieved using an intermediate single-element list because greenlet doesn't provide access to the return value of the target function (and neither does , which is why we opted for  above).There are downsides to using greenlets, though. First, they don't come with the standard library, you need to install the greenlet extension. Then, greenlet is inherently non-portable because the stack-switching code is not supported by the OS and the compiler and can be considered somewhat of a hack (although an extremely clever one). A Python targeting WebAssembly or JVM or GraalVM would be very unlikely to support greenlet. This is not a pressing issue, but it's definitely something to keep in mind for the long haul.Coroutines: asyncioAs of Python 3.5, Python provides native coroutines. Unlike greenlets, and similar to generators, coroutines are distinct from regular functions and must be defined using . Coroutines can't be easily executed from synchronous code, they must instead be processed by a scheduler which drives them to completion. The scheduler is also known as an event loop because its other job is to receive IO events and pass them to appropriate callbacks and coroutines. In the standard library, this is the role of the  module.Before implementing an asyncio-based , we must first resolve a hurdle. Unlike greenlet, asyncio is only able to suspend execution of coroutines, not of arbitrary functions. So we need to replace  and  with coroutines that do essentially the same thing. This is as simple as implementing them in the obvious way, only replacing  with , enabling the async iterator to suspend the coroutine while waiting for the next value to arrive:One could reasonably ask if providing a new pair of aggregate functions is cheating; after all, the previous solutions were careful to use existing  and  built-ins. The answer will depend on the exact interpretation of the question, but I would argue that the new functions are allowed because they are in no way specific to the task at hand. They do the exact same thing the built-ins do, but consuming async iterators. I suspect that the only reason such functions don't already exist somewhere in the standard library is due to coroutines and async iterators being a relatively new feature.With that out of the way, we can proceed to write  as a coroutine:Although this version is based on switching between coroutines inside a single thread, just like the one using greenlet, it looks different. asyncio doesn't provide explicit switching of coroutines, it bases task switching on the  suspension/resumption primitive. The target of  can be another coroutine, but also an abstract "future", a value placeholder which will be filled in later by some other coroutine. Once the awaited value becomes available, the event loop automatically resumes execution of the coroutine, with the  expression evaluating to the provided value. So instead of  switching to consumers, it suspends itself by awaiting a future that will arrive once all the consumers have observed the produced value. is an asynchronous generator, which is like an ordinary generator, except it creates an async iterator, which our aggregate coroutines are already prepared to accept by using . An async iterator's equivalent of  is called  and is a coroutine, allowing the coroutine that exhausts the async iterator to suspend while waiting for the new value to arrive. When a running async generator suspends on an , that is observed by  as a suspension of the implicit  invocation.  does exactly that when it waits for the values provided by  and, as they become available, transmits them to aggregate coroutines like  and . Waiting is realized using the  future, which carries the next element from . Awaiting that future inside  suspends the async generator, and with it the aggregate coroutine.The advantage of this approach compared to greenlets' explicit switching is that it makes it much easier to combine coroutines that don't know of each other into the same event loop. For example, one could have two instances of  running in parallel (in the same thread), or run a more complex aggregate function that invoked further async code to do calculations.The following convenience function shows how to run the above from non-asyncio code:PerformanceMeasuring and comparing performance of these approaches to parallel execution can be misleading because  and  do almost no processing, which over-stresses the overhead of parallelization. Treat these as you would treat any microbenchmarks, with a large grain of salt. Having said that, let's look at the numbers anyway!Measurements were produced using Python 3.6 The functions were run only once and given , their time measured by subtracting  before and after the execution. Here are the results: and : 0.66 ms - almost exact same time for both, with the  version being a bit faster.: 2.7 ms. This timing means very little because of non-deterministic buffering, so this might be measuring the time to start two threads and the synchronization internally performed by Python.: 1.29 seconds, by far the slowest option, ~2000 times slower than the fastest one. This horrible result is likely caused by a combination of the multiple synchronizations performed at each step of the iteration and their interaction with the GIL.: 25.5 ms, slow compared to the initial version, but much faster than the threaded version. With a sufficiently complex aggregate function, one can imagine using this version in production.: 351 ms, almost 14 times slower than the greenlet version. This is a disappointing result because asyncio coroutines are more lightweight than greenlets, and switching between them should be much faster than switching between fibers. It is likely that the overhead of running the coroutine scheduler and the event loop (which in this case is overkill given that the code does no IO) is destroying the performance on this micro-benchmark. using : 125 ms. This is more than twice the speed of regular asyncio, but still almost 5x slower than greenlet.Running the examples under PyPy doesn't bring significant speedup, in fact most of the examples run slightly slower, even after running them several times to ensure JIT warmup. The asyncio function requires a rewrite not to use async generators (since PyPy as of this writing implements Python 3.5), and executes in somewhat under 100ms. This is comparable to CPython+uvloop performance, i.e. better, but not dramatic compared to greenlet.
If it holds for your aggregate functions that ,then you could just cycle through your functions and feed them one element at a time, each time combining them with the result of the previous application, like  would do, e.g. like this:This is considerably slower (about 10-20 times) than just materializing the iterator in a list and applying the aggregate function on the list as a whole, or using  (which basically does the same thing, internally), but it has the benefit of using no additional memory.Note, however, that while this works well for functions like ,  or , it does not work for other aggregating functions, e.g. finding the mean or median element of an iterator, as . (For , you could of course just get the  and divide it by the number of elements, but computing e.g. the median taking just one element at a time will be more challenging.)


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.tee
https://docs.python.org/3/library/concurrent.futures.html
https://docs.python.org/3/library/threading.html#barrier-objects
https://docs.python.org/3/library/threading.html#condition-objects
https://docs.python.org/3/library/threading.html#thread-objects
https://docs.python.org/3/library/asyncio.html
