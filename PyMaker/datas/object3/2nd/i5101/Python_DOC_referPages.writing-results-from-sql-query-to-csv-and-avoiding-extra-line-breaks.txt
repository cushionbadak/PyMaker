Natural Text
I have to extract data from several different database engines. After this data is exported, I send the data to AWS S3 and copy that data to Redshift using a COPY command. Some of the tables contain lots of text, with line breaks and other characters present in the column fields. When I run the following code:Some of the columns that have carriage returns/linebreaks will create new lines:which causes the import process to fail. I can work around this by hard-coding for exceptions:But this takes a long time to process larger files, and seems like bad practice in general. Is there a faster way to export data from a SQL cursor to CSV that will not break when faced with text columns that contain carriage returns/line breaks?
I suspect the issue is as simple as making sure the Python CSV export library and Redshift's COPY import speak a common interface.  In short, check your delimiters and quoting characters and make sure both the Python output and the Redshift COPY command agree.With slightly more detail: the DB drivers will have already done the hard work of getting to Python in a well-understood form.  That is, each row from the DB is a list (or tuple, generator, etc.), and each cell is individually accessible.  And at the point you have a list-like structure, Python's CSV exporter can do the rest of the work and -- crucially -- Redshift will be able to COPY FROM the output, embedded newlines and all.  In particular, you should not need to do any manual escaping; the  or  functions should be all you need do.Redshift's COPY implementation understands the most common dialect of CSV by default, which is todelimit cells by a comma (),quote cells with double quotes (),and to escape any embedded double quotes by doubling ( â†’ ).To back that up with documentation from Redshift :... The default quote character is a double quotation mark ( " ). When the quote character is used within a field, escape the character with an additional quote character. ...However, your Python CSV export code uses a pipe () as the  and sets the  to double quote ().  That, too, can work, but why stray from the defaults?  Suggest using CSV's namesake and keeping your code simpler in the process:From there, tell COPY to use the CSV format (again with no need for non-default specifications):That should do it.
If you're doing  without a  clause, you could use  instead, with the right options:This, in my testing, results in literal '\n' instead of actual newlines, where writing through the csv writer gives broken lines.If you do need a  clause in production you could create a temporary table and copy it instead:(edit) Looking at your question again I see you mention "ever all different database engines". The above works with psyopg2 and postgresql but could probably be adapted for other databases or libraries.
Why write to the database after every row?
The problem is that you are using the Redshift  command with its default parameters, which use a pipe as a delimiter (see here and here) and require escaping of newlines and pipes within text fields (see here and here). However, the Python csv writer only knows how to do the standard thing with embedded newlines, which is to leave them as-is, inside a quoted string.Fortunately, the Redshift  command can also use the standard CSV format. Adding the  option to your  command gives you this behavior:Enables use of CSV format in the input data. To automatically escape delimiters, newline characters, and carriage returns, enclose the field in the character specified by the QUOTE parameter. The default quote character is a double quotation mark ( " ). When the quote character is used within a field, escape the character with an additional quote character."This is exactly the approach used by the Python CSV writer, so it should take care of your problems. So my advice would be to create a standard csv file using code like this:Then in Redshift, change your  command to something like this (note the added  tag):Alternatively, you could continue manually converting your fields to match the default settings for Redshift's COPY command. Python's  won't do this for you on its own, but you may be able to speed up your code a bit, especially for big files, like this:As another alternative, you could experiment with importing the query data into a  DataFrame with , doing the replacements in the DataFrame (a whole column at a time), then writing the table out with . Pandas has incredibly fast csv code, so this may give you a significant speedup.Update: I just noticed that in the end I basically duplicated @hunteke's answer. The key point (which I missed the first time through) is that you probably haven't been using the  argument in your current Redshift  command; if you add that, this should get easy. 


Answer URL
https://docs.python.org/3/library/csv.html#dialects-and-formatting-parameters
