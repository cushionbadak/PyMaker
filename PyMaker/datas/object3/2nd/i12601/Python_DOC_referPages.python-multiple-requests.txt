Natural Text
I have a situation to call multiple requests in a scheduler job to check live user status for 1000 users at a time. But server limits maximum up to 50 users in each hit of an API request. So using following approach with  loop its taking around 66 seconds for 1000 users (i.e for 20 API calls).So, Is there any workaround so that it should optimize the time required to fetch API?Does  provide any multiprocessing option to process such api requests in a single job?
You could try to apply python's Thread pool from the  module, if the server allows concurrent requests. That way you would parallelise the processing, instead of the scheduling itselfThere are some good examples provided in the documentation here (If you're using python 2, there is a sort of an equivalent modulee.g.If you want to use a Process pool, just make sure you don't use shared resources, e.g. queue, and write your data our independently


Answer URL
https://docs.python.org/3/library/concurrent.futures.html
