Natural Text
It's a common idiom in python to use context manager to automatically close files:Now I want to read contents of several files. Consumer of the data does not know or care if data comes from files or not-files. It does not want to check if the objects it received can be open or not. It just wants to get something to read lines from. So I create an iterator like this:This iterator may be used like this:(Note, that the same code could be used to consume not the open files, but lists of strings - that's cool!)The question is: is it safe to yield open files?Looks like "why not?". Consumer calls iterator, iterator opens file, yields it to consumer. Consumer processes the file and comes back to iterator for next one. Iterator code resumes, we exit 'with' block, the  object gets closed, go to next file, etc.But what if consumer never comes back to iterator for the next file? F.e. an exception occurred inside the consumer. Or consumer found something very exciting in one of the files and happily returned the results to whoever called it?Iterator code would never resume in this case, we would never come to the end of 'with' block, and the  object would never get closed!Or would it?
You bring up a criticism that has been raised before1.  The cleanup in this case is non-deterministic, but it will happen with CPython when the generator gets garbage collected.  Your mileage may vary for other python implementations...Here's a quick example:Running this script in CPython, I get:Basically, what we see is that for generators that are exhausted, the context manager cleans up when you expect.  For generators that aren't exhausted, the cleanup function runs when the generator is collected by the garbage collector.  This happens when the generator goes out of scope (or, IIRC at the next  cycle at the latest).However, doing some quick experiments (e.g. running the above code in ), I don't get all of my context managers cleaned up:So, the assertion that the context manager's  will get called for all python implementations is untrue.  Likely the misses here are attributable to pypy's garbage collection strategy (which isn't reference counting) and by the time  decides to reap the generators, the process is already shutting down and therefore, it doesn't bother with it...  In most real-world applications, the generators would probably get reaped and finalized quickly enough that it doesn't actually matter...Providing strict guaranteesIf you want to guarantee that your context manager is finalized properly, you should take care to close the generator when you are done with it2.  Uncommenting the  lines above gives me deterministic cleanup because a  is raised at the  statement (which is inside the context manager) and then it's caught/suppressed by the generator...FWIW, this means that you can clean up your generators using :1Most recently, some discussion has revolved around PEP 533 which aims to make iterator cleanup more deterministic.2It is perfectly OK to close an already closed and/or consumed generator so you can call it without worrying about the state of the generator.
Is it safe to combine 'with' and 'yield' in python?I don't think you should do this.Let me demonstrate making some files:Convince ourselves that the files are there:And here's a function that recreates your code:Here it looks like you can use the function:But let's create a list comprehension of all of the file objects first:And now we see they are all closed:This only works until the generator closes. Then the files are all closed.I doubt that is what you want, even if you're using lazy evaluation, your last file will probably be closed before you're done using it.


Answer URL
https://docs.python.org/3/reference/expressions.html#generator.close
https://docs.python.org/3/reference/expressions.html#generator.throw
