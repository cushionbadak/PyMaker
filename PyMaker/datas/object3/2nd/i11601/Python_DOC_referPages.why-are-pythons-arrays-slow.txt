Natural Text
I expected  to be faster than lists, as arrays seem to be unboxed.However, I get the following result:What could be the cause of such a difference?
The storage is "unboxed", but every time you access an element Python has to "box" it (embed it in a regular Python object) in order to do anything with it.  For example, your  iterates over the array, and boxes each integer, one at a time, in a regular Python  object.  That costs time.  In your , all the boxing was done at the time the list was created.So, in the end, an array is generally slower, but requires substantially less memory.Here's the relevant code from a recent version of Python 3, but the same basic ideas apply to all CPython implementations since Python was first released.Here's the code to access a list item:There's very little to it:   just returns the 'th object in the list (and all Python objects in CPython are pointers to a struct whose initial segment conforms to the layout of a ).And here's the  implementation for an  with type code :The raw memory is treated as a vector of platform-native   integers; the 'th  is read up; and then  is called to wrap ("box") the native  in a Python  object (which, in Python 3, which eliminates Python 2's distinction between  and , is actually shown as type ).This boxing has to allocate new memory for a Python  object, and spray the native 's bits into it.  In the context of the original example, this object's lifetime is very brief (just long enough for  to add the contents into a running total), and then more time is required to deallocate the new  object.This is where the speed difference comes from, always has come from, and always will come from in the CPython implementation.
To add to Tim Peters' excellent answer, arrays implement the buffer protocol, while lists do not.  This means that, if you are writing a C extension (or the moral equivalent, such as writing a Cython module), then you can access and work with the elements of an array much faster than anything Python can do.  This will give you considerable speed improvements, possibly well over an order of magnitude.  However, it has a number of downsides:You are now in the business of writing C instead of Python.  Cython is one way to ameliorate this, but it does not eliminate many fundamental differences between the languages; you need to be familiar with C semantics and understand what it is doing.PyPy's C API works to some extent, but isn't very fast.  If you are targeting PyPy, you should probably just write simple code with regular lists, and then let the JITter optimize it for you.C extensions are harder to distribute than pure Python code because they need to be compiled.  Compilation tends to be architecture and operating-system dependent, so you will need to ensure you are compiling for your target platform.Going straight to C extensions may be using a sledgehammer to swat a fly, depending on your use case.  You should first investigate NumPy and see if it is powerful enough to do whatever math you're trying to do.  It will also be much faster than native Python, if used correctly.
Tim Peters answered why this is slow, but let's see how to improve it.Sticking to your example of  (factor 10 smaller than your example to fit into memory here):This way also numpy needs to box/unbox, which has additional overhead. To make it fast one has to stay within the numpy c code:So from the list solution to the numpy version this is a factor 16 in runtime.Let's also check how long creating those data structures takesClear winner: NumpyAlso note that creating the data structure takes about as much time as summing, if not more. Allocating memory is slow.Memory usage of those:So these take 8 bytes per number with varying overhead. For the range we use 32bit ints are sufficient, so we can safe some memory.But it turns out that adding 64bit ints is faster than 32bit ints on my machine, so this is only worth it if you are limited by memory/bandwidth.


Answer URL
https://docs.python.org/3/c-api/buffer.html
