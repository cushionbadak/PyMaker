Natural Text
I have an array of 10-bytes (80-bits) Little Endian float values (or ). How can i read this values in python 3?The package  does not support  (may be I read the docs carelessly).The package  as same as package "struct" does not support .The package  supports  or  types. It's very good, but appending  in a tail of  to extend it to  or  is ugly, importing of this package takes a lot of time.The package  supports . It's many times faster then numpy, but  is machine-dependent and can be less then 80 bits, appending  in a tail of  to extend it to  is ugly too.UPDATE 1: test code at my gist.github.The function  is ugly, but it works. Now I'm looking for right way
Let me rewrite my answer in a more logical way: is machine dependent because the longdouble float type is not set in stone by the C standard and is dependent on the compiler :( but it is still your best you can have right now for high precision floats...If you plan to use numpy, numpy.longdouble is what your are looking for, numpy.float96 or numpy.float128 are highly misleading names. They do not indicate a 96- or 128-bit IEEE floating point format. Instead, they indicate the number of bits of alignment used by the underlying long double type. So e.g. on x86-32, long double is 80 bits, but gets padded up to 96 bits to maintain 32-bit alignment, and numpy calls this . On x86-64, long double is again the identical 80 bit type, but now it gets padded up to 128 bits to maintain 64-bit alignment, and numpy calls this . There's no extra precision, just extra padding. Appending  at the end of a  to make a  is ugly, but in the end it is just that as  is just a padded  and  is a  or  depending of the architecture of the machine you use.What is the internal precision of numpy.float128?
 can use 80-bit float if the compiler and platform support them:Whether [supporting higher precision] is possible in numpy depends on  the hardware and on the development environment: specifically, x86  machines provide hardware floating-point with 80-bit precision, and  while most C compilers provide this as their  type, MSVC  (standard for Windows builds) makes  identical to double  (64 bits). Numpy makes the compilerâ€™s long double available as   (and np.clongdouble for the complex numbers). You can  find out what your numpy provides with.I checked that  is  in stock  at PyPI as well as in Gohlke's build and  in  in CentOS 6.
The padding, or rather, memory alignment of extended-precision floats on a 4 (x32) or 16 (x64) byte boundary, is added - by recommendatation from Intel no less - to avoid a performance hit associated with handling non-aligned data on x86 CPUs. To give you an idea of the hit's magnitude, some figures from Microsoft show ~2 times difference for DWORDs.This layout is ingrained into the underlying C's  rather than being 's invention, so  doesn't attempt to provide any way around it to extract/insert only the "significant" part.So, adding padding by hand if you have raw data without padding looks like the way to go. You can speed up the process by writing directly to the underlying buffer:Or even attain much more speedup by porting the code to Cython (if using raw buffers, the speedup compared to regular array indexing can be as much as 100x! This would hurt the code's maintainability though so beware of premature optimization here).Alternatively, for an "interchange" format, you might consider using one that is not bound to internal representation, like .


Answer URL
https://docs.python.org/3/library/decimal.html
