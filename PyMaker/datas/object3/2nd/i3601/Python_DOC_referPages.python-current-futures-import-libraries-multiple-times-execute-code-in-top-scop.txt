Natural Text
for the following script (python 3.6, windows anaconda), I noticed that the libraries are imported as many as the number of the processors were invoked. And  are also executed multiple same amount of times.  I thought the processors will only be invoked for  call rather than the whole program. The actual  is a heavy cpu bounded task which will be executed for millions of times. Is this the right choice of framework for such task? 
 uses the  module to do its multiprocessing.And, as explained in the Programming guidelines, this means you have to protect any top-level code you don't want to run in every process in your  block:Make sure that the main module can be safely imported by a new Python interpreter without causing unintended side effects (such a starting a new process).... one should protect the “entry point” of the program by using …Notice that this is only necessary if using the  or  start methods. But if you're on Windows,  is the default. And, at any rate, it never hurts to do this, and usually makes the code clearer, so it's worth doing anyway.You probably don't want to protect your s this way. After all, the cost of calling  once per core may seem nontrivial, but that only happens at startup, and the cost of running a heavy CPU-bound function millions of times will completely swamp it. (If not, you probably didn't want to use multiprocessing in the first place…) And usually, the same goes for your  and  statements (especially if they're not capturing any closure variables or anything). It's only setup code that's incorrect to run multiple times (like that  in your example) that needs to be protected.The examples in the  doc (and in PEP 3148) all handle this by using the "main function" idiom:This has the added benefit of turning your top-level globals into locals, to make sure you don't accidentally share them (which can especially be a problem with , where they get actually shared with , but copied with , so the same code may work when testing on one platform, but then fail when deployed on the other).If you want to know why this happens:With the  start method,  creates each new child process by cloning the parent Python interpreter and then just starting the pool-servicing function up right where you (or ) created the pool. So, top-level code doesn't get re-run.With the  start method,  creates each new child process by starting a clean new Python interpreter, ing your code, and then starting the pool-servicing function. So, top-level code gets re-run as part of the .


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor
https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods
https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
