Natural Text
My practice now:I let my backend to catch the get request sent by the front-end page  to run my scrapy spider, everytime the page is refreshed or loaded. The crawled data will be shown in my front page. Here's the code, I call a subprocess to run the spider:It works well.What I want:However, the spider crawls a website which barely changes, so it's no need to crawl that often. So I want to run my scrapy spider every 30 minutes using the coroutine method just at backend.What I tried and succeeded:It works well too.But when I change the codes of  into this (which is basically the same as the first one):the spider was run only at the first time and crawled data was stored to presentcode.json successfully, but the spider never called after 20 seconds later.QuestionsWhat's wrong with my program? Is it because I called a subprocess in a coroutine and it is invalid?Any better thoughts to run a spider while the main application is running?Edit:Let me put the code of my web app init function here first:My thought is, the main app made coroutine event loop 'stuck' so the spider cannot be callback later after. Let me check the source code of  and ..
Probably not a complete answer, and I would not do it like you do. But calling  from within an  coroutine is definitely not correct. Coroutines offer cooperative multitasking, so when you call  from within a coroutine, that coroutine effectively stops your whole app until called process is finished.One thing you need to understand when working with  is that control flow can be switched from one coroutine to another only when you call  (or , or ,  and other shortcuts). If you do some long action without calling any of those then you block any other coroutines until this action is finished.What you need to use is  which will properly return control flow to other parts of your application (namely webserver) while the subprocess is running.Here is how actual  coroutine could look:


Answer URL
https://docs.python.org/3/library/asyncio-subprocess.html
