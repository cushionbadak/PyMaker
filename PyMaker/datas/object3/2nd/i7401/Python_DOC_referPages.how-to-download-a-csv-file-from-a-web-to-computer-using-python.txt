Natural Text
I have a small utility that I use to download a MP3 from a website on a schedule and then builds/updates a podcast XML file which I've obviously added to iTunes.The text processing that creates/updates the XML file is written in Python. I use wget inside a Windows  file to download the actual MP3 however. I would prefer to have the entire utility written in Python though.I struggled though to find a way to actually down load the file in Python, thus why I resorted to .So, how do I download the file using Python?
In Python 2, use urllib2 which comes with the standard library.This is the most basic way to use the library, minus any error handling.  You can also do more complex stuff such as changing headers.  The documentation can be found here.
One more, using :(for Python 3+ use 'import urllib.request' and urllib.request.urlretrieve)Yet another one, with a "progressbar"
In 2012, use the python requests libraryYou can run  to get it.Requests has many advantages over the alternatives because the API is much simpler. This is especially true if you have to do authentication. urllib and urllib2 are pretty unintuitive and painful in this case.2015-12-30People have expressed admiration for the progress bar. It's cool, sure. There are several off-the-shelf solutions now, including :This is essentially the implementation @kvance described 30 months ago.
The  in  opens a file (and erases any existing file) in binary mode so you can save data with it instead of just text.
Python 3Python 2 (thanks Corey) (thanks PabloG)
use wget module:
An improved version of the PabloG code for Python 2/3:
Wrote wget library in pure Python just for this purpose. It is pumped up  with these features as of version 2.0.
Simple yet  compatible way comes with  library:
I agree with Corey, urllib2 is more complete than urllib and should likely be the module used if you want to do more complex things, but to make the answers more complete, urllib is a simpler module if you want just the basics:Will work fine. Or, if you don't want to deal with the "response" object you can call read() directly:
Following are the most commonly used calls for downloading files in python:Note:  and  are found to perform relatively bad with downloading large files (size > 500 MB).  stores the file in-memory until download is complete.  
You can get the progress feedback with urlretrieve as well:
If you have wget installed, you can use parallel_sync.pip install parallel_syncDoc:https://pythonhosted.org/parallel_sync/pages/examples.htmlThis is pretty powerful. It can download files in parallel, retry upon failure , and it can even download files on a remote machine.

In python3 you can use urllib3 and shutil libraires.Download them by using pip or pip3 (Depending whether python3 is default or not)Then run this codeNote that you download  but use  in code
If speed matters to you, I made a small performance test for the modules  and , and regarding  I tried once with status bar and once without. I took three different 500MB files to test with (different files- to eliminate the chance that there is some caching going on under the hood). Tested on debian machine, with python2.First, these are the results (they are similar in different runs):The way I performed the test is using "profile" decorator. This is the full code: seems to be the fastest
Source code can be:
Just for the sake of completeness, it is also possible to call any program for retrieving files using the  package. Programs dedicated to retrieving files are more powerful than Python functions like . For example,  can download directories recursively (), can deal with FTP, redirects, HTTP proxies, can avoid re-downloading existing files (), and  can do multi-connection downloads which can potentially speed up your downloads.In Jupyter Notebook, one can also call programs directly with the  syntax:
I wrote the following, which works in vanilla Python 2 or Python 3.Notes:Supports a "progress bar" callback.Download is a 4 MB test .zip from my website.
You can use PycURL on Python 2 and 3.
urlretrieve and requests.get is simple, however the reality not.I have fetched data for couple sites, including text and images, the above two probably solve most of the tasks. but for a more universal solution I suggest the use of urlopen. As it is included in Python 3 standard library, your code could run on any machine that run Python 3 without pre-installing site-parThis answer provides a solution to HTTP 403 Forbidden when downloading file over http using Python. I have tried only requests and urllib modules, the other module may provide something better, but this is the one I used to solve most of the problems.
This may be a little late, But I saw pabloG's code and couldn't help adding a os.system('cls') to make it look AWESOME! Check it out : If running in an environment other than Windows, you will have to use something other then 'cls'. In MAC OS X and Linux it should be 'clear'.


Answer URL
https://docs.python.org/3/library/urllib.request.html#urllib.request.urlopen
https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve
