Natural Text
I have a python  setup (i.e. worker processes) with custom signal handling, which prevents the worker from cleanly using  itself. (See extended problem description below).The SetupThe master class that spawns all worker processes looks like the following (some parts stripped to only contain the important parts).Here, it re-binds its own s only to print ; actually the received signals are propagated down the process tree and must be handled by the workers themselves. This is achieved by re-binding the signals after workers have been spawned.The worker class bases  and implements its own -method.In this method, it connects to a distributed message queue and polls the queue for items forever. Forever should be: until the worker receives  or . The worker should not quit immediately; instead, it has to finish whatever calculation it does and will quit afterwards (once  is set to ).The ProblemSo far for the basic setup, where (almost) everything works fine:The master process spawns the desired number of workersEach worker connects to the message queueOnce a message is published, one of the workers receives itThe facade pattern (using a class named MessageRouter) routes the received message to the respective function and executes itNow for the problem: Target functions (where the  gets directed to by the  facade) may contain very complex business logic and thus may require multiprocessing.If, for example, the target function contains something like this:Then the processes spawned by the  will also redirect their signal handling for  and  to the worker's  function (because of signal propagation to the process subtree), essentially printing  and not stopping at all. I know, that this happens due to the fact that I have re-bound the signals for the worker before its own child-processes are spawned. This is where I'm stuck: I just cannot set the workers' signals after spawning its child processes, because I do not know whether or not it spawns some (target functions are masked and may be written by others), and because the worker stays (as designed) in its poll-loop. At the same time, I cannot expect the implementation of a target function that uses  to re-bind its own signal handlers to (whatever) default values.Currently, I feel like restoring signal handlers in each loop in the worker (before the message is routed to its target function) and resetting them after the function has returned is the only option, but it simply feels wrong.Do I miss something? Do you have any advice? I'd be really happy if someone could give me a hint on how to solve the flaws of my design here!
There is not a clear approach for tackling the issue in the way you want to proceed. I often find myself in situations where I have to run unknown code (represented as Python entry point functions which might get down into some C weirdness) in multiprocessing environments.This is how I approach the problem.The main loopUsually the main loop is pretty simple, it fetches a task from some source (HTTP, Pipe, Rabbit Queue..) and submits it to a Pool of workers. I make sure the KeyboardInterrupt exception is correctly handled to shutdown the service.The workersThe workers are managed by a Pool of workers from either  or from . If I need more advanced features such as timeout support I either use billiard or pebble.Each worker will ignore SIGINT as recommended here. SIGTERM is left as default.The serviceThe service is controlled either by systemd or supervisord. In either cases, I make sure that the termination request is always delivered as a SIGINT (CTL+C).I want to keep SIGTERM as an emergency shutdown rather than relying only on SIGKILL for that. SIGKILL is not portable and some platforms do not implement it."I whish it was that simple"If things are more complex, I'd consider the use of frameworks such as Luigi or Celery. In general, reinventing the wheel on such things is quite detrimental and gives little gratifications. Especially if someone else will have to look at that code.The latter sentence does not apply if your aim is to learn how these things are done of course.
I was able to do this using Python 3 and  with the  flavour. Another way Python 3 > Python 2!Where by "this" I mean:Have a main process with its own signal handler which just joins the children.Have some worker processes with a signal handler which may spawn...further subprocesses which do not have a signal handler.The behaviour on Ctrl-C is then:manager process waits for workers to exit.workers run their signal handlers, (an maybe set a  flag and continue executing to finish their job, although I didn't bother in my example, I just joined the child I knew I had) and then exit.all children of the workers die immediately.Of course note that if your intention is for the children of the workers not to crash you will need to install some ignore handler or something for them in your worker process  method, or somewhere.To mercilessly lift from the docs:When the program starts and selects the forkserver start method, a server process is started. From then on, whenever a new process is needed, the parent process connects to the server and requests that it fork a new process. The fork server process is single threaded so it is safe for it to use os.fork(). No unnecessary resources are inherited.Available on Unix platforms which support passing file descriptors over Unix pipes.The idea is therefore that the "server process" inherits the default signal handling behaviour before you install your new ones, so all its children also have default handling.Code in all its glory:
Since my previous answer was python 3 only, I thought I'd also suggest a more dirty method for fun which should work on both python 2 and python 3. Not Windows though... just uses  under the covers, so patch it to reset the signal handling in the child:You can call that at the start of the run method of your  processes (so that you don't affect the Manager) and so be sure that any children will ignore those signals.This might seem crazy, but if you're not too concerned about portability it might actually not be a bad idea as it's simple and probably pretty resilient over different python versions. 


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.set_start_method
https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
