Natural Text
This code is designed to illustrate how multithreaded code can step on its shared variableTypical output is:I was expecting to see small deviations from 1000 in the length and max values but numbers between 999 and 1500 are normal.Given that the while block should complete when counts reaches a length of 999, and that the append operation should be thread-safe, why is there so much variance in the results? I expected small errors, not like these.
The delay from when a task is submitted to the executor to when it begins executing is a random variable D with some probability distribution. We may assume that only this task is running (no background tasks, etc.) and that each code line takes one time quantum to execute. For simplicity, assume that the limitation to four concurrent tasks is not present. These are grossly simplified assumptions that will make analyzing this behavior at least somewhat tractable.Assume that D identically assumes the value of 0. That is, all workers begin executing immediately. This is the synchronous case. I think we can both agree that, in this event, you will always get a counter of 999 every time with no variation.Now, assume that D is uniformly distributed from 4 to 13, inclusive. That is, there is a 10% chance of the execution of any task being delayed by any of 4, 5, 6, 7, 8, 9, 10, 11, 12 or 13 time quanta. Let's say we want to loop until the counter equals 10. The best case is everything is delayed minimally. The  line will execute at times , and it kicks off tasks at times . The 10th task runs the line  at time . The loop condition is next checked at time , meaning that 11 iterations of the loop have completed and the 12th one will not run. Thus , and we have one task scheduled that has not yet run (but is still going to run sometime; in our case before you print the results, but in general there is a race condition with printing).In the worst case, there is a delay of 13, so change  to  and you get the loop condition failing the 15th time, meaning  and there are 4 tasks outstanding that may or may not complete before you format your print output.So, in the best case, you will have a list length from 11 to 12 (depending on the race condition with the printing of output), and in the worst case you will have a list length from 14 to 18 (depending on the race conditions). I would expect the results of lots of trials of this to be roughly normally distributed with mean around ~14.5 and standard deviation ~1.2.You would expect about the same magnitude of error regardless of the target, so by making the target bigger you would see relatively less error (though the same magnitude). This is because the magnitude of error depends only on the distribution of the random variable. This might be why you see it converging for higher values: your task scheduler has the same delay distribution, but it becomes less relevant compared to the overall expectation as you increase that expectation. The lower you make the target, the larger the effect becomes relatively speaking.Based on the numbers you're getting = say, from ~1000 to ~1500 - I would guess your delay in terms of our line-ops metric is something like 250-900 line-ops if the above model roughly holds. Of course, the distribution is probably not uniform over the range 250-900, but that might be a rough guess. You could check the distribution yourself using timers and compare to the timed average cycle length of your loop. Also, that you have a max workers given changes the analysis by increasing the effective delay above that of the underlying system (assuming the system can execute arbitrarily many processes truly in parallel).As you can see, even with all the simplifying assumptions, it's a murky analysis, so really nailing down exactly why you see your distribution on a real computer is going to be a tall order.


Answer URL
https://docs.python.org/3/faq/library.html#what-kinds-of-global-value-mutation-are-thread-safe
