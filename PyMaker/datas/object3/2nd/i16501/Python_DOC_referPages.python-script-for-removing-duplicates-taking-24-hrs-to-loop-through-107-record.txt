Natural Text
input t1P95P,71655,LINC-JP,pathogenicP95P,71655,LINC-JP,pathogenicP71P,71655,LINC-JP,pathogenicP71P,71655,LINC-JP,pathogenicoutput opP95P,71655,LINC-JP,pathogenicP71P,71655,LINC-JP,pathogenicmyCodeIn the above input line1,2 and line 3,4 are duplicates. Hence in output these duplicates are removed  the entries in my input files are around 10^7.Why is my code running since past 24 hrs for an input of 76Mb file. Also it has yet to complete one iteration of entire input file.It works fine for small files.Can anyone please point out the reason for this long time.How can I optimize my program ? thnx
It's not clear why you're building a huge string () that holds the same thing the file does, or what  is for. In terms of performance, you can look up  much faster if  is a  than if  is a . Also, a minor point; shorter variable names don't improve performance, so use good ones instead. I would suggest:If you do really need final, make it a list until the last moment (see commented-out lines) - gradual string concatenation means the creation of lots of unnecessary objects.
You're using an O(n2) algorithm, which scales poorly for larger files:You should consider using a set (i.e. make  a ) or  if duplicates will always be next to each other. These approaches will be O(n).
if you have access to a Unix system,  is a nice utility that is made for your problem.see https://www.cs.duke.edu/csl/docs/unix_course/intro-89.htmlI know this is a  question, but sometimes  is not the tool for the task.And you can always embed a system call in your python script.
There are a couple things that you are doing very inefficiently. The largest is that you made  a list, so the line  has to search through everything in the list already in order to check if q matches it. If you make l a set, the membership check can be done using a hash calculation and array lookup, which take the same (small) amount of time no matter how much you add to the set (though it will cause  not to be read in the order that it was written).Other little speedups that you can do include:Using a tupple  instead of a list for .You are writing your output file  every loop. Your OS will try to batch and background the writes, but it may be faster to just do one big write at the end. I am not sure on this point but try it.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.groupby
