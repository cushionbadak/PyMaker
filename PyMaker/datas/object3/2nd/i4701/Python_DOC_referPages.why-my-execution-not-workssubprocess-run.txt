Natural Text
How can I call an external command (as if I'd typed it at the Unix shell or Windows command prompt) from within a Python script?
Look at the subprocess module in the standard library:The advantage of subprocess vs. system is that it is more flexible (you can get the stdout, stderr, the "real" status code, better error handling, etc...).The official documentation recommends the subprocess module over the alternative os.system():The subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function [].The "Replacing Older Functions with the subprocess Module" section in the subprocess documentation may have some helpful recipes.Older versions of Python use call:
Here's a summary of the ways to call external programs and the advantages and disadvantages of each: passes the command and arguments to your system's shell.  This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection.  For example:  However, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, etc.  On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.  See the documentation. will do the same thing as  except that it gives you a file-like object that you can use to access standard input/output for that process.  There are 3 other variants of popen that all handle the i/o slightly differently.  If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don't need to worry about escaping anything.  See the documentation.The  class of the  module.  This is intended as a replacement for  but has the downside of being slightly more complicated by virtue of being so comprehensive.  For example, you'd say:instead of: but it is nice to have all of the options there in one unified class instead of 4 different popen functions.  See the documentation.The  function from the  module.  This is basically just like the  class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code.  For example:See the documentation.If you're on Python 3.5 or later, you can use the new  function, which is a lot like the above but even more flexible and returns a  object when the command finishes executing.The os module also has all of the fork/exec/spawn functions that you'd have in a C program, but I don't recommend using them directly.The  module should probably be what you use.Finally please be aware that for all methods where you pass the final command to be executed by the shell as a string and you are responsible for escaping it. There are serious security implications if any part of the string that you pass can not be fully trusted. For example, if a user is entering some/any part of the string. If you are unsure, only use these methods with constants. To give you a hint of the implications consider this code:and imagine that the user enters "my mama didnt love me && rm -rf /".
I typically use:You are free to do what you want with the  data in the pipe.  In fact, you can simply omit those parameters ( and ) and it'll behave like .
Some hints on detaching the child process from the calling one (starting the child process in background).Suppose you want to start a long task from a CGI-script, that is the child process should live longer than the CGI-script execution process.The classical example from the subprocess module docs is:The idea here is that you do not want to wait in the line 'call subprocess' until the longtask.py is finished. But it is not clear what happens after the line 'some more code here' from the example.My target platform was freebsd, but the development was on windows, so I faced the problem on windows first.On windows (win xp), the parent process will not finish until the longtask.py has finished its work. It is not what you want in CGI-script. The problem is not specific to Python, in PHP community the problems are the same.The solution is to pass DETACHED_PROCESS Process Creation Flag to the underlying CreateProcess function in win API.If you happen to have installed pywin32 you can import the flag from the win32process module, otherwise you should define it yourself:/* UPD 2015.10.27 @eryksun in a comment below notes, that the semantically correct flag is CREATE_NEW_CONSOLE (0x00000010) */On freebsd we have another problem: when the parent process is finished, it finishes the child processes as well. And that is not what you want in CGI-script either. Some experiments showed that the problem seemed to be in sharing sys.stdout. And the working solution was the following:I have not checked the code on other platforms and do not know the reasons of the behaviour on freebsd. If anyone knows, please share your ideas. Googling on starting background processes in Python does not shed any light yet.
I'd recommend using the subprocess module instead of os.system because it does shell escaping for you and is therefore much safer: http://docs.python.org/library/subprocess.html
If you want to return the results of the command, you can use . However, this is deprecated since version 2.6 in favor of the subprocess module, which other answers have covered well.
Note that this is dangerous, since the command isn't cleaned. I leave it up to you to google for the relevant documentation on the 'os' and 'sys' modules. There are a bunch of functions (exec* and spawn*) that will do similar things.
There are lots of different libraries which allow you to call external commands with Python. For each library I've given a description and shown an example of calling an external command. The command I used as the example is  (list all files). If you want to find out more about any of the libraries I've listed and linked the documentation for each of them.Sources:subprocess: https://docs.python.org/3.5/library/subprocess.htmlshlex: https://docs.python.org/3/library/shlex.htmlos: https://docs.python.org/3.5/library/os.htmlsh: https://amoffat.github.io/sh/plumbum: https://plumbum.readthedocs.io/en/latest/pexpect: https://pexpect.readthedocs.io/en/stable/fabric: http://www.fabfile.org/envoy: https://github.com/kennethreitz/envoycommands: https://docs.python.org/2/library/commands.htmlThese are all the libraries:Hopefully this will help you make a decision on which library to use :)subprocessSubprocess allows you to call external commands and connect them to their input/output/error pipes (stdin, stdout, and stderr). Subprocess is the default choice for running commands, but sometimes other modules are better.osos is used for "operating system dependent functionality". It can also be used to call external commands with  and  (Note: There is also a subprocess.popen). os will always run the shell and is a simple alternative for people who don't need to, or don't know how to use .shsh is a subprocess interface which lets you call programs as if they were functions. This is useful if you want to run a command multiple times.plumbumplumbum is a library for "script-like" Python programs. You can call programs like functions as in . Plumbum is useful if you want to run a pipeline without the shell.pexpectpexpect lets you spawn child applications, control them and find patterns in their output. This is a better alternative to subprocess for commands that expect a tty on Unix.fabricfabric is a Python 2.5 and 2.7 library. It allows you to execute local and remote shell commands. Fabric is simple alternative for running commands in a secure shell (SSH)envoyenvoy is known as "subprocess for humans". It is used as a convenience wrapper around the  module.commands contains wrapper functions for , but it has been removed from Python 3 since  is a better alternative.The edit was based on J.F. Sebastian's comment.
I always use  for this things like:But this seem to be a good tool:  (Python subprocess interface).Look an example:
Check the "pexpect" Python library, too.It allows for interactive controlling of external programs/commands, even ssh, ftp, telnet, etc. You can just type something like:
If you need the output from the command you are calling,then you can use subprocess.check_output (Python 2.7+).Also note the shell parameter.If shell is , the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a userâ€™s home directory. However, note that Python itself offers implementations of many shell-like features (in particular, , , , , , and ).
With Standard LibraryTheUse subprocess module (Python 3):It is the recommended standard way. However, more complicated tasks (pipes, output, input, etc.) can be tedious to construct and write.Note on Python version: If you are still using Python 2, subprocess.call works in a similar way.ProTip: shlex.split can help you to parse the command for , , and other  functions in case you don't want (or you can't!) provide them in form of lists:With External DependenciesIf you do not mind external dependencies, use plumbum:It is the best  wrapper. It's cross-platform, i.e. it works on both Windows and Unix-like systems. Install by .Another popular library is sh:However,  dropped Windows support, so it's not as awesome as it used to be. Install by .
This is how I run my commands. This code has everything you need pretty much
Update: is the recommended approach as of Python 3.5 if your code does not need to maintain compatibility with earlier Python versions. It's more consistent and offers similar ease-of-use as Envoy. (Piping isn't as straightforward though. See this question for how.)Here's some examples from the docs.Run a process:Raise on failed run:Capture output:Original answer:I recommend trying Envoy. It's a wrapper for subprocess, which in turn aims to replace the older modules and functions. Envoy is subprocess for humans.Example usage from the readme:Pipe stuff around too:
Without the output of the result:With output of the result:
https://docs.python.org/2/library/subprocess.html...or for a very simple command:
There is also Plumbum
 is OK, but kind of dated.  It's also not very secure.  Instead, try .   does not call sh directly and is therefore more secure than .Get more information here.
Calling an external command in PythonSimple, use , which returns a  object:Why?As of Python 3.5, the documentation recommends subprocess.run:The recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly.Here's an example of the simplest possible usage - and it does exactly as asked: waits for the command to successfully finish, then returns a  object. It may instead raise  (if you give it a  argument) or  (if it fails and you pass ).As you might infer from the above example, stdout and stderr both get piped to your own stdout and stderr by default.We can inspect the returned object and see the command that was given and the returncode:Capturing outputIf you want to capture the output, you can pass  to the appropriate  or :(I find it interesting and slightly counterintuitive that the version info gets put to stderr instead of stdout.)Pass a command listOne might easily move from manually providing a command string (like the question suggests) to providing a string built programmatically. Don't build strings programmatically. This is a potential security issue. It's better to assume you don't trust the input. Note, only  should be passed positionally.Full SignatureHere's the actual signature in the source and as shown by :The  and  are given to the  constructor.  can be a string of bytes (or unicode, if specify encoding or ) that will be piped to the subprocess's stdin.The documentation describes  and  better than I could:The timeout argument is passed to Popen.communicate(). If the timeout  expires, the child process will be killed and waited for. The  TimeoutExpired exception will be re-raised after the child process has  terminated.If check is true, and the process exits with a non-zero exit code, a  CalledProcessError exception will be raised. Attributes of that  exception hold the arguments, the exit code, and stdout and stderr if  they were captured.and this example for  is better than one I could come up with:Expanded SignatureHere's an expanded signature, as given in the documentation:Note that this indicates that only the args list should be passed positionally. So pass the remaining arguments as keyword arguments.PopenWhen use  instead? I would struggle to find use-case based on the arguments alone. Direct usage of  would, however, give you access to its methods, including , 'send_signal', 'terminate', and 'wait'.Here's the  signature as given in the source. I think this is the most precise encapsulation of the information (as opposed to ):But more informative is the  documentation:Execute a child program in a new process. On POSIX, the class uses  os.execvp()-like behavior to execute the child program. On Windows,  the class uses the Windows CreateProcess() function. The arguments to  Popen are as follows.Understanding the remaining documentation on  will be left as an exercise for the reader.
Use:os - This module provides a portable way of using operating system-dependent functionality.For the more  functions, here is the documentation.
It can be this simple:
 is convenient if you don't want to test return values. It throws an exception on any error.
I tend to use subprocess together with shlex (to handle escaping of quoted strings):
 does not allow you to store results, so if you want to store results in some list or something  works.
There is another difference here which is not mentioned previously. executes the <command> as a subprocess. In my case, I need to execute file <a> which needs to communicate with another program, <b>. I tried subprocess, and execution was successful. However <b> could not communicate with <a>.Everything is normal when I run both from the terminal.One more: (NOTE: kwrite behaves different from other applications. If you try the below with Firefox, the results will not be the same.)If you try , program flow freezes until the user closes kwrite. To overcome that I tried instead . This time program continued to flow, but kwrite became the subprocess of the console.Anyone runs the kwrite not being a subprocess (i.e. in the system monitor it must appear at the leftmost edge of the tree).
use the os moduleeg
I quite like shell_command for its simplicity.  It's built on top of the subprocess module.Here's an example from the docs:
Shameless plug, I wrote a library for this :Phttps://github.com/houqp/shell.pyIt's basically a wrapper for popen and shlex for now. It also supports piping commands so you can chain commands easier in Python. So you can do things like:
Under Linux, in case you would like to call an external command that will execute independently (will keep running after the python script terminates), you can use a simple queue as task spooler or the at commandAn example with task spooler:Notes about task spooler (): You could set the number of concurrent processes to be run ("slots") with:Installing  doesn't requires admin privileges. You can download and compile it from source with a simple , add it to your path and you're done.
In Windows you can just import the  module and run external commands by calling ,  and  as below:Output:


Answer URL
https://docs.python.org/3/library/shlex.html
https://docs.python.org/3/library/subprocess.html#subprocess.run
https://docs.python.org/3/library/subprocess.html#subprocess.run
https://docs.python.org/3/library/subprocess.html#popen-constructor
