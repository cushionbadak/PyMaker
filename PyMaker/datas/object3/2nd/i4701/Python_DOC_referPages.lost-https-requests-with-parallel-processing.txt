Natural Text
I use the two following class methods to request information from the Questrade API (http://www.questrade.com/api/documentation/rest-operations/market-calls/markets-quotes-id). I have over 11,000 stock symbols where I request the Questrade API with batches of 100 symbols. If I make over 110 HTTPS requests with Parallel class, then instead of getting 11,000 output I got 10,500 or 10,600. So I lost data with parallel processing.  Be aware that I used two python module here, i.e. joblib (https://github.com/joblib/joblib/issues/651) and requests (https://github.com/requests/requests). The following  loop worked perfectly, so I know my problem is with the Parallel class.How could I increase the performance of the last  loop without losing data?UPDATEA sample of  (simplified result) could be and  is simply  as seen in the above Questrade API link.UPDATE 2The Marat's answer was a good try but didn't give me a better result. The first test gave me 31,356 (or 10,452 if I divide that result by 3) instead of 10,900. The second test just gave me 0 or the process block completely.I found out that the  is 20. Link : http://www.questrade.com/api/documentation/rate-limiting. How could I increase the performance of the last  loop without losing data in considering that new information?
If you are not stuck with using  you could try some standard library parallel processing modules. In python2/3  is available and provides functions for mapping a task across parallel threads. A simplified version would look like this:There are asynchronous and iterable versions of  that you would probably want for larger sized jobs, and of course you could add parameters to your  task to avoid hard coding things like I did. A caveat with using  is that any arguments passed to it have to be picklable.In python3 the concurrent.futures module actually has a nice example of multithreaded url retrieval in the docs. With a little effort you could replace  in that example with your  function. There is a version of  backported to python2 as the  module, as well.These might require a bit more work in refactoring, so if there is a solution that sticks with  feel free to prefer that. On the off-chance that your problem is a bug in , there are plenty of ways you could do this in a multithreaded fashion with standard library (albeit with some added boilerplate).
Most likely, it happens because some of HTTP calls fail due to network load. To test, change :Much less likely:  is not thread safe. If the snippet above didn't help, try guarding  with a lock:


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example
