Natural Text
I use the library multiprocessing in a flask-based web application to start long-running processes. The function that does it is the following: When I want to deploy some code on this web application, I restart a service  that restarts gunicorn, served by nginx. My problem is that this restart kills all children processes started by this application as if a SIGINT signal were sent to all children. How could I avoid that ?EDIT: After reading this post, it appears that this behavior is normal. The answer suggests to use the subprocess library instead. So I reformulate my question: how should I proceed if I want to start long-running tasks (which are python functions) in a python script and make sure they would survive the parent process OR make sure the parent process (which is a gunicorn instance) would survive a deployement ?  FINAL EDIT: I chose @noxdafox answer since it is the more complete one. First, using process queuing systems might be the best practice here. Then as a workaround, I can still use multiprocessing but using the python-daemon context (see here ans here) inside the function wrapper. Last, @Rippr suggests using subprocess with a different process group, which is cleaner than forking with multiprocessing but involves having standalone functions to launch (in my case I start specific functions from imported libraries).     
I would recommend against your design as it's quite error prone. Better solutions would de-couple the workers from the server using some sort of queuing system (, , , ...).Nevertheless, here's a couple of "hacks" you could try out.Turn your child processes into UNIX daemons. The python daemon module could be a starting point.Instruct your child processes to ignore the  signal. The service orchestrator might work around that by issuing a  or  signal if child processes refuse to die. You might need to disable such feature. To do so, just add the following line at the beginning of the  function:
Ultimately, this problem comes down to misunderstanding of what it means to do the deployment.  In non-enterprise languages like Python (compared to enterprise ones like Erlang), it is generally understood that deployment wipes out any of the preceding artefacts of running the process.  As such, it would clearly be a bug if your old children/functions don't actually terminate once a new deployment is performed.To play the devil's advocate, it is even unclear from your question/spec of what your actual expectation for the deployment is â€” do you simply expect your old "functions" to run forever?  How do those functions get started in the first place?  Who's supposed to know whether or not those "functions" were modified in a given deployment, and whether or not they're supposed to be restarted and in what fashion?  A lot of consideration to these very questions is given in Erlang/OTP (which are unrelated to Python), and, as such, you can't simply expect the machine to read your mind when you use a language like Python that's not even designed for such a use-case.As such, it may be a better option to separate the long-running logic from the rest of the code, and perform the deployment appropriately.  As the other answer mentions, this may involve spawning a separate UNIX  directly from within Python, or maybe even using an entirely separate logic to handle the situation.
Adding on to @noxdafox's excellent answer, I think you can consider this alternative:Basically, the child processes are killed because they belong to the same process group as the parent. By adding the  parameter, you are simply requesting the child processes to spawn in their own process groups which means they will not receive the terminate signal.Explanation taken from here.


Answer URL
https://docs.python.org/3/library/multiprocessing.html?highlight=process#multiprocessing.Process
