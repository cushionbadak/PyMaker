Natural Text
I have written the program (below) to:read a huge text file as then  using a specific column value to split the data and store as list of dataframes.then pipe the data to  to process each dataframe in parallel.Everything is fine, the program works well on my small test dataset. But, when I pipe in my large data (about 14 GB), the memory consumption exponentially increases and then freezes the computer or gets killed (in HPC cluster). I have added codes to clear the memory as soon as the data/variable isn't useful. I am also closing the pool as soon as it is done. Still with 14 GB input I was only expecting 2*14 GB memory burden, but it seems like lot is going on. I also tried to tweak using  but I am not seeing any difference in optimization in both test vs. large file.I think improvements to this code is/are required at this code position, when I start . but, I am posting the whole code.Test example: I created a test file ("genome_matrix_final-chr1234-1mb.txt") of upto 250 mb and ran the program. When I check the system monitor I can see that memory consumption increased by about 6 GB. I am not so clear why so much memory space is taken by 250 mb file plus some outputs. I have shared that file via drop box if it helps in seeing the real problem. https://www.dropbox.com/sh/coihujii38t5prd/AABDXv8ACGIYczeMtzKBo0eea?dl=0 Can someone suggest, How I can get rid of the problem?My python script:Update for bounty hunters:I have achieved multiprocessing using  but the code is causing a big memory burden (input test file ~ 300 mb, but memory burden is about 6 GB). I was only expecting 3*300 mb memory burden at max. Can somebody explain, What is causing such a huge memory requirement for such a small file and for such small length computation. Also, i am trying to take the answer and use that to improve multiprocess in my large program. So, addition of any method, module that doesn't change the structure of computation part (CPU bound process) too much should be fine. I have included two test files for the test purposes to play with the code. The attached code is full code so it should work as intended as it is when copied-pasted. Any changes should be used only to improve optimization in multiprocessing steps.
PrerequisiteIn Python (in the following I use 64-bit build of Python 3.6.5) everything is an object. This has its overhead and with  we can see exactly the size of an object in bytes:When fork system call used (default on *nix, see ) to create a child process, parent's physical memory is not copied and copy-on-write technique is used. Fork child process will still report full RSS (resident set size) of the parent process. Because of this fact, PSS (proportional set size) is more appropriate metric to estimate memory usage of forking application. Here's an example from the page:Process A has 50 KiB of unshared memoryProcess B has 300 KiB of unshared memoryBoth process A and process B have 100 KiB of the same shared memory regionSince the PSS is defined as the sum of the unshared memory of a process and the proportion of memory shared with other processes, the PSS for these two processes are as follows:PSS of process A = 50 KiB + (100 KiB / 2) = 100 KiBPSS of process B = 300 KiB + (100 KiB / 2) = 350 KiBThe data frameNot let's look at your  alone.  will help us.justpd.pyNow let's use the profiler:We can see the plot:and line-by-line trace:We can see that the data frame takes ~2 GiB with peak at ~3 GiB while it's being built. What's more interesting is the output of . But  ("deep" means introspection of the data deeply by interrogating  s, see below) gives:Huh?! Looking outside of the process we can make sure that  's figures are correct.  also shows the same value for the frame (most probably because of custom ) and so will other tools that use it to estimate allocated , e.g. .Gives:So where do these 7.93 GiB come from? Let's try to explain this. We have 4M rows and 34 columns, which gives us 134M values. They are either  or  (which is a 64-bit pointer; see using pandas with large data for detailed explanation). Thus we have  ~1022 MiB only for values in the data frame. What about the remaining ~ 6.93 GiB?String interningTo understand the behaviour it's necessary to know that Python does string interning. There are two good articles (one, two) about string interning in Python 2. Besides the Unicode change in Python 3 and PEP 393 in Python 3.3 the C-structures have changed, but the idea is the same. Basically, every short string that looks like an identifier will be cached by Python in an internal dictionary and references will point to the same Python objects. In other word we can say it behaves like a singleton. Articles that I mentioned above explain what significant memory profile and performance improvements it gives. We can check if a string is interned using  field of :Then:With two strings we can also do identity comparison (addressed in memory comparison in case of CPython).Because of that fact, in regard to  , the data frame allocates at most 20 strings (one per amino acids). Though, it's worth noting that Pandas recommends categorical types for enumerations.Pandas memoryThus we can explain the naive estimate of 7.93 GiB like:Note that  is 58 bytes, not 50 as we've seen above for 1-character literal. It's because PEP 393 defines compact and non-compact strings. You can check it with .Actual memory consumption should be ~1 GiB as it's reported by , it's twice as much. We can assume it has something to do with memory (pre)allocation done by Pandas or NumPy. The following experiment shows that it's not without reason (multiple runs show the save picture):I want to finish this section by a quote from fresh article about design issues and future Pandas2 by original author of Pandas.pandas rule of thumb: have 5 to 10 times as much RAM as the size of your datasetProcess treeLet's come to the pool, finally, and see if can make use of copy-on-write. We'll use  (available form an Ubuntu repository) to estimate process group memory sharing and  to write down system-wide free memory. Both can write JSON. We'll run original script with . We'll need 3 terminal windows. Then  produces:The sum chart () looks like:Note that two charts above show RSS. The hypothesis is that because of copy-on-write it's doesn't reflect actual memory usage. Now we have two JSON files from  and . I'll the following script to covert the JSON files to CSV.First let's look at  memory.The difference between first and minimum is ~4.15 GiB. And here is how PSS figures look like:And the sum:Thus we can see that because of copy-on-write actual memory consumption is ~4.15 GiB. But we're still serialising data to send it to worker processes via . Can we leverage copy-on-write here as well?Shared dataTo use copy-on-write we need to have the  be accessible globally so the worker after fork can still read it. Let's modify code after  in  like the following:Remove  that goes later.And modify first lines of  like:Now let's re-run it. Free memory:Process tree:And its sum:Thus we're at maximum of ~2.9 GiB of actual memory usage (the peak main process has while building the data frame) and copy-on-write has helped! As a side note, there's so called copy-on-read, the behaviour of Python's reference cycle garbage collector, described in Instagram Engineering (which led to  in issue31558). But  doesn't have an impact in this particular case.UpdateAn alternative to copy-on-write copy-less data sharing can be delegating it to the kernel from the beginning by using . Here's an example implementation from High Performance Data Processing in Python talk. The tricky part is then to make Pandas to use the mmaped Numpy array.
When you use  a number of child processes will be created using the  system call. Each of those processes start off with an exact copy of the memory of the parent process at that time. Because you're loading the csv before you create the  of size 3, each of those 3 processes in the pool will unnecessarily have a copy of the data frame. ( as well as  will exist in the current process as well as in each of the 3 child processes, so 4 copies of each of these structures will be in memory)Try creating the  before loading the file (at the very beginning actually) That should reduce the memory usage.If it's still too high, you can:Dump gen_matrix_df_list to a file, 1 item per line, e.g: Use  on an iterator over the lines that you dumped in this file, e.g.:(Note that  takes a  tuple in the example above, not just a value)I hope that helps.NB: I haven't tested the code above. It's only meant to demonstrate the idea.
I had the same issue. I needed to process a huge text corpus while keeping a knowledge base of few DataFrames of millions of rows loaded in memory. I think this issue is common so I will keep my answer oriented for general purposes.A combination of settings solved the problem for me (1 & 3 & 5 only might do it for you):Use  (or ) instead of . This will iterate over data lazily than loading all of it in memory before starting processing.Set a value to  parameter. This will make  faster too.Set a value to  parameter.Append output to disk than in memory. Instantly or every while when it reaches a certain size.Run the code in different batches. You can use itertools.islice if you have an iterator. The idea is to split your  to three or more lists, then you pass the first third only to  or , then the second third in another run, etc. Since you have a list you can simply slice it in the same line of code.
GENERAL ANSWER ABOUT MEMORY WITH MULTIPROCESSINGYou asked: "What is causing so much memory to be allocated". The answer relies on two parts. First, as you already noticed, each  worker gets it's own copy of the data (quoted from here), so you should chunk large arguments. Or for large files, read them in a little bit at a time, if possible.By default the workers of the pool are real Python processes forked   using the multiprocessing module of the Python standard library when   n_jobs != 1. The arguments passed as input to the Parallel call are   serialized and reallocated in the memory of each worker process.This can be problematic for large arguments as they will be  reallocated n_jobs times by the workers.Second, if you're trying to reclaim memory, you need to understand that python works differently than other languages, and you are relying on del to release the memory when it doesn't. I don't know if it's best, but in my own code, I've overcome this be reassigning the variable to a None or empty object.FOR YOUR SPECIFIC EXAMPLE - MINIMAL CODE EDITINGAs long as you can fit your large data in memory twice, I think you can do what you are trying to do by just changing a single line. I've written very similar code and it worked for me when I reassigned the variable (vice call del or any kind of garbage collect). If this doesn't work, you may need to follow the suggestions above and use disk I/O:FOR YOUR SPECIFIC EXAMPLE - OPTIMAL MEMORY USAGEAs long as you can fit your large data in memory once, and you have some idea of how big your file is, you can use Pandas read_csv partial file reading, to read in only nrows at a time if you really want to micro-manage how much data is being read in, or a [fixed amount of memory at a time using chunksize], which returns an iterator5. By that I mean, the nrows parameter is just a single read: you might use that to just get a peek at a file, or if for some reason you wanted each part to have exactly the same number of rows (because, for example, if any of your data is strings of variable length, each row will not take up the same amount of memory). But I think for the purposes of prepping a file for multiprocessing, it will be far easier to use chunks, because that directly relates to memory, which is your concern. It will be easier to use trial & error to fit into memory based on specific sized chunks than number of rows, which will change the amount of memory usage depending on how much data is in the rows. The only other difficult part is that for some application specific reason, you're grouping some rows, so it just makes it a little bit more complicated. Using your code as an example: 


Answer URL
https://docs.python.org/3/library/sys.html#sys.getsizeof
https://docs.python.org/3/library/itertools.html#itertools.islice
