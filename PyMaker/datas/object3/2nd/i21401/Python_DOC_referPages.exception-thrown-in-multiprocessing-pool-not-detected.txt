Natural Text
It seems that when an exception is raised from a multiprocessing.Pool process, there is no stack trace or any other indication that it has failed. Example: prints 1 and stops silently. Interestingly, raising a BaseException instead works. Is there any way to make the behavior for all exceptions the same as BaseException?
I have a reasonable solution for the problem, at least for debugging purposes. I do not currently have a solution that will raise the exception back in the main processes. My first thought was to use a decorator, but you can only pickle functions defined at the top level of a module, so that's right out.Instead, a simple wrapping class and a Pool subclass that uses this for  (and hence ). I'll leave  as an exercise for the reader.This gives me:
Maybe I'm missing something, but isn't that what the  method of the Result object returns? See Process Pools.class multiprocessing.pool.AsyncResultThe class of the result returned by Pool.apply_async() and Pool.map_async().get([timeout])  Return the result when it arrives. If timeout is not None and the result does not arrive within  timeout seconds then multiprocessing.TimeoutError is raised. If the remote  call raised an exception then that exception will be reraised by get().So, slightly modifying your example, one can doWhich gives as resultThis is not completely satisfactory, since it does not print the traceback, but is better than nothing.UPDATE: This bug has been fixed in Python 3.4, courtesy of Richard Oudkerk. See the issue get method of multiprocessing.pool.Async should return full traceback.
The solution with the most votes at the time of writing has a problem:As @dfrankow noted, it will wait on , which ruins the point of running a task asynchronously. So, for better efficiency (in particular if your worker function  takes a long time) I would change it to:Advantages: the worker function is run asynchronously, so if for example you are running many tasks on several cores, it will be a lot more efficient than the original solution.Disadvantages: if there is an exception in the worker function, it will only be raised after the pool has completed all the tasks. This may or may not be the desirable behaviour. EDITED according to @colinfang's comment, which fixed this. 
I've had success logging exceptions with this decorator:with the code in the question, it'sSimply decorate the function you pass to your process pool. The key to this working is  otherwise multiprocessing throws a .code above gives

I created a module RemoteException.py that shows the full traceback of a exception in a process. Python2. Download it and add this to your code:
I'd try using pdb:
Since you have used , I guess the use case is want to do some synchronize tasks. Use callback for handling is another option. Please note this option is available only for python3.2 and above and not available on python2.7.
Since there are already decent answers for  available, I will provide a solution using a different approach for completeness.For  the following solution seems to be the simplest:Advantages:very little coderaises an exception in the main processprovides a stack traceno external dependenciesFor more info about the API please check: https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutorAdditionally, if you are submitting a large number of tasks and you would like your main process to fail as soon as one of your tasks fail, you can use the following snippet:All of the other answers fail only once all tasks have been executed.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.apply_async
