Natural Text
I'm trying to implement an edge list for a MultiGraph in Python.What I've tried so far: has constant-time deletion of all edges between two vertices (e.g. ) but linear-time random selection on those edges (e.g. ). Note that you have to do a selection on  (vs.  itself). has constant-time random selection () but linear-time deletion of all elements equal to a given edge ().Question: is there a Python data structure that would give me both constant-time random selection and deletion?
I don't think what you're trying to do is achievable in theory.If you're using weighted values to represent duplicates, you can't get constant-time random selection. The best you could possibly do is some kind of skip-list-type structure that lets you binary-search the element by weighted index, which is logarithmic.If you're not using weighted values to represent duplicates, then you need some structure that allows you to store multiple copies. And a hash table isn't going to do itâ€”the dups have to be independent objects (e.g., ),, meaning there's no way to delete all that match some criterion in constant time.If you can accept logarithmic time, the obvious choice is a tree. For example, using :To select one at random:The documentation doesn't seem to guarantee that this won't do something O(n). But fortunately, the source for both 3.3 and 2.7 shows that it's going to do the right thing. If you don't trust that, just write .To delete all copies of an edge, you can do it like this:Or:The documentation explains the exact performance guarantees for every operation involved. In particular,  is constant, while indexing, slicing, deleting by index or slice, bisecting, and removing by value are all logarithmic, so both operations end up logarithmic.(It's worth noting that  is a B+Tree; you might get better performance out of a red-black tree, or a treap, or something else. You can find good implementations for most data structures on PyPI.)As pointed out by senderle, if the maximum number of copies of an edge is much smaller than the size of the collection, you can create a data structure that does it in time quadratic on the maximum number of copies. Translating his suggestion into code:(You could, of course, change the replace-list-with-list-comprehension line with a three-liner find-and-update-in-place, but that's still linear in the number of dups.)Obviously, if you plan to use this for real, you may want to beef up the class a bit. You can make it look like a  of edges, a  of s of multiple copies of each edge, a , etc., by implementing a few methods and letting the appropriate / fill in the rest.So, which is better? Well, in your sample case, the average dup count is half the size of the list, and the maximum is 2/3rds the size. If that were true for your real data, the tree would be much, much better, because  will obviously blow away . On the other hand, if dups were rare, senderle's solution would obviously be better, because  is still 1 if  is 1. Of course for a 3-element sample, constant overhead and multipliers are going to dominate everything. But presumably your real collection isn't that tiny. (If it is, just use a ...)If you don't know how to characterize your real data, write both implementations and time them with various realistic inputs.


Answer URL
