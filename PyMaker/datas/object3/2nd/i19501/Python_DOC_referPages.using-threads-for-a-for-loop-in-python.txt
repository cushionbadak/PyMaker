Natural Text
rucksack1 = Rucksack(100)This is a small algo for the knapsack-problem. I have to parallelize the code somehow, but I don't get the thread module so far. I think the only place to work with parallelisation is the for-loop, right? So, I tried this:but the results are strange. Nothing happens and if I make a keyboardinterrupt, the result is correct, but thats definetly not the sense of the implementation. Can you tell me what is wrong with the second code or where else I could use perallelisation soundly. Thanks.
First, since your code is CPU-bound, you will get very little benefit from using threads for parallelism, because of the GIL, as bereal explains. Fortunately, there are only a few differences between threads and processes—basically, all shared data must be passed or shared explicitly (see Sharing state between processes for details).Second, if you want to data-parallelize your code, you have to lock all access to mutable shared objects. From a quick glance, while  and  look immutable, the  object is a shared global that you modify all over the place. If you can change your code to return a value up the chain, that's ideal. If not, if you can accumulate a bunch of separate return values and merge them after processing is finished, that's usually good too. If neither is feasible, you will need to guard all access to  with a lock. See Synchronization between processes for details.Finally, you ask where to put the parallelism. The key is to find the right dividing line between independent tasks.Ideally you want to find a large number of mid-sized jobs that you can queue up, and just have a pool of processes each picking up the next one. From a quick glance, the obvious places to do that are either at the recursive call to , or at each iteration of the  loop. If they actually are independent, just use , as in the  example. (If you're on Python 3.1 or earlier, you need the  module, because it's not built into the stdlib.)If there's no easy way to do this, often it's at least possible to create a small number (N or 2N, if you have N cores) of long-running jobs of about equal size, and just give each one its own . For example:One last note: If you finish your code and it looks like you've gotten away with implicitly sharing globals, what you've actually done is written code that usually-but-not-always works on some platforms, and never on others. See the Windows section of the  docs to see what to avoid—and, if possible, test regularly on Windows, because it's the most restrictive platform.You also ask a second question:Can you tell me what is wrong with the second code.It's not entirely clear what you were trying to do here, but there are a few obvious problems (beyond what's mentioned above).You don't create a thread anywhere in the code you showed us. Just creating variables with "thread" in the name doesn't give you parallelism. And neither does adding locks—if you don't have any threads, all locks can do is slow you down for no reason.From your description, it sounds like you were trying to use the  module, instead of . There's a reason that the very top of the  documentation tells you to not use it and use  instead.You have a lock protecting your thread count (which shouldn't be needed at all), but no lock protecting your . You will get away with this in most cases in Python (because of the same GIL issue mentioned above—your threads are basically not going to run concurrently, and therefore they're not going to have races), but it's still a very bad idea (especially if you don't understand exactly what those "most cases" are).However, it looks like your  function is based on the body of the  loop in . If that's a good place to parallelize, you're in luck, because creating a parallel task out of each iteration of a loop is exactly what  and  are best at. For example, this code:… can, of course, be written as:And it can be trivially parallelized, without even having to understand what futures are about, as:


Answer URL
