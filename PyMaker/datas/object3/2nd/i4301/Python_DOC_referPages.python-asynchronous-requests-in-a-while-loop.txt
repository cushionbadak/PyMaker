Natural Text
I currently have a program which uses a  loop to receive messages as a subscriber from a . Every time I get a new message, I need to make an HTTP request to a server, which will then send a response. This call takes around 1 second to make. I would like to have a way to call the HTTP request and then loop again, get another message from , and call another HTTP request without waiting for the first one.The pseudo-code would look something like this:Is there any way to do this in Python?I've looked into libraries like  and  as well as looking at  and I haven't figured out a solution.
Yes, there are several ways to do this in Python :Their main difference is the cost-of-operations and performance.A )The simplest and cheapest ever is to use Tkinter-native infrastructure of independent -orchestrated processing, where independent serial-processing cheaply takes place in a co-orchestrated manner, with additional soft-real-time benefits, if one may wish to use them. Given this option, one may soft-schedule the  using either an  or even  ( as your code regularly goes into a blocking-mode of the  receiver ) scheduling methods.B )Another, way smarter Tkinter-native infrastructure tool may do a similar job more efficiently, given the  would actually become a Tkinter's -instance, that has been equipped with a -monitoring-tool. This way any value-change of the  will auto-trigger a , without the code taking any further steps ( but the correct setup of such  monitor ). I love this Tkinter-tool, indeed for many powerful Live-GUI designs.C )Last, but not least, one may design a scalable performance ZeroMQ workflow from inside of your -loop, where any such received  gets marshalled into a pool of remote-workers over another . This approach can help easily "mask" the worker-side processing latency by adding more remote-workers into the pool of workers, besides the elementary trick of getting the async -method and other associated work done "outside" of the said -loop. The almost linear performance scaling + latency masking of this option may go beyond the shared GIL-lock stepping, so if these are the core design-features, this is the way to go.


Answer URL
https://docs.python.org/3/library/queue.html#queue.Queue
https://docs.python.org/3/library/threading.html
