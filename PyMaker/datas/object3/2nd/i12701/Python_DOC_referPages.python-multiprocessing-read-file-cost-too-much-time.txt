Natural Text
there is a function in my code that should read the file .each file is about 8M,however the reading speed is too low,and to improve that i use the multiprocessing.sadly,it seems it got blocked.i wanna know is there any methods to help solve this and improve the reading speed?my code is as follows:and the call function:
You have a few problems. First, you're not parallelizing. You do:over and over, dispatching a task, then immediately calling  which waits for it to complete before you dispatch any additional tasks; you never actually have more than one worker running at once. Store all the results without calling , then call  later. Or just use  or related methods and save yourself some hassle from manual individual result management, e.g. (using  to minimize overhead since you're just sorting anyway):Second,  has to pickle and unpickle all arguments and return values sent between the main process and the workers, and it's all sent over pipes that incur system call overhead to boot. Since your file system isn't likely to gain substantial speed from parallelizing the reads, it's likely to be a net loss, not a gain.You might be able to get a bit of a boost by switching to a thread based pool; change the  to  and you'll get a version of  implemented in terms of threads; they don't work around the CPython GIL, but since this code is almost certainly I/O bound, that hardly matters, and it removes the pickling and unpickling as well as the IPC involved in worker communications.Lastly, if you're using Python 3.3 or higher on a UNIX like system, you may be able to get the OS to help you out by having it pull files into the system cache more aggressively. If you can open the file, then use  on the file descriptor ( on file objects) with either  or  it might improve read performance when you read from the file at some later point by aggressively prefetching file data before you request it.


Answer URL
https://docs.python.org/3/library/os.html#os.posix_fadvise
