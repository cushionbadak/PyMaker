Natural Text
In python it is possible to share ctypes objects between multiple processes. However I notice that allocating these objects seems to be extremely expensive.Consider following code:The output is:There are two things that puzzle me: Why is explicit allocation and initialization compared to passing a numpy array so much faster?Why is allocating shared memory in python so extremely expensive?  only takes . There are several orders of magnitude between those timings.
Sample CodeI rewrote your sample code a little bit to look into this issue. Here's where I landed, I'll use it in my answer below::Note that I added two test cases that do not use shared memory (and use regular a  array instead).:Test resultsThe results from running the above code using Python 3.6a0 (specifically ) are:What's interesting is that if you change , the results scale linearly. For example, using  (10 times bigger), you get something that's pretty much 10 times slower:Speed differenceIn the end, the speed difference lies in the hot loop that is called to initialize the array by copying every single value over from the Numpy array () to the new array (). This makes sense, because as we noted speed scales linearly with array size.When you pass the Numpy array as a constructor argument, the function that does this is . However, if you assign using , then it's  that does the job.Again, what matters here are the hot loops. Let's look at them. hot loop (slower): hot loop (faster): As it turns out, the majority of the speed difference lies in using  vs. . Indeed, if you change the code for  to use  instead of  (), and recompile Python, the new results become:Still a bit slower, but not by much.In other words, most of the overhead is caused by a slower hot loop, and mostly caused by the code that  wraps around .This code might appear like little overhead at first read, but it really isn't.  actually calls into the entire Python machinery to resolve the  method and call it. This eventually resolves in a call to , but only after a large number of levels of indirection (which a direct call to  would bypass entirely!)Going through the rabbit hole, the call sequence looks a bit like this: points to . calls into . calls into And on and on until we eventually get to ..!In other words, we have C code in  that's calling Python code () in a hot loop. That's slow.Why ?Now, why does Python use  in  and not  in ? That's because if it did, it would be bypassing the hooks that are exposed to the developer in Python-land.Indeed, you can intercept calls to  by subclassing the array and overriding  ( in Python 2). It will be called once, with a  argument for the index.Likewise, defining your own  also overrides the logic in the constructor. It will be called N times, with an integer argument for the index.This means that if  directly called into , then you would lose something:  would no longer be called in the constructor, and you wouldn't be able to override the behavior anymore.Now can we try to retain the faster speed all the while still exposing the same Python hooks? Well, perhaps, using this code in  instead of the existing hot loop:Using this will call into  once with a slice argument (on Python 2, it would call into ). We still go through the Python hooks, but we only do it once instead of N times.Using this code, the performance becomes:Other overheadI think the rest of the overhead may be due to the tuple instantiation that takes place when calling  on the array object (note the , and the fact that  expects a tuple for ) â€” this presumably scales with  as well.Indeed, if you replace  with  in the test case, then the performance results become almost identical. With :There's probably still something smaller going on, but ultimately we're comparing two substantially different hot loops. There's simply little reason to expect them to have identical performance.I think it might be interesting to try calling  from  for the hot loop and see the results, though!Baseline speedNow, to your second question, regarding allocating shared memory.Note that there isn't really a cost to allocating shared memory. As noted in the results above, there isn't a substantial difference between using shared memory or not.Looking at the Numpy code ( is implemented here), we can finally understand why it's so much faster than :  doesn't appear to make calls to Python "user-land" (i.e. no call to  or ).That doesn't necessarily explain all the difference, but you'd probably want to start investigating there.
Not an answer (the accepted answer explains this quite well), but for those looking for how to fix this, here's how: Don't use s slice assignment operator.As noted in the accepted answer, s slice assignment operator doesn't take advantage of the fact that you're copying between two wrappers around C-style arrays of identical type and size. But  implements the buffer protocol, so you can wrap it in a  to access it in an "even more raw" way (and it will make  win, because you can only do this after constructing the object, not as part of construction):In tests solving this problem on another question, the time to copy using a  wrapper is less than 1% of the time required to copy with s normal slice assignment.One trick here is that the sizes of the elements of the output of  are , and on a 64 bit system,  is 64 bits, so on 64 bit Python, you need another round of copying to coerce it to the right size (or you need to declare the  to be of a type that matches the size of ). Even if you do need to make that temporary copy though, it's still much cheaper with a :As you can see, even when you need to copy the  to resize the elements first, the total time is less than 3% of the time required using 's own slice assignment operator.If you avoid the temporary copy by making the size of the  match the source, the cost drops further:which gets us down to 0.85% of the  slice assignment time; at this point, you're basically running at  speeds; the rest of your actual Python code will swamp the miniscule amount of time spent on data copying.
This should be a comment, but I do not have enough reputation :-(Starting with Python 3.5, shared arrays in Linux are created as temp files mapped to memory (see https://bugs.python.org/issue30919). I think this explains why creating a Numpy array, which is created in memory, is faster than creating and initializing a large shared array.To force Python to use shared memory, a workaround is to execute these two lines of code (ref. No space left while using Multiprocessing.Array in shared memory):


Answer URL
https://docs.python.org/3/library/stdtypes.html#memoryview
