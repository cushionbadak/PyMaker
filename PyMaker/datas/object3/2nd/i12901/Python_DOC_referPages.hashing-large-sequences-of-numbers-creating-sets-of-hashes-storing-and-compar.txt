Natural Text
I am trying to find the best way to compare large sets of numerical sequences to other large sets, in order to rank them against each other. Maybe the following toy example clarifies the issue, where lists a, b, and c represent shingles of size 3 in a time series.The similarity among these sets is:So in this example, we can see that set a and b are the most similar with a score of 0.4.I am having a design problem:1) Since each set will be composed of ~1000 shingles, do I gain speed by transforming every shingle into a unique hash and then comparing hashes?2) Initially, I have over 10,000 sets to compare so I think I am much better off storing the shingles (or hashes, depending on answer to 1) in a database or pickling. Is this a good approach?3) As a new set is added to my workflow, I need to rank it against all existing sets and display, let's say, the top 10 most similar. Is there a better approach than the one in the toy example?
1) Members of a set have to be hashable, so python is already computing hashes. Storing sets of hashes of items would be duplicated effort, so there's no need to do that. 2) The complexity of the set intersection and union is approximately linear. The Jaccard isn't comptationally expensive, and 10,000 sets isn't that many (about 50 million1 computations). It will probably take an hour to compute your initial results, but it won't take days. 3) Once you have all of your combinations, ranking another set against your existing results means doing only 10,000 more comparisons. I can't think of a simpler way than that. I'd say just do it. If you want to go faster, then you should be able to use a multiprocessing approach fairly easily with this dataset. (Each computation is independent of the other ones, so they can all run in parallel).Here's an example adapted from the  examples (Python3). [1]: 
1) This is done internally anyways when constructing the .2) I'm not sure you'll be able to stay with python for your size of the data set, so I'd suggest using some simple (text) format so it can be easily loaded eg in C/C++. Do you need to store the shingles at all? What about generating them on the fly?3) If you need all to all comparison for your initial data set, something like google-all-pairs or ppjoin will surely help. It works by reducing the candidate set for each comparison using predefined similarity threshold. You can modify the code to keep the index for further searches.
You should definitely consider utilizing multi-cores as this problem is very suitable for this task. You might consider PyPy, as I see 2-3X speedup comparing to Python 3 for large set comparison. Then you might checkout part 1: resemblance with the jaccard coefficient for a magic C++ implementation to get further speed-ups. This C++ / OpenMP solution is the fastest I have tested yet.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html
