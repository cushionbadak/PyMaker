Natural Text
I have a huge binary file(several GB) that has the following dataformat:4 subsequent bytes form one composite datapoint(32 bits) which consists of:I need to access both signed integers and the flag bits separately and append to a list or some smarter datastructure (not yet decided). At the moment I'm using the following code to read it in:This code works but it's too slow for my purposes (2s for 100k datapoints with 4byte each). I would need a factor of 10 in speed at least. The profiler says that the code spends it's time mostly in string formatting(to get the bits) and in _unpack_integer(). Unfortunately I am not sure how to proceed here. I'm thinking about either using cython or directly writing some c code to do the read in. I also tried Pypy ant it gave me huge performance gain but unfortunately it needs to be compatible to a bigger project which doesn't work with Pypy.
I would recommend trying ctypes, if you already have a c/c++ library that recognizes the data-strcture. The benefits are, the datastructues are still available to your python while the 'loading' would be fast. If you already have a c library to load the data you can use the function call from that library to do the heavy lifting and just map the data into your python structures. I'm sorry I won't be able to try out and provide proper code for your example (perhaps someone else cane) but here are a couple of tips to get you startedMy take on how one might create bit vectors in python:https://stackoverflow.com/a/40364970/262108The approach I mentioned above which I applied to a similar problem that you described. Here I use ctypes to create a ctypes data-structure (thus enabling me to use the object as any other python object), while also being able to pass it along to a C library:https://gist.github.com/lonetwin/2bfdd41da41dae326afb
Thanks to the hint by Jean-Fran√ßois Fabre I found a suitable sulution using bitmasks which gives me a speedup of factor 6 in comparison to the code in the question. It has now a throuput of around 300k datapoints/s.Also I neglected using the admittedly nice named tuples and replaced it by a list because I found out this is also a bottleneck.The code now looks likeThe line profiler says:So there is not one clear bottleneck anymore. Probably now it's as fast as it gets in pure Python. Or does anyone have an idea for further speedup?


Answer URL
https://docs.python.org/3/library/ctypes.html
