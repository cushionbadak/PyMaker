Natural Text
First question ever here on stack,I'm on my first few challenges on hackerrank and am stuck on the "how many deletion to anagramize 2 words". I've seen some other solutions on the web but i can't figure out why mine is that much slower. I seem to have a "correct" algorithm since i computed some test cases and found the corresponding expected outputSo i'm trying to figure out if the general idea of my algorithm is the  bottleneck or if it's some error in it that is. Thanks!
You could use line profiling here.  You can use .First incorporate your function in a script and decorate it with .  Here's that script:Then in IPython/JupyterQt call the command below.  You might need to change the path based on what you're directory looks like:The result shows you some useful stats line-for-line.  It looks like  being nested is your culprit.  You're evaluating the line below 136,000 times.  That is, the operations you're running within the loops take just as long per hit, but they're hardly ever evaluated, so as a whole they're not what's eating up your time.Still, the runtime doesn't seem too bad.  14.7 ms for  and  on my machine.
You code has O(n3) complexity (n being length of  and ): You loop every character in , compare those with every character in , and then check whether that index is in the list of already matched characters, which also has linear complexity.As a quick fix, you could make  a , thus reducing the complexity to to O(n²). But you can do better than that: Just count all the individual characters in  and . Do not use  for this, or you will have O(n²) again; instead, use a  mapping characters to their counts, loop  and  once, and update those counts accordingly. Finally, just sum the difference of those counts for  and .Or, using Python's libraries, you could just create two  for  and  and compare those.


Answer URL
https://docs.python.org/3/library/collections.html#collections.Counter
