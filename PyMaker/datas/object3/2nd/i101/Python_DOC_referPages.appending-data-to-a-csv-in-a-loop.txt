Natural Text
I have up to 8 seperate Python processes creating temp files in a shared folder.  Then I'd like the controlling process to append all the temp files in a certain order into one big file.  What's the quickest way of doing this at an os agnostic shell level?
Just using simple file IO:That's about as OS agnostic as it gets. It's also fairly simple, and the performance ought to be about as good as using anything else.
Not aware of any shell-level commands for appending one file to another. But appending at 'python level' is sufficiently easy that I am guessing python developers did not think it necessary to add it to the library.The solution depends on the size and structure of the temp files you are appending. If they are all small enough that you don't mind reading each of them into memory, then the answer from Rafe Kettler (copied from his answer and repeated below) does the job with the least amount of code.If reading files fully into memory is not possible or not an appropriate solution, you will want to loop through each file and read them piece-wise. If your temp file contains newline-terminated lines which can be read individually into memory, you might do something like thisAlternatively - something which will always work - you may choose a buffer size and just read the file piece-wise, e.g.The input/output tutorial has a lot of good info.
Rafe's answer was lacking proper open/close statements, e.g.However, be forewarned that if you want to sort the contents of the bigfile, this method does not catch instances where the last line in one or more of your temp files has a different EOL format, which will cause some strange sort results.  In this case, you will want to strip the tempfile lines as you read them, and then write consistent EOL lines to the bigfile (i.e. involving an extra line of code).
We can use above code to read all the contents from all the file present in current directory and store into temp.txt file.
Use fileinput:This is more memory efficient than @RafeKettler's answer as it doesn't need to read the whole file into memory before writing to .
Try this.  It's very fast (much faster than line-by-line, and shouldn't cause a VM thrash for large files), and should run on about anything, including CPython 2.x, CPython 3.x, Pypy, Pypy3 and Jython.  Also it should be highly OS-agnostic.  Also, it makes no assumptions about file encodings.There is one (nontrivial) optimization I've left out: It's better to not assume anything about a good blocksize, instead using a bunch of random ones, and slowly backing off the randomization to focus on the good ones (sometimes called "simulated annealing").  But that's a lot more complexity for little actual performance benefit.You could also make the os.write keep track of its return value and restart partial writes, but that's only really necessary if you're expecting to receive (nonterminal) *ix signals.
In this code, you can indicate the path and name of the input/output files, and it will create the final big file in that path:
Simple & Efficient way to copy data from multiple files to one big file, Before that you need to rename your files to (int) eg. 1,2,3,4...etc, Code:


Answer URL
https://docs.python.org/3/library/fileinput.html#module-fileinput
