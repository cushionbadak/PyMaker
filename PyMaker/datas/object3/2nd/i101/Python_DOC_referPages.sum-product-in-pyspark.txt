Natural Text
I have a pyspark dataframe like thisI have Another dataframe like thisI want to divide every column in df1 by the respective value in df2. Hence df3 will look likeUltimately, I want to add colA and colB to get the final df4 per ID
The idea is to join both the DataFrames together and then apply the  operation. Since,  contains the column names and the respective value, so we need to  it first and then join with the main table . (Pivoting is an expensive operation, but it should be fine as long as the DataFrame is small.)The code is fairly generic, so that we need not need to specify the column names on our own. We find the column names we need to operate on. Except  we need all.Pivoting the , which we will join to .We can change the column names, so that we don't have a duplicate name for every column. We do so, by adding a suffix  on all the names.Next we join the tables with a Cartesian join. (Note that you may run into memory issues if  is large.)Finally adding the columns by dividing them with the corresponding value first.  applies function  of two arguments, cumulatively, to the items of the sequence.Note: OP has to be careful with the division with 0. The snippet just above can be altered to take this condition into account.


Answer URL
https://docs.python.org/3/library/functools.html#functools.reduce
