Natural Text
 returns a list instance. Is there a version that returns a generator instead? Are there any reasons against having a generator version? 
It is highly probable that  uses fairly minimal memory overhead.Demo:edit: I have just confirmed that this takes constant memory in python 3.2.1, assuming my testing methodology was correct. I created a string of very large size (1GB or so), then iterated through the iterable with a  loop (NOT a list comprehension, which would have generated extra memory). This did not result in a noticeable growth of memory (that is, if there was a growth in memory, it was far far less than the 1GB string).
The most efficient way I can think of it to write one using the  parameter of the  method. This avoids lots of memory use, and relying on the overhead of a regexp when it's not needed.  [edit 2016-8-2: updated this to optionally support regex separators]This can be used like you want...While there is a little bit of cost seeking within the string each time find() or slicing is performed, this should be minimal since strings are represented as continguous arrays in memory.
This is generator version of  implemented via  that does not have the problem of allocating too many substrings.EDIT: Corrected handling of surrounding whitespace if no separator chars are given.
Here is my implementation, which is much, much faster and more complete than the other answers here. It has 4 separate subfunctions for different cases.I'll just copy the docstring of the main  function:Split the string  by the rest of the arguments, possibly omittingempty parts ( keyword argument is responsible for that).This is a generator function.When only one delimiter is supplied, the string is simply split by it. is then  by default.When multiple delimiters are supplied, the string is split by longestpossible sequences of those delimiters by default, or, if  is set to, empty strings between the delimiters are also included. Note thatthe delimiters in this case may only be single characters.When no delimiters are supplied,  is used, so the effectis the same as , except this function is a generator.This function works in Python 3, and an easy, though quite ugly, fix can be applied to make it work in both 2 and 3 versions. The first lines of the function should be changed to:
Did some performance testing on the various methods proposed (I won't repeat them here). Some results: (default    = 0.3461570239996945manual search (by character) (one of Dave Webb's answer's) = 0.8260340550004912 (ninjagecko's answer)     = 0.698872097000276 (one of Eli Collins's answers)      = 0.7230395330007013 (Ignacio Vazquez-Abrams's answer) = 2.023023967998597 recursion = N/A††The recursion answers ( with ) fail to complete in a reasonable time, given s speed they may work better on shorter strings, but then I can't see the use-case for short strings where memory isn't an issue anyway.Tested using  on:This raises another question as to why  is so much faster despite its memory usage.
No, but it should be easy enough to write one using .EDIT:Very simple, half-broken implementation:
I don't see any obvious benefit to a generator version of .  The generator object is going to have to contain the whole string to iterate over so you're not going to save any memory by having a generator.If you wanted to write one it would be fairly easy though:
I wrote a version of @ninjagecko's answer that behaves more like string.split (i.e. whitespace delimited by default and you can specify a delimiter).Here are the tests I used (in both python 3 and python 2):python's regex module says that it does "the right thing" for unicode whitespace, but I haven't actually tested it.Also available as a gist.
If you would also like to be able to read an iterator (as well as return one) try this:Usage
I wanted to show how to use the find_iter solution to return a generator for given delimiters and then use the pairwise recipe from itertools to build a previous next iteration which will get the actual words as in the original split method.note: I use prev & curr instead of prev & next because overriding next in python is a very bad ideaThis is quite efficient
 offers an analog to  for iterators. is a third-party package.

Need is for me, at least, with files used as generators.This is version I did in preparation to some huge files with empty line separated blocks of text (this would need to be thoroughly tested for corner cases in case you would use it in production system):
here is a simple response


Answer URL
https://docs.python.org/3/library/re.html
