Natural Text
As the title suggests, I'm working on a site written in python and it makes several calls to the urllib2 module to read websites. I then parse them with BeautifulSoup. As I have to read 5-10 sites, the page takes a while to load. I'm just wondering if there's a way to read the sites all at once? Or anytricks to make it faster, like should I close the urllib2.urlopen after each read, or keep it open?Added: also, if I were to just switch over to php, would that be faster for fetching and Parsi g HTML and XML files from other sites? I just want it to load faster, as opposed to the ~20 seconds it currently takes
I'm rewriting Dumb Guy's code below using modern Python modules like  and .Best time for  is 2s. Best time for  is 0.9s.Also it is incorrect to say  is useless in Python because of GIL. This is one of those case when thread is useful in Python because the the threads are blocked on I/O. As you can see in my result the parallel case is 2 times faster.
Edit: Please take a look at Wai's post for a better version of this code. Note that there is nothing wrong with this code and it will work properly, despite the comments below.The speed of reading web pages is probably bounded by your Internet connection, not Python.You could use threads to load them all at once.
It is maby not perfect. But when I need the data from a site. I just do this:
As a general rule, a given construct in any language is not slow until it is measured.In Python, not only do timings often run counter to intuition but the tools for measuring execution time are exceptionally good.
Scrapy might be useful for you. If you don't need all of its functionality, you might just use twisted's  instead. Asynchronous IO in one thread is going to be way more performant and easy to debug than anything that uses multiple threads and blocking IO.
Not sure why nobody mentions  (if anyone knows why this might be a bad idea, let me know):There are a few caveats with  pools. First, unlike threads, these are completely new Python processes (interpreter). While it's not subject to global interpreter lock, it means you are limited in what you can pass across to the new process. You cannot pass lambdas and functions that are defined dynamically. The function that is used in the  call must be defined in your module in a way that allows the other process to import it.The , which is the most straightforward way to process multiple tasks concurrently, doesn't provide a way to pass multiple arguments, so you may need to write wrapper functions or change function signatures, and/or pass multiple arguments as part of the iterable that is being mapped.You cannot have child processes spawn new ones. Only the parent can spawn child processes. This means you have to carefully plan and benchmark (and sometimes write multiple versions of your code) in order to determine what the most effective use of processes would be.Drawbacks notwithstanding, I find multiprocessing to be one of the most straightforward ways to do concurrent blocking calls. You can also combine multiprocessing and threads (afaik, but please correct me if I'm wrong), or combine multiprocessing with green threads.
1) Are you opening the same site many times, or many different site?  If many different sites, I think urllib2 is good. If doing the same site over and over again, I have had some personal luck with urllib3 http://code.google.com/p/urllib3/2) BeautifulSoup is easy to use, but is pretty slow. If you do have to use it, make sure to decompose your tags to get rid of memory leaks.. or it will likely lead to memory issues (did for me). What do your memory and cpu look like?  If you are maxing your CPU, make sure you are using real heavyweight threads, so you can run on more than 1 core.
How about using pycurl? You can apt-get it by
First, you should try multithreading/multiprocessing packages. Currently, the three popular ones are multiprocessing;concurrent.futures and [threading][3]. Those packages could help you to open multi url at the same time, which could increase the speed.More importantly, after using multithread processing, and if you try to open hundreds urls at the same time, you will find urllib.request.urlopen is very slow, and opening and read the context become the most time-consuming part. So if you want to make it even faster, you should try requests packages, requests.get(url).content() is faster than urllib.request.urlopen(url).read(). So, here I list two example to do fast multi url parsing, and the speed is faster than the other answers.The first example use classical threading package and generate hundreds thread at the same time. (One trivial shortcoming is it cannot keep the original order of the ticker.)The second example uses multiprocessing package, and it is little more straightforward. Since you just need to state the number of pool and map the function. The order will not change after fetching the context and the speed is similar to the first example but much faster than other method.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html
