Natural Text
I have something like  I want to achieve something likebut as म takes 4 bytes while बि takes 8 bytes I am not able to get to that straight.So what could be done to achieve that? In Python.
The algorithm for splitting text into grapheme clusters is given in Unicode Annex 29, section 3.1. I'm not going to implement the full algorithm for you here, but I'll show you roughly how to handle the case of Devanagari, and then you can read the Annex for yourself and see what else you need to implement.The  module contains the information you need to detect the grapheme clusters.In Devanagari, each grapheme cluster consists of an initial letter, optional pairs of virama (vowel killer) and letter, and an optional vowel sign. In regular expression notation that would be . You can tell which is which by looking up the Unicode category for each code point:Letters are category  (Letter, Other), vowel signs are category  (Mark, Spacing Combining), virama is category  (Mark, Nonspacing) and spaces are category  (Separator, Space).So here's a rough approach to split out the grapheme clusters:
So, you want to achieve something like thisMy advice is to ditch the idea that string indexing corresponds to the characters you see on the screen.  Devanagari, as well as several other scripts, do not play well with programmers who grew up with Latin characters.  I suggest reading the Unicode standard chapter 9 (available here).It looks like what you are trying to do is break a string into grapheme clusters.  String indexing by itself will not let you do this.  Hangul is another script which plays poorly with string indexing, although with combining characters, even something as familiar as Spanish will cause problems.You will need an external library such as ICU to achieve this (unless you have lots of free time).  ICU has Python bindings.Note how some of these "characters" (grapheme clusters) have length 2, and some have length 1.  This is why string indexing is problematic: if I want to get grapheme cluster #69450 from a text file, then I have to linearly scan through the entire file and count.  So your options are:Build an index (kind of crazy...)Just realize that you can't break on every character boundary.  The break iterator object is capable of going both forwards AND backwards, so if you need to extract the first 140 characters of a string, then you look at index 140 and iterate backwards to the previous grapheme cluster break, that way you don't end up with funny text.  (Better yet, you can use a word break iterator for the appropriate locale.)  The benefit of using this level of abstraction (character iterators and the like) is that it no longer matters which encoding you use: you can use UTF-8, UTF-16, UTF-32 and it all just works.  Well, mostly works.
You can achieve this with a simple regex for any engine that supports DemoUnfortunately, Python's re does not support the \X grapheme match.Fortunately, the proposed replacement, regex, does support :
Indic and non Latin scripts like Hangul do not generally follow the idea of matching string indices to code points. It's generally a pain working with Indic scripts. Most characters are two bytes with some rare ones extending into three. With Dravidian, it's no defined order. See the Unicode specification for more details. That said,check here for some ideas about unicode and python with C++. Finally,as said by Dietrich, you might want to check out ICU too. It has bindings available for C/C++ and java via icu4c and icu4j respectively. There's some learning curve involved, so I suggest you set aside some loads of time for it. :)
There's a pure-Python library called  which provides a number of utilities including a grapheme cluster iterator which provides the behaviour you described:It claims to implement the full Unicode text segmentation algorithm described in http://www.unicode.org/reports/tr29/tr29-21.html.


Answer URL
