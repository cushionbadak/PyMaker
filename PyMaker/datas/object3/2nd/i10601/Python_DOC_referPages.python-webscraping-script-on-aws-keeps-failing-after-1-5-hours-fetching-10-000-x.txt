Natural Text
I wrote a Python script that runs from an AWS instance and fetches xml files off an S3 server to place in a folder on the instance. The script works fine, except for the fact that after about an hour and a half, or about the time it takes to fetch 10,000-15,000 xmls, I get the following error:Following this error, I am told that the folder I tell the script to place the fetched xml in can't be found, i.e. I have tried running this script both from ssh, using screen and using nohup, but I get the same issue each time. As I have about 200,000 xmls to fetch, I'd like to just run this script once and go do something else for the 20+ hours it needs to run. For reference, the script I wrote is below:
I don't know the first thing about python, but the problem seems apparent enough, all the same.When the S3 error occurs, you , which skips the rest of the instructions within the closest loop, and continues with the next value, from the top of the loop... and this skips  at the end of the loop, so your current working directory is still .  On the next iteration,  will, of course, fail, because that is trying to find a directory called  inside the then-current directory, which is of course already , so that would of course fail.There are a couple of lessons, here.  Don't keep switching in and out of a directory using  -- that's very bad form, prone to subtle bugs.  Case in point, see above.  Use absolute paths.  If you really want relative paths, that's fine, but don't do it this way.  Capture the working directory at startup or configure a base working directory and use that to fully-qualify your paths with chdir.Design your code to anticipate occasional errors from S3, or any web service, from any provider, and retry 5XX errors after a brief, incrementing delay -- exponential backoff.


Answer URL
https://docs.python.org/3/reference/simple_stmts.html#continue
