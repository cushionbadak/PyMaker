Natural Text
I am working at a company that deals with phish and fake Facebook accounts.  I want to show my dedication to the "mission".  We are unable to passively monitor facebook pages for when they are removed.  I am thinking a web crawler but I am curious on how to design one that constant checks a specific link to see if the Facebook page is still active or not?  I hope this made sense?  
Yes! You can use crawling. However, if you want it to be as fast as possible, crawling may not be the best way to do it. If you're interested this is how I'd do it using HTTPConnection. also, unfortunately, the link has to be completely broken.If you need more information then you will most likely have to use an API or web crawler to check if the link is broken(Thus meaning it has to link to nowhere),If it returns '302 Found' then it should be an active web page,I hope this helps! Please tell me if this isn't what you wanted. :)Thanks,~Coolq
You can send a http request to tell the account is active or not by it's response status, Python has some standard library, you may have a look at Internet Protocols and Support.Personally, I will recommend use requests:If you really care about speed or performance, you should use multiprocessing or asynchronous i/o (like gevent) in Python.If you are focus on crawl,you may have a look at ScrapyHere you notice one of the main advantages about Scrapy: requests are  scheduled and processed asynchronously. This means that Scrapy doesnâ€™t  need to wait for a request to be finished and processed, it can send  another request or do other things in the meantime. This also means  that other requests can keep going even if some request fails or an  error happens while handling it.
https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch/answer/Raghavendran-BaluOne of the best articles I have read about Crawlers.A web crawler might sound like a simple fetch-parse-append system, but watch out! you may over look the complexity. I might deviate from the question intent by focussing more on architecture than implementation specifics.I believe it is necessary because, to build a web scale crawler, the architecture of the crawler is more important than the choice of language/ framework.Architecture:A bare minimum crawler needs at least these components:HTTP Fetcher : To retrieve web page from the server.Extractor: Minimal support to extract URL from page like anchor links.Duplicate Eliminator: To make sure same content is not extracted twice unintentionally. Consider it as a set based data structure.URL Frontier: To prioritize URL that has to fetched and parsed. Consider it as a priority queueDatastore: To store retrieve pages and URL and other meta data.A good starting point to learn about architecture is:Web Crawling Crawling the WebMercator: A scalable, extensible Web crawlerUbiCrawler: a scalable fully distributed web crawlerIRLbot: Scaling to 6 billion pages and beyond(single-sever crawler) and MultiCrawler: a pipelined architectureProgramming Language: Any high level language with good network library that you are comfortable with is fine. I personally prefer Python/Java. As your crawler project might grow in terms of code size it will be hard to manage if you develop in a design-restricted programming language. While it is possible to build a crawler using just unix commands and shell script, you might not want to do so for obvious reasons.Framework/Libraries: Many frameworks are already suggested in other answers. I shall summarise here:Apache Nutch and Heritrix (Java): Mature, Large scale, configurableScrapy (Python): Technically a scraper but can be used to build a crawler.You can also visit https://github.com/scrapinghub/distributed-frontera - URL frontier and data storage for Scrapy, allowing you to run large scale crawls.node.io (Javascript): Scraper.  Nascent, but worth considering, if you are ready to live with javascript.For Python: Refer Introduction to web-crawling in PythonCode in Python: https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch/answer/Rishi-Giri-1Suggestions for scalable distributed crawling:It is better to go for a asynchronous model, given the nature of the problem.Choose a distributed data base for data storage ex. Hbase.A distributed data structure like redis is also worth considering for URL frontier and duplicate detector.For more Information visit: https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch References:Olston, C., & Najork, M. (2010). Web crawling. Foundations and Trends in Information Retrieval, 4(3), 175-246.Pant, G., Srinivasan, P., & Menczer, F. (2004). Crawling the web. In Web Dynamics (pp. 153-177). Springer Berlin Heidelberg.Heydon, A., & Najork, M. (1999). Mercator: A scalable, extensible web crawler.World Wide Web, 2(4), 219-229.Boldi, P., Codenotti, B., Santini, M., & Vigna, S. (2004). Ubicrawler: A scalable fully distributed web crawler. Software: Practice and Experience, 34(8), 711-726.Lee, H. T., Leonard, D., Wang, X., & Loguinov, D. (2009). IRLbot: scaling to 6 billion pages and beyond. ACM Transactions on the Web (TWEB), 3(3), 8.Harth, A., Umbrich, J., & Decker, S. (2006). Multicrawler: A pipelined architecture for crawling and indexing semantic web data. In The Semantic Web-ISWC 2006 (pp. 258-271). Springer Berlin Heidelberg.


Answer URL
https://docs.python.org/3/library/internet.html
