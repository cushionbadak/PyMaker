Natural Text
I have many files containing :data: numbers that I have to use/manipulate, formatted in a specific way, specified in the following, rows that I need just as they are (configurations of the software use these files). The files most of time are huge, many millions of rows, and can't be handled fast enough with bash. I have made a script that checks each line to see if it's data, writing them to another file (without calculations), but it's very slow (many thousand rows per second).The data is formatted in a way like this:I have to make another file, using $data, that should be the results of some operation with it.The portions of file that contains numbers can be distinguished by the presence of this occurrence:and the same:at the end.I've made before a C++ program that makes the operation I want,  but for files containing columns of numbers only. I don't know how to ignore the text that I don't have to modify and handle the way the data is formatted.Where do I have to look to solve my problem smartly?Which should be the best way to handle data files, formatted in different ways, and make math with them? Maybe Python?
Are you sure that the shell isn't fast enough?  Maybe your bash just needs improved. :)It appears that you want to print every line after a line with just a  until you get to a closing .  So...And, with your provided test data:I'll bet you'll find that to be roughly as fast as anything else you would write.  If I was going to be using this often, I'd be inclined to add several more error checks - but if your data is well-formed, this will work fine.Alternatively, you could just use sed.performance note edit:I was comparing python implementations below, so I thought I'd test these as well.  The sed solution runs about identically to the fastest python implementation on the same data - less than one second (0.9 seconds) to filter ~80K lines.  The bash version takes 42.5 seconds to do it.  However, just replacing  with  above (which is ksh93, on Ubuntu 13.10) and making no other changes to the script reduces runtime down to 10.5 seconds.  Still slower than python or sed, but that's part of why I hate scripting in bash.I also updated both solutions to remove the opening and closing parens, to be more consistent with the other answers.
Here is something which should perform well on huge data, and it's using Python 3:This uses mmap to map the file into memory. Then you can access it easily without having to worry about reading it in. It also is very efficient, since it can avoid unneccessary copying (from the page cache to the application heap).
I guess we need a perl solution, too.On my system, this runs slightly slower than the fastest python implementation from above (while also doing the parenthesis removal), and about the same as
Using C provides much better speed than bash/ksh or C++(or Python, even though saying that stings). I created a text file containing 18 million lines containing the example text duplicated 1 million times. On my laptop, this C program works with the file in 1 second, while the Python version takes 5 seconds, and running the bash version under ksh(because it's faster than bash) with the edits mentioned in that answer's comments takes 1 minute 20 seconds(a.k.a 80 seconds). Note that this C program doesn't check for errors at all except for the non-existent file. Here it is:If the fact it's completely unsafe freaks you out, here's a C++ version, which took 2 seconds:EDIT: As MvG pointed out in the comments, I wasn't benching the Python version fairly. It doesn't take 24 seconds as I originally stated, but 5 instead.


Answer URL
https://docs.python.org/3/library/mmap.html
