Natural Text
Assume that we have a list of strings and we want to create a string by concatenating all element in this list. Something like this: Since strings are immutable objects, I expect that python creates a new str object and copy contents of result and element at each iteration. It makes  time complexity, M is the length of each element and N is the size of the list.However, my experiment shows that it runs in linear time. I suspect that python uses something like stringbuffer under the hood. So, it doesn't create new object at each iteration.Now consider a slightly different implementation. The only difference is one extra assignment.I know that  line does not create a new object.  just points to the same object. So, this little change shouldn't affect the performance much.However, there is a huge difference. And it looks like foo2 function is O(N^2) while foo is O(N).My question is how does python achieve linear time in string concatenation without breaking other language components like immutable object assignment? and how does that extra line affect the performance that much? I searched a bit in the cpython implementation but couldn't find exact location. UpdateHere is the line profiling results.result for foo functionresult for foo2 functionSomehow  line affects the performance of  line.
Having another name pointing to the same object kills the optimisation. The optimisation basically works by resizing the string object and appending in place. If you have more than one references to that object, you can't resize without affecting the other reference. With strings being immutable, allowing this would be a serious flaw of the implementation.increased the reference count for the string object named by  thereby prohibiting the optimisation.The full list of checks performed in the case of  (which eventually translates to ) can be seen in the  function. Among other things, it checks that the reference count of the object is equal to one, that it isn't interned and that it isn't a string subclass.There's a couple more checks in the  statement guarding this optimisation, if you want a more thorough list.Though not the basic issue of your question, future readers might be curious about how to efficiently perform string concatenations. Besides similar questions on S.O, the Python FAQ also has an entry on this.
Actually, the behavior you are observing is determined by the behavior of the memory-allocator of the C-runtime on your OS.CPython has an optimization, that if the unicode-object has only one reference, it can be changed in-place - nobody would register that the unicode-object loss its immutability for a moment. See my answer to this SO-question for more details.In , there is another reference to the unicode object (), which prevents the in-place-optimization: Changing it in-place would break the immutability, because it could be observed through .However, even with the inplace optimization, it is not obvious, why  behavior can be avoided, as unicode object doesn't overallocate and thus has to exend the underlying buffer at every addition, which naively would mean a copy of the whole content (i.e. ) in every step.However, most of the time  (differently than +copy) can be done in , because if the memory directly behind the the allocated buffer is free, it can be used to extend the original without copying.An interesting detail is that there is no guarantee, that  will run in : If the memory is fragemented (e.g. in a long running process).  wont be able to extend the buffer without copying the data and thus the running time will become . Thus one should not rely on this optimization to avoid quadratic running time.


Answer URL
https://docs.python.org/3/faq/programming.html#what-is-the-most-efficient-way-to-concatenate-many-strings-together
