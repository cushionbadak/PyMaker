Natural Text
I'm writing a machine learning program with the following components:A shared "Experience Pool" with a binary-tree-like data structure.N simulator processes. Each adds an "experience object" to the pool every once in a while. The pool is responsible for balancing its tree.M learner processes that sample a batch of "experience objects" from the pool every few moments and perform whatever learning procedure. I don't know what's the best way to implement the above. I'm not using Tensorflow, so I cannot take advantage of its parallel capability. More concretely, I first think of Python3's built-in  library. Unlike , however,  module cannot have different processes update the same global object. My hunch is that I should use the server-proxy model. Could anyone please give me a rough skeleton code to start with? Is MPI4py a better solution?Any other libraries that would be a better fit? I've looked at , , etc. It's not obvious to me how to adapt them to my use case.
Based on the comments, what you're really looking for is a way to update a shared object from a set of processes that are carrying out a CPU-bound task. The CPU-bounding makes  an obvious choice - if most of your work was IO-bound,  would have been a simpler choice.  Your problem follows a simpler server-client model: the clients use the server as a simple stateful store, no communication between any child processes is needed, and no process needs to be synchronised.Thus, the simplest way to do this is to:Start a separate process that contains a server.Inside the server logic, provide methods to update and read from a single object.Treat both your simulator and learner processes as separate clients that can periodically read and update the global state. From the server's perspective, the identity of the clients doesn't matter - only their actions do. Thus, this can be accomplished by using a customised manager in  as so:Now for all of your clients you can just do:You may then launch one  and as many  as you want. Is This The Best Design?Not always. You may run into race conditions in that your learners may receive stale or old data if they are forced to compete with a simulator node writing at the same time. If you want to ensure a preference for latest writes, you may additionally use a lock whenever your simulators are trying to write something, preventing your other processes from getting a read until the write finishes. 


Answer URL
https://docs.python.org/3/library/multiprocessing.html#customized-managers
https://docs.python.org/3/library/multiprocessing.html#synchronization-between-processes
https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes
https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues
https://docs.python.org/3/library/multiprocessing.html#customized-managers
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.managers.BaseProxy._callmethod
