Natural Text
This is my first question here.I started learning python a few days ago and i have a problem.I made some python files that each of them runs a for loop and appends the results to a list.So each file has its own list.For example file1.py produces list1, and file2.py produces list2 etc...My goal, is to combine all these lists together, so i am making a separate "main.py" file and import the list names and then combine them together like this:and that is working fine as expected.But the problem is that this method is very slow, because it is importing the lists one by one in serial in the order i have them imported.For example, when i run it, it is importing first the list1 and when the list1 is completed it starts the list2 and then the list3 etc.. and finally combines them together.So, because i have 400 lists on 400 different files, this is taking a very long time.Is there any way to import and combine all the lists together in parallel?Like with multi-threading or any other method?Note, that i don't care about the order of the items in the combined list.
You could spawn multiple reader processes (via a Pool, preferably) that feed a Queue, with a single consumer that reads from it. You can do this with  as well; some relevant sample code can be found here.Note that in this case the consumer probably should not collect the results into a single list, but rather it should run the actual operation you want to perform on each element as they come out of the queue.However...I made some python files that each of them runs a for loop and appends the results to a list. So each file has its own list.Why? It sounds like this is way more complicated than it should be, but without knowing what you're actually trying to accomplish, it's impossible to say for sure.Without more information, if you have this volume of data to deal with, it sounds like your scripts should be generating CSV files (or they should be combined into a single script that generates a single CSV file). Even using an RDBMS might be a better idea than regenerating these data sets every time they're imported, unless they change very often.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.pool
https://docs.python.org/3/library/itertools.html
