Natural Text
CSV file may not be clean (lines with inconsistent number of elements), unclean lines would need to be disregarded.String manipulation is required during processing.Example input:Desired output: float32 (pseudo-date, seconds in the day, f3, f4)The CSV file is also very big, the numpy array in memory would be expected to take 5-10 GB, CSV file is over 30GB.Looking for an efficient way to process the CSV file and end up with a numpy array.Current solution: use csv module, process line by line and use a list() as a buffer that later gets turned to numpy array with asarray(). Problem is, during the turning process memory consumption is doubled and the copying process adds execution overhead.Numpy's genfromtxt and loadtxt don't appear to be able to process the data as desired.
If you know in advance how many rows are in the data, you could dispense with the intermediate  and write directly to the array.
did you think for using pandas read_csv (with engine='C')I find it as one of the best and easy solutions to handling csv. I worked with 4GB file and it worked for me. 
I think i/o capability of pandas is the best way to get data into a numpy array. Specifically the read_csv method will read into a pandas DataFrame. You can then access the underlying numpy array using the as_matrix method of the returned . 


Answer URL
https://docs.python.org/3/library/mmap.html
