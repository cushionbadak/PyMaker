Natural Text
So basically I have a list of 300 values and different averages associated with each one.I have a -loop that generates a list of ten of these values at random, and writes it to excel if certain conditions are met based on their averages.The code runs fine if I loop through 10 million times or less, but that is orders of magnitudes too small. Even if I just double the for loop counter to 20 million my computer becomes unusable while it is running. I want to iterate the loop 100 million or 1 billion times even. I want it to run slowly in the background, I don't care if it takes 24 hours to get to the results. I just want to use my computer while it's working. Currently, if the for loop goes past 10 million the memory and disk usage of my laptop go to 99%. Using  and python 3.3Comp specs: Intel Core i7 4700HQ (2.40GHz) 8GB Memory 1TB HDD NVIDIA GeForce GTX 850M 2GB GDDR3Code snippet:
First, I suspect your real problem is that you're just retaining too much memory, causing your computer to run into VM swap, which makes your entire computer slow to a crawl. You should really look into fixing that instead of just trying to make it happen periodically throughout the day instead of constantly.In particular, it sounds like you're keeping a list of 10N values around forever. Do you really need to do that?If not, start freeing them. (Or don't store them in the first place. One common problem a lot of people have is that they need 1 billion values, but only one at a time, once through a loop, and they're storing them in a list when they could be using an iterator. This is basically the generic version of the familiar  problem.)If so, look into some efficient disk-based storage instead of memory, or something more compact like a NumPy array instead of a list.But meanwhile, if you want to reduce the priority of a program, the easiest way to do that may be externally. For example, on most platforms besides Windows, you can just launch your script with  and the OS will give everything else more CPU time than your program.But to answer your direct question, if you want to slow down your script from inside, that's pretty easy to do: Just call  every so often. This asks the OS to suspend your program and not give you any resources until the specified number of seconds have expired. That may be only approximate rather than absolutely nothing for exactly N seconds, but it's close enough (and as good as you can do).For example:If  takes 18ms, you'll burn CPU for 1.8 seconds then sleep for 10 and repeat. I doubt that's exactly the behavior you want (or that it takes 18ms), but you can tweak the numbers. Or, if the timing is variable, and you want the sleep percentage to be consistent, you can measure times and sleep every N seconds since the last sleep, instead of every N reps.
Do not slow down. Rather re-design & step in for HPCFor high performance processing on just "a few" items ( list of 300 values )the best would consist of:avoid file access ( even if sparse as noted in OP ) -- cache TruePOSITIVEs in  that is being -ed at the end or upon a string length limit or marshalled to another, remote, logging machine.move all highly iterative processing, said to span  --  cycles, onto massively-parallel GPU/CUDA kernel-processing on GPU you already have in the laptop, both to free your CPU-resources and to get benefits of 640-threads delivering about 1.15 TFLOPs of a parallel-engine computing horsepower, as opposed to just a few, GUI-shared, MFLOPs from the CPU-cores.


Answer URL
https://docs.python.org/3/library/time.html#time.sleep
