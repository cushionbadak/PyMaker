Natural Text
The problemMy application is extracting a list of zip files in memory and writing the data to a temporary file. I then memory map the data in the temp file for use in another function. When I do this in a single process, it works fine, reading the data doesn't affect memory, max RAM is around 40MB. However when I do this using concurrent.futures the RAM goes up to 500MB.I have looked at this example and I understand I could be submitting the jobs in a nicer way to save memory during processing. But I don't think my issue is related, as I am not running out of memory during processing. The issue I don't understand is why it is holding onto the memory even after the memory maps are returned. Nor do I understand what is in the memory, since doing this in a single process does not load the data in memory. Can anyone explain what is actually in the memory and why this is different between single and parallel processing?PS I used  for measuring the memory usageCodeMain code:Other functions:Helper code for dataI can't share my actual data, but here's some simple code to create files that demonstrate the issue:
The problem is that you're trying to pass an  between processes, and that doesn't work.The simplest solution is to instead pass the filename, and have the child process  the same file.When you pass an argument to a child process or pool method via , or return a value from one (including doing so indirectly via a ), it works by calling  on the value, passing the pickle across processes (the details vary, but it doesn't matter whether it's a  or a  or something else), and then unpickling the result on the other side.A  is basically just an  object with an  allocated in the ped memory.And Python doesn't know how to pickle an  object. (If you try, you will either get a  or a  error, depending on your Python version.)A  can be pickled, because it's just a subclass of —but pickling and unpickling it actually copies the data and gives you a plain in-memory array. (If you look at , it's .) It would probably be nicer if it gave you an error instead of silently copying all of your data (the pickle-replacement library  does exactly that: ), but it doesn't.It's not impossible to pass the underlying file descriptor between processes—the details are different on every platform, but all of the major platforms have a way to do that. And you could then use the passed fd to build an  on the receiving side, then build a  out of that. And you could probably even wrap this up in a subclass of . But I suspect if that weren't somewhat difficult, someone would have already done it, and in fact it would probably already be part of , if not  itself.Another alternative is to explicitly use the shared memory features of , and allocate the array in shared memory instead of a .But the simplest solution is, as I said at the top, to just pass the filename instead of the object, and let each side  the same file. This does, unfortunately, mean you can't just use a delete-on-close  (although the way you were using it was already non-portable and wouldn't have worked on Windows the same way it does on Unix), but changing that is still probably less work than the other alternatives.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#exchanging-objects-between-processes
https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor
https://docs.python.org/3/library/pickle.html
https://docs.python.org/3/library/mmap.html
https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes
