Natural Text
I have a general question. It concerns the writing of a large number of large text files. The content of the textfile is based on a dataset and differs for each textfile. The basic question is how to do so most efficiently.More specifically, I want to perform spatially explicit model runs (crop model). The model requires input files to be in txt format. So if I wish to run the model for a large number of raster cells - I need a textfile for each cell (1000s). The efficiency problem occurs when writing the weather input based on climate projections. They are in daily time steps for up to 100 years - eg. 36500 lines (each 8 variables) extracted from the dataset and written to each textfile.My first attempt was to create a for loop that loops through each location (i.e. each textfile) and for each textfile loops through each daily climate timestep to create the whole string for the climate file and then write it to the text file (I also tested writing each time step to file but efficiency was similar).This approach takes ca. 1-2min per file on my (a bit old) machine. For a raster of 70x80 cells ca. 7days. Of course I could scale down the number of locations and select less timesteps - but, nevertheless, I wonder whether there is a more effective way to do this?As from research I believe that the for loop that strings/writes each line together/to file is the bottleneck, I wonder whether pulling the data into an array or dataframe and then saving to cv would be quicker? Or what do you suggest as the most suitable approach for this operation?Thank you in advance!Bests regards, Anton. Here the code:Please let me know if I should provide additional code/info etc. as I am new to programming and teaching myself for a month now - apologies is things are a bit messy. I used cProfile to check the code:
It may help you to test how long each step is taking by doing something like here. It looks like your code is trying to store lots of information in memory. You can test this by running this for each read-in; Try and rule these out first. If its a memory issue then try reading in parts of the files in chunks rather than the whole file.Appreciate this isn't a full answer but maybe it'll get you started
First you'll need to find out whether it is the datageneration or the writing that is taking the time. Easiest would be to seperate the program in logical functions, and use timeit to time these.calculating the dataThere is a lot of repetition in calculating the model, so we can easily abstract thatThat part is easily times like this:By this abstraction of the calculation, it is also possible to see whether the results have already been calculated. If they are, there is perhaps no reason to do the calculation again# writing the datasetCan be more easily done with That can be timed like this:


Answer URL
https://docs.python.org/3/library/timeit.html
