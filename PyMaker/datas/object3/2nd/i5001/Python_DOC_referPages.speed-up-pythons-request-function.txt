Natural Text
I have a script that takes a list file of query IDs and extracts the organism and sequence from uniprot, the code works well, however it is very slow. I want to process approximately 4 million sequences through it, but it takes around 5 min to parse through 100 sequences:The code uses python's retrieve module. I've read online that I can use the .session() attribute, however when I try this I get the following error:The code is listed here:Example input format:Example output format:EDP09046   R6X9    A0A251R6X9_PRUPE    Prunus persica (Peach) (Amygdalus persica)  MEENHAPALESIPNGDHEAATTTNDFNTHIHTNNDHGWQKVTAKRQRKTKPSKADSINNLNKLVPGVTIAGGEGVFRSLEKQSEDRRRRILEAQRAANADADSLAPVRSKLRSDDEDGEDSDDESVAQNVKAEEAKKSKPKKPKKPKVTVAEAAAKIDDANDLSAFLIDISASYESKEDIQLMRFADYFGRAFSAVTAAQFPWVKMFRESTVAKLADIPLSHISEAVYKTSVDWISQRSLEALGSFILWSLDSILADLASQVAGAKGSKKSVQNVSSKSQVAIFVVVAMVLRKKPDVLISILPTLRENSKYQGQDKLPVIVWAISQASQGDLAVGLHSWAHIVLPLVSGKGSNPQSRDLILQLAERILSTPKARTILVNGAVRKGERLVPPSAFEILIGVTFPAPSARVKATERFEAIYPTLKAVALAGSPRSKAMKQVSLQILSFAVKAAGESIPALSNEATGIFIWCLTQHADCFKQWDKVYQENLEASVAVLKKLSDQWKEHSAKLAPFDPMRETLKSFRHKNEKMLASGEDEAHQEKLIKDADKYCKTLLGKSSRGSGCKKSVALAVVALAVGAAVMSPNMESWDWDLEKLRVTISSFFDCan anyone suggest some ways that I may improve this code to make it faster?Thanks in advance!
Requests are almost always the slowest portion of any networking code so you'll absolutely want to batch your IDs. Uniprot has a batching capability in it's API. There's a Perl example on that page that should help you get started â€“ I'd look at what the batch size limit is and go for the largest as a starting point (it's likely much smaller than 4,000,000). As noted on the Uniprot site, there's also an ID mapping service that may fit the bill.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example
