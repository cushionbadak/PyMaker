Natural Text
I want to remove the new line character in CSV file field's data. The same question is asked by multiple people in SO/other places. However the provided solutions are in scripting. I'm looking for a solution in programming languages like PYTHON or in Spark(not only these two) as I have pretty big files.Previously asked questions on the same topic:Remove New Line Character from CSV file's string columnReplace new line character between double quotes with spaceRemove New Line from CSV file's string columnhttps://unix.stackexchange.com/questions/222049/how-to-detect-and-remove-newline-character-within-a-column-in-a-csv-fileI have a CSV file of size ~1GB and want to remove the new line characters in field's data. The schema of the CSV file varies dynamically, so I can't hard code the schema. The line break doesn't always appear before a comma, it appears randomly even within a field.Sample Data:Expected Output:Newline character can be in any field's data.Edit:Screenshot as per the code:
If you are using pyspark then I would suggest you to go with sparkContext's  function to read the file, since your file needs to be read as whole text for parsing appropriately.After reading it using , you should parse by replacing end of line characters by , and do some additional formattings so that whole text can be broken down into groups of eight strings. You should get output as If you would like to convert all the array rdd rows into strings of rows then you can addand you should get 
You can use ,  and  modules as follows:If you want this  as an output  file use:
It could use a bit cleaning but here is some code that would do what you want. Works for line breaks within a field and before a comma. If more requirements needed, some tweaking could be done:I'm storing the corrected rows in  but if you don't want to load into memory, just use the  variable in every loop were pointed out in the comment
This one is a basic one with simple preprocessing before reading it through csv. Result :
The basic idea in this solution is to get fixed length chunks (of length equal to the number of columns in the first row) using the grouper recipe. Since it doesn't read the entire file at once, it wouldn't blow up your memory usage with large files.One assumption being made here is the absence of empty cells in the input csv.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools-recipes
