Natural Text
Generally, when you want to iterate over a portion of a list in Python, the easiest thing to do is just slice the list. But the slice operator creates a new list, which is not even necessary to do in many cases.  Ideally, I'd like some kind of slicing function that creates generators, as opposed to new list objects.  Something similar to this could be accomplished by creating a generator expression that uses a  to return only certain portions of the list:But this is sort of cumbersome.  I'm wondering if there is a better, more elegant way to do this.  So, what's the easiest way to slice a list so that a generator expression is created instead of a new list object?
Use itertools.islice:From the doc:Make an iterator that returns selected elements from the iterable
Before I start, to be clear, the correct order of selecting between slicing approaches is usually:Use regular slicing (the cost of copying all but the longest of inputs is usually not meaningful, and the code is much simpler). If the input might not be a sliceable sequence type, convert it to one, then slice, e.g. . This is simpler, and for most cases, typically faster, than any other approach.If regular slicing isn't viable (the input isn't guaranteed to be a sequence and converting to a sequence before slicing might cause memory issues, or it's huge and the slice covers most of it, e.g. skipping the first 1000 and last 1000 elements of a 10M element , so memory might be a concern),  is usually the correct solution as it's simple enough, and the performance cost is usually unimportant.If, and only if, 's performance is unacceptably slow (it adds some overhead to producing every item, though admittedly it's quite a small amount) and the amount of data to be skipped is small, while the data to be included is huge (e.g. the OP's scenario of skipping a single element and keeping the rest), keep readingIf you find yourself in case #3, you're in a scenario where 's ability to bypass initial elements (relatively) quickly isn't enough to make up for the incremental cost to produce the rest of the elements. In that case, you can improve performance by reversing your problem from selecting all elements after  to discarding all elements before .For this approach, you manually convert your input to an iterator, then explicitly pull out and discard  values, then iterate what's left in the iterator (but without the per-element overhead of ). For example, for an input of , your options for selecting elements 1 through the end are:If the number of elements to discard is larger, it's probably best to borrow the  recipe from the  docs:which makes the approaches generalize for skipping  elements to:Performance-wise, this wins by a meaningful amount for most large inputs (exception:  itself on Python 3 is already optimized for plain slicing; plain slicing can't be beat on actual  objects).  microbenchmarks (on CPython 3.6, 64 bit Linux build) illustrate this (the definition of  in the setup is just a way to make the lowest overhead approach to running out an iterable so we minimize the impact of the stuff we're not interested in):Obviously, the extra complexity of my solution isn't usually going to be worth it, but for moderate sized inputs (10K elements in this case), the performance benefit is clear;  was the worst performer (by a small amount), plain slicing was slightly better (which reinforces my point about plain slicing almost always being the best solution when you have an actual sequence), and the "convert to iterator, discard initial, use rest" approach won by a huge amount, relatively speaking (well under half the time of either of the under solutions).That benefit won't show up for tiny inputs, because the fixed overhead of loading/calling /, and especially , will outweigh the savings:but as you can see, even for 10 elements the -free approach isn't much worse; by 100 elements, the -free approach is faster than all competitors, and by 200 elements, the generalized + beats all competitors (obviously it doesn't beat -free given the 180 ns overhead of , but that's made up for by generalizing to skipping  elements as a single step, rather than needing to call  repeatedly for skipping more than one element). Plain  rarely wins in the "skip a few, keep a lot" case due to the per element overhead the wrapper exacts (it didn't clearly beat eager slicing in the microbenchmarks until around 100K elements; it's memory efficient, but CPU inefficient), and it will do even worse (relative to eager slicing) in the "skip a lot, keep a few" case.
Try itertools.islice:http://docs.python.org/library/itertools.html#itertools.islice


Answer URL
https://docs.python.org/3/library/itertools.html#itertools-recipes
