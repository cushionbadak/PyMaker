Natural Text
I transformed the existing code which was in python pasted below was in pyspark. Python code:Converted Pyspark Code:I ran both programs but didn't see any significant performance improvement. What am I missing? Please could you shed some thoughts?Also, Should I use 'reduce' as well? I am currently using only 'map'.
If you want to parallely process something in PySpark, don't  back to a Python listObviously,  needs changed as well to both accept and return a PySpark Then, you would have something like this 
In addition to the collection issue others have pointed out, your PySpark implementation may be slower simply because Spark was not meant for your current use case.Fundamentally, Spark aims to speed up operations on very large, distributed data sets (multiple machines), not local parallelization. To achieve this, it uses overhead structures and processes.For single/small datasets, this overhead could easily become dominant and slow down your solution. This article discusses the use of Hadoop, which is very similar. You may have tried multiprocessing instead?If you're sure that Spark is right for you, it maybe helpful to post a new question detailing your Spark set up, how you're measuring your performance, and your data set.
I think it perfectly makes sense that you don't see any speed up. You are first creating an RDD (so you distribute data) then you collect them to run your second function which is the analysis function. In effect, you destroy what your first function did by collecting all your data to the driver machine that goes on to apply your displaySentiment() function. So what you in effect do is to run the program in the driver machine which is just one machine. Hence no speed up.


Answer URL
https://docs.python.org/3/library/multiprocessing.html
