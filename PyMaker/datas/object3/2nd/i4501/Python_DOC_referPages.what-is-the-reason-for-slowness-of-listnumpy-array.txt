Natural Text
It is well known, that if  is a numpy array, then  is faster than , for example:That means, the naive  version is about factor  slower than the special-function . However, comparing it to the the performance of the build-in -module:we can see, that one should probably say, that  is slow rather than  is fast, because  is as fast as the special function.Another observation: -module and  benefit from the small-integer-pool (i.e. when values are in range ), but this is not the case for :As we can see the faster versions are about 2 times faster, but the slow version is as slow as before.My question: what slows  down compared to ?Edit:One more observation, for Python2.7, it takes longer if the integers are bigger (i.e. cannot be hold by ):but still faster, than the slow list-version.
Here is a partial answer explaining your observation re the small integer pool:As we can see  seems to try and create elements of native python type whereas the array iterator (which is what is used by the list constructor) doesn't bother.Indeed, the C implementation of  (source here) uses  which is the equivalent to Python , not - as one might assume - 
Basically, the answer of Paul Panzer explains what happens: in the slow  version the resulting elements of the list are not python-integers, but are numpy-scalars, e.g. . This answer just elaborates a little bit and connects the dots.I didn't make a systematic profiling, but when stopped in the debugger, every time both versions were in the routines which created the integer-object, so it is very probably this is where the lion's share of the execution time is spent, and the overhead doesn't play a big role.The -version, iterator calls , which has an special treatment for one-dimensional arrays and calls , which is a quite generic function and doesn't use the machinery of the Pythons-integer-creation. It happens to be slower than the Python-version, there is also no integer-pool for small values.The  version calls , which eventually uses Python's , which shows all the observed behaviors and happens to be faster than  numpy-functionality (probably, because it is not the normal way of using numpy, not many optimizations were done).There is a small difference for Python2 compared to Python3: Python2 has two different classes of integers: the one, more efficient, for numbers up to 32bit and the one for arbitrary big numbers - thus for the bigger numbers the most general (and thus more costly)  path must be taken - this also can be observed.
Constructing a list with  iterates over something and collects the result of the iteration into a new list.If  is slower than  one could assume that iterating over  is slower than iterating over . Let's verify that: Yep, iterating over a numpy array seems to be slower. This is where I must start speculating. Numpy arrays are flexible. They can have an arbitrary number of dimensions with various strides. Array arrays are always flat. This flexibility probably likely comes at a cost, which manifests in more complex iteration.


Answer URL
https://docs.python.org/3/c-api/long.html#c.PyLong_FromLong
