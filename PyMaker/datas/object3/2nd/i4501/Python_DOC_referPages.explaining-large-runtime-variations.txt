Natural Text
I am a bit baffled by the following profiling results, and would like to hear some explanation in order to make sense of them. I thought I would take the inner-product as a simple function to compare different possible implementations:The first implementation  is a "naive" one, where we first create a new list of pairwise products, and then sum its elements. I thought the second implementation  would be a bit smarter, because it eliminates the need to create a new list. And the third implementation  uses Numpy's dot function.In order to profile these functions, I am using the following:Scenario 1: Python listsoutput:So the "smarter" implementation  actually performs worse than the naive one , and Numpy is much faster than both. But then..Scenario 2: Python arraysI thought maybe using a numeric container, like array, would perhaps enable certain optimisations under the hood.output:Nope. If anything, it made things worse for the pure-Python implementations, accentuating the difference between the "naive" and "smart" versions, and now Numpy is 3 orders of magnitude faster!QuestionsQ1. The only way I can make sense of these results is if Numpy actually copies the data before processing it in Scenario 1, whereas it only "points" to it in scenario 2, does that sound reasonable?Q2. Why is my "smart" implementation performing systematically slower than the "naive" one? If my hunch for Q1 is correct, it's entirely possible that it is faster to create a new array if  does something smart under the hood. Is that it?Q3. 3 orders of magnitude! How is that possible? Are my implementations really dumb, or are there some magical processor instructions to compute the dot product?
Q1Python lists contain pointers to python objects, while arrays contain those numbers directly. The underlying numpy code however expects it to be a contiguous array. So when passed a list, it has to read the value out of the float into a new array for every element of the list.As noted in the comments, using numpy's built-in arrays is even better.Q2Getting a value from a generator is (from memory) slighly less expensive as a python function call. This is a lot more expensive than the list comprehension, where everything but the  is handled inside of the interpreter. A list has to be produced, which is expensive, but this cost appears to be quickly ammortized.Q3Numpy is three orders of magnitued faster because it is built on very highly optimized low-level libraries. Depending on the backend used, it might even use multiple threads. Python has to deal with a lot of overhead to make things easier for you the programmer at every step of the way, so it really isn't a fair race.BonusI made my generator suggestion because usually constructing the list is significant overhead. However, in this case it is moot as it appears that sum() over a list is faster than over an iterator.
The generators / yield mechanics does cost some CPU cycles. What it saves you is memory when you don't want the whole sequence at once, or helps when you want to interleave several dependent computations to lower your latency aka time to the first item in the sequence.Using a  function on an array just lets it run regular C code over a contiguous chunk of memory, without dereferencing  objects from pointers in a list. So it becomes insanely fast (which is the whole point of ).


Answer URL
https://docs.python.org/3/library/array.html
