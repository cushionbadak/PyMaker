Natural Text
I'm relatively new to Python and requests, so I'm not sure the best way to go about this.I need to send a large amount of POST requests to a URL. Right now, I'm simply using a loop and sending the request, which yields roughly 100 posts every 10 - 30 seconds, depending on the internet. I'm looking for a way to do this faster and with more posts. Multiprocessing was recommended to me, but my knowledge here is very lacking (I've already frozen my computer trying to spawn too many processes).How can I effectively implement multiprocessing to increase my results?
Here is a code sample taken from http://skipperkongen.dk/2016/09/09/easy-parallel-http-requests-with-python-and-asyncio/ which may solve your problem. It uses the requests library to make the request and asyncio for the asynchronous calls. The only change you'd have to make is from a GET call to a POST call.This was written in Python 3.5 (as expressed in the article)I would also recommend reading the entire article as it shows time comparisons when using lots of threads.
There's no reason to use multiprocessing here. Making requests of HTTP servers is almost entirely I/O-bound, not CPU-bound, so threads work just fine.And the very first example of using  in the stdlib's  documentation does exactly what you're asking for, except with  instead of .If you're doing anything complicated, look at .If you really do need to use multiprocessing for some reason (e.g., you're doing a whole lot of text processing on each result, and you want to parallelize that along with the requesting), you can just switch the  to a  and change nothing else in your code.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example
