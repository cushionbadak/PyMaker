Natural Text
Edit: Running Apple MBP 2017 Model 14,3 with 2.8GHz i7 4-cores:I have a list of objects I'm performing object methods on in python once for each object.  The process is for a genetic algorithm so I'm interested in speeding it up. Basically, each time I update the environment with data from the data list, the object (genome) performs a little bit of math including taking values from the environment, and referencing it's own internal values.I'm doing:If I run this:It is much faster than if I try to split this up using multiprocessing/ map:If I run with object_list length of 200 (instead of 20000) the total time is 14.8 seconds in single threaded mode.If I run with the same in multiprocessing mode the total time is... still waiting... ok... 211 seconds.Also it doesn't appear to do what the function says it should at all.  What am I missing here?  When I check the values of each object they do not appear to have been updated at all.
When you use multiprocessing, you're serializing and transferring the data both ways. In this case, that includes each object you indend to call update_values on. I'm guessing that you're also iterating on your models, meaning they'll be sent back and forth quite a lot. Furthermore, map() returns a list of results, but process_object just returns None. So you've serialized a model, sent it to another process, had that process run and update the model, then send a None back and toss away the updated model, before tossing away the list of None results. If you were to return the models:Your program might actually produce some results, but almost certainly still slower than you wish. In particular your process pool will not have the data_list or similar things (the "environment"?) - it only receives what you passed through Pool.map(). You may want to consider using other tools such as tensorflow or MPI. At least read up on sharing state between processes. Also, you probably shouldn't be recreating your process pool for every iteration; that's very expensive on some platforms, such as Windows. 
I would split up the parallelization a little bit differently. It's hard to tell what's happening with , but I would parallelize the call to that too. Why leave it out? You could wrap the whole operation you're interested in, in some function, right?Also, this is important: you need to make sure that you only open up the pool if you're in the main process. So add the line


Answer URL
https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes
https://docs.python.org/3/library/pickle.html
