Natural Text
I have a multiple csv files with the following content:And I have about 10 millions raws of this data. I need to normalise this data and split it into tables.  I suppose there will be tables: stations, bikes, rides. In terms of OLAP rides are facts, and stations and bikes are dimensions. I'm very new to data analysis so I could use incorrect terms. But I'm trying to use this approach.So the question is how to write this data into database as optimal as it possible? The approach I can imagine is following:But if I have 10 millions of rows, this approach will make ~40 millions of queries to database, which looks terrible and not optimal.Is there more optimal approaches/algorithms/technologies to do it? I'm going to use python and psql for it, if it's important.
You can probably economize on queries by memoizing the function that creates the unique records, for example:If the input is sorted by station, then subsequent calls to  will not query the database once the record is created. Even if it's not perfectly sorted, this could help.You can batch the saving of rides. Accumulate records and then call an  function (will depend on the libraries you are using).You could pre-process the data to create separate CSV files, then load each file.
According to the PostgreSQL documentation,  command is an optimum approach for populating a table with a large number of rows. At the other hand, for processing csv files pandas library is one of the best tools.So the below steps can be an acceptable solution:


Answer URL
https://docs.python.org/3/library/functools.html#functools.lru_cache
