Natural Text
Problem:Replacing multiple string patterns in a large text file is taking a lot of time. (Python)Scenario:I have a large text file with no particular structure to it. But, it contains several patterns. For example, email addresses and phone numbers. The text file has over 100 different such patterns and the file is of size 10mb (size could increase). The text file may or may not contain all the 100 patterns. At present, I am replacing the matches using  and the approach for performing replaces looks as shown below.This approach is taking a lot of time for large files. Is there a better way to optimize it?I am thinking of replacing  with  but not sure how much that would help.
you could use lineprofiler to find which lines in your code take the most timeanother thing, I think you're building the string too large in memory, maybe you can make use of generators
You may obtain slightly better results doing :However, further optimization can be achieved knowing what kind of processing you apply. In fact the last line will be the one that takes up all CPU power (and memory allocation). If regexes can be applied on a per-line basis, you can achieve great results using the multiprocessing package. Threading won't give you anything because of the GIL (https://wiki.python.org/moin/GlobalInterpreterLock)


Answer URL
https://docs.python.org/3/howto/functional.html#generator-expressions-and-list-comprehensions
