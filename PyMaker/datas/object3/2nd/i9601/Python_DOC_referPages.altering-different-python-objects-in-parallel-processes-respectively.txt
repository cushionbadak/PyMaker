Natural Text
In a NutshellI want to change complex python objects concurrently, whereby each object is processed by a single process only. How can I do this (most efficiently)? Would implementing some kind of pickling support help? Would that be efficient?Full ProblemI have a python data structure  that basically consists of a  array and a dictionary and maps arbitrary indices to rows in the array. In my case, all keys are integers.Now I have multiple such s and want to fill them in . Since  is expensive, I want to run it in parallel. Note that  may add many rows to . Every process alters its own . I do not need concurrent access to the s.In , I change entries in the  (that is, I change the internal  array), I add entries to the  (that is, I add another index to the dictionary and write a new value in the internal array). Eventually, I would like to be able to exchange 's internal  array when it becomes too small. This does not happen often and I could perform this action in the non-parallel part of my program, if no better solution exists. My own attempts were not successful even without the array exchange.I have spent days researching on shared memory and python's multiprocessing module. Since I will finally be working on linux, the task seemed to be rather simple: the system call  allows to work with copies of the arguments efficiently. My thought was then to change each  in its own process, return the changed version of the object, and overwrite the original object. To save memory and save the work for copying, I used in addition sharedmem arrays to store the data in . I am aware that the dictionary must still be copied.What I get is a segmentation fault error. I was able to circumvent this error by creating deepcopies of the s to  and saving them into . I do not really understand why this is necessary, and copying my (potentially very large) arrays frequently (the while loop takes long) is not what seems to be efficient to me. However, at least it worked to a certain extent. Nevertheless, my program has some buggy behaviour at the 3rd iteration due to the shared memory. Therefore, I think that my way is not optimal.I read here and here that it is possible to save aribtrary numpy arrays on the shared memory using . However, I would still need to share the whole , which includes in particular a dictionary, which in turn is not pickable. How could I achieve my goals in an efficient way? Would it be possible (and efficient) to make my object pickable somehow? All solutions must run with python 3 and full numpy/scipy support on 64bit Linux.EditI found here that it is somehow possible to share arbitrary objects using Multiprocessing "Manager" classes and user-defined proxy classes. Will this be efficient? I would like to exploit that I do not need concurrent access to the objects, even though they are not handled in the main process. Would it be an option to create a manager for each object that I want to process? (I might still have some misconceptions about how mangers work.)
This seems like a fairly complex class, and I am not able to completely anticipate if this solution will work in your case. A simple compromise for such a complex class is to use .If this does not answer your question then it would be good with a minimal, working, example.The objects are cloned when sent to the child process, you need to return the result (as changes to the object will not propagate back to the main process) and handle how you want to store them.Note that changes to class variables will propagate to other objects in the  same process, e.g. if you have more tasks than processes, changes to class   variables will be shared among the instances running in the same process. This is usually undesired behavior.This is a high-level interface to parallelization.  uses the  module and can only be used with pickable objects. I suspect that  has performance similar to "sharing state between processes". Under the hood,  is using , and should exhibit similar performance as  (except when using very long iterables with map).  does seem to be the intended future API for concurrent tasks in python.If you can, it is usually faster to use the  (which can just be swapped for the ). In this case the object is shared between the processes, and an update to one will propagate back to the main thread.As mentioned the fastest option is probably to re-structure  so that it only uses objects that can be represented by  or .If  does not work, and you cannot optimize , you may be stuck with using a . There are good examples on how to do that here. The greatest performance gain is often likely to be found in .   And, as I mentioned, the overhead of using threads is less than that of processes.


Answer URL
https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled
