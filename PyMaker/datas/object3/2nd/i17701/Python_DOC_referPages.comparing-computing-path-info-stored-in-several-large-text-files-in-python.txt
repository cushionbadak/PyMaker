Natural Text
I have a bunch of flat files that basically store millions of paths and their corresponding info (name, atime, size, owner, etc)I would like to compile a full list of all the paths stored collectively on the files.  For duplicate paths only the largest path needs to be kept.  There are roughly 500 files and approximately a million paths in the text file.  The files are also gzipped.  So far I've been able to do this in python but the solution is not optimized as for each file it basically takes an hour to load and compare against the current list.  Should I go for a database solution?  sqlite3?  Is there a data structure or better algorithm to go about this in python?  Thanks for any help!
So far I've been able to do this in python but the solution is not optimized as for each file it basically takes an hour to load and compare against the current list.If "the current list" implies that you're keeping track of all of the paths seen so far in a , and then doing  for each line, then each one of those searches takes linear time. If you have 500M total paths, of which 100M are unique, you're doing O(500M*100M) comparisons.Just changing that  to a , and changing nothing else in your code (well, you need to replace  with , and you can probably remove the  check entirely… but without seeing your code it's hard to be specific) makes each one of those checks take constant time. So you're doing O(500M) comparisons—100M times faster.Another potential problem is that you may not have enough memory. On a 64-bit machine, you've got enough virtual memory to hold almost anything you want… but if there's not enough physical memory available to back that up, eventually you'll spend more time swapping data back and forth to disk than doing actual work, and your program will slow to a crawl.There are actually two potential sub-problems here.First, you might be reading each entire file in at once (or, worse, all of the files at once) when you don't need to (e.g., by decompressing the whole file instead of using , or by using  but then doing  or , or whatever). If so… don't do that. Just iterate over the lines in each , .Second, maybe even a simple  of however many unique lines you have is too much to fit in memory on your computer. In that case, you probably want to look at a database. But you don't need anything as complicated as sqlite. A  acts like a  (except that its keys and values have to be byte strings), but it's stored on-dict, caching things in memory where appropriate, instead of stored in memory, paging to disk randomly, which means it will go a lot faster in this case. (And it'll be persistent, too.) Of course you want something that acts like a , not a … but that's easy. You can model a set as a  whose keys are always . So instead of , it's just . Yeah, that wastes a few bytes of disk space over building your own custom on-disk key-only hash table, but it's unlikely to make any significant difference.


Answer URL
