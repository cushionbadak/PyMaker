Natural Text
I'm new in Scrapy. I have thousands of url,xpath tuples and values in a database. These urls are from different domains (not allways, there can be 100 urls from the same domain). Now I want to get these values every 2 hours as fast as possible but to be sure that I don't overload any of these. Can't figure out how to do that. My thoughts:I could create one Spider for every different domain, set it's parsing rules and run them at once. Is it a good practice? EDIT: I'm not sure how it would work with outputting data into database according to concurrency.EDIT2:I can do something like this - for every domain there is a new spider. But this is impossible to do having thousands of different urls and it's xpaths.
From the example you posted in edit2, it looks like all your classes are easily abstractable by one more level. How about this:?  Next, you could group the data from database by xpathsAD concurency: your database should handle concurent writes so I do not see a problem thereEdit: Related to the timeouts: I Do not know how scrapy works under the hood i.e. if it uses some sort of paralelization and whether it runs asynchronously in the background. But from what you wrote I guess it does and when you fire up 1k scrapers each firing multiple requests at time your hardware cant handle that much traffic (disclaimer, this is just a guess!).There might be a native way to do this, but a possible workaround is to use multiprocessing + Queue:But please bear in mind that this is a vanilla approach for paralell execution completely ignoring Scrapy abilities. I have found This blogpost which uses  to achieve (what I think is) the same thing. But since I've never used twisted I can't comment on that
if you are thinking about  can't handle multiple domains at once because of the  parameters, remember that it is optional.If no  parameter is set in the spider, it can work with every domain it gets.
If I understand correctly you have map of domain to xpath values and you want to pull xpath depending on what domain you crawl?Try something like:


Answer URL
https://docs.python.org/3/library/multiprocessing.html
