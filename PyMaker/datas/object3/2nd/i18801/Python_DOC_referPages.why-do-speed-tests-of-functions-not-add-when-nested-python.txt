Natural Text
I am trying to optimize some code, so I thought I would look into exactly where my bottlenecks were. I have four functions that wrap eachother like: So I tested each individually as well as a whole. When individually i essentially precomputed the previous function. However, I assumed they would add up to the total time. But they didn't, it grew significantly as I combined them. So I decided to look at it at a smaller scale. I wrote this to testI got back test as .085 seconds, test2 as .125 seconds and test3 as .171 seconds. This confounded me in two ways. 1) Why isn't test3 .21 seconds, and 2) Why was it shorter as opposed to my problem of it getting much longer?
Since you haven't given us code that reproduces your original problem, it's hard to do anything but guess… but I can make some guesses here.When you compose two very small functions, the more often you run it, the more likely you are to have the bytecode to both functions, the globals and locals dictionaries, etc. all in your cache.On the other hand, when you compose two very large functions, you're very likely to push part of the outer function out of cache each time the inner function runs, so you end up spending more time in cache refills than actually interpreting your code.On top of that, you're forgetting about the cost of calling a function. In Python, that's not just a function call—you normally call functions by their global name, and a  can be very slow. If you've written toy composition like this:… you don't pay for that lookup as often as if you do this:… but you can pay almost nothing for it by copying  into the appropriate . For the two examples above:Your test functions include setup costs in what you're timing.For example, if you're using Python 2.x, the  could take a significant fraction of the total time. But  only does that twice, while  only does it once. So, it's perfectly reasonable that the savings in  were enough to be noticeable in the toy test. But in your real-life test, where each loop takes, say, 100x longer, the cost of the  call is insignificant.It's also worth noting that if you create enough memory, you can end up triggering  calls or even VM swapping—which are, respectively, slow and mind-numbingly slow, and which are also both much more variable and unpredictable than the usual costs of running code in a loop. That may not be an issue just creating and destroying a few 1M-item lists (which should be on the order of 20-80MB), but it could be.Finally, you haven't shown us how you're doing the timing, how you're repeating the tests, how you're aggregating the results, etc., so it's quite possible that your tests just aren't valid.
Big amount of time takes list generation "range(1000000)" (assuming you are using python 2.X). In test3 you are creating this list only once. When you sum the time, you are summing 2 times creation of list.You can use profiler to know what takes the time http://docs.python.org/2/library/profile.html


Answer URL
