Natural Text
FOR PYTHON 2.7 (I took a shot of using encode in 3 and am all confused now...would love some advice how to replicate this test in python 3....)For the Euro character (€) I looked up what its utf8 Hex code point was using this tool. It said it was  0x20AC. For Latin1 (again using Python2 2.7), I used decode to get its Hex code point:Then I used this print statement for both of them, and this is what I got:for utf8:for latin-1:What in the heck happened? Why did encode return 'â‚¬' for utf-8? Also...it seems that Latin1 hex code points CAN be different then their utf8 counterparts (I have a colleague who believes different -- says that Latin1 is just like ASCII in this respect). But the presence of different code points seems to suggest otherwise to me...HOWEVER the reason why python 2.7 is reading the Windows CP1252 'x80' is a real mystery to me....is this the standard for latin-1 in python 2.7??
You've got some serious misunderstandings here. If you haven't read the Unicode HOWTOs for Python 2 and Python 3, you should start there.First, UTF-8 is an encoding of Unicode to 8-bit bytes. There is no such thing as UTF-8 code point . There is a Unicode code point U+20AC, but in UTF-8, that's three bytes: , , .And that explains your confusion here:Why did encode return 'â‚¬' for utf-8?It didn't. It returned the byte string . You then ed  that out to your console. Your console is presumably in CP-1252, so it interpreted those bytes as if they were CP-1252, which gave you .Meanwhile, when you write this:The console isn't giving Python Unicode, it's giving Python bytes in CP-1252, which Python just stores as bytes. The CP-1252 for the Euro sign is . So, this is the same as typing:But in Latin-1,  isn't the Euro sign, it's an invisible control character, equivalent to Unicode U+0080. So, when you call , you get back . Which is exactly what you're seeing.The reason you can't reproduce this in Python 3 is that in Python 3, , and plain string literals, are Unicode strings, not byte strings. So, when you write this:… the console gives Python some bytes, which Python then automatically decodes with the character set it guessed for the console (CP-1252) into Unicode. So, it's equivalent to writing this:… or this:Also, you keep saying "hex code points" to mean a variety of different things, none of which make any sense.A code point is a Unicode concept. A  string in Python is a sequence of code points. A  is a sequence of bytes, not code points. Hex is just a way of representing a number—the hex number , or , is the same thing as the decimal number , and the hex number  is the same thing as the decimal number .That sequence of bytes doesn't have any inherent meaning as text on its own; it needs to be combined with an encoding to have a meaning. Depending on the encoding, some code points may not be representable at all, and others may take 2 or more bytes to represent.Finally:Also...it seems that Latin1 hex code points CAN be different then their utf8 counterparts (I have a colleague who believes different -- says that Latin1 is just like ASCII in this respect).Latin-1 is a superset of ASCII. Unicode is also a superset of the printable subset of Latin-1; some of the Unicode characters up to U+FF (and all printable characters up to U+7F) are encoded in UTF-8 as the byte with the same value as the code point, but not all. CP-1252 is a different superset of the printable subset of Latin-1. Since there is no Euro sign in either ASCII or Latin-1, it's perfectly reasonable for CP-1252 and UTF-8 to represent it differently.


Answer URL
