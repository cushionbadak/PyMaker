Natural Text
So, I have a large 30k line program I've been writing for a year. It basically gathers non-normalized and non-standardized data from multiple sources and matches everything up after standardizing the sources.I've written most everything with ordered dictionaries. This allowed me to keep the columns ordered, named and mutable, which made processing easier as values can be assigned/fixed throughout the entire mess of code.However, I'm currently running out of RAM from all these dictionaries. I've since learned that switching to namedtuples will fix this, the only problem is that these aren't mutable, so that brings up one issue in doing the conversion. I believe I could use a class to eliminate the immutability, but will may RAM savings be the same? Another option would be to use namedtuples and reassign them to new namedtouples every time a value needs to change (i.e. NewTup=Tup(oldTup.odj1, oldTup.odj2, "something new"). But I think I'd need an explicit way to destroy the old one afterwords or space could become an issue again.The bottom line is my input files are about 6GB on disk (lots of data). I'm forced to process this data on a server with 16GB RAM and 4 GB swap. I originally programmed all the rows of these various I/O data sets with dictionaries, which is eating too much RAM... but the mutable nature and named referencing was a huge help in faster development, how do I cut my addition to dictionaries so that I can utilize the cost savings of other objects without rewriting the entire application do to the immutable nature of tuples.SAMPLE CODE:*Later on these values are often changed by bringing in other data sets.
Just write your own class, and use  to keep the memory footprint to a minimum:This can get a little verbose perhaps, and if you need to use these as dictionary keys you'll have more typing to do.There is a project that makes creating such classes easier: :Classes created with the  library automatically take care of proper equality testing, representation and hashability.Such objects are the most memory efficient representation of data Python can offer. If that is not enough (and it could well be that it is not enough), you need to look at offloading your data to disk. The easiest way to do that is to use a SQL database, like with the bundled  SQLite library. Even if you used a  temporary database, the database will manage your memory load by swapping out pages to disk as needed.
It seems to me, that your main problem is you trying to create a database entirely in memory. You should use actual database like MySQL or PostgreSQL. You can use nice ORM like peewee or django orm for interaction with databases.On the other hand, if you can't hadle whole data, you can split your data into parts you can handle.Module "TinyDB" (http://tinydb.readthedocs.io/en/latest/) can help you to keep using dictionaries and don't run of RAM.


Answer URL
https://docs.python.org/3/library/sqlite3.html
