Natural Text
I am building a simple webcrawler in Python. I will have to go through ~50k websites and I want to speed up the process with some multithreading.I have defined a crawler class to crawl through each website as a meta-object of Thread:Then in a main function, I iterate through batches of 10 URLs from the full list of URLs and create a Crawler object for each URL:The problem is that, for each batch, I have to wait for all threads to finish. This is unefficient as when I have, say 9 websites with 100 pages and only 1 website with 10,000 pages, the 9 websites will be done a in a matter of minutes, but I will have to wait an hour for the 10,000 pages large website to finish before I can proceed to the next batch.To optimize things, it would be better to start with 10 Crawler threads, then, every time a Crawler thread is finished, create a new Crawler with the next url in the list, until the list is done. I'm thinking I could get rid of the join() and have a while loop over the length of , adding a new thread everytime the length drops below 10, but that sounds a bit hackish. I was looking into python's  but judging from the examples at https://docs.python.org/3/library/queue.html, I would still have to rely on  and therefore wait for all the threads in the queue to have executed.Is there any way to add something like an "event listener" to a thread, so that whenever a thread finishes I can update the thread list with a new thread ? 
Perhaps have a look at the Queue again, you don't need a join per batch or at all.You can put all of the 50K websites into the queue. I would that perhaps call  and the limited number of threads are usually called something like . Each worker then picks up an item from the queue, processes it and continues to pick up items from the queue until done. What done means varies. One suggestion is to put  on the queue for each worker and each worker will stop once it sees a . But there are other signals you could use. You could then use  to wait for all worker threads to finish. In that case a worker doesn't need to be a daemon. (You wouldn't want to create a separate Thread per URL)For example:There are alternatives to that. If you are not actually looking at it as an exercise but want to get something done, then I would recommend https://scrapy.org/


Answer URL
https://docs.python.org/3/library/queue.html
