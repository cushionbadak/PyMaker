Natural Text
I'm trying to import a large size of JSON FILE from Amazon S3 into AWS RDS-PostgreSQL using Python. But, these errors occured,Traceback (most recent call last):File "my_code.py", line 67, in file_content = obj['Body'].read().decode('utf-8').splitlines(True)File "/home/user/asd-to-qwe/fgh-to-hjk/env/local/lib/python3.6/site-packages/botocore/response.py", line 76, in readchunk = self._raw_stream.read(amt)File "/home/user/asd-to-qwe/fgh-to-hjk/env/local/lib/python3.6/site-packages/botocore/vendored/requests/packages/urllib3/response.py", line 239, in readdata = self._fp.read()File "/usr/lib64/python3.6/http/client.py", line 462, in reads = self._safe_read(self.length)File "/usr/lib64/python3.6/http/client.py", line 617, in _safe_readreturn b"".join(s)MemoryError// my_code.pyAre there any solutions to these problems? Any help would do, thank you so much!
A significant savings can be had by avoiding slurping your whole input file into memory as a  of lines.Specifically, these lines are terrible on memory usage, in that they involve a peak memory usage of a  object the size of your whole file, plus a  of lines with the complete contents of the file as well:For a 1 GB ASCII text file with 5 million lines, on 64 bit Python 3.3+, that's a peak memory requirement of roughly 2.3 GB for just the  object, the , and the individual s in the . A program that needs 2.3x as much RAM as the size of the files it processes won't scale to large files.To fix, change that original code to:Given that  appears to be usable for lazy streaming this should remove both copies of the complete file data from memory. Using  means  is lazily read and decoded in chunks (of a few KB at a time), and the lines are iterated lazily as well; this reduces memory demands to a small, largely fixed amount (the peak memory cost would depend on the length of the longest line), regardless of file size.Update:It looks like  doesn't implement the  ABC. It does have its own documented API though, that can be used for a similar purpose. If you can't make the  do the work for you (it's much more efficient and simple if it can be made to work), an alternative would be to do:Unlike using , it doesn't benefit from bulk decoding of blocks (each line is decoded individually), but otherwise it should still achieve the same benefits in terms of reduced memory usage.


Answer URL
https://docs.python.org/3/library/io.html#class-hierarchy
https://docs.python.org/3/library/io.html#io.BufferedIOBase
