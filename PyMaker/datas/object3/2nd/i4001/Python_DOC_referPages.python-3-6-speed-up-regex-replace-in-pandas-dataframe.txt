Natural Text
I'm using the following code to read 1 million rows of SQL data and replace any control characters which might appear in the data, the only problem is that is slow and it is definitely the 'replace' which is slowing it down. Anyone have any suggestion about taking a different approach or tweaking to might make the code any faster?Takes 16 minutes to read, clean and write out 1 million rows (of 31 fields.)
You don't need regex for this to begin with - you're just replacing 'special' characters with an empty space in an one-to-one replacement - but apart from that you hardly need to parse and turn your data into a DataFrame to begin with.You can work directly with a DB connection and export the columns using the built-in  module without ever venturing into ,  and similar heavyweights that add unnecessary overhead for your use case.So, first things first, instead of regex you can create a translation table and use it with  to clean up any string:This allows you to quickly and effortlessly translate all of the special characters, defined within the ranges of , into a space on any string, for example:And while we're at it, we can create a small function to handle the translation attempts for any passed field so we don't need to check the type when iterating over our database data:Now all we need is to connect to the database, and execute our query, which depends on the database that you're using - I'll write the next example as if you were using SQLite but most database drivers use Python Database API and are largely interchangeable so the code should work with minimal changes:And, finally, we can iterate over the results, process each field and store all into a CSV:This avoids all unnecessary conversions and should work as fast as Python and your database is capable of. You could, technically, squeeze out some more speed by inspecting the  and creating a replacement map only for the strings in your result set (instead of attempting to process each field) but it probably won't add much to the overall speed.So, putting it all together:As a test I've created a  table in SQLite with 22  fields (each filled with 10-50 random characters in the  range), interspersed with  and  fields and, on my system, it cleaned up the data and produced the  in under 56 seconds. YMMV, of course, but it certainly shouldn't take 16 minutes.


Answer URL
https://docs.python.org/3/library/stdtypes.html?#str.translate
