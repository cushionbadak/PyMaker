Natural Text
In a python script, I have a large dataset that I would like to apply multiple functions to. The functions are responsible for creating certain outputs that get saved to the hard drive. A few things of note:the functions are independentnone of the functions return anythingthe functions will take variable amounts of timesome of the functions may fail, and that is fineCan I multiprocess this in any way that each function and the dataset are sent separately to a core and run there? This way I do not need the first function to finish before the second one can kick off? There is no need for them to be sequentially dependent.Thanks!
Since your functions are independent and only read data, as long as it is not an issue if your data is modified during the execution of a function, then they are also thread safe.Use a thread pool (click) . You would have to create a task per function you want to run.Note: In order for it to run on more than one core you must use Python Multiprocessing. Else all the threads will run on a single core. This happens because Python has a Global Interpreter Lock (GIL). For more information Python threads all executing on a single coreAlternatively, you could use DASK , which augments the data in order to run some multi threading. While adding some overhead, it might be quicker for your needs.
I was in a similar situation as yours, and used Processes with the following function:This has a major drawback: all Processes in a bucket have to finish before a new bucket can start. I tried to use a JoinableQueue to avoid this, but could not make it work.Example:Hope it can help.


Answer URL
https://docs.python.org/3/library/multiprocessing.html
https://docs.python.org/3/library/multiprocessing.html?highlight=process#module-multiprocessing
