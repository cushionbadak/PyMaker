Natural Text
NEWEST UPDATE:I'm reducing my question to how to get all links from a site, including sublinks of each page etc, recursively.I think I know how to get all sublinks of one page:How do I recursively ensure all links on the site are also harvested and written onto the same file? So I tried this, and it's not even compiling.
Answering your question, this is how I would fetch all links of a page with beautilfulsoup and save them to a file:This will, however, not prevent cicles (which would result in infinite recursion). In order to do so you may use a  to store already visited links and not visit them again.You should really consider using something like Scrapy for this kind of task. I think a  is what you should look into.For the purpose of extracting the urls from the  domain you may do something like this:And run it withand you get the urls in a  format on the  file.


Answer URL
https://docs.python.org/3/tutorial/datastructures.html#sets
