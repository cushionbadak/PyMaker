Natural Text
Currently I am playing with Python performance, trying to speed up my programs (usually those which compute heuristics). I always used lists, trying not to get into  arrays.But recently I've heard that Python has  so I thought I would try that one.I wrote a piece of code to measure an  vs. a , as I use it in many places in my code:I was expecting a slight performance improvement when using . Well, this is what happened:So, according to  the  is 2013x faster than the . I definitely didn't expect that. So I've searched through SO, python docs etc. and the only thing I found was that the objects in array have to be first wrapped into -s, so this could slow things down, but I was expecting this to happen when creating an -instance, not when random accessing it (which I believe is what  does).So where's the catch?Am I doing something wrong?Or maybe I shouldn't use standard arrays and go straight to s?
where's the catch?The initial test, as proposed above does not compare apples to apples:not mentioning the python-2.7, where  creates indeed a RAM-allocated data-structure, whereas  resembles a python-3 re-formulated object ( as seen below ) a generator-will never be comparable to whatever smart RAM-allocated data-structure.The generator's object intrinsic  spits out the length, where still no counting takes place, does it? ( glad it did not, it would not fit into even  of RAM ..., yet it can "live" in py3+ as an object ) Quantitatively fair testing? Better test engineering details are needed:Go well above a few tens of MB, so as to avoid false expectations from InCACHE-computing artifacts, that will never scale-out to real-world problem sizes:Go into RAM-feasible, yet above InCACHE-horizon sizings:go straight to  arrays ?Definitely a wise step to test either. Vectorised internalities may surprise, and often do a lot :o)Depends a lot on your other code, if numpy-strengths may even boost some other parts of your code-base. Last but not least, beware of premature optimisations and scaling. Some -domain traps could be coped with if can spend more in -domain, yet most dangerous are lost InCACHE-locality, where no tradeoffs may help. So, better do not prematurely lock on promising detail, at a cost of loosing a global scale performance target.


Answer URL
https://docs.python.org/3/library/array.html
