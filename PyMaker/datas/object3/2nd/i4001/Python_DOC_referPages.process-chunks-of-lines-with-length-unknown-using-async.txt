Natural Text
I have a file with a url on each line. I am running an async requests call using aiohttp on these urls in batches.The file is huge and my memory is tiny. I don't know how many lines are in the file and reading the whole thing line by line with a counter will take an age.How can I:grab 100,000 lines into a listprocess thosepause file readgrab next 100,000 lines= repeat until I'm grabbing the remainder?I was thinking along the lines of async but I may have massively misunderstood this library.
The "pause" you are looking for is precisely what  does. It blocks the current coroutine (allowing others to make progress) until the one it awaits finishes. Your code is basically correct, except for the use of  around . Here is a more complete version (untested):When implementing , you need to take care to enable parallel execution, but also to allow awaiting completion of the whole batch. The key ingredient for that are coroutine combinators, functions that aggregate multiple coroutines into a single awaitable object. One that is powerful and very simple to use is : starts the given coroutines and, when ed, blocks the current one until all of them complete. This allows  to pause reading until the whole batch has been downloaded.Finally,  could look like this:All functions accept a session object created in  because creating a new session for each request is strongly discouraged in the documentation.Finally, to run the whole thing, use something like:
Reading and processing the read data asynchronously doesn't make it faster. Instead, you might use for async when you have a long task like data processing and want to render something whilst doing so. I would just read the file into memory, then process the data. Of course, if you can't do such thing because it doesn't fit into memory I would use  which @user4815162342 mentioned in their answer. 


Answer URL
https://docs.python.org/3/library/asyncio-task.html#asyncio.gather
