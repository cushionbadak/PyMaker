Natural Text
How would you go about employing and/or implementing a case class equivalent in PySpark?
As mentioned by Alex Hall a real equivalent of named product type, is a .Unlike , suggested in the other answer, it has a number of useful properties:Has well defined shape and can be reliably used for structural pattern matching:In contrast  are not reliable when used with keyword arguments:although if defined with positional arguments:the order is preserved.Define proper typesand can be used whenever type handling is required, especially with single:and multiple dispatch:and combined with the first property, there can be used in wide ranges of pattern matching scenarios.  also support standard inheritance and type hints. don't:Provide highly optimized representation. Unlike  objects, tuple don't use  and carry field names with each instance. As a result there are can be order of magnitude faster to initialize:compared to different  constructors:and are significantly more memory efficient (very important property when working with large scale data):compared to equivalent Finally attribute access is order of magnitude faster with :compared to equivalent operation on  object:Last but not least  are properly supported in Spark SQLSummary:It should be clear that  is a very poor substitute for an actual product type, and should be avoided unless enforced by Spark API.It should be also clear that  is not intended to be a replacement of a case class when you consider that, it is direct equivalent of  - type which is pretty far from an actual product, and behaves like  (depending on a subclass, with names added). Both Python and Scala implementations were introduced as an useful, albeit awkward interface between external code and internal Spark SQL representation. See also:It would be a shame not to mention awesome MacroPy developed by Li Haoyi and its port (MacroPy3) by Alberto Berti:which comes with a rich set of other features including, but not limited to, advanced pattern matching and neat lambda expression syntax.Python  (Python 3.7+).
If you go to sql-programming-guide in Inferring the Schema Using Reflection section, you will see  being defined as case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as Sequences or Arrays.with example as In the same section, if you switch to python i.e. pyspark, you will see  being used and defined as Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by looking at the first row.with example as So the conclusion of the explanation is that  can be used as  in pyspark


Answer URL
https://docs.python.org/3/library/typing.html#typing.NamedTuple
https://docs.python.org/3/library/dataclasses.html
