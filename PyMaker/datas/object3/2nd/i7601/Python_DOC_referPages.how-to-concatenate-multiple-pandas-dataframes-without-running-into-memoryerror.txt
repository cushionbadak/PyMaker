Natural Text
I have three DataFrames that I'm trying to concatenate.This results in a MemoryError. How can I resolve this?Note that most of the existing similar questions are on MemoryErrors occuring when reading large files. I don't have that problem. I have read my files in into DataFrames. I just can't concatenate that data.
I advice you to put your dataframes into single csv file by concatenation. Then to read your csv file.Execute that:If this solution isn't enougth performante, to concat larger files than usually. Do:Then run bash command:Or concat csv files in python :After read:
Similar to what @glegoux suggests, also  can write in append mode, so you can do something like:
The problem is, like viewed in the others answers, a problem of memory. And a solution is to store data on disk, then  to build an unique dataframe.With such huge data, performance is an issue.csv solutions are very slow, since conversion in text mode occurs.HDF5 solutions are shorter, more elegant and faster since using binary mode.I propose a third way in binary mode, with pickle, which seems to be even faster, but more technical and needing some more room. And a fourth, by hand.  Here the code:Better solutions :    For homogeneous dataframes, we can do even better :And some tests on (little, 32 Mb) data to compare performance. you have to multiply by about 128 for 4 Gb.A check :Of course all of that must be improved and tuned to fit your problem.For exemple  df3 can be split in chuncks of size 'total_memory_size - df_total_size' to be able to run . I can edit it if you give more information on your data structure and size if you want. Beautiful question !
Kinda taking a guess here, but maybe:Obviously, you could do that more as a loop but the key is you want to delete df2, df3, etc. as you go.  As you are doing it in the question, you never clear out the old dataframes so you are using about twice as much memory as you need to.More generally, if you are reading and concatentating, I'd do it something like this (if you had 3 CSVs:  foo0, foo1, foo2):In other words, as you are reading in files, you only keep the small dataframes in memory temporarily, until you concatenate them into the combined df, concat_df.  As you currently do it, you are keeping around all the smaller dataframes, even after concatenating them.
Dask might be good option to try for handling large dataframes - Go through Dask Docs
You can store your individual dataframes in a HDF Store, and then call the store just like one big dataframe.
Another option: 1) Write  to .csv file: 2) Open .csv file, then append :3) Repeat Step 2 with 
I'm grateful to the community for their answers. However, in my case, I found out that the problem was actually due to the fact that I was using 32 bit Python. There are memory limits defined for Windows 32 and 64 bit OS. For a 32 bit process, it is only 2 GB. So, even if your RAM has more than 2GB, and even if you're running the 64 bit OS, but you are running a 32 bit process, then that process will be limited to just 2 GB of RAM - in my case that process was Python.I upgraded to 64 bit Python, and haven't had a memory error since then!Other relevant questions are: Python 32-bit memory limits on 64bit windows, Should I use Python 32bit or Python 64bit, Why is this numpy array too big to load?
I've had a similar performance issues while trying to concatenate a large number of DataFrames to a 'growing' DataFrame.My workaround was appending all sub DataFrames to a list, and then concatenating the list of DataFrames once processing of the sub DataFrames has been completed. This will bring the runtime to almost half.


Answer URL
https://docs.python.org/3/library/pickle.html#module-pickle
