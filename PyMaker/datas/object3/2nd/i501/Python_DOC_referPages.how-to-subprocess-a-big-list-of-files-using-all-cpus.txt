Natural Text
I need to convert 86,000 TEX files to XML using the LaTeXML library in the command line. I tried to write a Python script to automate this with the  module, utilizing all 4 cores.The script results in  and slows down my computer. From looking at Activity Monitor, it looks like this script is trying to create 86,000 conversion subprocesses at once, and each process is trying to open a file. Maybe this is the result of  -- maybe I need to not use map in conjunction with , since I just have too many commands to call? What would be an alternative?I've spent all day trying to figure out the right way to approach bulk subprocessing. I'm new to this part of Python, so any tips for heading in the right direction would be much appreciated. Thanks!
In , the  statements spawns a  subprocess.Without a blocking call such as , the  ends even while  continues to run in the background.Since  ends, the Pool sends the associated worker process another task to run and so  is called again.Once again another  process is spawned in the background.Pretty soon, you are up to your eyeballs in  processes and the resource limit on the number of open files is reached.The fix is easy: add  to tell  to wait until the  process has finished.Regarding :As martineau pointed out, there is a warning in the multiprocessing docs thatcode that spawns new processes should not be called at the top level of a module.Instead, the code should be contained inside a  statement.In Linux, nothing terrible happens if you disregard this warning.But in Windows, the code "fork-bombs". Or more accurately, the codecauses an unmitigated chain of subprocesses to be spawned, because on Windows  is simulated by spawning a new Python process which then imports the calling script. Every import spawns a new Python process. Every Python process tries to import the calling script. The cycle is not broken until all resources are consumed.So to be nice to our Windows-fork-bereft brethren, useSometimes processes require a lot of memory. The only reliable way to free memory is to terminate the process.  tells the  to terminate each worker process after it completes 1 task. It then spawns a new worker process to handle another task (if there are any). This frees the (memory) resources the original worker may have allocated which could not otherwise have been freed.In your situation it does not look like the worker process is going to require much memory, so you probably don't need .In , the  statements spawns a  subprocess.Without a blocking call such as , the  ends even while  continues to run in the background.Since  ends, the Pool sends the associated worker process another task to run and so  is called again.Once again another  process is spawned in the background.Pretty soon, you are up to your eyeballs in  processes and the resource limit on the number of open files is reached.The fix is easy: add  to tell  to wait until the  process has finished.The  affects how many tasks a worker performs before sending the result back to the main process. Sometimes this can affect performance, especially if interprocess communication is a signficant portion of overall runtime.In your situation,  takes a relatively long time (assuming we wait until  finishes) and it simply returns . So interprocess communication probably isn't a significant portion of overall runtime. Therefore, I don't expect you would find a significant change in performance in this case (though it never hurts to experiment!).In plain Python,  should not be used just to call a function multiple times.For a similar stylistic reason, I would reserve using the  methods for situations where I cared about the return values.So instead ofyou might consider usinginstead.The iterable passed to any of the  functions is consumedimmediately.  It doesn't matter if the iterable is an iterator. There is nospecial memory benefit to using an iterator here.  returns aniterator, but it does not handle its input in any especially iterator-friendlyway. No matter what type of iterable you pass, upon calling the  function the iterable isconsumed and turned into tasks which are put into a task queue.Here is code which corroborates this claim:version1.py:version2.py:Running  and  both produce the same result. Crucially, you will notice that  is printed 10 times very quickly atthe beginning of the run, and then there is a long pause (while the calculationis done) before the program ends.If the generator  were somehow consumed slowly by ,we should expect  to be printed slowly as well. Since  isprinted 10 times and quickly, we can see that the iterable  is beingcompletely consumed well before the tasks are completed.Running these programs should hopefully give you confidence that and  are putting tasks in the queueessentially in the same way: immediate after the call is made.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#programming-guidelines
https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming
