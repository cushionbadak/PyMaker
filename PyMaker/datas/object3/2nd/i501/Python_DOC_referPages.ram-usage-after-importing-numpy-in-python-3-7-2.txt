Natural Text
I run conda 4.6.3 with python 3.7.2 win32. In python, when I import numpy, i see the RAM usage increase by 80MB. Since I am using multiprocessing, I wonder if this is normal and if there is anyway to avoid this RAM overhead? Please see below all the versions from relevant packages (from conda list):python...........3.7.2 h8c8aaf0_2mkl_fft...........1.0.10 py37h14836fe_0mkl_random..1.0.2 py37h343c172_0numpy...........1.15.4 py37h19fb1c0_0numpy-base..1.15.4 py37hc3f5095_0thanks!
You can't avoid this cost, but it's likely not as bad as it seems. The  libraries (a copy of C only , plus all the Python  extension modules) occupy over 60 MB on disk, and they're all going to be memory mapped into your Python process on import; adding on all the Python modules and the dynamically allocated memory involved in loading and initializing all of them, and 80 MB of increased reported RAM usage is pretty normal.That said:The C libraries and Python extension modules are memory mapped in, but that doesn't actually mean they occupy "real" RAM; if the code paths in a given page aren't exercised, the page will either never be loaded, or will be dropped under memory pressure (not even written to the page file, since it can always reload it from the original DLL).On UNIX-like systems, when you  ( does this by default everywhere but Windows) that memory is shared between parent and worker processes in copy-on-write mode. Since the code itself is generally not written, the only cost is the page tables themselves (a tiny fraction of the memory they reference), and both parent and child will share that RAM.Sadly, on Windows,  isn't an option (unless you're running Ubuntu  on Windows, in which case it's only barely Windows, effectively Linux), so you'll likely pay more of the memory costs in each process. But even there, , the C library backing large parts of , will be remapped per process, but the OS should properly share that read-only memory across processes (and large parts, if not all, of the Python extension modules as well).Basically, until this actually causes a problem (and it's unlikely to do so), don't worry about it.
[NumPy]: NumPyis the fundamental package for scientific computing with Python.It is a big package, designed to work with large datasets and optimized (primarily) for speed. If you look in its __init__.py (which gets executed when importing it (e.g.: )), you'll notice that it imports lots of items (packages / modules):Those items themselves, may import othersSome of them are extension modules (.pyds (.dlls) or .sos) which get loaded into the current process (their dependencies as well)I've prepared a demo.code.py:Output:And the (before and after import) screenshots ([MS.Docs]: Process Explorer):As a personal remark, I think that ~80 MiB (or whatever the exact amount is), is more than decent for the current "era", which is characterized by ridiculously high amounts of hardware resources, especially in the memories area. Besides, that would probably insignificant, compared to the amount required by the arrays themselves. If it's not the case, you should probably consider moving away from numpy.There could be a way to reduce the memory footprint, by selectively importing only the modules containing the features that you need (my personal advice is against it), and thus going around __init__.py:You'd have to be an expert in numpy's internalsModules must be imported "manually" (by file name), using [Python 3]: importlib - The implementation of import (or alternatives)Their dependents will be imported / loaded as well (and because of this, I don't know how much free memory you'd gain)


Answer URL
https://docs.python.org/3/library/importlib.html#importlib.import_module
