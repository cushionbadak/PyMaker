Natural Text
I have a large csv file and I need to process every row to count some words. I need to use some MPI approach to distribute data processing among multiple process. Currently, I'm using scatter/gather in  library. The problem is that I need to create an array with length equal to number of the processes. But I get a memory error when creating the list for large row counts. Is there another way to transfer large data among these processes?
You bascially have the following options:Process the data in chunks, e.g. read 200k rows, scatter, collect results, repeat.Read the data locally, e.g.  of the file on each rank. This can be difficult to do efficiently. You cannot efficiently seek to a specific line in a csv file. So you have to separate the file by size,  to the position where you split it, find the next newline and work from there until the first newline after the end of your part of the file.Combine both.But then again, you could just process the file serially line by line, throwing away each line after you counted the words of it.P.S. Consider the  module.


Answer URL
https://docs.python.org/3/library/csv.html
