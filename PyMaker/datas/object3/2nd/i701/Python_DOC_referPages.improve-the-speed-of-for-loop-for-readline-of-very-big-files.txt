Natural Text
I am trying to improve the speed of my for loop for reading lines from very big files. I have two files, I am taking information from first file line by line in a  loop and matching these each to lines from a second file through an  statement. Since both the files have millions of lines, this is taking too long.I am posting my code here; How can I improve the loop statement to increase the speed of execution?Between 0 and 100 lines of the  file match the  filter.
You have a O(N * M) loop over file I/O, that is very slow indeed. You can improve per-line processing by using the  module to do parse each line into a list for you in C code, and drop the redundant  calls (you already have strings), but your real problem is the nested loop.You can easily avoid that loop. There may be millions of rows in your second file, but you already filter those rows to a much smaller number, between 0 and 100. That can be trivially held in memory and accessed per  value in next to no time.Store the information from each row in a dictionary; pre-parse the 2nd column integer, and store the whole row for output to the output file in the  list:After building that dictionary, you can look up matches against  in O(1) time, so your loop over  is a straightforward loop with a cheap operation for each row. When you find a match in the  dictionary, you only need to loop over the associated list to filter on the  integer values:I strongly recommend against storing the whole line from the  file, certainly not as a Python printable representation. I don't know how you plan to use that data, but at least consider using JSON to serialise the  list to a string representation. JSON is readable by other platforms and faster to parse back into a suitable Python data structure.


Answer URL
https://docs.python.org/3/library/functions.html#repr
