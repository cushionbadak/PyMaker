Natural Text
I have a script that takes in  and loops over them to generate an output file per input file, so this is a case which can be easily parallelized I think.I have a 8 core machine.I tried on using  flag on this command:But I can't make it work, i.e. specific question is: how to use parallel in bash with a python command in Linux, along with the arguments for the specific case mentioned above.There is a Linux parallel command (), which I read somewhere can do this job but I don't know how to use it.Most of the internet resources explain how to do it in python but can it be done in bash?Please help, thanks.I have a folder with 2 files, i just want to create their duplicates with a different name parallely in this example. 
You can just use an ordinary shell  command, and append the  background indicator to the python command inside the :Of course, assuming your python code will generate separate outputs by itself.It is just this simple. Although not usual - in general people will favor using Python itself to control the parallel execution of the loop, if you can edit the program. One nice way to do is to use  in Python to create a worker pool with 8 workers - the shell approach above will launch all instances in parallel at once.Assuming your code have a  function that takes in a filename, your Python code could be written as:This won't depend on special shell syntax, and takes care of corner cases, and number-or-workers handling, which could be hard to do  properly from bash.
Based on your comment,@Ouroborus no, no consider this opensource.com/article/18/5/gnu-parallel i want to run a python program along with this parallel..for a very specific case..if an arbitrary convert program can be piped to parallel ..why wouldn't a python program?I think this might help: wasn't chosen arbitrarily. It was chosen because it is a better known program that (roughly) maps a single input file, provided via the command line, to a single output file, also provided via the command line.The typical shell  loop can be used to iterate over a list. In the article you linked, they show an exampleThis (again, roughly) takes a list of file names and applies them, one by one, to a command template and then runs that command.The issue here is that  would necessarily wait until a command is finished before running the next one and so may under-utilize today's multi-core processors. acts a kind of replacement for . It makes the assumption that a command can be executed multiple times simultaneously, each with different arguments, without each instance interfering with the others.In the article, they show a command using that is equivalent to the previous  command. The difference (still roughly) is that  runs several variants of the templated command simultaneously without necessarily waiting for each to complete.For your specific situation, in order to be able to use , you would need to:Adjust your python script so that it takes one input (such as a file name) and one output (also possibly a file name), both via the command line.Figure out how to setup  so that it can receive a list of those file names for insertion into a command template to run your python script on each of those files individually.
It is unclear from your question how you run your tasks in serial. But if we assume you run:then the simple way to parallelize this would be:If you have a list of files with one line per file then use:It will run one job per cpu thread in parallel. So if  contains 1000000 names, then it will not run them all at the same time, but only start a new job when one finishes.


Answer URL
https://docs.python.org/3/library/multiprocessing.html?highlight=process
