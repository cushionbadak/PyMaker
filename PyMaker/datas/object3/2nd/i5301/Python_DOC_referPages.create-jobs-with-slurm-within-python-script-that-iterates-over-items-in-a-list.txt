Natural Text
Background:I wrote a python script to convert files from format to another. This code uses a text file () as input and iterates through source directory names listed in that text file (several hundred directories, each with thousands of files), converting their contents and storing them in a specified output directory.  Issue:To save time, I would like to use this script on a high performance cluster (HPC) and create jobs to convert the files in parallel, rather than sequentially iterating through each directory in the list.  I am new both to python and to HPCs. Our lab had previously written primarily in BASH and had not had access to an HPC environment, but we recently gained access to the HPC and the decision has been made to switch to Python, so everything is pretty new.  Question:Is there a module in python that will allow me to create jobs within the python script? I've found documentation on the multiprocessing and the subprocess python modules, but it isn't clear to me how I would use them. Or is there perhaps a different approach I should take? I've also read a number of posts here on stackoverflow about using slurm and python together, but I'm stymied with too much information and not enough knowledge to distinguish which thread to pick up. Any help is greatly appreciated.Environment:HPC: Red Hat Enterprise Linux Server release 7.4 (Maipo)python3/3.6.1slurm 17.11.2  Housekeeping part of the code: Part I'm stuck at: Edit 1:dcm2niix is the software used for conversion and is available on the HPC. It takes the following flags and paths  . Edit 2 (solution):
This is a possible solution for your code. It has not been tested.


Answer URL
https://docs.python.org/3/library/subprocess.html
https://docs.python.org/3/library/subprocess.html
