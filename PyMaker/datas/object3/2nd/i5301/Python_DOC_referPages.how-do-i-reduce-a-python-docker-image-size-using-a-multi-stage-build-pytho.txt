Natural Text
I am looking for a way to create multistage builds with python and Dockerfile:For example, using the following images: 1st image: install all compile time requirements, and install all needed python modules2nd image: copy all compiled / built packages from first image to the second, without the compilers themselves (gcc, postgers-dev, python-dev etc..)The final objcetive is to have a smaller image, running python and the python packages that I need. In short: how can I 'wrap' all the compiled modules (site-packages / external libs) that were created in the first image, and copy them in a 'clean' manner, to the 2nd image.
ok so my solution is using wheel, it lets us compile on first image, create wheel files for all dependencies and install them in the second image, without installing the compilersYou can see my answer regarding this in the following blog posthttps://galnevis.wixsite.com/website/single-post/2018/02/10/Python-and-Docker-multistage-build
I hope the following examples helpsyou can also tar them in the first building image and extract the image in the smaller base image.or create a official package like deb, rpm with only the minimum of files.maybe you only need base python and a venv which you could transfer as a tar etc.it is easier to find infos to make a clean package for your task - so you can google whats the best for your project and maybe install the package
The docs on this explain exactly how to do this.https://docs.docker.com/engine/userguide/eng-image/multistage-build/#before-multi-stage-buildsBasically you do exactly what you've said. The magic of multistage build feature though is that you can do this all from one dockerfile.ie:This builds a go binary, then the next image runs the binary. The first image has all the build tools and the seccond is just a base linux machine that can run a binary.


Answer URL
https://docs.python.org/3/library/zipapp.html
