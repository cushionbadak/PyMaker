Natural Text
I have a huge list of proxies (70k) and I have this script:And the operation of set(proxies) so the duplicates removing, is really slow. Is there a way to speed up this using threads?
No, threading won't speed this up, for three reasons:The Python GIL prevents Python code from being executed in parallel; threads executing Python code can only be run concurrently. For the same amount of CPU work, the same amount of time or more is required.To be able to add to the same datastructure from multiple threads, you'd have to add locking, slowing down threading more.Your code is slow because it is wasting cycles, because you are recreating the set object each iteration and then discarding it again. This is sucking up all the time as  continues to grow, so in the end you created sets for each size of , from length 1 all the way up to length 70k, approaching 5 million steps to throw away 70k sets.You should produce the set once. You can do so in a set comprehension:


Answer URL
https://docs.python.org/3/library/hashlib.html#hashlib.hash.update
