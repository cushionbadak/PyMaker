Natural Text
I've created the following function to pull data out of a file. It works ok, but gets very slow for larger files. The file might look like this (line numbers added for clarity):With the above example, lines 1234570 through 1234572 will be yielded.Since my files are large, there are a couple things I don't like about my function. First is that it reads the entire file into memory; I do this so I can use line indexing in order to parse the data out. Second is that the same lines in the file are iterated over many times- this gets very expensive for a large file. I have fiddled around trying to use iterators to get through the file a single time, but haven't been able to crack it. Any suggestions? 
If you only want a small portion of the file, I would use itertools.islice. This function will not store any data but the data you want in memory.Here's an example:If you use Python 3.3 or newer, you can also simplify this by using the  statement:This will not cache the lines you've already read from the file though. If you want to to this, I suggest that you store all read lines in a dictionary with the line number as key, and only pull the data from the file if needed. 
Out of left field a bit.  But if you have control over your files you could move the data to an sqlite3 db.  Also take a look at mmap and linecache.  I imagine these last two are just wrappers around random access files.  i.e. you could roll your own by scanning the files once, then building an index->offset lookup table and using seek.  Some of these approaches assume you have some control of the files you're reading? Also depends on whether you read a lot and write infrequently (if so building an index is not such a bad idea).


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.islice
https://docs.python.org/3/whatsnew/3.3.html#pep-380-syntax-for-delegating-to-a-subgenerator
