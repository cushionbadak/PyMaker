Natural Text
I'm building a web application for processing ~60,000 (and growing) large files, perform some analysis and return a "best guess" that needs to be verified by a user. The files will be refined by category to avoid loading every file, but I'm still left with a scenario where I might have to process 1000+ files at a time.These are large files that can take up to 8-9 seconds each to process, and in a 1000+ file situation it is impractical to have a user wait 8 seconds between reviews or 2 hours+ while the files are processed before hand.To overcome this, I've decided to use multiprocessing to spawn several workers, each of which will pick from a queue of files, process them and insert into an output queue. I have another method that basically polls the output queue for items and then streams them to the client when one becomes available.This works well, until a portion of the way through when the queue arbitrarily stops returning items. We're using gevent with Django and uwsgi in our environment and I'm aware that child process creation via multiprocessing in the context of gevent yields an undesired event loop state in the child. Greenlets spawned before forking are duplicated in the child. Therefore, I've decided to use gipc to assist in the handling of the child processes.An example of my code (I cannot post my actual code):I expect the output to show the current count of the queue after processing and yielding an item:However, the script only manages to yield a few items before failing:My question is: What exactly is happening? Why can't I  from the queue? How can I return the item I expect and avoid this?
What is the actual exception that is being thrown when you can't get an item? You're blindly catching all exceptions that could be thrown. In addition, why not just use  without a timeout as well? You immediately try again without doing anything else. Might as just make the call to get a block until an item is ready.With regards to the problem I think what is happening is that  is closing the pipes associated with your queue and thus breaking the queue. I expect an  is being thrown rather than . See this bug report for details.As an alternative, you could use a process pool, initiate the pool before any  stuff happens (meaning you don't have to worry about the event loop issue). Submit jobs to the pool using  and you should be fine. Your start function would look something like:


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.imap_unordered
