Natural Text
I'm fairly new to Python and especially new to working with large amounts of data. I'm working on a little fun project, which is effectively an upscale of something I've done before in another language.For now, I'm loading a sizeable (100mb+) text document, breaking it up into words and then determining the frequencies of what words follow each prefix (each prefix being one or more words). Fairly simple and fun to implement in Python, I ended up with something along the lines of:(the function name, its argument and the use of a global "samples" is just temporary while I work out the kinks)This works well, taking about 20 seconds or so to process a 60mb plaintext dump of top articles from Wikipedia. Stepping up the sample size (the "samples" variable) from 1 to even 4 or 5 however, greatly increases memory usage (as expected -- there are ~10 million words and for each, "samples" many words are sliced and joined into a new string). This quickly approaches and reaches the memory limit of 2 gigabytes.One method I've applied to alleviate this problem is to delete the initial words from memory as I iterate, as they are no longer needed (the list of words could simply be constructed as part of the function so I'm not modifying anything passed in).This does help, but not by much, and at the cost of some performance.So what I'm asking is if this kind of approach is fairly efficient or recommended, and if not, what would be more suitable? I may be missing some method to avoid redundantly creating strings and copying lists, seeing as most of this is new to me.I'm considering:Chunking the symbols/words list, processing, dumping the relationships to disk and combining them after the factWorking with something like Redis as opposed to keeping the relationships within Python the whole time at allCheers for any advice or assistance!
The main advice when working with large strings in Python in case you need to do lot of changes, is:Convert string to listDo all the work with listWhen finished, convert to strings backThe reason is that string in Python is immutable. Every action as  for example, forces Python to allocate new memory, copy needed string, and return it as new string object. Because of that, when you need to do many changes to string (change by index, splitting), you better work with lists, which are mutable.
Re. speeding up the process, I've found it useful to use try/except blocks to update hashtables. For example:Rather than using 'in' to check for a key and then updating/creating the key which would require another check for that key, the above code eliminates one check and thus works faster.
Use, , and do  on that iter, i.e. Read more Which is more efficient, a for-each loop, or an iterator? Although it's explained with Java, I think the concepts are same.


Answer URL
https://docs.python.org/3/library/shelve.html
