Natural Text
I'm trying to use numpy/pandas to constuct a sliding window style comparator. I've got list of lists each of which is a different length. I want to compare each list to to another list as depicted below:The window diff for lists[0] and lists[1] would give 25 using the following window sliding technique shown in the image below. Because lists[1] is the shorter path we shift it once to the right, resulting in 2 windows of comparison. If you sum the last row in the image below we get the total difference between the two lists using the two windows of comparison; in this case a total of 25. To note we are taking the absolute difference.The function should aggregate the total window_diff between each list and the other lists, so in this caseI wanted to know if there was a quick route to doing this in pandas or numpy. Currently I am using a very long winded process of for looping through each of the lists and then comparing bitwise by shifting the shorter list in accordance to the longer list. My approach works fine for short lists, but my dataset is 10,000 lists long and some of these lists contain 60 or so datapoints, so speed is a criteria here. I was wondering if numpy, pandas had some advice on this? ThanksSample problem data
Steps :Among each pair of lists from the input list of lists create sliding windows for the bigger array  and then get the absolute difference against the smaller one in that pair. We can use  to get those sliding windows.Get the total sum and store this summation as a pair-wise differentiation.Finally sum along each row and col on the  array from previous step and their summation is final output.Thus, the implementation would look something like this -For really heavy datasets, here's one method using one more level of looping - is inspired from .Sample input, output -
Just for completeness of @Divakar's great answer and for its application to very large datasets:It does not create unnecessary large datasets and updates a single vector while iterating only over the upper triangular array.It will still take a while to compute, as there is a tradeoff between computational complexity and larger-than-ram datasets. Solutions for larger than ram datasets often rely on iterations, and python is not great at it. Iterating in python over a large dataset is slow, very slow.Translating the code above to cython could speedup things a bit.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.combinations
