Natural Text
I would like to use multiprocessing pool with an iterator in order to execute a function in a thread splitting the iterator in N elements until the iterator is finish.My question, is this script the right way to do it? Is there a better way?Probably something is wrong with that script, because I got the following AssertionError at the 
If I have to guess what's primarily wrong with your code, I'd say it's in passing your  to your process function  - the way  works is to unpack the arguments passed to it, so your  function actually retreives  arguments instead of one argument with a list of  elements. This causes an immediate error before your process function even gets the chance to start. If you change your call to  it might start working... You also would be defeating the purpose of iterators and you just might convert your whole input iterator into a list and feed slices of  to  and be done with it.But you asked if there is a 'better' way to do it. While 'better' is a relative category, in an ideal world,  comes with a handy  (and ) method intended to consume iterables and spread them over the selected pool in a lazy fashion (so no running over the whole iterator before processing), so all you need to build are your iterator slices and pass it to it for processing, i.e.:(btw. your code wouldn't give you the last slice for all  sizes that are not multiples of 100)But... it would be wonderful if it actually worked as advertised. While a lot of it has been fixed over the years,  will still take a large sample of your iterator (far larger than the actual pool processes' number) when producing its own iterator, so if that's a concern you'll have to get down and dirty yourself, and you're on the right track -  is the way to go when you want to control how to feed your pool, you just need to make sure you feed your pool properly:And now your  iterator will be called only when the next 100 results are needed (when your  process function exits cleanly or not).UPDATE - The previously posted code had a bug in it on closing queues in the end if a captured result is desired, this one should do the job nicely. We can easily test it with some mock functions:Now we can intertwine them like:And the result is (of course, it will differ from system to system):Proving that our generator/iterator is used to collect data only when there's a free slot in the pool to do the work ensuring a minimal memory usage (and/or I/O pounding if your iterators ultimately do that). You won't get it much more streamlined than this. The only additional, albeit marginal, speed up you can obtain is to reduce the wait time (but your main process will then eat more resources) and to increase the allowed  size (at the expense of memory) which is locked to the number of processes in the above code - if you use  it will load 3 more iterator slices ensuring lesser latency in situations when a process ends and a slot in the pool frees up.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.apply_async
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.imap
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.imap_unordered
