Natural Text
I have an example dataset in a csv. It only has ~50K rows. I want to test performance of SQL queries against this dataset but 50K rows is too small for this purpose. What is the best way to take the existing csv and create a new one that is N times larger than the original one and each row is duplicated N times?For example if N = 5And the input csv is:The desired output csv would be:bash, python, or SQL solutions welcomed bash or python solution preferred because I am testing across multiple database platforms
Using bash:The  command displays the header.The  runs 5 times the command  that displays the content of  except the first line ( sets the offset to the second line).
Since you have not specified against which RDBMS you are planning to execute your SQL, I will give you a solution for PostgreSQL.First, you can copy your CSV data into PostgreSQL using the COPY command.Then you can use the generate_series function to expand your data like this (CTE csv is just for testing purposes):Adjust according to your needs. This one generates 99 times what's in CTE csv.
I have made small python script for the purpose:And call it: For better arguments validation/parsing use argsparse. 


Answer URL
https://docs.python.org/3/library/argparse.html
