Natural Text
I am working on a big data problem and am stuck with some concurrency and async io issues. The problem is as follows:1) Have multiple huge files (~4gb each x upto 15) which I am processing using ProcessPoolExecutor from concurrent.futures module this way :2) Now in each file, I want to go line by line, process line to create a particular json, group such 2K jsons together and hit an API with that request to get response. Here is the code:3) Now the problem, the number of lines in each file being really large and that API call blocking and bit slow to respond, the program is taking huge amount of time to complete.4) What I want to achieve is to make that API call async so that I can keep processing next batch of 2000 when that API call is happening.5) Things I tried till now : I was trying to implement this using asyncio but there we need to collect the set of future tasks and wait for completion using event loop. Something like this:6) I am really not understanding this because this is indirectly the same as previous as it waits to collect all tasks before launching them. Can someone help me with what should be the correct architecture of this problem ? How can I call the API async way, without collecting all tasks and with ability to process next batch parallely ?
I am really not understanding this because this is indirectly the  same as previous as it waits to collect all tasks before launching  them.No, you wrong here. When you create asyncio.Task with  it starts executing  coroutine immediately. This is how tasks in asyncio work:Output:Problem with your approach is that  is not actually asynchronous: it does large amount of CPU-related job without returning control to your asyncio event loop. It's a problem - function blocks event loop making impossible tasks to be executed.Very simple, but effective solution I think you can use - is to return control to event loop manually using  after a few amount of executing , for example, on reading each line:Upd:there will be more than millions of requests to be done and hence I am  feeling uncomfortable to store future objects for all of them in a  listIt makes much sense. Nothing good will happen if you run million parallel network requests. Usual way to set limit in this case is to use synchronization primitives like asyncio.Semaphore.I advice you to make generator to get  from file, and acquire  before adding new task and release it on task ready. You will get clean code protected from many parallel running tasks.This will look like something like this:


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor
https://docs.python.org/3/library/asyncio.html
https://docs.python.org/3/library/asyncio-task.html#asyncio.Task
https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore
