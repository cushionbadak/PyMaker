Natural Text
Using python's  module, the following contrived example runs with minimal memory requirements:Uncomment the creation of the  and you'll find that each spawned process allocates the memory for a copy of the ! This is a minimal example of a much larger project that I can't figure out how to workaround; multiprocessing seems to make a copy of everything that is global. I don't need a shared memory object, I simply need to pass in , and process it without the memory overhead of the entire program.Side observation: What's interesting is that  inside  gives the same value, suggesting that somehow that might not be copies...
Because of the nature of , any variables in the global namespace of your  module will be inherited by the child processes (assuming you're on a Posix platform), so you'll see the memory usage in the children reflect that as soon as they're created. I'm not sure if all that memory is really being allocated though, as far as I know that memory is shared until you actually try to change it in the child, at which point a new copy is made. Windows, on the other hand, doesn't use  - it re-imports the main module in each child, and pickles any local variables you want sent to the children. So, using Windows you can actually avoid the large global ending up copied in the child by only defining it inside an  guard, because everything inside that guard will only run in the parent process:Now, in Python 2.x, you can only create new  objects by forking if you're using a Posix platform. But on Python 3.4, you can specify how the new processes are created, by using contexts. So, we can specify the  context, which is the one Windows uses, to create our new processes, and use the same trick:If you need 2.x support, or want to stick with using  to create new  objects, I think the best you can do to get the reported memory usage down is immediately delete the offending object in the child:
What is important here is which platform you are targeting.Unix systems processes are created by using Copy-On-Write (cow) memory. So even though each process gets a copy of the full memory of the parent process, that memory is only actually allocated on a per page bases (4KiB)when it is modified.So if you are only targeting these platforms you don't have to change anything.If you are targeting platforms without cow forks you may want to use python 3.4 and its new forking contexts  and , see the documentationThese methods will create new processes which share nothing or limited state with the parent and all memory passing is explicit.But not that that the spawned process will import your module so all global data will be explicitly copied and no copy-on-write is possible. To prevent this you have to reduce the scope of the data.e.g top output with spawn:and with fork:note how its more than 100%, due to copy-on-write


Answer URL
https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#contexts-and-start-methods
https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#contexts-and-start-methods
https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#contexts-and-start-methods
