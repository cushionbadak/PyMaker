Natural Text
I was tracking down an out of memory bug, and was horrified to find that python's multiprocessing appears to copy large arrays, even if I have no intention of using them. Why is python (on Linux) doing this, I thought copy-on-write would protect me from any extra copying? I imagine that whenever I reference the object some kind of trap is invoked and only then is the copy made. Is the correct way to solve this problem for an arbitrary data type, like a 30 gigabyte custom dictionary to use a ? Is there some way to build Python so that it doesn't have this nonsense?Running:
I'd expect this behavior -- when you pass code to Python to compile, anything that's not guarded behind a function or object is immediately ed for evaluation.In your case,  has to be evaluated -- unless your actual code has different behavior,  not being called has nothing to do with it.  To see an immediate example:To apply this to your code, put  behind a function and call it as your need it, otherwise, Python is trying to be hepful by having that variable available to you at runtime.If  doesn't change, you could write a function that gets or sets it on an object that you keep around for the duration of the process. edit: Coffee just started working.  When you make a new process, Python will need to copy all objects into that new process for access.  You can avoid this by using threads or by a mechanism that will allow you to share memory between processes such as shared memory maps or shared ctypes
The problem was that by default Linux checks for the worst case memory usage, which can indeed exceed memory capacity. This is true even if the python language doesn't exposure the variables. You need to turn off "overcommit" system wide, to achieve the expected COW behavior.See https://www.kernel.org/doc/Documentation/vm/overcommit-accounting


Answer URL
https://docs.python.org/3/library/multiprocessing.html
