Natural Text
Requests is a really nice library. I'd like to use it for download big files (>1GB).The problem is it's not possible to keep whole file in memory I need to read it in chunks. And this is a problem with the following codeBy some reason it doesn't work this way. It still loads response into memory before save it to a file.UPDATEIf you need a small client (Python 2.x /3.x) which can download big files from FTP, you can find it here. It supports multithreading & reconnects (it does monitor connections) also it tunes socket params for the download task. 
With the following streaming code, the Python memory usage is restricted regardless of the size of the downloaded file:Note that the number of bytes returned using  is not exactly the ; it's expected to be a random number that is often far bigger, and is expected to be different in every iteration.See http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow for further reference.
It's much easier if you use  and :This streams the file to disk without using excessive memory, and the code is simple.
Your chunk size could be too large, have you tried dropping that - maybe 1024 bytes at a time? (also, you could use  to tidy up the syntax)Incidentally, how are you deducing that the response has been loaded into memory?It sounds as if python isn't flushing the data to file, from other SO questions you could try  and  to force the file write and free memory;
Not exactly what OP was asking, but... it's ridiculously easy to do that with :Or this way, if you want to save it to a temporary file:I watched the process:And I saw the file growing, but memory usage stayed at 17 MB. Am I missing something?


Answer URL
https://docs.python.org/3/library/shutil.html#shutil.copyfileobj
