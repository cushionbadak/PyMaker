Natural Text
I'm currently using a 4 Giga-byte sized file as Open-addressing Hash table.In order to read each offset I'm using file.seek() function for a 1-byte (char) data. I want to optimize the size of the file using buckets (saving up space on offsets that have no data), for the optimization to be best I want to know how much bytes are cached into memory when ever I am using file.seek()?That way I can tune the buckets so the file will require less space but the disk I/O reads won't increase.
 approach will be very memory efficient but also very slow. You will want to align everything by the page boundary though, thus I suggest that you do not cross the 4 kiB boundaries.Instead of using , if you are using 64-bit processor, map the entire file in memory using . Then you can use the rule that pages are usually 4 kiB in size, thus aligning everything on the 4 kiB boundary. This is most certainly faster than dummily using ; though it might end up consuming more memory, the operating system can fine tune to your access patterns.On Python 3 you would use  as follows:


Answer URL
https://docs.python.org/3/library/mmap.html
