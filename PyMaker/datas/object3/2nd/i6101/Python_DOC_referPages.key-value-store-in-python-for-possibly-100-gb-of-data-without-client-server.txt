Natural Text
There are many solutions to serialize a small dictionary: /, , , , or even by using .But when dealing with possibly 100 GB of data, it's not possible anymore to use such modules that would possibly rewrite the whole data when closing / serializing. is not really an option because it uses a client/server scheme.Question: Which key:value store, serverless, able to work with 100+ GB of data, are frequently used in Python?I'm looking for a solution with a standard "Pythonic"  syntax:Note: BsdDB (BerkeleyDB) seems to be deprecated. There seems to be a LevelDB for Python, but it doesn't seem well-known - and I haven't found a version which is ready to use on Windows. Which ones would be the most common ones?
You can use sqlitedict which provides key-value interface to SQLite database.SQLite limits page says that theoretical maximum is 140 TB depending on  and . However, default values for Python 3.5.2-2ubuntu0~16.04.4 ( 2.6.0), are  and . This gives ~1100 GB of maximal database size which fits your requirement.You can use the package like:UpdateAbout memory usage. SQLite doesn't need your dataset to fit in RAM. By default it caches up to  pages, which is barely 2MiB (the same Python as above). Here's the script you can use to check it with your data. Before run:sqlitedct.pyRun it like . In my case it produces this chart:And database file:
I would consider HDF5 for this.  It has several advantages:Usable from many programming languages.Usable from Python via the excellent h5py package.Battle tested, including with large data sets.Supports variable-length string values.Values are addressable by a filesystem-like "path" ().Values can be arrays (and usually are), but do not have to be.Optional built-in compression.Optional "chunking" to allow writing chunks incrementally.Does not require loading the entire data set into memory at once.It does have some disadvantages too:Extremely flexible, to the point of making it hard to define a single approach.Complex format, not feasible to use without the official HDF5 C library (but there are many wrappers, e.g. ).Baroque C/C++ API (the Python one is not so).Little support for concurrent writers (or writer + readers).  Writes might need to lock at a coarse granularity.You can think of HDF5 as a way to store values (scalars or N-dimensional  arrays) inside a hierarchy inside a single file (or indeed multiple such files).  The biggest problem with just storing your values in a single disk file would be that you'd overwhelm some filesystems; you can think of HDF5 as a filesystem within a file which won't fall down when you put a million values in one "directory."
First, bsddb (or under it's new name Oracle BerkeleyDB) is not deprecated.From experience LevelDB / RocksDB / bsddb are slower than wiredtiger, that's why I recommend wiredtiger.wiredtiger is the storage engine for mongodb so it's well tested in production. There is little or no use of wiredtiger in Python outside my AjguDB project; I use wiredtiger (via AjguDB) to store and query wikidata and concept which around 80GB.Here is an example class that allows mimick the python2 shelve module. Basically,it's a wiredtiger backend dictionary where keys can only be strings:Here the adapted test program from @saaj answer:Using the following command line:I generated the following diagram:When write-ahead-log is active:This is without performance tunning and compression.Wiredtiger has no known limit until recently, the documentation was updated to the following:WiredTiger supports petabyte tables, records up to 4GB, and record numbers up to 64-bits.http://source.wiredtiger.com/1.6.4/architecture.html


Answer URL
https://docs.python.org/3/library/contextlib.html#contextlib.closing
