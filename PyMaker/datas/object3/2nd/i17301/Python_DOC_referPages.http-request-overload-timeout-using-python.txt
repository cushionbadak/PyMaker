Natural Text
I have a python script running which basically requests for a 1000 urls over http and logs their response. Here is the function that downloads the url page.Now this runs smoothly for 100-200 urls in the pipeline but after that the response starts to get very slow and ultimately the response just times out. I am, guessing this is because of the request overload. Is there some efficient way to do this without overloading requests?
I don't know where the issue comes from, but if it is related to having too much requests in the same process, you could try multiprocessing, as a workaround.It may also speed up the whole thing, as you can do multiple tasks at the same time (for instance, a process downloading while another is writing to the disk, â€¦). I did this for a similar thing, it was really better (increased total download speed too)


Answer URL
