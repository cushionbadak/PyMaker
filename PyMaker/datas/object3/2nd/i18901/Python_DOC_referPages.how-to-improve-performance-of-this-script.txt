Natural Text
With the help of python community I started learning python to process around 500 millions (40G) of data and wrote the following script. Input File Format -Output ReportPython ScriptYou can see the output file (report) is ordered by date and also concatenation of the fields. I am spending more time on sorting the datetime column (maybe). I am a new comer to Python. I really appreciate any help in improving my script to reduce the processing time. Hope I am making sense. FYI : I am making sure the input file is sorted by studentid and processing in batches.
I can't even imagine wanting to do this with any ammount of profiling or algorythm optimization.  This strikes me as a database problem.  load data in to a database (sqlite comes with python)add the date indexdump your outputpython is the glue language to do the formatting and parsing.  Not the language to roll your own management of 40G of data.
I think the best thing you can do is a pigeonhole sort. The this works if you know the bounds and spread of your time data, i.e. about the biggest and smallest times. Pigeonholing works by segregating your data into discrete pigeonholes. For example, if your data is distributed about the month then you could have an array where every index is an array representing 1 hour of every day of that month. You walk the list, placing your data into that array. When you place the data into that array, then you can sort it into that sublist.This algorithm is super efficient IF you choose your pigeonholes properly. For example, if all your data was within the same day and your pigeon hole was by day then this algorithm performs no better than yours. Another note is that you pigeonholes don't have to be evenly spread out. If almost all your data is in the same hour on different days, then add smaller time steps in that hour and bigger ones for the rest.Counting sort is another option (also has a better runtime), but since your file is so big I think you might run into memory issues or lag from constant disk writes. Worth looking into but be careful with those delays.EDIT: Just realized that your file is 40g big so to account for that memory issue, instead of an array of arrays, you could have a series of files with known names that contain your pigeonholed lists.


Answer URL
