Natural Text
I am generating 100 random integers and I want to store them in a sorted array. The first approach that I tried was using a binary search to find the proper index to store each number at and then insert the number at that index. This way, after 100th random number, I will have a sorted array. Binary search has a time complexity of  and  method has a time complexity of  so the final Big-O should be  right?Below is the code for this approach:When I printed the difference between elapsed and begin time, I got 4.2414 microseconds.My second approach was to simply add all random numbers in an array/list and then use the built in  method to sort it. The time complexity for  method is . The elapsed time for this approach was 1.9407 microseconds. I don't understand that if the time complexities for both methods are same then what makes the second approach so much faster?
Your binary search insertion is O(N^2); each insertion has to move up to O(N) elements up one step to the right, and you do this N times. However, even if it was O(NlogN), the constant cost of the sorting code is far lower than your Python code could match.If you do want to stick to bisect insertion sorting, rather than re-invent the (admittedly simple)  wheel, do use the  module. This module comes with a C-optimised implementation.It even has  functions, which note:Keep in mind that the O(log n) search is dominated by the slow O(n) insertion step.Another tip: don't use wall-clock time to measure algorithms. Use the  module, which disables the garbage collector, uses the most accurate clock available, and runs your test multiple times to eliminate external factors.Next, don't include creating the random values, you don't want to time how fast those can be produced; produce one list, up front, and re-use it for all timings.Next, use a proper  function, yours is broken for any ; there is no need to test for  for example. The following avoids an off-by-one error and uses  as a parameter rather than a global:Use  to call it, the start and end values are filled in for you.Now you can conduct a proper timed test:As you can see, with larger input lists, the  function is lagging further and further behind in performance.
First, neither version of your code is O(Nlog(N)). Your version with the binary search isn't, due to the  calls. Your version with  isn't either, because you're calling  on every insertion!The  calls win because  is implemented in C. Interpreted Python code has a lot of overhead that the C implementation of  gets to avoid. Also,  is smart enough to take advantage of existing order in the input, so it doesn't need to go through the full effort of an O(Nlog(N)) sort every time.


Answer URL
https://docs.python.org/3/library/bisect.html
https://docs.python.org/3/library/bisect.html#bisect.insort_left
https://docs.python.org/3/library/timeit.html
