Natural Text
I'm trying to write a huge dataset ~146m rows to CSV. I've tried this:This code works, but it eats all my RAM and aborts. How can I optimize it?
 will consume the whole iterable before submitting parts of it to the pool's workers. That's why you get memory issues.You should use  instead in order to avoid this. See this post for a thorough explanation.That being said, I sincerely doubt that multiprocessing will speed up your program in the way you wrote it since the bottleneck is disk I/O. Opening, appending, and closing the file over and over again is hardly quicker than one sequential write. Parallel writing to a single file is just not possible.Assuming that the generation of  takes some time, there could be a speedup if you write your program like this:I assume you don't care about order, otherwise you wouldn't have chosen multiprocessing in the first place, right?This way it's not the main process which generates the lists of rows but the worker processes. As soon as one worker process has finished a list, it will return it to the main process which then will append its entries to the file. The worker then fetches a new row and starts building another list.It might also be better to use more pandas functionality in the program in general (I assume you're using pandas dataframes because of ). For example you could create a new Dataframe instead of a list of rows and make  compatible to  objects so you don't have to call it on every entry.
Try to write the data in terms of chunks.Read your data frame ( assuming you writing from a data frame) in parts i.e in terms of some chunks.Write each chunk once,this performs quite faster.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.imap
