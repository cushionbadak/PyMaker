Natural Text
I am following this book http://doughellmann.com/pages/python-standard-library-by-example.htmlAlong with some online references.  I have some algorithm setup for multiprocessing where i have a large array of dictionaries and do some calculation.  I use multiprocessing to divide the indexes on which the calculations are done on the dictionary.  To make the question more general, I replaced the algorithm with just some array of return values.  From finding information online and other SO, I think it has to do with the join method.  The structure is like so, Generate some fake data, call the manager function for multiprocessing, create a Queue, divide data over  the number of index.  Loop through the number of processes to use, send each process function the correct index range.  Lastly join the processes and print out the results.What I have figured out, is if the function used by the processes is trying to return a range(0,992), it works quickly, if the range(0,993), it hangs.  I tried on two different computers with different specs. The code is here:Is there something about these numbers specifically or am I just missing something basic that has nothing to do with these numbers?From my searches, it seems this is some memory issue with the join method, but the book does not really explain how to solve this using this setup.  Is it possible to use this structure (i understand it mostly, so it would be nice if i can continue to use this) and also pass back large results.  I know there are other methods to share data between processes, but thats not what I need, just return the values and join them to one array once completed.
I can't reproduce this on my machine, but it sounds like items in  into the queue haven't been flushed to the underlying pipe. This will cause a deadlock if you try to terminate the process, according to the docs:As mentioned above, if a child process has put items on a queue (and  it has not used JoinableQueue.cancel_join_thread), then that process  will not terminate until all buffered items have been flushed to the  pipe. This means that if you try joining that process you may get a  deadlock unless you are sure that all items which have been put on the  queue have been consumed. Similarly, if the child process is  non-daemonic then the parent process may hang on exit when it tries to  join all its non-daemonic children.If you're in this situation. your  calls will hang forever, because there's still buffered data in the queue. You can avoid it by consuming from the queue before you join the processes:This doesn't affect the way the code works, each  call will block until the result is placed on the queue, which has the same effect has calling  on all processes prior to calling . The only difference is you avoid the deadlock.


Answer URL
https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#pipes-and-queues
