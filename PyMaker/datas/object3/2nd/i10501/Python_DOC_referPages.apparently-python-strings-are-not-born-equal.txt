Natural Text
I'm trying to wrap my brain around the 'text encoding standards'. When interpreting a bunch of bytes as 'text', one has to know which 'encoding sheme' applies. Possible candidates that I know of:ASCII: Very basic encoding scheme, supports 128 characters.CP-1252: Windows encoding scheme for the Latin alphabet. Also known as 'ANSI'.UTF-8: A coding scheme for the Unicode table (1.114.112 characters). Represents each character with one byte if possible, more bytes if needed (max. 4 bytes).UTF-16: Another coding scheme for the Unicode table (1.114.112 characters). Represents each character with min 2 bytes, max 4 bytes.UTF-32: Yet another coding scheme for the Unicode table. Represents each character with 4 bytes.. . .Now I would expect that Python consistently uses one encoding scheme for its built-in String type. I did the following test, and the result makes me shiver. I start to believe that Python is not consistently sticking to one encoding scheme to store its Strings internally. In other words: Python Strings seem to be 'not born equal'..EDIT :I forgot to mention that I'm using Python 3.x . Sorry :-)1. The testI have two simple text files in a folder:  and . As you can guess, the first is encoded in the  encoding scheme, also known as . The latter is encoded in . In my test, I open each file and read out its content. I assign the content to a native Python String variable. Then I close the file. After that, I create a new file and write the content of the String variable to that file. Here is the code to do all that:2. The result I would expectLet us suppose that Python consistently sticks to one internal coding scheme - for example  - for all its Strings. Assigning other content to a String would lead to some sort of implicit conversion. Under these assumptions, I would expect both output files to be of the  type:3. The result I getThe result I get is this:From these results, I have to conclude that the String variable  somehow stores the encoding scheme it adheres to.Many people tell me in their answers:When no encoding is passed explicitly,  uses the preferred  system encoding both for reading and for writing.I just cannot wrap my brain around that statement. If open() uses the 'preferred system encoding' - say  as example - then both  outputs should be encoded in that way, wouldn't they?4. Questions..My test raises several questions to me:(1) When I open a file to read its content, how does Python know the encoding scheme of that file? I did not specify it when opening the file.(2) Apparently a Python String can adhere to any encoding scheme supported by Python. So not all Python Strings are born equal. How do you find out the encoding scheme of a particular String, and how do you convert it? Or how do you make sure your freshly created Python String is of the expected type?(3) When I create a file, how does Python decide in what encoding scheme the file will be created? I did not specify the encoding scheme when creating those files in my test. Nevertheless, Python made a different (!) decision in each case.5. Extra information (based on the comments to this question):Python version: Python 3.x (installed from Anaconda)Operating system: Windows 10Terminal: Standard Windows command prompt Some questions raised about the temporary variable . Apparently the instruction  does not work for the ANSI case. An exception is thrown. But in the python terminal window, I can simply type the variable name  and get the file content printed out.Encoding detection of files: Bottom right corner of Notepad++ for first check, online tool for double check: https://nlp.fi.muni.cz/projects/chared/The output files  and  do not exist at the start of the test. They are created at the very moment that I issue the  command with the  option.6. The actual files (for completeness):I got several comments encouraging me to share the actual files on which I'm doing this test. Those files were quite large, so I've trimmed them down and re-did the tests. Results are similar. Here are the files (PS: of course, my files contain source code, what else?):myAnsi.txtThe print statement of the  variable leads to the following exception:But just typing the name of the variable prints out the contents without problems:myUtf.txt
When no encoding is passed explicitly,  uses the preferred system encoding both for reading and for writing (not sure exactly how the preferred encoding is detected on Windows).So, when you write:All four files are opened using the same encoding, both for reading and for writing.You have to pass  or  if you want to be sure that files are opened using these encodings:(As a side note, I'm not a Windows expert, but I think you can write  instead of .)Apart from that, you have to consider that some characters are represented in the same way with different encodings. For example, the string  has the same representation in ASCII, CP-1252 or UTF-8. In general, you have to use some non-ASCII characters to see some differences:Not only that, but some byte strings can be perfectly valid in two distinct encodings, even though they can have different meanings, so that when you try to decode a file with the wrong encoding you don't get an error, but a weird string:For the record, Python uses UTF-8, UTF-16 or UTF-32 to represent strings internally. Python tries to use the "shortest" representation, even though UTF-8 and UTF-16 are used without continuation bytes, so that lookups are always O(1).In short, you have read two files using the system encoding and written two files using the same encoding (therefore without any transformation). The content of the files you have read are compatible with both CP-1252 and UTF-8.
CP-1252 is basically a byte for byte codec; it can decode arbitrary bytes, including the bytes from UTF-8 encoding. So effectively, assuming you're on Windows using a Western locale, where the default encoding provided to  is , if you never work with the string in Python, just read and write it, you may as well have just read and written in binary mode. You'd only see a problem if you tried to use the string in ways that exposed the problem.For example, consider the following test file with a single UTF-8 encoded character in it:The actual bytes in that file are .If you read that file in , it will happily do so, because every byte is a legal  byte:But it's not the string , it's what those two bytes happen to represent in :  (you can print them or check the length and you'll see that, assuming your console encoding handles non-ASCII)If you're just writing it back though, without using it, you'd never see this; the output step is (default) encoding  as , which restores the original bytes that you expect.Your problem is that most files are legal  text files (and it's possible Python will silently read unassigned bytes as equivalent Unicode ordinals; I know it does so for  for unassigned bytes like ), and when they're legal, reading as such and writing back in the same encoding is non-mutating.
To fully grasp the answer we need to look at the documentation a bit. Let's start with the open() function. According to the Python 3.* documentation open() returns a file object, and is most commonly used with two arguments: open(filename, mode). 1This means that we are dealing with a file object which could mean raw binary, buffered binary or in this case, text files 2.  But how can this text file object know it's encoding? Well again, according to the documentation A file object able to read and write str objects. Often, a text file actually accesses a byte-oriented datastream and handles the text encoding automatically.3So there we have it, it's handle automatically. And since both of those formats fall within the supported codecs. Python knows how to encode your files on writes given the file object. 
What you're hoping for is unfortunately impossible.Files do not contain encoding information and therefore it's impossible to read them as text without providing an encoding or assuming one.When doing things like  there is no one in the world that can tell for sure what encoding to use to translate the byte stream contained in the file into unicode points if the file contains characters beyond ASCII. Note that's even possible that a file doesn't contain text in a single encoding (just concatenate one encoded with  to another using  instead).IIRC russian locale for example can contain any byte from  to  an thus any file can be interpreted as containing russian locale text without decoding errors.There are libraries that try to solve this problem by statistical-based guessing, but they're just guessing, not telling for sure.Python 3 strings are unicode and therefore when reading a text file it applies a decoding using the "system default" unless provided a different one explicitly.If the system default is not the correct one but can decode all the 8-bit bytes contained you will just get wrong content silently. When writing back the string it will use the system default again thus rewriting the same bytes in output.This is what happened to you.If the system default encoding however is not able to decode the file content you will get a  exception. Error detection depends on the file content and the encodings used.For example reading  encoded in  as if it were an  content (default on my system) you get an error as the character  in  is  and that byte cannot be present in a valid  file preceded by  ().Doing the opposite however (i.e. reading  encoded in  as if it were ) will apparently "work" but will wrongly give as text  (i.e. an uppercase  with a tilde on top  and a non-breaking space ).


Answer URL
https://docs.python.org/3/tutorial/inputoutput.html
https://docs.python.org/3/glossary.html#term-file-object
https://docs.python.org/3/glossary.html#term-text-file
