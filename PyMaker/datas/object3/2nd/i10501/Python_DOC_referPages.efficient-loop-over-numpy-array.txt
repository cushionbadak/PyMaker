Natural Text
Versions of this question have already been asked but I have not found a satisfactory answer.Problem: given a large numpy vector, find indices of the vector elements which are duplicated (a variation of that could be comparison with tolerance). So the problem is ~O(N^2) and memory bound (at least from the current algorithm point of view). I wonder why whatever I tried Python is 100x or more slower than an equivalent C code.I tried using numpy iterators but they are even worse (~ x4-5)http://docs.scipy.org/doc/numpy/reference/arrays.nditer.htmlUsing N=10,000 I'm getting 0.1 sec in C, 12 sec in Python (code above), 40 sec in Python using np.nditer, 50 sec in Python using np.ndindex. I pushed it to N=160,000 and the timing scales as N^2 as expected.
Since the answers have stopped coming and none was totally satisfactory, for the record I post my own solution.It is my understanding that it's the assignment which makes Python slow in this case, not the nested loops as I thought initially. Using a library or compiled code eliminates the need for assignments and performance improves dramatically.TestsThe performance of compiled version (with @jit uncommented) is close to C code performance ~0.1 - 0.2 sec. Perhaps eliminating the last loop could improve the performance even further. The difference in performance is even stronger when using approximate comparison using eps while there is very little difference for the compiled version.This is ~ 200x difference. In the real code, I had to put both loops in the function as well as use a function template with variable types so it was a bit more complex but not very much.
Python itself is a highly-dynamic, slow, language. The idea in numpy is to use vectorization, and avoid explicit loops. In this case, you can use . You can start withNow, for example, to find the sum:To find the indices of i that are equal, you can doTiming
The obvious question is why you want to do this in this way. NumPy arrays are intended to be opaque data structures â€“ by this I mean NumPy arrays are intended to be created inside the NumPy system and then operations sent in to the NumPy subsystem to deliver a result. i.e. NumPy should be a black box into which you throw requests and out come results.So given the code above I am not at all suprised that NumPy performance is worse than dreadful.The following should be effectively what you want, I believe, but done the NumPy way:
This solution using the numpy_indexed package has complexity n Log n, and is fully vectorized; so not terribly different from C performance, in all likelihood.
As an alternative to Ami Tavory's answer, you can use a Counter from the collections package to detect duplicates. On my computer it seems to be even faster. See the function below which can also find different duplicates.
Approach #1You can simulate that iterator dependency criteria for a vectorized solution using a . This is based on  that dealt with multiplication involving . For performing the elementwise equality of each element in  against its all elements, we can use . Finally, we can use  to get the count, as it's supposed to be very efficient in summing purposes on boolean arrays.So, we would have a solution like so -If you only care about the count , we could have two more approaches as listed next.Approach #2We can avoid the use of the triangular matrix and simply get the entire count and just subtract the contribution from diagonal elements and consider just one of either lower of upper triangular regions by just halving the remaining count as the contributions from either ones would be identical.So, we would have a modified solution like so -Approach #3Here's an entirely different approach that uses the fact the count of each unique element plays a cumsumed contribution to the final total. So, with that idea in mind, we would have a third approach like so -
This runs in 8 ms compared to 18 s for your code and doesn't use any strange libraries. It's similar to the approach by @vs0, but I like  more.  It should be approximately O(N).
I wonder why whatever I tried Python is 100x or more slower than an equivalent C code.Because Python programs are usually 100x slower than C programs.You can either implement critical code paths in C and provide Python-C bindings, or change the algorithm. You can write an O(N) version by using a  that reverses the array from value to index.Edit:If you need to test against an eps with abs(vect[i] - vect[j]) < eps you can then normalize the values up to epsLike this:


Answer URL
https://docs.python.org/3/library/collections.html#collections.Counter
