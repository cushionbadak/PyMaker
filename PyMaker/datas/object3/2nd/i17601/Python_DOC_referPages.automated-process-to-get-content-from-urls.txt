Natural Text
I have a huge list of URL's from a client which I need to run through so i can get content from the pages. This content is in different tags within the page.I am looking to create an automated service to do this which I can leave running to complete.I want the automated process to load each page and get the content from particular html tags, then process some this content to ensure the html is correct.If possible I want to generate one XML or JSON file, but I can settle for an XML or JSON file per page.What is the best way to do this, preferably something I can run off a mac or a linux server.The list of URL's are to an external site.Is there something I can already use or an example somewhere which will help me.Thanks
This is a perfect application of BeautifulSoup, IMHO.  Here is a tutorial on a similar process. It is certainly a headstart.
Scrapy is an excellent framework for spidering and scraping.I think you'll find it will involve a little more learning overhead based on the Requests + Beautiful Soup or LXML tutorial mentioned by tim-cook in his answer. However if you're writing a lot of scraping / parsing logic it should guide toward a pretty well-factored (readiable, maintainable) codebase.So, if it's a one-off run I'd go with Beautiful Soup + Requests. If it'll be re-used, extended and maintained over time then Scrapy would be my pick.


Answer URL
