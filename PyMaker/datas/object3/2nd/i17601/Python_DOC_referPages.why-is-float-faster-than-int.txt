Natural Text
Experimenting with some code and doing some microbenchmarks I just found out that using the  function on a string containing an integer number is a factor 2 faster than using  on the same string.It gets even stranger when testing  which runtime is shorter than the bare .I tested the code under Windows 7 running cPython 2.7.6 and Linux Mint 16 with cPython 2.7.6. I have to add that only Python 2 is affected, Python 3 shows a way smaller (not remarkable) difference between the runtimes.I know that the information I get by such microbenchmarks are easy to misuse, but I'm curious why there is such a difference in the functions' runtime.I tried to find the implementations of  and  but I can not find it in the sources.
 has lots of bases.*, 0*, 0x*, 0b*, 0o* and it can be long, it takes time to determine the base and other thingsif the base is set, it saves a lot of timeas @Martijn Pieters metions the code the  and 
 has to account for more possible types to convert from than  has to. When you pass a single object to  and it is not already an integer, then various things are tested for:if it is an integer already, use it directlyif the object implements the  method, call it and use the resultif the object is a C-derived subclass of , reach in and convert the C integer value in the structure to an  object.if the object implements the  method, call it and use the resultif the object is a string, convert it to an integer with the base set to 10.None of these tests are executed when you pass in a base argument, the code then jumps straight to converting a string to an int, with the selected base. That’s because there are no other accepted types, not when there is a base given. As a result, when you pass in a base, suddenly creating an integer from a string is a lot faster:When you pass a string to , the first test made is to see if the argument is a string object (and not a subclass), at which point it is being parsed. There’s no need to test other types. So the  call makes a few more tests than  or . Of those tests, tests 1, 2, and 3 are quite fast; they are just pointer checks. But the fourth test uses the C equivalent of , which is relatively expensive. This has to test the instance, and the full MRO of the string, and there is no cache, and in the end it raises an , formatting an error message that no-one will ever see. All work that's pretty useless here.In Python 3, that  call has been replaced with code that is a lot faster. That's because in Python 3, there is no need to account for old-style classes so the attribute can be looked up directly on the type of the instance (the class, the result of ), and class attribute lookups across the MRO are cached at this point. No exceptions need to be created.  objects implement the  method, which is why  is faster; you never hit the  attribute test at step 4 as step 2 produced the result instead. If you wanted to look at the C code, for Python 2, look at the  method first. After parsing the arguments, the code essentially does this:The no-base case calls the  function, which does this:where  is essentially a wrapper for , so parsing the string with base 10.In Python 3,  was removed, leaving only , renamed to  on the Python side. In the same vein,  has replaced . So now we look at , and testing for a string is done with  instead of :So again when no base is set, we need to look at , which executes:Note the  call, this is the special method lookup implementation; it eventually uses , which uses a cache; since there is no  method that cache will forever return a null after the first MRO scan. This method also never raises an exception, it just returns either the requested method or a null.The way  handles strings is unchanged between Python 2 and 3, so you only need to look at the Python 2  function, which for strings is pretty straightforward:So for string objects, we jump straight to parsing, otherwise use  to look for actual  objects, or things with a  method, or for string subclasses. This does reveal a possible optimisation: if  were to first test for  before all those other type tests it would be just as fast as  when it comes to strings.  rules out a string subclass that has a  or  method so is a good first test. To address other answers blaming this on base parsing (so looking for a , ,  or  prefix, case insensitively), the default  call with a single  string argument does look for a base, the base is hardcoded to 10. It is an error to pass in a string with a prefix in that case:Base prefix parsing is only done if you explicitly set the second argument to :Because no testing is done for  the  prefix parsing case is just as fast as setting  explicitly to any other supported value:
This is not a full answer, just some data and observations.Profiling results from x86-64 Arch Linux, Python 2.7.14, on a 3.9GHz Skylake i7-6700k running Linux 4.15.8-1-ARCH.  : 0.0854 usec per loop.  : 0.196  usec per loop.  (So about a factor of 2)floatIDK why the heck Python is messing around with the x87 control word, but yes, the tiny  function really runs  and then reloads it into  as an integer return value with , but probably spends more of its time writing and checking the stack canary from .It's weird because  uses SSE2 () for FP math, not x87.  (The hot part with this input is mostly integer, but there are  and  in there.)  x86-64 code normally only uses x87 for  (80-bit float).   stands for David Gay's string to double.  Interesting blog post about how it works under the hood.Note that this function only takes 9% of the total run time.  The rest is basically interpreter overhead, compared to a C loop that called  in a loop and threw away the result.intNotice that  takes 13% of the total time for , vs. 30% of the total for .  That's about the same absolute time, and  is taking twice as much time.  Plus more functions taking more small chunks of time.I haven't figured out what  does, or what it's spending its time on.  7% of its cycle counts are charged to a  instruction near the start; i.e. loading a 16-byte arg that was passed by reference (as the 2nd function arg).  This may be getting more counts than it deserves if whatever stored that memory was slow to produce it.  (See this Q&A for more about how cycle counts get charge to instructions on out-of-order execution Intel CPUs where lots of different work is in flight every cycle.)  Or maybe it's getting counts from a store-forwarding stall if that memory was written recently with separate narrower stores.It's surprising that  is taking so much time.  From looking at the instruction profile within it, it's getting short strings, but not exclusively 1-byte strings.  Looks like a mix of  bytes and  bytes.  Might be interesting to set a breakpoint in gdb and see what args are common.The float version has a  (maybe looking for a  decimal point?), but no  of anything.  It's surprising that the  version has to redo a  inside the loop at all.The actual  function takes 2% of the total time, run from  (3% of the total time).  These are "self" times, not including their children, so allocating memory and deciding on the number base is taking more time than parsing the single digit.An equivalent loop in C would run ~50x faster (or maybe 20x if we're generous), calling  on a constant string and discarding the result.int with explicit baseFor some reason this is as fast as .The profile by function looks pretty similar to the  version, too.


Answer URL
https://docs.python.org/3/reference/datamodel.html#special-method-lookup
