Natural Text
What's going onI'm collecting data from a few thousand network devices every few minutes in Python 2.7.8 via package . I'm also using  so that I can access the (more efficient) Net-SNMP command .I'm trying to cut down how much memory my script uses. I'm running three instances of the same script which sleeps for two minutes before re-querying all devices for data we want. When I created the original script in  they would use less than 500MB when active simultaneously. As I've converted this over to Python, however, each instance hogs 4GB each which indicates (to me) that my data structures need to be managed more efficiently. Even when idle they're consuming a total of 4GB.Code ActivityMy script begins with creating a list where I open a file and append the hostname of our target devices as separate values. These usually contain 80 to 1200 names.From there I set up the SNMP sessions and execute the requestsBecause of how both SNMP packages behave, the device responses are parsed up into lists and stored into one giant data structure. For example,I'm having to iterate through each response, combine related data, then output each device's complete response. This is a bit difficult For example,Each device has a varying number of responses. I can't loop through expecting every device having a uniform arbitrary number of values to combine into a string to write out to a CSV.How I'm handling the dataI believe it is here where I'm consuming a lot of memory but I cannot resolve how to simplify the process while simultaneously removing visited data.Currently I'm creating a new dictionary, checking that the device response is not null, then appending the response value to a string that will be used to write out to the CSV file.The problem with this is that I'm essentially cloning the existing dictionary, meaning I'm using twice as much system memory. I'd like to remove values that I've visited in  when I move them to  so that I'm not using so much RAM. Is there an efficient method of doing this? Is there also a better way of reducing the complexity of my code so that it's easier to follow?The CulpritThanks to those who answered. For those in the future that stumble across this thread due to experiencing similar issues: the  package is the culprit behind the large use of system memory. The  function creates a thread for each host but does so all at once rather than putting some kind of upper limit. Since each instance of my script would handle up to 1200 devices that meant 1200 threads were instantiated and queued within just a few seconds. Using the  function was slower but still fast enough to suit my needs. The difference between the two was 4GB vs 250MB (of system memory use).
If the device responses are in order and are grouped together by host, then you don't need a dictionary, just three lists:
The memory consumption was due to instantiation of several workers in an unbound manner.I've updated fastsnmpy (latest is version 1.2.1 ) and uploaded it to  PyPi.  You can do a search from PyPi for 'fastsnmpy', or grab it  directly from my PyPi page here at FastSNMPyJust finished updating the docs, and posted them to the project page at fastSNMPy DOCSWhat I basically did here is to replace the earlier model of unbound-workers with a process-pool from multiprocessing. This can be passed in as an argument, or defaults to 1.You now have just 2 methods for simplicity. snmpwalk(processes=n) and snmpbulkwalk(processes=n)You shouldn't see the memory issue anymore. If you do, please ping me on github.
You might have an easier time figuring out where the memory is going by using a profiler: https://pypi.python.org/pypi/memory_profilerAdditionally, if you're already already tweaking the fastsnmpy classes, you can just change the implementation to do the dictionary based results merging for you instead of letting it construct a gigantic list first. How long are you hanging on to the session? The result list will grow indefinitely if you reuse it. 


Answer URL
https://docs.python.org/3/library/functions.html#zip
