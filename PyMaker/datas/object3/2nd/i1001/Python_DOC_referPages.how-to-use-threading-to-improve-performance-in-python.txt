Natural Text
I have a list of  that has around . And also a list of  that have around . For each sentence I want to extract  from  in the order of sentence and write it to output.For example, my python programme looks as follows.The output is;However, the order of the output is not important to me. i.e my output could also look likes follows (In other words the lists inside the  can be shuffled).However, due to to the length of my  and  this program is still quite slow.Is it possible to further improve the performance (in terms of time) using multithreading in python?
This answer will address improving performance without using concurrency.The way you structured your search you are looking for 13 million unique things in each sentence. You said it takes 3-5 minutes for each sentence and that the word lengths in  range from one to ten.I think you can improve the search time by making a set of  (either initially when constructed or from your list) then splitting each sentence into strings of one to ten (consecutive) words and testing for membership in the set. Example of a sentence split into 4 word strings:Process:Adapting an itertools recipe (pairwise) you can automate that process of making n-word strings from a sentence:Testing each sentence looks like thisI made a set of 13e6 random strings with 20 characters each to approximate .Testing a four or forty character string for membership in  consistently takes about 60 nanoseconds.  A one hundred word sentence has 955 one to ten word strings so searching that sentence should take ~60 microseconds.The first sentence from your example  has 195 possible concepts (one to ten word strings). Timing for the following two functions is about the same: about 140 microseconds for  and 150 microseconds for :So these are just approximations since I'm not using your actual data but it should speed things up quite a bit.After testing with your example data I found that  won't work if a concept appears twice in a sentence.So here it is all together with the concepts listed in the order they are found in each sentence.  The new version of  will take longer but the added time should be relatively small.  If possible would you post a comment letting me know how much longer it is than the original? (I'm curious).
Whether multi-threading will yield an actual performance increase, does not just depend on the implementation in Python and the amount of data, it also depends on the hardware executing the program. In some cases, where the hardware offers no advantage, multi-threading may end up slowing things down due to increased overhead.However, assuming you're running on a modern standard PC or better, you may see some improvement with multi-threading. The problem then is to set up a number of workers, pass the work to them and collect the results.Staying close to your example structure, implementation and naming:User @wwii asked about multiple threads not really affecting performance for cpu-bound problems. Instead of using multiple threads, accessing the same output variable, you could also use multiple processes, access a shared output queue, like this:More overhead still, but using CPU resources available on other cores as well.
Here are two solutions using concurrent.futures.ProcessPoolExecutor which will distribute the tasks to different processes. Your task appears to be cpu bound not i/o bound so threads probably won't help. has a  parameter: the docs say sending chunks of greater than one item of the iterable could be beneficial. The function would need to be refactored to account for that. I tested this with a function that would just return what it was sent but regardless of the chunksize I specified the test function only returned single items. Â¿go figure?One drawback with Multiprocessing is that the data must be serialized/pickled to be sent to the process which takes time and might be significant for a compiled regular expression that large - it might defeat the gains from multiple processes.I made a set of 13e6 random strings with 20 characters each to approximate your compiled regex.Pickling to an io.BytesIO stream takes about 7.5 seconds  and unpickling from a io.BytesIO stream takes 9 seconds. If using a multiprocessing solution, it may be beneficial to pickle the concepts object (in whatever form) to the hard drive once then have each process unpickle from the hard drive rather than pickling/unpickling on each side of the IPC each time  a new process is created, definitely worth testing - YMMV . The pickled set is 380 MB on my hard drive.When I tried some experiments with concurrent.futures.ProcessPoolExecutor I kept blowing up my computer because each process needed its own copy of the set and my computer just doesn't have enough ram.I'm going to post another answer dealing with the method of testing for concepts in sentences.


Answer URL
https://docs.python.org/3/library/io.html#io.BytesIO
