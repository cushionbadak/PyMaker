Natural Text
I'm using the subprocess module to start a subprocess and connect to it's output stream (stdout). I want to be able to execute non-blocking reads on its stdout. Is there a way to make .readline non-blocking or to check if there is data on the stream before I invoke ? I'd like this to be portable or at least work under Windows and Linux.here is how I do it for now (It's blocking on the  if no data is avaible):
, ,  won't help in this case.A reliable way to read a stream without blocking regardless of operating system is to use :
I have often had a similar problem; Python programs I write frequently need to have the ability to execute some primary functionality while simultaneously accepting user input from the command line (stdin). Simply putting the user input handling functionality in another thread doesn't solve the problem because  blocks and has no timeout. If the primary functionality is complete and there is no longer any need to wait for further user input I typically want my program to exit, but it can't because  is still blocking in the other thread waiting for a line. A solution I have found to this problem is to make stdin a non-blocking file using the fcntl module:In my opinion this is a bit cleaner than using the select or signal modules to solve this problem but then again it only works on UNIX...
Python 3.4 introduces new provisional API for asynchronous IO --  module. The approach is similar to -based answer by @Bryan Ward -- define a protocol and its methods are called as soon as data is ready:See "Subprocess" in the docs.There is a high-level interface  that returns  objects that allows to read a line asynchroniosly using  coroutine (with / Python 3.5+ syntax): performs the following tasks:start subprocess, redirect its stdout to a piperead a line from subprocess' stdout asynchronouslykill subprocesswait for it to exitEach step could be limited by timeout seconds if necessary.
Try the asyncproc module. For example:The module takes care of all the threading as suggested by S.Lott.
Use select & read(1).  For readline()-like:
You can do this really easily in Twisted. Depending upon your existing code base, this might not be that easy to use, but if you are building a twisted application, then things like this become almost trivial. You create a  class, and override the  method. Twisted (depending upon the reactor used) is usually just a big  loop with callbacks installed to handle data from different file descriptors (often network sockets). So the  method is simply installing a callback for handling data coming from .  A simple example demonstrating this behavior is as follows:The Twisted documentation has some good information on this.If you build your entire application around Twisted, it makes asynchronous communication with other processes, local or remote, really elegant like this. On the other hand, if your program isn't built on top of Twisted, this isn't really going to be that helpful. Hopefully this can be helpful to other readers, even if it isn't applicable for your particular application.
One solution is to make another process to perform your read of the process, or make a thread of the process with a timeout.Here's the threaded version of a timeout function:http://code.activestate.com/recipes/473878/However, do you need to read the stdout as it's coming in?Another solution may be to dump the output to a file and wait for the process to finish using p.wait().
Disclaimer: this works only for tornadoYou can do this by setting the fd to be nonblocking and then use ioloop to register callbacks. I have packaged this in an egg called tornado_subprocess and you can install it via PyPI:now you can do something like this:you can also use it with a RequestHandler
Existing solutions did not work for me (details below). What finally worked was to implement readline using read(1) (based on this answer). The latter does not block:Why existing solutions did not work:Solutions that require readline (including the Queue based ones) always block. It is difficult (impossible?) to kill the thread that executes readline. It only gets killed when the process that created it finishes, but not when the output-producing process is killed.Mixing low-level fcntl with high-level readline calls may not work properly as anonnn has pointed out.Using select.poll() is neat, but doesn't work on Windows according to python docs.Using third-party libraries seems overkill for this task and adds additional dependencies.
This version of non-blocking read doesn't require special modules and will work out-of-the-box on majority of Linux distros.
I add this problem to read some subprocess.Popen stdout.Here is my non blocking read solution:
Here is my code, used to catch every output from subprocess ASAP, including partial lines. It pumps at same time and stdout and stderr in almost correct order.Tested and correctly worked on Python 2.7 linux & windows.
Adding this answer here since it provides ability to set non-blocking pipes on Windows and Unix.All the  details are thanks to @techtonik's answer.There is a slightly modified version to be used both on Unix and Windows systems.Python3 compatible (only minor change needed).Includes posix version, and defines exception to use for either.This way you can use the same function and exception for Unix and Windows code.To avoid reading incomplete data, I ended up writing my own readline generator (which returns the byte string for each line).Its a generator so you can for example...
The select module helps you determine where the next useful input is.However, you're almost always happier with separate threads.  One does a blocking read the stdin, another does wherever it is you don't want blocked.
why bothering thread&queue?unlike readline(), BufferedReader.read1() wont block waiting for \r\n, it returns ASAP if there is any output coming in.
In my case I needed a logging module that catches the output from the background applications and augments it(adding time-stamps, colors, etc.).I ended up with a background thread that does the actual I/O. Following code is only for POSIX platforms. I stripped non-essential parts. If someone is going to use this beast for long runs consider managing open descriptors. In my case it was not a big problem.
I have created a library based on J. F. Sebastian's solution. You can use it.https://github.com/cenkalti/what
Working from J.F. Sebastian's answer, and several other sources, I've put together a simple subprocess manager. It provides the request non-blocking reading, as well as running several processes in parallel. It doesn't use any OS-specific call (that I'm aware) and thus should work anywhere.It's available from pypi, so just . Refer to the project page for examples and full docs.
EDIT: This implementation still blocks. Use J.F.Sebastian's answer instead.I tried the top answer, but the additional risk and maintenance of thread code was worrisome.Looking through the io module (and being limited to 2.6), I found BufferedReader. This is my threadless, non-blocking solution.
I recently stumbled upon on the same problemI need to read one line at time from stream ( tail run in subprocess )in non-blocking mode I wanted to avoid next problems: not to burn cpu, don't read stream by one byte (like readline did ), etcHere is my implementationhttps://gist.github.com/grubberr/5501e1a9760c3eab5e0ait don't support windows (poll), don't handle EOF,but it works for me well
This is a example to run interactive command in subprocess, and the stdout is interactive by using pseudo terminal. You can refer to: https://stackoverflow.com/a/43012138/3555925
My problem is a bit different as I wanted to collect both stdout and stderr from a running process, but ultimately the same since I wanted to render the output in a widget as its generated.I did not want to resort to many of the proposed workarounds using Queues or additional Threads as they should not be necessary to perform such a common task as running another script and collecting its output.After reading the proposed solutions and python docs I resolved my issue with the implementation below. Yes it only works for POSIX as I'm using the  function call. I agree that the docs are confusing and the implementation is awkward for such a common scripting task. I believe that older versions of python have different defaults for  and different explanations so that created a lot of confusion. This seems to work well for both Python 2.7.12 and 3.5.2. The key was to set  for line buffering and then  to process as a text file instead of a binary which seems to become the default when setting .ERROR, DEBUG and VERBOSE are simply macros that print output to the terminal.This solution is IMHO 99.99% effective as it still uses the blocking  function, so we assume the sub process is nice and outputs complete lines. I welcome feedback to improve the solution as I am still new to Python.
This solution uses the  module to "read any available data" from an IO stream. This function blocks initially until data is available, but then reads only the data that is available and doesn't block further.Given the fact that it uses the  module, this only works on Unix.The code is fully PEP8-compliant.
I also faced the problem described by Jesse and solved it by using "select" as Bradley, Andy and others did but in a blocking mode to avoid a busy loop. It uses a dummy Pipe as a fake stdin. The select blocks and wait for either stdin or the pipe to be ready. When a key is pressed stdin unblocks the select and the key value can be retrieved with read(1). When a different thread writes to the pipe then the pipe unblocks the select and it can be taken as an indication that the need for stdin is over. Here is some reference code:
I have the original questioner's problem, but did not wish to invoke threads. I mixed Jesse's solution with a direct read() from the pipe, and my own buffer-handler for line reads (however, my sub-process - ping - always wrote full lines < a system page size). I avoid busy-waiting by only reading in a gobject-registered io watch. These days I usually run code within a gobject MainLoop to avoid threads.The watcher isAnd the main program sets up a ping and then calls gobject mail loop.Any other work is attached to callbacks in gobject.
Here is a module that supports non-blocking reads and background writes in python:https://pypi.python.org/pypi/python-nonblockProvides a function,nonblock_read which will read data from the stream, if available, otherwise return an empty string (or None if the stream is closed on the other side and all possible data has been read)You may also consider the python-subprocess2 module,https://pypi.python.org/pypi/python-subprocess2which adds to the subprocess module. So on the object returned from "subprocess.Popen" is added an additional method, runInBackground. This starts a thread and returns an object which will automatically be populated as stuff is written to stdout/stderr, without blocking your main thread.Enjoy!


Answer URL
https://docs.python.org/3/library/queue.html#queue.Queue.get_nowait
https://docs.python.org/3/library/asyncio-subprocess.html
https://docs.python.org/3/library/asyncio-subprocess.html#process
https://docs.python.org/3/library/asyncio-stream.html#asyncio.StreamReader
