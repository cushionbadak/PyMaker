Natural Text
I just found this issue through debugging my code. I had a list of messages as strings that I was trying to concatenate together, and I wanted to add a newline to the end of every message.Approach 1:This was extremely slow - after around the 100,000th message, adding each message took about 2-3s, and around the 300,000th message, this process basically stopped.Approach 2:This approach finished concatenating all 1.6 million messages in less than a second.What I'm wondering is why is the second approach so much faster than the first?
 isn't a single operation that joins , , and  into a single string. It is two operations,  and , which means copying the contents of  twice; once to copy  into , and again when  gets copied into the result of . Since, in your example,  is the string that keeps getting longer, you are at best doubling the amount of data being copied at each step.The best approach is to avoid all the temporary str object created by , and use : operates with each string directly, without the need to iteratively append them to an initial empty string one at a time.  figures out, by scanning , how long the resulting string needs to be, allocates enough memory for it, then sequentially copies the data from each element of  into place one at a time.
Well, since  is executed as , one can see that the order of computation is the following:. This has to copy the huge string  because strings are immutable.. This has to copy the (even more) huge string  because strings are immutable.So, there are two huge copies involved, while in the second version,  (like in your second example), only one such copy is needed. The latter approach will obviously be faster.
Python's strings are immutable and contiguous. The former means they can't be modified, and the latter means they're stored in one place in memory. This is unlike e.g. a rope data structure, where appending data is a cheap operation that need only form a new node for the end. It means that the concatenation operation must copy both input strings each time, and with something like , since  is left associative, copies all of  twice. The usual solution is keeping all the small strings until the whole set is completed, and using  to perform the concatenations in one pass. This would only copy each component string once, instead of a geometric (proportional to square) number of times. Another option, to build a buffer as you go along, is to use . That will give you a file-like object, a bit like a  in some other languages, from which you can extract the final string. We also have operations like  that can accept iterables, so the join may not be needed at all. My guess as for why the second implementation managed to be so much faster (not just about twice as fast), is that there are optimizations in place that can sometimes permit CPython not to perform the copy of the left operand at all.  appears to have precisely such an optimization, based on , wherein it can mutate an object if the reference count is precisely 1, the string has never been hashed, and a few other conditions. This would typically apply to a local variable where you use , and presumably the compiler managed to generate such behaviour when there wasn't a second operator in the same assignment. 


Answer URL
https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str
https://docs.python.org/3/reference/expressions.html#evaluation-order
https://docs.python.org/3/library/stdtypes.html#str.join
https://docs.python.org/3/library/io.html#io.StringIO
https://docs.python.org/3/library/io.html#io.IOBase.writelines
https://docs.python.org/3/library/stdtypes.html#str.join
