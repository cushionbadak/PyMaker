Natural Text
If I have about 10+ million little tasks to process in python (convert images or so), how can I create queue and save progress in case of crash in processing. To be clear, how can I save progress or stop process whatever I want and continue processing from the last point.Also how to deal with multiple threads in that case?In general question is how to save progress on processed data to file. Issue if it huge amount of very small files, saving file after each iteration will be longer than processing itself...Thanks! (sorry for my English if its not clear)
First of I would suggest not to go for multi-threading. Use multi-processing instead. Multiple threads do not work synchronously in python due to GIL when it comes to computation intensive task. To solve the problem of saving result use following sequenceGet the names of all the files in a list and divide the list into chunks.Now assign each process one chunk.Append names of processed files after every 1000 steps to some file(say monitor.txt) on system(assuming that you can process 1000 files again in case of failure).In case of failure skip all the files which are saved in the monitor.txt for each process.You can have monitor_1.txt, monitor_2.txt ... for each process so you will not have to read the whole file for each process.Following gist might help you. You just need to add code for the 4th point. https://gist.github.com/rishibarve/ccab04b9d53c0106c6c3f690089d0229
I/O operations like saving files are always relatively slow. If you have to process a large batch of files, you will be stuck with a long I/O time regardless of the number of threads you use. The easiest is to use multithreading and not multiprocessing, and let the OS's scheduler figure it all out. The docs have a good explanation of how to set up threads. A simple example would beOne solution that might be faster is to create a separate process that does the I/O. Then you use a  to queue the files from the `data process thread', and let the I/O thread pick these up and process them one after the other.This way the I/O never has to rest, which will be close to optimal. I don't know if this will yield a big advantage over the threading based solution, but as is generally the case with concurrency, the best way to find out is to do some benchmarks with your own application.One issue to watch out for is that if the data processing is much faster, then the  can grow very big. This might have a performance impact, depending on your system amongst other things. A quick workaround is to pause the data processing if the queue gets to large.Remember to write all multiprocessing code in Python in a script with theguard, and be aware that some IDEs don't play nice with concurrent Python code. The safe bet is to test your code by executing it from a terminal.


Answer URL
https://docs.python.org/3/library/threading.html#threading.Thread
