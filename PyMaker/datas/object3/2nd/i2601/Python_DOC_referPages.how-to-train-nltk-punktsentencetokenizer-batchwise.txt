Natural Text
I am trying to split financial documents to sentences. I have ~50.000 documents containing plain English text. The total file size is ~2.6 GB. I am using NLTK's  with the standard English pickle file. I additionally tweaked it with providing additional abbreviations but the results are still not accurate enough. Since NLTK PunktSentenceTokenizer bases on the unsupervised algorithm by Kiss & Strunk (2006) I am trying to train the sentence tokenizer based on my documents, based on training data format for nltk punkt.Unfortunately, when running the code, I got an error, that there is not sufficient memory. (Mainly because I first concatenated all the files to one big file.)Now my questions are:How can I train the algorithm batchwise and would that lead to a lower memory consumption? Can I use the standard English pickle file and do further training with that already trained object?I am using Python 3.6 (Anaconda 5.2) on Windows 10 on a Core I7 2600K and 16GB RAM machine.
As described in the source code:Punkt Sentence TokenizerThis tokenizer divides a text into a list of sentences  by using an unsupervised algorithm to build a model for abbreviation  words, collocations, and words that start sentences. It must be  trained on a large collection of plaintext in the target language  before it can be used.It is not very clear what a large collection really means. In the paper, there are no information given about learning curves (when it is sufficiant to stop learning process, because enough data was seen). Wall Street Journal corpus is mentioned there (it has approximately 30 million words). So it is very unclear if you can simply trim your training corpus and have less memory footprints.There is also an open issue on your topic saying something about 200 GB RAM and more. As you can see there, NLTK has probably not a good implementation of the algorithm presented by Kiss & Strunk (2006).I cannot see how to batch it, as you can see in the function signature of -method (NLTK version 3.3):But there are probably more issues, e.g. if you compare the signature of given version 3.3 with the git tagged version 3.3, there is a new parameter  which might be helpful and indicates a possible batch-process or a possible merge with an already trained model:Anyway, I would strongly recommend not using NLTK's Punkt Sentence Tokenizer if you want to do sentence tokenization beyond playground level. Nevertheless, if you want to stick to that tokenizer, I would simply recommend using also the given models and not train new models unless you have a server with huge RAM memory.


Answer URL
https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files
