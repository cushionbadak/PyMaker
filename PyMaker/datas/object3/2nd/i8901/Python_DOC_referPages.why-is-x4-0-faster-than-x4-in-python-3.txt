Natural Text
Why is  faster than ? I am using CPython 3.5.2.I tried changing the power I raised by to see how it acts, and for example if I raise x to the power of 10 or 16 it's jumping from 30 to 35, but if I'm raising by 10.0 as a float, it's just moving around 24.1~4.I guess it has something to do with float conversion and powers of 2 maybe, but I don't really know.I noticed that in both cases powers of 2 are faster, I guess since those calculations are more native/easy for the interpreter/computer. But still, with floats it's almost not moving.  but  TigerhawkT3 pointed out that it doesn't happen outside of the loop. I checked and the situation only occurs (from what I've seen) when the base is getting raised. Any idea about that?
Why is  faster than  in Python 3*?Python 3  objects are a full fledged object designed to support an arbitrary size; due to that fact, they are handled as such on the C level (see how all variables are declared as  type in ). This also makes their exponentiation a lot more trickier and tedious since you need to play around with the  array it uses to represent its value to perform it. (Source for the brave. -- See: Understanding memory allocation for large integers in Python for more on s.) Python  objects, on the contrary, can be transformed to a C  type (by using ) and operations can be performed using those native types. This is great because, after checking for relevant edge-cases, it allows Python to use the platforms'  (C's , that is) to handle the actual exponentiation:where  and  are our original s as C s.For what it's worth: Python  for me is a factor  faster, and shows the inverse behaviour.The previous fact also explains the discrepancy between Python 2 and 3 so, I thought I'd address this comment too because it is interesting.In Python 2, you're using the old  object that differs from the  object in Python 3 (all  objects in 3.x are of  type). In Python 2, there's a distinction that depends on the value of the object (or, if you use the suffix ):The  you see here does the same thing s do, it gets safely converted into a C  when exponentiation is performed on it (The  also hints the compiler to put 'em in a register if it can do so, so that could make a difference):this allows for a good speed gain. To see how sluggish s are in comparison to s, if you wrapped the  name in a  call in Python 2 (essentially forcing it to use  as in Python 3), the speed gain disappears:Take note that, though the one snippet transforms the  to  while the other does not (as pointed out by @pydsinger), this cast is not the contributing force behind the slowdown. The implementation of  is. (Time the statements solely with  to see).[...] it doesn't happen outside of the loop. [...] Any idea about that?This is CPython's peephole optimizer folding the constants for you. You get the same exact timings either case since there's no actual computation to find the result of the exponentiation, only loading of values:Identical byte-code is generated for  with the only difference being that the  loads the float  instead of the int :So the times are identical.*All of the above apply solely for CPython, the reference implementation of Python. Other implementations might perform differently.
If we look at the bytecode, we can see that the expressions are purely identical. The only difference is a type of a constant that will be an argument of . So it's most certainly due to an  being converted to a floating point number down the line.Update: let's take a look at Objects/abstract.c in the CPython source code: calls , which is too long to paste here, so here's the link.It calls the  slot of , passing  as an argument.Finally, in  at line 686 of Objects/floatobject.c we see that arguments are converted to a C  right before the actual operation:
Because one is correct, another is approximation.


Answer URL
https://docs.python.org/3/c-api/float.html#c.PyFloat_AsDouble
