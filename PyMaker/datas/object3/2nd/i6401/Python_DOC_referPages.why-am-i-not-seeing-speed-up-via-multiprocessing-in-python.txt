Natural Text
I am trying to parallelize an embarrassingly parallel for loop (previously asked here) and settled on this implementation that fit my parameters:The reason I need to use a proxy object (shared between processes) is the first element in the shared proxy list  that is a list of large objects (each about 200-300MB). This  list usually has 5-25 elements. I typically need to run this program on a HPC cluster.Here is the question, when I run this program with 32 processes and 50GB of memory (num_repetitions=200, with datasets being a list of 10 objects, each 250MB), I do not see a speedup even by factor of 16 (with 32 parallel processes). I do not understand why - any clues? Any obvious mistakes, or bad choices? Where can I improve this implementation? Any alternatives?I am sure this has been discussed before, and the reasons can be varied and very specific to implementation - hence I request you to provide me your 2 cents. Thanks.Update: I did some profiling with cProfile to get a better idea - here is some info, sorted by cumulative time.The profiling info now sorted by More profiling info sorted by  info:Update 2The elements in the large list  I mentioned earlier are not usually as big - they are typically 10-25MB each. But depending on the floating point precision used, number of samples and features, this can easily grow to 500MB-1GB per element also. hence I'd prefer a solution that can scale.Update 3:The code inside holdout_trial_compare_datasets uses method GridSearchCV of scikit-learn, which internally uses joblib library if we set n_jobs > 1 (or whenever we even set it). This might lead to some bad interactions between multiprocessing and joblib. So trying another config where I do not set n_jobs at all (which should to default no parallelism within scikit-learn). Will keep you posted.
Based on discussion in the comments, I did a mini experiment, compared three versions of implementation:v1: basically as same as your approach, in fact, as  will unpack  immediately,  not involved here, data passed to worker with the internal queue of .v2: v2 made use , work function will receive a  object, it fetches shared data via a internal connection to a server process.v3: child process share data from the parent, take advantage of  system call.results taken on a 16 core E5-2682 VPC, it is obvious that v3 scales better: 
Looking at your profiler output I would say that the shared object lock/unlock overhead overwhelms the speed gains of multithreading. Refactor so that the work is farmed out to workers that do not need to talk to one another as much. Specifically, if possible, derive one answer per data pile and then act on the accumulated results. This is why Queues can seem so much faster: they involve a type of work that does not require an object that has to be 'managed' and so locked/unlocked. Only 'manage' things that absolutely need to be shared between processes. Your managed list contains some very complicated looking objects...A faster paradigm is: and then
If you do not need a complex shared object, then only use a list of the most simple objects imaginable.Then tell the workers to acquire the complex data that they can process in their own little world.Try:I hope that this makes sense. It should not take too much time to refactor in this direction. And you should see that {method 'acquire' of '_thread.lock' objects} number drop like a rock when you profile.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#proxy-objects
