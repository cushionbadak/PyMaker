Natural Text
Background: The POSIX-defined  data type is a signed integer of variable-size depending on the environment.  For 64-bit build environments it seems to be consistently a 64-bit off_t.  For 32-bit systems the size of off_t depends on the environment (usually controlled with _FILE_OFFSET_BITS and related).I'm using Python's ctypes to access some library calls for functions that use the  data type.  ctypes does not have a type for  so mapping such an API via a Structure or otherwise requires choosing some other types that are defined by ctypes, namely one of , , ,  and .In searching around for solutions by others I've seen all sorts, most of them guesses and limited to one of the environments, some of them flat-out wrong (using an unsigned type) and none of them portable.Is there a robust, portable way to determine the size of  for the build of a given Python interpreter?Thanks!
There is no support for this in a standard, and as you yourself point out, it is going to vary on 32 bit machines based on the flags that were used to compile the library, not Python. (That is, you need to know if the library routines you are using were compiled with -D_FILE_OFFSET_BITS=64 or not...)GCC seems to define  as  pretty consistently. So that's probably the "safest" choice. Or you could just go with 64 bits by default. In either case, an option to override the behavior and specify the number of bits might be valuable.Is there a function you can call that will return an ? Maybe you could fill a buffer with 0xFF, then  and  to see how many zeroes get written?


Answer URL
https://docs.python.org/3/library/ctypes.html
