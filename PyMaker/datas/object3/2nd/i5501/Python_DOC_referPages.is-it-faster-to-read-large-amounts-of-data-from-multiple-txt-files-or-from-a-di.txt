Natural Text
I am working on a personal project (using Python 3) that will retrieve weather information for any city in the United States.  My program prompts the user to enter as many city-state combinations as they wish, and then it retrieves the weather information and creates a weather summary for each city entered.  Behind the scenes, I'm essentially taking the State entered by the user, opening a .txt file corresponding to that State, and then getting a weather code that is associated with the city entered, which I then use in a URL request to find weather information for the city.  Since I have a .txt file for every state, I have 50 .txt files, each with a large number of city-weather code combinations.Would it be faster to keep my algorithm the way that it currently is, or would it be faster to keep all of this data in a dictionary? This is how I was thinking about storing the data in a dictionary:I'd be happy to provide some of my code or elaborate if necessary.Thanks!
If you have a large datafile, you will spend days shifting through the file and putting the values in the .py file. If it is a small file I would use a dictionary, but if it were a large file a .txt file.Other possible solutions are: sqlitepickleshelveOther ResourcesBasic data storage with Pythonhttps://docs.python.org/3/library/persistence.htmlhttps://docs.python.org/3/library/pickle.htmlhttps://docs.python.org/3/library/shelve.html
It almost certainly would be much faster to preload the data from the files, if you're using the same python process for many user requests. If the process handles just one request and exits, this approach would be slower and use more memory. For some number of requests between "one" and "many", they'd be about equal on speed.For a situation like this I would probably use sqlite, for which python has built-in support. It would be much faster than scanning text files without the time and memory overhead of loading the full dictionary.
It is probably not a very good idea to have a large amount of text files, because it will slow down in large or numerous director(y|ies) access. But If you have large data records, you might wish to choose an intermediate solution, in indexing one data file and load the index in a dictionary.


Answer URL
https://docs.python.org/3/library/pickle.html
https://docs.python.org/3/library/shelve.html
https://docs.python.org/3/library/persistence.html
https://docs.python.org/3/library/pickle.html
https://docs.python.org/3/library/shelve.html
