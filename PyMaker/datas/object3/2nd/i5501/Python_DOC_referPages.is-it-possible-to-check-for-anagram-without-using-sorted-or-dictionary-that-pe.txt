Natural Text
I'm currently studying and I was told not to use "magic" python tricks like sorted() or string splitting in python. I mistook this as only using arrays to check if a input string is an anagram. So while the first thing I thought of was to use a dictionary I did not pursue it because I thought it was banned. Using the dictionary I can count up the frequency of the letters by using the letter as a key and the count as the value and counting down (subtract) the frequency of letters encountered in the input string and doing a loop for the frequency dictionary to see if there's any zeros. So...with a false notion of the restriction set upon me, I created a nested loop like the following (problem assumes equal length, no space)Obviously this is undesirable as the big o notation is O(n^2) to the solution using dictionary which would be O(3n), two iterations to count the frequency, and a final iteration to check if any entry in the dictionary has a non zero frequency (which would mean this is not an anagram).So this is some comprehension problem on my end but instead of moving on, I thought to myself is it possible to produce a better performing anagram checker that is better than my solution O(n^2) without using dictionary and just relying on array/list?I had another idea but I stopped:1) Convert the list of string characters into a list of numbers - but this means I need to still loop over the reference character (original) to find the number position.It's been eating away at me, and I realize that I am overthinking such simple algorithm questions....but still curious if there is a solution meeting the criteria.
The pythonic way of answering this question would be using a  object:But since those are restricted, you can fall back to vanilla dictionaries (also known as hashmaps, a fundamental ingredient in so many efficient algorithms).A high level process is as follows. First, build a hashmap of counts for . Repeat the process for . Finally, compare the two hashmaps for equality.First, the helper function - And now, the driver - Complexity Analysis - building each hashmap takes O(N) time, and performing comparison is O(N) as well, since you have to, one, test if the keys are identical, and two, compare the values of corresponding keys. All in all, a linear algorithm.Since hashmaps and hash sets are so commonplace, you should not argue that this is restricted, unless you plan on implementing your own hash map with arrays and open addressing.And no, there is no efficient algorithm that does not rely on hashmaps or something more complicated. Not unless you go with viraptor's answer, which is basically an array version of a hashmap (!), but having a unique entry for every single character in the ASCII set. For example, the count for ASCII character 65 would be accessed with , and so on. So, you'd need to have an array big enough to fit every ASCII character.Things are manageable for just the ASCII letters, but the plot thickens when you consider other, broader encodings (unicode). In the end, it's a lot more space efficient to just use a hashmap. 
Here's an alternative method that works in linear time for "reasonable" length words. The Algorithm runs O(n) if you don't count the arbitrary precision multiplications.The logic, If you assign each letter to a prime number. The multiplication of these primes for 2 anagrams will be the same.I hope reduce doesn't count as a magic function.
It depends on the definition of your inputs. Do you want to handle all characters, or just the printable / latin-1 set? Do you care about theoretical complexity, or real performance?For the first question - if you don't care about characters encoding to more than one byte, you can create a list with 256 elements and instead of indexing into a dict, index that array instead. For each character add/remove  at a specific position in the list. It's the same complexity as your dict solution: . (counting in the array is  since it's got predefined size)For the second question - if you want to use characters without limit, you can do the same thing, but create a list with  elements and index by unicode character number. It's not going to be faster than the dictionary solution, but again - the complexity remains .
If magic functions are disallowed, you can build your own CounterThen you can simply fall back on cᴏʟᴅsᴘᴇᴇᴅ's solution


Answer URL
https://docs.python.org/3/library/collections.html#collections.Counter
