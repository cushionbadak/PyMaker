Natural Text
I tested this:With the following partial output:There are two calls for read syscall with different number of requested bytes.When I repeat the same using dd command, just one read syscall is triggered using the exact number of bytes requested.I have googled this without any possible explanation. Is this related to page size or any Python memory management?Why does this happen?
I did some research on exactly why this happens.Note: I did my tests with Python 3.5. Python 2 has a different I/O system with the same quirk for a similar reason, but this was easier to understand with the new IO system in Python 3.As it turns out, this is due to Python's BufferedReader, not anything about the actual system calls.You can try this code:If you try to strace this code, you will find:Our original file object was a BufferedReader:If we call  on this, then we throw away the BufferedReader portion and just get the FileIO, which is what talks to the kernel. At this layer, it'll read everything at once.So the behavior that we're looking for is in BufferedReader. We can look in  in the Python source, specifically the function . In our case, where the file has not yet been read from until this point, we dispatch to .Now, this is where the quirk we see comes from:Essentially, this will read as many full "blocks" as possible directly into the output buffer. The block size is based on the parameter passed to the  constructor, which has a default selected by a few parameters:So this code will read as much as possible without needing to start filling its buffer. This will be 65536 bytes in this case, because it's the largest multiple of 4096 bytes less than or equal to 65600. By doing this, it can read the data directly into the output and avoid filling up and emptying its own buffer, which would be slower.Once it's done with that, there might be a bit more to read. In our case, , so it needs to read at least 64 more bytes. But yet it reads 4096! What gives? Well, the key here is that the point of a BufferedReader is to minimize the number of kernel reads we actually have to do, as each read has significant overhead in and of itself. So it simply reads another block to fill its buffer (so 4096 bytes) and gives you the first 64 of these.Hopefully, that makes sense in terms of explaining why it happens like this.As a demonstration, we could try this program:With this, strace tells us:Sure enough, this follows the same pattern: as many blocks as possible, and then one more., in a quest for high efficiency of copying lots and lots of data, would try to read up to a much larger amount at once, which is why it only uses one read. Try it with a larger set of data, and I suspect you may find multiple calls to read.TL;DR: the BufferedReader reads as many full blocks as possible (64 * 4096) and then one extra block of 4096 to fill its buffer.EDIT:The easy way to change the buffer size, as @fcatho pointed out, is to change the  argument on :( ... )The optional buffering argument specifies the fileâ€™s desired buffer size: 0 means unbuffered, 1 means line buffered, any other positive value means use a buffer of (approximately) that size (in bytes). A negative buffering means to use the system default, which is usually line buffered for tty devices and fully buffered for other files. If omitted, the system default is used.This works on both Python 2 and Python 3.


Answer URL
https://docs.python.org/3/library/functions.html#open
