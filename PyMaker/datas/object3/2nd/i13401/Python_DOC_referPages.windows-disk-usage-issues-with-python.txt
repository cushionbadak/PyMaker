Natural Text
I am executing the python code that follows.I am running it on a folder ("articles") which has a couple hundred subfolders and 240,226 files in all.I am timing the execution. At first the times were pretty stable but went non-linear after 100,000 files. Now the times (I am timing at 10,000 file intervals) can go non_linear after 30,000 or so (or not). I have the Task Manager open and correlate the slow-downs to 99% Disk usage by python.exe. I have done gc-collect(). dels etc., turned off Windows indexing. I have re-started Windows, emptied the trash (I have a few hundred GBs free). Nothing helps, the disk usage seems to be getting more erratic if anything.Sorry for the long post - Thanks for the help
There are some things that could be optimized. At first, is you open files, close them as well. A  block will do that easily. BTW in Python 2  is a bad choice for a variable name, it is built-in function's name.You can remove one disc read by doing string comparisons instead of the glob. And last but not least:  spits out the results cleverly, so don't buffer them into a list, process everything inside one loop. This will save a lot of memory.That is what I can advise from the code. For more details on what is causing the I/O you should use profiling. See https://docs.python.org/2/library/profile.html for details.


Answer URL
https://docs.python.org/3/library/os.html#os.scandir
https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-471
https://docs.python.org/3/library/fnmatch.html
