Natural Text
TL;DRIs there a way to wait on multiple futures, and yield from them as they are completed in a given order?Long storyImagine you have two data sources. One gives you  mapping, the other gives you  mapping. You want to compute .There is too much data to just load it, but both data sources support paging/iterating and ordering by .So you write something likeand the same for age, and then you iterate over  and .What is wrong with that? What happens is:you request one page of names and agesyou get themyou process the data until you reach the end of a page, let's say, agesyou request another page of agesyou process the data until ...Basically, you are not waiting for any requests while you process data.You could use  to send all requests and wait for all data, but then:when the first pages arrives, you still wait for othersyou run out of memoryThere is  which allows you to send all requests and process pages as you get results, but you will get pages out of order, so you will not be able to do the processing.Ideally, there would be a function that would make the first request, and as the response comes, make the second request and yield results from the first at the same moment.Is that possible?
There are a lot of things going on in your question; I'll try to get to all of them.Is there a way to wait on multiple futures, and yield from them as they are completed in a given order?Yes. Your code can  or  any number of futures in sequence. If you are talking about s specifically and you want these tasks to be executing concurrently, they simply need to be assigned to the loop (done when you  or ) and the loop needs to be running.As for yielding from them in sequence, you can establish what that sequence is in the first place as you create the tasks. In a simple example where you have created all of the tasks/futures before you start to process their results, you could use a  to store the task futures and finally pull from the list:That example will only process one result at a time, but will still allow any of the scheduled getter tasks to continue doing their thing while it's waiting for the next one in the sequence to complete. If the processing order didn't matter as much and we wanted to process more than one potentially-ready result at a time, then we could use a  instead of a , with more than one  task ing the getter tasks from the . If we wanted to get even more advanced, we can do as Vincent suggests in a comment to your question and use an  instead of a . With such a queue, you can have a producer adding tasks to the queue running concurrently with the task-processing consumers.Using a , or  for sequencing futures for processing has a disadvantage though, and that's that you are only processing as many futures concurrently as you have running processor tasks. You could create a new processor task every single time you queued up a new future to be processed, but at that point, this queue becomes a completely redundant data structure because asyncio already gives you a queue-like object where every thing added gets processed concurrently: the event loop. For every task we schedule, we can also schedule its processing. Revising the above example:Now let's say that our getter might return multiple things (kind of like your scenario) and each of those things needs some processing. That brings me to a different asyncio design pattern: sub-tasks. Your tasks can schedule other tasks on the event loop. As the event loop is run, the order of your first tasks will still be maintained, but if any one of them ends up waiting on something, there's a chance one of your sub-tasks will get started in the midst of things. Revising the above scenario, we might pass the loop to our coroutine so the coroutine can schedule the tasks that processes its results:All these advanced patterns start tasks in the order you want, but might finish processing them in any order. If you truly need to process the results in the exact same order that you started getting those results, then stick to a single processor pulling result futures from a sequence or yielding from an .You'll also notice that to ensure tasks starting in a predictable order, I explicitly schedule them with . While  and  will happily take coroutine objects and schedule/wrap them as s, they have problems with scheduling them in a predictable order as of me writing this. See asyncio issue #432.OK, let's get back to your specific case. You have two separate sources of results, and those results need to be joined together by a common key, an . The patterns I mentioned for getting things and processing those things don't account for such a problem, and I don't know the perfect pattern for it off the top of my head. I'll go through what I might do to attempt this though.We need some objects to maintain the state of what we know and what we've done so far for the sake of correlating that knowledge as it grows.We know we'll be getting this information in "pages" via the iterators you mentioned, so let's start there in designing our tasks:Those coroutines schedule the name/age pair of an id to be processedâ€”more specifically, the name and age futures for an id. Once started, the processor will await both futures' results (joining them, in a sense).OK, we've got tasks for getting/iterating the pages and subtasks for processing what's in those pages. Now we need to actually schedule the getting of those pages. Back to your problem, we've got two datasources we want to pull from, and we want to pull from them in parallel. We assume the order of information from one closely correlates to the order of information from another, so we interleave the processing of both in the event loop.Now that we have scheduled the top level tasks, we can finally actually do the things: may not solve all of your parallel processing woes. You mentioned that the "processing" each page to iterate takes forever. If it takes forever because it's awaiting responses from a server, then this architecture is a neat lightweight approach to do what you need (just make sure the i/o is being done with asyncio loop-aware tools).If it takes forever because Python is crunching numbers or moving things around with CPU and memory, asyncio's single-threaded event loop doesn't help you much because only one Python operation is happening at a time. In this scenario, you may want to look into using  with a pool of Python interpreter processes if you'd like to stick with asyncio and the sub-task pattern. You could also develop a solution using the  library with a process pool instead of using asyncio.Note: The example generator you gave might be confusing to some because it uses  to delegate generation to an inner generator. It just so happens that asyncio coroutines use the same expression to await a future result and tell the loop it can run other coroutines' code if it wants.
asyncio has no such functionality but you may write a simple wrapper around  for yielding data in-order.It may be built using small sliding window buffer for storing newer completed data while older result is not available yet.


Answer URL
https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop.run_in_executor
https://docs.python.org/3/library/concurrent.futures.html
https://docs.python.org/3/library/asyncio-queue.html#queue
