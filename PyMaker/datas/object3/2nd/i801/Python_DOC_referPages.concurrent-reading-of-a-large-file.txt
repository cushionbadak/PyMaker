Natural Text
I'm creating a python pipeline to work on very large binary files (i.e. 50+ GB's). They are BAM files, a type of format used to represent genomes. My script is currently bottlenecked by two very computationally expensive subprocess calls.These two commands take ~80% of the computation time of every run of the pipeline, so I need to figure out a way to speed this process. They read the data from the same file. I would like to know the best route forward to make this more efficient. Basically, is there a particular flavor of concurrency that would work best? Or is there some other interesting way to do this?Thanks!Commands:subprocess.call('samtools view -b -f 68 {} >{}_unmapped_one.bam'.format(self.file_path, self.file_prefix), shell=True)subprocess.call('samtools view -b -f 132 {} > {}_unmapped_two.bam'.format(self.file_path, self.file_prefix), shell=True)
For what you're describing, and the code you shared I can think of several ways to improve performance.You are spawning subprocesses inside the program's shell to handle the file, this approach would vary based on the hardware that the program is running on, if this is a multiprocessor environment, it could be a good approach.Consider using the pysam library which wraps the low level hstlib APIDepending on the application flow you develop - you may be able to significantly improve performance by extending concurrent activities with asyncio. a recent article by Brad Salomon shortly explains on the benefits of multiprocessing and multithreading and deep dives into asyncio.If you do end up using asyncio on a unix based system, I also recommend looking at uvloop which wraps libuv as an event loop


Answer URL
https://docs.python.org/3/library/profile.html
