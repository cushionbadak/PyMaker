Natural Text
It is my understanding that the  function, which is actually an object type in Python 3, generates its contents on the fly, similar to a generator. This being the case, I would have expected the following line to take an inordinate amount of time, because in order to determine whether 1 quadrillion is in the range, a quadrillion values would have to be generated: Furthermore: it seems that no matter how many zeroes I add on, the calculation more or less takes the same amount of time (basically instantaneous). I have also tried things like this, but the calculation is still almost instant: If I try to implement my own range function, the result is not so nice!! What is the  object doing under the hood that makes it so fast? Martijn Pieters' answer was chosen for its completeness, but also see abarnert's first answer for a good discussion of what it means for  to be a full-fledged sequence in Python 3, and some information/warning regarding potential inconsistency for  function optimization across Python implementations. abarnert's other answer goes into some more detail and provides links for those interested in the history behind the optimization in Python 3 (and lack of optimization of  in Python 2). Answers by poke and by wim provide the relevant C source code and explanations for those who are interested. 
The Python 3  object doesn't produce numbers immediately; it is a smart sequence object that produces numbers on demand. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.The object also implements the  hook, and calculates if your number is part of its range. Calculating is a O(1) constant time operation. There is never a need to scan through all possible integers in the range.From the  object documentation:The advantage of the  type over a regular  or  is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the ,  and  values, calculating individual items and subranges as needed).So at a minimum, your  object would do:This is still missing several things that a real  supports (such as the  or  methods, hashing, equality testing, or slicing), but should give you an idea.I also simplified the  implementation to only focus on integer tests; if you give a real  object a non-integer value (including subclasses of ), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original Python issue that implemented the containment test.
The fundamental misunderstanding here is in thinking that  is a generator. It's not. In fact, it's not any kind of iterator.You can tell this pretty easily:If it were a generator, iterating it once would exhaust it:What  actually is, is a sequence, just like a list. You can even test this:This means it has to follow all the rules of being a sequence:The difference between a  and a  is that a  is a lazy or dynamic sequence; it doesn't remember all of its values, it just remembers its , , and , and creates the values on demand on .(As a side note, if you , you'll notice that  uses the same  type as . How does that work? A  doesn't use anything special about  except for the fact that it provides a C implementation of , so it works fine for  too.)Now, there's nothing that says that  has to be constant time—in fact, for obvious examples of sequences like , it isn't. But there's nothing that says it can't be. And it's easier to implement  to just check it mathematically (, but with some extra complexity to deal with negative steps) than to actually generate and test all the values, so why shouldn't it do it the better way?But there doesn't seem to be anything in the language that guarantees this will happen. As Ashwini Chaudhari points out, if you give it a non-integral value, instead of converting to integer and doing the mathematical test, it will fall back to iterating all the values and comparing them one by one. And just because CPython 3.2+ and PyPy 3.x versions happen to contain this optimization, and it's an obvious good idea and easy to do, there's no reason that IronPython or NewKickAssPython 3.x couldn't leave it out. (And in fact CPython 3.0-3.1 didn't include it.)If  actually were a generator, like , then it wouldn't make sense to test  this way, or at least the way it makes sense wouldn't be obvious. If you'd already iterated the first 3 values, is  still  the generator? Should testing for  cause it to iterate and consume all the values up to  (or up to the first value )?
Use the source, Luke!In CPython,  (a method wrapper) will eventually delegate to a simple calculation which checks if the value can possibly be in the range.  The reason for the speed here is we're using mathematical reasoning about the bounds, rather than a direct iteration of the range object.  To explain the logic used: Check that the number is between  and , andCheck that the stride value doesn't "step over" our number.  For example,  is in  because:, and.The full C code is included below, which is a bit more verbose because of memory management and reference counting details, but the basic idea is there:The "meat" of the idea is mentioned in the line:As a final note - look at the  function at the bottom of the code snippet.  If the exact type check fails then we don't use the clever algorithm described, instead falling back to a dumb iteration search of the range using !  You can check this behaviour in the interpreter (I'm using v3.5.0 here):
To add to Martijn’s answer, this is the relevant part of the source (in C, as the range object is written in native code):So for  objects (which is  in Python 3), it will use the  function to determine the result. And that function essentially checks if  is in the specified range (although it looks a bit more complex in C).If it’s not an  object, it falls back to iterating until it finds the value (or not).The whole logic could be translated to pseudo-Python like this:
If you're wondering why this optimization was added to , and why it wasn't added to  in 2.7:First, as Ashwini Chaudhary discovered, issue 1766304 was opened explicitly to optimize . A patch for this was accepted and checked in for 3.2, but not backported to 2.7 because "xrange has behaved like this for such a long time that I don't see what it buys us to commit the patch this late." (2.7 was nearly out at that point.)Meanwhile:Originally,  was a not-quite-sequence object. As the 3.1 docs say:Range objects have very little behavior: they only support indexing, iteration, and the  function.This wasn't quite true; an  object actually supported a few other things that come automatically with indexing and ,* including  (via linear search). But nobody thought it was worth making them full sequences at the time.Then, as part of implementing the Abstract Base Classes PEP, it was important to figure out which builtin types should be marked as implementing which ABCs, and / claimed to implement , even though it still only handled the same "very little behavior". Nobody noticed that problem until issue 9213. The patch for that issue not only added  and  to 3.2's , it also re-worked the optimized  (which shares the same math with , and is directly used by ).** This change went in for 3.2 as well, and was not backported to 2.x, because "it's a bugfix that adds new methods". (At this point, 2.7 was already past rc status.)So, there were two chances to get this optimization backported to 2.7, but they were both rejected.* In fact, you even get iteration for free with  and indexing, but in 2.3  objects got a custom iterator. Which they then lost in 3.x, which uses the same  type as .** The first version actually reimplemented it, and got the details wrong—e.g., it would give you . But Daniel Stutzbach's updated version of the patch restored most of the previous code, including the fallback to the generic, slow  that pre-3.2  was implicitly using when the optimization doesn't apply.
The other answers explained it well already, but I'd like to offer another experiment illustrating the nature of range objects:As you can see,  a range object is an object that remembers its range and can be used many times (even while iterating over it), not just a one-time generator.
It's all about a lazy approach to the evaluation and some extra optimization of .Values in ranges don't need to be computed until real use, or even further due to extra optimization.By the way, your integer is not such big, consider  is pretty fastdue to optimization - it's easy to compare given integer just with min and max of range.but: is pretty slow.(in this case, there is no optimization in , so if python receives unexpected float, python will compare all numbers)You should be aware of an implementation detail but should not be relied upon, because this may change in the future.
Here is similar implementation in . You can see how  done in O(1) time.
TL;DRThe object returned by  is actually a  object. This object implements the iterator interface so you can iterate through its values sequentially, just like a generator, but it also implements the  interface which is actually what gets called when an object appears on the right hand side of the  operator. The  method returns a bool of whether or not the item is in the object. Since  objects know their bounds and stride, this is very easy to implement in O(1). 


Answer URL
https://docs.python.org/3/library/stdtypes.html#typesseq-range
https://docs.python.org/3/reference/datamodel.html#object.__contains__
https://docs.python.org/3/library/stdtypes.html#range
https://docs.python.org/3/reference/datamodel.html#object.__contains__
https://docs.python.org/3/library/stdtypes.html#typesseq
