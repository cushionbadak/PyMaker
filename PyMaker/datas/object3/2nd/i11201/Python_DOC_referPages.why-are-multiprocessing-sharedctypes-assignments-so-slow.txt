Natural Text
Here's a little bench-marking code to illustrate my question:Basically I want a numpy array to be shared between multiple processes because it's big and read-only. This method works great, no extra copies are made and the actual computation time on the processes is good. But the overhead of creating the shared array is immense.This post offered some great insight into why certain ways of initializing the array are slow (note that in the example above I'm using the faster method). But the post doesn't really describe how to really improve the speed to numpy like performance.Does anyone have any suggestions on how to improve the speed? Would some cython code make sense to allocate the array?I'm working on a Windows 7 x64 system.
This is slow for the reasons given in your second link, and the solution is actually pretty simple: Bypass the (slow)  slice assignment code, which in this case is inefficiently reading one raw C value at a time from the source array to create a Python object, then converts it straight back to raw C for storage in the shared array, then discards the temporary Python object, and repeats  times.But you don't need to do it that way; like most C level things,  implements the buffer protocol, which means you can convert it to a , a view of the underlying raw memory that implements most operations in C-like ways, using raw memory operations if possible. So instead of doing:use  to manipulate it as a raw bytes-like object and assign that way ( already implements the buffer protocol, and 's slice assignment operator seamlessly uses it):Note, the time for the latter is milliseconds, not seconds; copying using  wrapping to perform raw memory transfers takes less than 1% of the time to do it the plodding way  does it by default!
Just put a numpy array around the shared array:then time:No need to figure out how to cast the memoryview (as I had to in python3 Ubuntu 16) and mess with reshaping (if  has more dimensions, since  flattens). And use  to double check data types just like any numpy array.  :)
On ms-windows when you create a , a new Python interpreter will be spawned which then imports your program as a module. (This is why on ms-windows you should only create  and  from within a  block.) This will recreate your array, which should take about the same time as creating it originally did. See the programming guidelines, especially concerning the  start method which has to be used on ms-windows.So probably a better way is to create a memory mapped numpy array using . Write the array to disk in the parent process. (On ms-windows this must be done in the  block, so it's only called once). Then in the  function use  in read-only mode to read the data.


Answer URL
https://docs.python.org/3/library/stdtypes.html#memoryview
https://docs.python.org/3/library/multiprocessing.html#programming-guidelines
