Natural Text
I have such a code;Basically, I want to tokenize sentences from a given text file, which involves more than 37 M sentences. Since I use nltk library for tokenizing, it takes more than 1 day for me to finish the process.For this reason, I have decided to do multithreading, so basically I split the given text file into 32 pieces, and process them parallely.But it seems like multithreading doesn't change the speed. Is my process slow because of I have used many cores in multithreading? May decreasing the number of cores lead to better performance? 
The  module won't take advantage of multiple processing cores, it will only share time between threads in the same process on one core.  If you want to spread your processing over many cores (which most likely will reduce the total time your program takes to execute), the multiprocessing package  is what you want to use. 


Answer URL
https://docs.python.org/3/library/multiprocessing.html
