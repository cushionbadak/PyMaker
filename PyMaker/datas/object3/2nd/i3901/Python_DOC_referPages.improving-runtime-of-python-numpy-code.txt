Natural Text
I have a code which reassigns bins to a large  array. Basically, the elements of the large array has been sampled at different frequency and the final goal is to rebin the entire array at fixed bins . The code is kind of slow for the array I have. Is there any good way to improve the runtime of this code? A factor of few would do for now. May be some  magic would do.
Keep the code simple and than optimizeIf you have an idea what algorithm you want to code write a simple reference implementation. From this you can go two ways using Python. You can try to vectorize the code or you can compile the code to get good performance.Even if  or  were implementet in Numba, it would be very hard for any compiler to make efficient binary code from your example. The only thing I have rewritten is a more efficient approach of digitize for scalar values.EditIn the Numba source code there is a more efficient implimentation of digitize for scalar values.CodePerformance
This seems to be trivially parallelizable:You've got an outer loop that you run 90 times.Each time, you're not mutating any shared arrays except … and that, only to store into a unique row.It looks like most of the work inside the loop is numpy array-wide operations, which will release the GIL.So (using the  backport of , since you seem to be on 2.7):If this works, there are two tweaks to try, either of which might be more efficient. We don't really care which order the results come back in, but  queues them up in order. This can waste a bit of space and time. I don't think it will make much difference (presumably, the vast majority of your time is presumably spent doing the calculations, not writing out the results), but without profiling your code, it's hard to be sure. So, there are two easy ways around this problem.Using  lets us use the results in whatever order they finish, rather than in the order we queued them. Something like this:Alternatively, we can make the function insert the rows directly, instead of returning them. This means we're now mutating a shared object from multiple threads. So I think we need a lock here, although I'm not positive (numpy's rules are a bit complicated, and I haven't read you code that thoroughly…). But that probably won't hurt performance significantly, and it's easy. So:That  in all of my examples should be tuned for your machine. Too many threads is bad, because they start fighting each other instead of parallelizing; too few threads is even worse, because some of your cores just sit there idle.If you want this to run on a variety of machines, rather than tuning it for each one, the best guess (for 2.7) is usually:But if you want to squeeze the max performance out of a specific machine, you should test different values. In particular, for a typical quad-core laptop with hyperthreading, the ideal value can be anywhere from 4 to 8, depending on the exact work you're doing, and it's easier to just try all the values than to try to predict.
I think you get a small boost in the performance by replacing  with actual multiplication.Your code is rather slow at , which I believe can be much faster with , although I couldn't get it quite work for multidimensional arrays you have. May be someone here can add to that.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.as_completed
