Natural Text
Python provides the following three modules that deal with C types and how to handle them: for C structs for arrays such as those in C for C functions, which necessarily entails dealing with C’s type systemWhile  seems more general and flexible (its main task being “a foreign function library for Python”) than  and , there seems to be significant overlap in functionality between these three modules when the task is to read binary data structures. For example, if I wanted to read a C structI could use  as follows:On the other hand, using  works equally well (although a bit more verbose):(Aside: I do wonder where the trailing  went in this version, though…)This seems to me like it violates the principles in “The Zen of Python”:There should be one—and preferably only one—obvious way to do it.So how did this situation with several of these similar modules for binary data handling arise? Is there a historical or practical reason? (For example, I could imagine omitting the  module entirely and simply adding a more convenient API for reading/writing C structs to .)
Disclaimer: this post is speculation based on my understanding of the "division of labor" in Python stdlib, not on factual referenceable info.Your question stems from the fact that "C structs" and "binary data" tend to be used interchangeably, which, while correct in practice, is wrong in a technical sense. The  documentation is also misleading: it claims to work on "C structs", while a better description would be "binary data", with some disclaimers about C compatibility.Fundamentally, ,  and  do different things.  deals with converting Python values into binary in-memory formats.  deals with efficiently storing a lot of values.  deals with the C language(*). The overlap in functionality stems from the fact that for C, the "binary in-memory formats" are native, and that "efficiently storing values" is packing them into a C-like array.You will also note that  lets you easily specify endianness, because it deals with packing and unpacking binary data in many different ways it can be packed; while in  it is more difficult to get non-native byte order, because it uses the byte order that is native to C.If your task is reading binary data structures, there's increasing levels of abstraction:Manually splitting the byte array and converting parts with  and the likeDescribing the data with a format string and using  to unpack in one goUsing a library like Construct to describe the structure declaratively in logical terms. don't even figure here, because for this task, using  is pretty much taking a round-trip through a different programming language. The fact that it works just as well for your example is incidental; it works because C is natively suited to expressing many ways of packing binary data. But if your struct was mixed-endian, for instance, it would be very difficult to express in . Another example is half-precision float which doesn't have a C equivalent (see here).In this sense, it's also very reasonable that  use  - after all, "packing and unpacking binary data" is a subtask of "interfacing with C".On the other hand, it would make no sense for  to use : it would be like using the  library for character encoding conversions because it's a task that an e-mail library can do.(*) well, basically. More precise would be something like "C-based environments", i.e., how modern computers work on low level due to co-evolution with C as the primary systems language.


Answer URL
https://docs.python.org/3/library/struct.html
https://docs.python.org/3/library/array.html
https://docs.python.org/3/library/ctypes.html
https://docs.python.org/3/library/struct.html#format-characters
