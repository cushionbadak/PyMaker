Natural Text
I understand a few things based on the following link (I could be wrong!):http://docs.python.org/2/glossary.html#term-bytecode is a cached file and is only generated if the module is imported somewhere else is to help loading performance, not execution performance.running  does not generate  unless  is imported somewhere.Python has a bytecode compiler (used to generate )Python's virtual machine executes byte-code.So, when I run , if  is not imported anywhere, does Python actually create an in-memory bytecode?The missing  seems to break the idea of Python VM.This question is extended to code execution in Python interpreter (running  in the terminal). I believe CPython (or just about any language implementation) can't do pure interpretation. I think the core of the question is: Does the VM actually read the  file? I assume VM loads the  into the execution environment.
Your points 1 to 5 are correct, with the exception (if we're precise) of the point 4.  The Python interpreter has a part called the bytecode compiler that turns source code into , which you can inspect by typing  for any function .  This is the real bytecode that is interpreted.  These code objects may then, as a separate step, be saved inside  files.Here are the operations in more details.  The bytecode compiler runs only once per module, when you load the  and each of the modules it imports.  It's not a too long operation, but it still takes some time, particularly if your module imports a lot of other modules.  This is where  files enter the picture.  After an  statement has invoked the bytecode compiler, it tries to save the resulting  inside a  file.  The next time, if the  file already exists and the  file has not been modified, the  is reloaded from there.  This is just an optimization: it avoids the cost of invoking the bytecode compiler.  In both cases the result is the same: a  was created in memory and is going to be interpreted.It only works for  statements, not for example for the main module (i.e. the  in the command line ).  The idea is that it should not really matter --- where the bytecode compiler would loose time in a typical medium-to-large program is in compiling all directly and indirectly imported modules, not just compiling .
Interesting ... the first thing I did was call for and the first option I see is to disable automatic pyc and pyo file generation on import, though thats probably cause its alphabetical order.lets run some tests so it only generated the pyc file when it was imported.now in order to check which files are being used I'll use OS X dtruss similar to linux truss to do a full trace ... from the looks of it python did not even touch test.pyc file at all!well thats interesting it looks like it opened test.py then test.pyc what happens when we delete the pyc file.it first open test.py then it 'tried' to open test.pyc which returned an error then it called unlink and generated the pyc file again ... interesting, I thought it would check.what if we delete the original py file.no surprise there it couldn't open test.py but it still continued, to this day Im not sure if this is actually 'ok' python should give out some kind of warning, I've being burned a couple of times by this, accidentally deleting my files, running my tests and feeling a sigh of relief as they pass only to start sweating when I can't seem to find the source code! After this tests we an assume python only uses pyc files either directly when invoked such as   or indirectly when imported, otherwise it doesn't seem to use them.Supposedly CPythons compiler was designed to be fairly fast, it doesn't do much type checking and it probably generates very high level byte-code so most of the work load is actually done by the virtual machine ... it probably does a single pass, lexing->compiler->byte-code in one go, it does this every time, it reads a python file from the command line or when importing and no pyc file is present in that case it creates it.this may be why some other implementations are faster since they take more time to compile but generate far rawer byte-codes that can be well optimized.Its extremely difficult to build a virtual machine to do pure interpretive efficiently ... Its all about balance, the more powerful your bytecode the simpler your compiler can be but the more complex and slow your virtual machine has to be and vice-versa ...
Python is incapable of directly executing source code (unlike some other scripting languages which do ad hoc parsing, e.g. Bash). All Python source code must be compiled to bytecode, no matter what the source is. (This includes e.g. code run through  and ). Generating bytecode is rather expensive because it involves running a parser, so caching the bytecode (as .pyc) speeds up module loading by avoiding the parsing phase.The difference between  and  is simply that the latter doesn't cache the bytecode that is generated.


Answer URL
