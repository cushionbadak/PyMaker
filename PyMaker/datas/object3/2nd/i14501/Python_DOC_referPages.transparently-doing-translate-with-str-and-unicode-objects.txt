Natural Text
This is the implementation I am using to abstract away the differences for  between unicode and str:Here I am losing some of the power of the  (namely, translating one character to a string), but at least I have a uniform interface that I can use to translate unicode and plain strings, without having to care about the type.What I do not like is:this implementation relies in checking the type of the string in order to call the right functionI can not do  (I must do , which means I can not chain function calls like Is there a better way to implement a transparent ?
this implementation relies in checking the type of the string in order to call the right functionWell, what else could it do? You want to do different things for different types, and you can't monkeypatch the types to do it in dot-syntax OO style, so how can you automatically dispatch on types? What you're looking for is external dispatch. Python can do this in 3.4+ (only dispatching on the first argument, not all arguments like CLOS or Dylan… although there are multiple-dispatch libraries on PyPI) with , and there's a backport on PyPI that works back to 2.6. So, you can do this:Notice also that I just used  and  instead of  and . As the docs say, those types are just aliases, and they aren't really necessary. All they do is make your code less backward-compatible. (And they don't help for forward compatibility with 3.x; 3.0 just removed the unnecessary aliases instead of making  and  both aliases for  and adding a …)If you don't want to use a library off PyPI or implement the same thing yourself, and instead want manual type-switching, you probably want  rather than .I can not do txt.translate(...) (I must do translate(txt, ...)That's true; you can't monkeypatch  and . But so what?which means I can not chain function calls like txt[:50].translate(...)Sure, but you can chain function calls like . While that might look anti-idiomatic in a "everything-is-a-method" language like Java or Ruby, it's perfectly fine in Python. Especially since it's pretty rare to chain more than 2 or 3 calls in Python anyway. After all, the next thing after that  is going to have to be a  call or a comprehension, and those aren't done by methods in Python.Here I am losing some of the power of the  (namely, translating one character to a string)Yes, that's pretty much inherent in the lowest-common-denominator design. And so is some performance loss.  and  aren't really doing exactly the same thing. The former is a table-based translation, because that's a great optimization when you only have 256 possible values, but it does mean you give up some flexibility and power. The latter is a dict-based translation, because a table would be a pessimization for 1.1 million values, but that means you get some extra flexibility and power.So, here, you're giving up the performance of  (especially since you have to build the  on the fly for each translation), and the flexibility of , to get the worst of both worlds.If you actually know the encodings of your  strings (and they actually do represent text—after all,  can also be useful for binary data…), you could instead write this by just . But then if you know the encodings, you might as well just have  instead of  in the first place.But I think a better solution might to be wrap up  in a way that returns a tuple of two tables for , and a tuple of one dict for . Then you can just call the native  for either, instead of wrapping .Unfortunately, you can't use  for this, because any of the arguments may be , which means we're back to explicit type-switching.Now you can do this:But really, I'm not sure where this would be useful in the first place. How can you even call  or  without knowing whether your  and  are  or ?


Answer URL
https://docs.python.org/3/library/functools.html#functools.singledispatch
