Natural Text
I want to compress files and compute the checksum of the compressed file using python. My first naive attempt was to use 2 functions:However, it leads to the compressed file being written and then re-read. With many files (> 10 000), each several MB when compressed, in a NFS mounted drive, it is slow.How can I compress the file in a buffer and then compute the checksum from this buffer before writing the output file?The file are not that big so I can afford to store everything in memory. However, a nice incremental version could be nice too.The last requirement is that it should work with multiprocessing (in order to compress several files in parallel).I have tried to use  but the returned string miss the header of a gzip file.Edit: following @abarnert sggestion, I used python3 :This produce a correct gzip file but the output is different at each run (the md5 is different):The  program doesn't have this problem:I guess it's because the  module use the current time by default when creating a file (the  program use the modification of the input file I guess). There is no way to change that with .I was thinking to create a  in read/write mode, controlling the mtime but there is no such mode for .Inspired by @zwol suggestion I wrote the following function which correctly sets the filename and the OS (Unix) in the header:The output is the same at different run. Moreover the output of  is the same than :However, md5 is different: is also different:I guess it's because the  program and the python  module (which is based on the C library ) have a slightly different algorithm.
Wrap a  object around an  object.  (In Python 2, use  instead.)  After you close the , you can retrieve the compressed data from the  object (using ), hash it, and write it out to a real file.Incidentally, you really shouldn't be using MD5 at all anymore.
I have tried to use  but the returned string miss the header of a gzip file.Of course. That's the whole difference between the  module and the  module;  just deals with zlib-deflate compression without gzip headers,  deals with zlib-deflate data with gzip headers.So, just call  instead, and the code you wrote but didn't show us should just work.As a side note:You almost certainly want to open the file in  mode here. You don't want to convert  into  (if on Windows), or decode the binary data as  text (if on Python 3), so open it in binary mode.Another side note:Don't use line-based APIs on binary files. Instead of this:â€¦ do this:Or, if the files are too large to read into memory all at once:And one last point:With many files (> 10 000), each several MB when compressed, in a NFS mounted drive, it is slow.Does your system not have a tmp directory mounted on a faster drive?In most cases, you don't need a real file. Either there's a string-based API (, , , etc.), or the file-based API only requires a file-like object, like a .But when you do need a real temporary file, with a real file descriptor and everything, you almost always want to create it in the temporary directory.* In Python, you do this with the  module. For example:If you need an actual filename, rather than a file object, you can use .* The one exception is when you only want the temporary file to eventually  it to a permanent location. In that case, of course, you usually want the temporary file to be in the same directory as the permanent location. But you can do that with  just as easily. Just remember to pass .


Answer URL
https://docs.python.org/3/library/gzip.html#gzip.GzipFile
https://docs.python.org/3/library/io.html#io.BytesIO
https://docs.python.org/3/library/zlib.html
https://docs.python.org/3/library/gzip.html
https://docs.python.org/3/library/gzip.html#gzip.compress
https://docs.python.org/3/library/tempfile.html
https://docs.python.org/3/library/zlib.html#zlib.compress
