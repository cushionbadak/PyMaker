Natural Text
I'd really like to create a  array from a csv file, however, I'm having issues when the file is ~50k lines long (like the MNIST training set). The file I'm trying to import looks something like this: It works fine for something thats 10k lines long, like the validation set: If I do the same with the training data (larger file), I'll get a c-style segmentation fault. Does anyone know any better ways besides breaking the file up and then piecing it together?The end result is that I'd like to pickle the arrays into a similar  file but I can't do that if I can't read in the data. Any help would be greatly appreciated. 
I think you really want to track down the actual problem and solve it, rather than just work around it, because I'll bet you have other problems with your NumPy installation that you're going to have to deal with eventually.But, since you asked for a workaround that's better than manually splitting the files, reading them, and merging them, here are two:First, you can split the files programmatically and dynamically, instead of manually. This avoids wasting a lot of your own human effort, and also saves the disk space needed for those copies, even though it's conceptually the same thing you already know how to do.As the  docs make clear, the  argument can be a pathname, or a file object (open in  mode), or just a generator of lines (as ). Of course a file object is itself a generator of lines, but so is, say, an  of a file object, or a group from a . So:If you don't want to install , you can also just copy the 2-line  implementation from the Recipes section of the  docs, or even inline zipping the iterators straight into your code.Alternatively, you can parse the CSV file with the stdlib's  module instead of with NumPy:This is obviously going to be a lot slowerâ€¦ but if we're talking about turning 50ms of processing into 1000ms, and you only do it once, who cares?


Answer URL
https://docs.python.org/3/library/itertools.html#itertools-recipes
https://docs.python.org/3/library/csv.html
