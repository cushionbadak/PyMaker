Natural Text
I have a (potentially quite big) dictionary and a list of 'possible' keys. I want to quickly find which of the keys have matching values in the dictionary. I've found lots of discussion of single dictionary values here and here, but no discussion of speed or multiple entries.I've come up with four ways, and for the three that work best I compare their speed on different sample sizes below - are there better methods? If people can suggest sensible contenders I'll subject them to the analysis below as well.Sample lists and dictionaries are created as follows: Method 1: the  keyword:32 function calls in 0.018 seconds Method 2: error handling:32 function calls in 0.087 seconds Method 3: set intersection:26 function calls in 0.046 seconds Method 4: Naive use of :This is a cautionary tale - it was my first attempt and BY FAR the slowest!12 function calls in 248.552 seconds EDIT: Bringing the suggestions given in the answers into the same framework that I've used for consistency. Many have noted that more performance gains can be achieved in Python 3.x, particularly list comprehension-based methods. Many thanks for all of the help!Method 5: Better way of performing intersection (thanks jonrsharpe):25 function calls in 0.037 seconds Method 6: List comprehension (thanks jonrsharpe):24 function calls in 0.020 seconds Method 7: Using the  keyword (thanks jonrsharpe):25 function calls in 0.026 secondsFor methods 1-3 and 5-7 I timed them as above with list/dictionaries of length 1000, 10000, 100000, 1000000, 10000000 and 100000000 and show a log-log plot of time taken. Across all lengths the intersection and in-statement method perform better. The gradients are all about 1 (maybe a bit higher), indicating O(n) or perhaps slightly super-linear scaling.
Of a couple of additional methods I've tried, the fastest was a simple list comprehension:This runs the same process as your fastest approach, , but more quickly. For comparison, the quickest -based way was  results:Having added @abarnert's suggestion:and re-run the timing I now get: and  have switched places, so I re-ran again:So it looks like the set approach is slower than the list, but the difference between the list and list comprehension is (surprisingly, to me at least) is a bit variable. I'd say just pick one, and not worry about it unless it becomes a real bottleneck later.
First, I think you're on 2.7, so I'll do most of this with 2.7. But it's worth noting that if you're really interested in optimizing your code, the 3.x branch continues to get performance improvements, and the 2.x branch never will. And why are you using CPython instead of PyPy?Anyway, some further micro-optimizations to try (in addition to the ones in jonrsharpe's answer:Caching attribute and/or global lookups in local variables (it's called  for a reason). For example:But for some operator special methods, like  and , that may not be worth doing. Of course you won't know until you try:Meanwhile, Jon's  answer already optimizes out the  entirely by using a listcomp, and we just saw that optimizing out the lookups he does have probably won't help. Especially in 3.x, where the comprehension is going to be compiled into a function of its own, but even in 2.7 I wouldn't expect any benefit, for the same reasons as in the explicit loop. But let's try just to be sure:Surprisingly (at least to me), this time it actually helped. Not sure why.But what I was really setting up for was this: another advantage of turning both the filter expression and the value expression into function calls is that we can use  and :But that gain is largely 2.x-specific; 3.x has faster comprehensions, while its  is slower than 2.x's  or .You don't need to convert both sides of a set intersection to a set, just the left side; the right side can be any iterable, and a dict is already an iterable of its keys.But, even better, a dict's key view ( in 3.x,  in 2.7) is already a set-like object, and one backed by the dict's hash table, so you don't need to transform anything. (It doesn't have quite the same interface—it has no  method but its  operator takes iterables, unlike , which has an  method that takes iterables but its  only takes sets. That's annoying, but we only care about performance here, right?)That last one didn't help (although using Python 3.4 instead of 2.7, it was 10% faster…), but the first one definitely did.In real life, you may also want to compare the sizes of the two collections to decide which one gets setified, but here that information is static, so there's no point writing the code to test it.Anyway, my fastest result was the  on 2.7, by a pretty good margin. On 3.4 (which I didn't show here), Jon's listcomp was fastest (even fixed to return the values rather than the keys), and faster than any of the 2.7 methods. Also, 3.4's fastest set operation (using the key view as a set and the list as an iterable) were a lot closer to the iterative methods than in 2.7.
So, that's an 8% speedup just by using a later Python. (And the speedup was closer to 20% on the listcomp and dict-key-view versions.) And it's not because Apple's 2.7 is bad or anything, it's just that 3.x has continued to get optimizations over the past 5+ years, while 2.7 has not (and never will again).And meanwhile:That's a 7000000x speedup just by typing 5 extra characters. :)I'm sure it's cheating here. Either the JIT implicitly memoized the result, or it's noticed that I didn't even look at the result and pushed that up the chain and realized it didn't need to do any of the steps, or something. But this actually happens in real life sometimes; I've had a huge mess of code that spent 3 days debugging and trying to optimize before realizing that everything it did was unnecessary…At any rate, speedups on the order of 10x are pretty typical from PyPy even when it can't cheat. And it's a lot easier than tweaking attribute lookups or reversing the order of who gets turned into a set for 5%.Jython is more unpredictable—sometimes almost as fast as PyPy, sometimes much slower than CPython. Unfortunately,  is broken in Jython 2.5.3, and I just broke my Jython 2.7 completely by upgrading from rc2 to rc3, so… no tests today. Similarly, IronPython is basically Jython redone on a different VM; it's usually faster, but again unpredictable. But my current version of Mono and my current version of IronPython aren't playing nice together, so no tests there either.


Answer URL
https://docs.python.org/3/library/profile.html#introduction-to-the-profilers
