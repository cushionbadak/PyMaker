Natural Text
I have a list with numbers that are an integer: . This list can contain > 1 million items.I have a dictionary  that has as key an integer, with a list as value that contains object with a minimum and maximum range. This dictionary consists now of about 500k keys.I am now looping through the list:where I check if there is a match of a number of  in the ranges of , and if so, I return the key which will be used further on. When I run this, I see that it takes about 40 seconds to process 1000 numbers from the list. This means that if I have 1 million numbers, I need more than 11 hours to process.The expected output is returning the keys from  that are matching within the range and the  number used to find that key, i.e.  in function . What are the recommended ways in Python to optimize this algorithm?
Your list of  is sorted, so do the opposite: Loop the dictionaries in  and use  to binary-search the matching candidates. This will reduce the complexity from  to  for  dictionaries,  candidates, and  matching candidates on average.(Note: I changed the format of your  from a  of  with just a single element each to just a , which makes much more sense.)Output:If the  are not sorted in reality, or if you want the results to be sorted by candidate instead of by dictionary, you can just sort either as a pre- or post-processing step.
With a little bit of reorganisation, your code becomes a classic interval tree problem.Have a look at this package https://pypi.org/project/intervaltree/The only divergence from a normal interval tree is that you have some items that cover multiple intervals, however it would be easy enough to break them into individual intervals, e.g. {16.1: {"start": 15, "end": 20}, 16.2: {"start": 16, "end": 18}}By using the intervaltree package, a balanced binary search tree is created which is much more efficient than using nested for loops. This solution is O(logn) for searching each candidate, whereas a for loop is O(n). If there are 1MM+ candidates, the intervaltree package is going to be considerably faster than the accepted nested for loop answer.
Even though this question has an accepted answer, i would add for the sake of others that this type of scenario really justifies creating a reverse lookup. It is a 1 time headache that will save a lot of practical time as candidate list grows longer. Dictionary lookups are O(1) and if you need to perform multiple lookups, you should consider creating a reverse mapping as well.Output:


Answer URL
https://docs.python.org/3/library/bisect.html
