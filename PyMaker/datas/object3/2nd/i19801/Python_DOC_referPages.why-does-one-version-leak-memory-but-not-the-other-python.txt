Natural Text
Both these functions compute the same thing (the numbers of integers such that the length of the associated Collatz sequence is no greater than n) in essentially the same way. The only difference is that the first one uses sets exclusively whereas the second uses both sets and lists.The second one leaks memory (in IDLE with Python 3.2, at least), the first one does not, and  I have no idea why. I have tried a few "tricks" (such as adding  statements) but nothing seems to help (which is not surprising, those tricks should be useless).I would be grateful to anybody who could help me understand what goes on.If you want to test the code, you should probably use a value of  in the 55 to 65 range, anything above 75 will almost certainly result in a (totally expected) memory error.EDIT : This also happens when using the interactive mode of the interpreter (without IDLE), but not when running the script directly from a terminal (in that case, memory usage goes back to normal some time after the function has returned, or as soon as there is an explicit call to ).
CPython allocates small objects (obmalloc.c, 3.2.3) out of 4 KiB pools that it manages in 256 KiB blocks called arenas. Each active pool has a fixed block size ranging from 8 bytes up to 256 bytes, in steps of 8. For example, a 14-byte object is allocated from the first available pool that has a 16-byte block size. There's a potential problem if arenas are allocated on the heap instead of using mmap (this is tunable via mallopt's ), in that the heap cannot shrink below the highest allocated arena, which will not be released so long as 1 block in 1 pool is allocated to an object (CPython doesn't float objects around in memory).Given the above, the following version of your function should probably solve the problem. Replace the line  with the following 3 lines:After deallocating the containers and all referenced objects (releasing arenas back to the system), this returns a new  with the expression . The heap cannot shrink as long as there's a reference to the first result object. In this case that gets automatically deallocated when the function returns.If you're testing this interactively without the "plus 0" step, remember that the REPL (Read, Eval, Print, Loop) keeps a reference to the last result accessible via the pseudo-variable  "". In Python 3.3 this shouldn't be an issue since the object allocator was modified to use anonymous mmap for arenas, where available. (The upper limit on the object allocator was also bumped to 512 bytes to accommodate 64-bit platforms, but that's inconsequential here.)Regarding manual garbage collection,  does a full collection of tracked container objects, but it also clears freelists of objects that are maintained by built-in types (e.g. frames, methods, floats). Python 3.3 added additional API functions to clear freelists used by lists (), dicts (), and sets (). If you'd prefer to keep the freelists intact, use .
I doubt it leaks, I bet it is just that garbage collection doesn't kick in yet, so memory used keeps growing. This is because every round of outer loop, the previous current list becomes elgible for garbage collection, but will not be garbage collected until whenever.Furthermore, even if it is garbage collected, memory isn't normally released back to the OS, so you have to use whatever Python method to get current used heap size.If you add garbage collection at end of every outer loop iteration, that may reduce memory use a bit, or not, depending on how exactly Python handles its heap and garbage collection without that.
You do not have a memory leak. Processes on linux do not release memory to the OS until they exit. Accordingly, the stats you will see in e.g.  will only ever go up. You only have a memory leak if after running the same, or smaller size of job, Python grabs more memory from the OS, when it "should" have been able to reuse the memory it was using for objects which "should" have been garbage collected.


Answer URL
