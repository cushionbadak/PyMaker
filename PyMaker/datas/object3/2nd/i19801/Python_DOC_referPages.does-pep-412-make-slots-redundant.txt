Natural Text
PEP 412, implemented in Python 3.3, introduces improved handling of attribute dictionaries, effectively reducing the memory footprint of class instances.   was designed for the same purpose, so is there any point in using  any more?In an attempt to find out the answer myself, I run the following test, but the results don't make much sense:Python 3.3 Results:Python 2.7 Results:I would have expected the size to differ at least for Python 2.7, so I assume there is something wrong with the test.
No, PEP 412 does not make  redundant.First, Armin Rigo is right that you're not measuring it properly. What you need to measure is the size of the object, plus the values, plus the  itself (for  only) and the keys (for  only).Or you could do what he suggests:When I run this on 64-bit CPython 3.4 on OS X, I get  for  and  for . So, it looks like a  instance takes 88 bytes, while a  instance takes 256 bytes.How is this possible?Because there are still two differences between  and a key-split .First, the hash tables used by dictionaries are kept below 2/3rds full, and they grow exponentially and have a minimum size, so you're going to have some extra space. And it's not hard to work out how much space by looking at the nicely-commented source: you're going to have 8 hash buckets instead of 5 slots pointers.Second, the dictionary itself isn't free; it has a standard object header, a count, and two pointers. That might not sound like a lot, but when you're talking about an object that's only got a few attributes (note that most objects only have a few attributesâ€¦), the dict header can make as much difference as the hash table.And of course in your example, the values, so the only cost involved here is the object itself, plus the the 5 slots or 8 hash buckets and dict header, so the difference is pretty dramatic. In real life,  will rarely be that much of a benefit.Finally, notice that PEP 412 only claims:Benchmarking shows that memory use is reduced by 10% to 20% for object-oriented programsThink about where you use . Either the savings are so huge that not using  would be ridiculous, or you really need to squeeze out that last 15%. Or you're building an ABC or other class that you expect to be subclassed by who-knows-what and the subclasses might need the savings. At any rate, in those cases, the fact that you get half the benefit without , or even two thirds the benefit, is still rarely going to be enough; you'll still need to use .The real win is in the cases where it isn't worth using ; you'll get a small benefit for free.(Also, there are definitely some programmers who overuse the hell out of , and maybe this change can convince some of them to put their energy into micro optimizing something else not quite as irrelevant, if you're lucky.)
The problem is , which rarely returns what you expect.  For example in this case it counts the "size" of an object without accounting for the size of its .  I suggest you retry by measuring the real memory usage of creating 100'000 instances.Note also that the Python 3.3 behavior was inspired by PyPy, in which  makes no difference, so I would expect it to make no difference in Python 3.3 too.  As far as I can tell,  is almost never of any use now.


Answer URL
