Natural Text
I have a list of 20 file names, like . I want to write a Python script to concatenate these files into a new file. I could open each file by , read line by line by calling , and write each line into that new file. It doesn't seem very "elegant" to me, especially the part where I have to read//write line by line. Is there a more "elegant" way to do this in Python?
This should do itFor large files:For small files:â€¦ and another interesting one that I thought of:Sadly, this last method leaves a few open file descriptors, which the GC should take care of anyway. I just thought it was interesting
Use .It automatically reads the input files chunk by chunk for you, which is more more efficient and reading the input files in and will work even if some of the input files are too large to fit into memory:
That's exactly what fileinput is for:For this use case, it's really not much simpler than just iterating over the files manually, but in other cases, having a single iterator that iterates over all of the files as if they were a single file is very handy. (Also, the fact that  closes each file as soon as it's done means there's no need to  or  each one, but that's just a one-line savings, not that big of a deal.)There are some other nifty features in , like the ability to do in-place modifications of files just by filtering each line.As noted in the comments, and discussed in another post,  for Python 2.7 will not work as indicated. Here slight modification to make the code Python 2.7 compliant
I don't know about elegance, but this works:
What's wrong with UNIX commands ? (given you're not working on Windows) :  does the job ( you can call it from python with subprocess if you want)
An alternative to @inspectorG4dget answer (best answer to date 29-03-2016). I tested with 3 files of 436MB. @inspectorG4dget solution: 162 secondsThe following solution : 125 secondsThe idea is to create a batch file and execute it, taking advantage of "old good technology". Its semi-python but works faster. Works for windows. 
A simple benchmark shows that the shutil performs better.
Check out the .read() method of the File object:http://docs.python.org/2/tutorial/inputoutput.html#methods-of-file-objectsYou could do something like:or a more 'elegant' python-way:which, according to this article: http://www.skymind.com/~ocrow/python_string/ would also be the fastest.
If the files are not gigantic:If the files are too big to be entirely read and held in RAM, the algorithm must be a little different to read each file to be copied in a loop by chunks of fixed length, using  for example.
If you have a lot of files in the directory then  might be a better option to generate a list of filenames rather than writing them by hand.



Answer URL
https://docs.python.org/3/library/shutil.html#shutil.copyfileobj
