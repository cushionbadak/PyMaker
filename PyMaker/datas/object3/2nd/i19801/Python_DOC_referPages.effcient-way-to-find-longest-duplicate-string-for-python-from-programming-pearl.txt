Natural Text
From Section 15.2 of Programming PearlsThe C codes can be viewed here: http://www.cs.bell-labs.com/cm/cs/pearls/longdup.cWhen I implement it in Python using suffix-array:I found it very slow for the  step. I think it's slow because Python need to pass the substring by value instead of by pointer (as the C codes above).The tested file can be downloaded from hereThe C codes need only 0.3 seconds to finish.But for Python codes, it never ends on my computer (I waited for 10 minutes and killed it)Does anyone have ideas how to make the codes efficient?  (For example, less than 10 seconds)
My solution is based on Suffix array. It is constructed by Prefix doubling of Longest common prefix. The worst-case complexity is O(n (log n)^2). The task "iliad.mb.txt" takes 4 seconds on my laptop. The code is good documented inside functions  and . The latter function is short and can be easily modified e.g. for searching of 10 longest non overlapping substrings. This Python code is faster than the original C code (copy here)  from the question, if duplicate strings are longer than 10000 characters.I prefer this solution over more complicated O(n log n) because Python has a very fast list sorting (list.sort), probably faster than necessary linear time operations in the method from that article, that should be O(n) under very special presumptions of random strings together with a small alphabet (typical for DNA genom analyze). I read in Gog 2011 that worsest-case O(n log n) of my algorithm can be in practice faster than many O(n) algorithm, that can not use CPU memory cache.The code in another answer based on grow_chains is 19 times slower than the original example from the question, if the text contain a repeated string 8 kB long. Long repeated texts are not typical for classical literature, but they are frequent e.g. in "independent" school homeworks collections. The program should not freeze on it.I wrote an example and tests with the same code for Python 2.7, 3.3 - 3.6.
The main problem seems to be that python does slicing by copy: https://stackoverflow.com/a/5722068/538551You'll have to use a memoryview instead to get a reference instead of a copy. When I did this, the program hung after the  function (which was very fast).I'm sure with a little work, you can get the rest working.Edit:The above change will not work as a drop-in replacement because  does not work the same way as .  For example, try the following C code:And compare the result to this python:The C code prints  on my machine while the python version prints . It looks like the example  code is abusing the return value of  (it IS used in  after all). I couldn't find any documentation on when  will return something other than , but adding a  to  in the original code showed a lot of values outside of that range (3, -31, 5 were the first 3 values).To make sure that  wasn't some error code, if we reverse test1 and test2, we'll get .
The translation of the algorithm into Python: allows to get a substring without copying:It takes 5 seconds on my machine for the . In principle it is possible to find the duplicate in O(n) time and O(n) memory using a suffix array augmented with a lcp array.Note:  is deprecated by  versionMore memory efficient version (compared to longest_duplicate_small()):It takes 17 seconds on my machine for the . The result is:I had to define custom functions to compare  objects because  comparison either raises an exception in Python 3 or produces wrong result in Python 2:Related questions:Find the longest repeating string and the number of times it repeats in a given stringfinding long repeated substrings in a massive string
This version takes about 17 secs on my circa-2007 desktop using totally different algorithm:The basic idea is to create a list of substrings and positions where they occure, thus eliminating the need to compare same strings again and again. The resulting list look like . Unique strings are removed. Then every list member grows by 1 character and new list is created. Unique strings are removed again. And so on and so forth...


Answer URL
