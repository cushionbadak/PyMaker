Natural Text
I have a Python 3.x program that processes several large text files that contain sizeable arrays of data that can occasionally brush up against the memory limit of my puny workstation.  From some basic memory profiling, it seems like when using the generator, the memory usage of my script balloons to hold consecutive elements, using up to twice the memory I expect.I made a simple, stand alone example to test the generator and I get similar results in Python 2.7, 3.3, and 3.4.  My test code follows,  is a modifed version of this function from an SO question which uses  and agrees with  as I watch it.   is probably a more cross-platform method:In practice I'll process data coming from such a generator loop, saving just what I need, then discard it.When I run the above script, and two large elements come in series (the data size can be highly variable), it seems like Python computes the next before freeing the previous, leading to up to double the memory usage.The crazy belt-and-suspenders-and-duct-tape approach , , and  does nothing.I'm pretty sure the generator itself is not doubling up on memory because otherwise a single large value it yields would increase the peak usage, and in the same iteration a large object appeared; it's only large consecutive objects.How can I save my memory?
The problem is in the generator function; particularly in the statement:Suppose you have old content in the data variable. When you run this statement, it first computes the result, thus you have 2 these arrays in memory; old and new. Only then is data variable changed to point to the new structure and the old structure is released. Try to modify the iterator function to:
Have you tried using the gc module? There you can get a list of the objects that still reference your large data between loops, check if its in the list of unreachable but unfreed objects, or enable some debugs flags.With luck, a simple call to  after each loop may fix your problem in a single line.
Instead of:Try:The problem is simply that the generator's  local variable keeps a reference to the yielded list, preventing it from ever being garbage collected until the generator resumes and discards the reference.In other words, doing  outside the generator has no effect on garbage collection unless that's the only reference to the data. Avoiding a reference inside the generator makes that true.AddendumIf you have to manipulate the data, first, you can use a hack like this to drop the reference before yielding it:


Answer URL
