Natural Text
I am in the process of improving a program that parses XML and categorises and indexes its subtrees. The actual program is too large to show here, so I have brought it down to a minimal test case showing the issue I encounter.The idea is:Process XML files in a directory, one by oneProcess all  nodes in a file, in parallelDuring that process, the process needs read/write access to shared variables so that for instance we can check how many times an attribute has occurred in total, or keep track of file handlesNote that in the actual code there are some more caveats:simply returning new values per process and then merging them in the main thread seems not advisable and presumably quite slow because the actual data structure are s of four levels deep consisting of , s, s, and s, as well as dict-to-filehandle, and  objects;I tried using threads (with ) and even though there was some gain (I calculated around 5% improvement in speed), this is not good enough for me;the actual data I am working with can consist of XML files of more than 60GB, or up to 15 million  tags per file. That is the main reason I want to run things in parallel - there is just so much data. That means that the nested objects get quite big as well, so merging/sharing these objects between processes may be a bottleneck in itself.Example code:Sample XML (save it to an XML file and put it inside a directory; use that directory as the first argument of ):, in case you wish to set up a local environment for testing (the above script probably works with 3.4 and up, though):Depending on how you named the XML, the output of the script will be something as follows:This shows that the variables that I want to be shared between processes is indeed not shared or modified. (I am aware that this happens because the process and its variables are forked.) In this dummy example it may not seem very important (even though it is clear that now I can't close the opened file handles), but in the large program these and other variables are modified and used inside the forked processes. The question is, then, how to make that work.I read about Pipe() and Queue() and it seems to me that I would need . In addition, because I will be reading and writing very often from the different processes to the same object I think I need a  as well. This is the part that I am uncertain about, however. I tried reading this topic but it only confused me more. Furthermore, the comments suggest that even on 3.6.4 there may be issues when using complex objects. Remember that the actual data that I am sharing is nested and consists of different types.The question, in summary, thus is: how can I re-write the above example code to ensure that all es have (non-blocking) read/write access to instance variables inside  and methods that are called from within that process? I am willing to update my current Python version (3.6.4) to 3.7, and to use additional libraries.
The  library lets you make use of parallelism in concurrent Python code. Without  the Python GIL tends to get in the way of true parallel execution, but you should see  code as no different from other concurrency techniques. Basically, the biggest difference between  and threads, is that state is shared via slow-ish IPC calls.This means you need to carefully handle shared resources. Your current implementation doesn't do a great job of this; you have multiple concurrent tasks access shared resources without regard for what others may be doing. There are multiple opportunities for race conditions in your code, where multiple tasks can write to the same file, or where a nested data structure is updated without regard for other updates.When you have to update shared data structures or files, you usually can pick between two options:Use synchronisation; anyone that needs to mutate a resource needs to obtain a shared lock first, or use some other form of synchronisation primitive to coordinate access.Make a single task responsible for mutating the resource. This usually involves one or more queues.Note that you'll have to pass either of these objects (synchronisation primitives or queues) to child processes explicitly, see the programming guidelines; don't use references on an instance to share state.For your case, I'd go with queues and dedicated tasks; your bottleneck is the data processing, writing data to disk and updating a few data structures with the results of the analysis tasks is relatively fast in comparison.So use a single task to write to files; just put the serialised XML string together with the  value into a dedicated queue, and have a separate task that pulls these from the queue and writes them to files. This separate task is then responsible for all file access, including opening and closing. This serialises file access and removes the possibility for race conditions and clobbered writes. If the data for these files comes in so thick and fast as to make this task a bottleneck, create tasks per target file.Do the same for the shared data structures; send mutations to a queue, leave it to a dedicated task to merge the data. Updating proxy objects is not really suitable because their changes propagate to other processes via RPC calls, increasing the chances for race conditions, and locking won't guarantee that the data is consistent across all task processes!For your simple example, updates to the  object are not actually shared; each child process inherits a copy when it forks and updates that local copy, and the parent process will never see the changes made. So you'd use a local, new  instance, and push that into a queue. A dedicated task can then receive these from the queue and update a local  instance with the values by using , again ensuring that updates are serialised.To illustrate, here is a contrived example counting Lorem Ipsum data; a series of  tasks do the counting, but pass the  object they produce to a queue for a separate collating task to combine into a final word count. A separate logging task writes data from a logging queue to disk:which produces something like:and a largish  with information pushed to the logging queue.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#synchronization-primitives
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue
https://docs.python.org/3/library/multiprocessing.html#all-start-methods
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.cancel_join_thread
https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues
