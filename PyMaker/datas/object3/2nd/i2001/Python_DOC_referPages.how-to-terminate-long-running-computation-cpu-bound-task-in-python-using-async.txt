Natural Text
Similar Question (but answer does not work for me): How to cancel long-running subprocesses running using concurrent.futures.ProcessPoolExecutor?Unlike the question linked above and the solution provided, in my case the computation itself is rather long (CPU bound) and cannot be run in a loop to check if some event has happened.Reduced version of the code below:The idea is that there is a main simulation loop that runs and monitors three bot threads. Each of these bot threads then perform some reasoning but also start a really long background process using , which may end up running longer their own threshold/max execution time for reasoning on things.As you can see in the code above, I attempted to  these tasks when a timeout occurs. Though this is not really cancelling the actual computation, which keeps happening in the background and the  loop doesn't terminate until after all the long running computation have finished.How do I terminate such long running CPU-bound computations within a method?Other similar SO questions, but not necessarily related or helpful:asyncio: Is it possible to cancel a future been run by an Executor?How to terminate a single async task in multiprocessing if that single async task exceeds a threshold time in PythonAsynchronous multiprocessing with a worker pool in Python: how to keep going after timeout?
How do I terminate such long running CPU-bound computations within a method?The approach you tried doesn't work because the futures returned by  are not cancellable. Although asyncio's  tries to propagate the cancellation, it is simply ignored by  once the task starts executing.There is no fundamental reason for that. Unlike threads, processes can be safely terminated, so it would be perfectly possible for  to return a future whose  terminated the corresponding process. Asyncio coroutines have defined cancellation semantics and would automatically make use of it. Unfortunately,  returns a regular , which assumes the lowest common denominator and treats a running future as untouchable.As a result, to cancel tasks executed in subprocesses, one must circumvent the  altogether and manage one's own processes. The challenge is how to do this without reimplementing half of . One option offered by the standard library is to (ab)use  for this purpose, because it supports reliable shutdown of worker processes. A  could work as follows:Instead of spawning a fixed number of processes, spawn a fixed number of 1-worker pools.Assign tasks to pools from an asyncio coroutine. If the coroutine is canceled while waiting for the task to finish in the other process, terminate the single-process pool and create a new one.Since everything is coordinated from the single asyncio thread, don't worry about race conditions such as accidentally killing a process which has already started executing another task. (This would need to be prevented if one were to support cancellation in .)Here is a sample implementation of that idea:A minimalistic test case showing cancellation:Note how the CPU usage never exceeds 3 cores, and how it starts dropping near the end of the test, indicating that the processes are  being terminated as expected.To apply it to the code from the question, make  an instance of  and change  to .


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
https://docs.python.org/3/library/concurrent.futures.html#future-objects
https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#module-multiprocessing.pool
https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#multiprocessing.pool.Pool.terminate
