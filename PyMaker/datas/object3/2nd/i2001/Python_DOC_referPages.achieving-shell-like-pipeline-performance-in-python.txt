Natural Text
[Edit: Read accepted answer first. The long investigation below stems from a subtle blunder in the timing measurement.]I often need to process extremely large (100GB+) text/CSV-like files containing highly redundant data that cannot practically be stored on disk uncompressed. I rely heavily on external compressors like lz4 and zstd, which produce stdout streams approaching 1GB/s.As such, I care a lot about the performance of Unix shell pipelines. But large shell scripts are difficult to maintain, so I tend to construct pipelines in Python, stitching commands together with careful use of .This process is tedious and error-prone, so I'd like a "Pythonic" way to achieve the same end, managing the stdin/stdout file descriptors in Python without offloading to . However, I've never found a method of doing this without greatly sacrificing performance.Python 3's documentation recommends replacing shell pipelines with the  method on . I've adapted this example to create the following test script, which pipes 3GB of  into a useless , which outputs nothing:Output:The  approach is more than twice as slow as . If we raise the input size to 90GB (), we confirm this is not a constant-time overhead:My assumption up to now was that  is simply a high-level abstraction for connecting file descriptors, and that data is never copied into the Python process itself. As expected, when running the above test  uses 100% CPU but  uses near-zero CPU and RAM.Given that, why is my pipeline so slow? Is this an intrinsic limitation of Python's ? If so, what does  do differently under the hood that makes it twice as fast?More generally, are there better methods for building large, high-performance subprocess pipelines in Python?
You're timing it wrong. Your  calls don't start and stop a timer; they just return a number of seconds since some arbitrary starting point. That starting point probably happens to be the first  call here, but it could be any point, even one in the future.The actual time taken by the  method is 4.862174164 - 2.412427189 = 2.449746975 seconds, not 4.862174164 seconds. This timing does not show a measurable performance penalty from .
Also, take this into account, for :Changed in version 3.3.1: bufsize now defaults to -1 to enable  buffering by default to match the behavior that most code expects. In  versions prior to Python 3.2.4 and 3.3.1 it incorrectly defaulted to 0  which was unbuffered and allowed short reads. This was unintentional  and did not match the behavior of Python 2 as most code expected.
In python3 there is "the python way" and "the one we don't mention". (Though it pains me to abuse RAM, there does seem to be rather a lot of it available these days.)


Answer URL
https://docs.python.org/3/library/subprocess.html#replacing-shell-pipeline
