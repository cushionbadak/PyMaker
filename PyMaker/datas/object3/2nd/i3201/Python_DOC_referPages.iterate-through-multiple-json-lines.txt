Natural Text
I've got a file with hundreds of JSON lines in it. I wrote a little python script that will let me extract some data but it only works for one line. I'm now wondering how I can loop through all the lines in my file if there's multiple.What I have so far is:which works for data like:but I would like to also make it work for data like:How can I get this done?My script currently returns:When i run the large file.
I've got a file with hundreds of JSON lines in it.No you don't, and that's the problem.Hundreds of JSON texts is not a valid JSON file. A valid JSON file is just one text. Which is why  is returning an error.Hundreds of JSON texts that each fit on exactly one line with newlines in between them is a valid file in other formats like JSONlines or NDJ. It's still not a valid JSON file, so you can't use , but you could use a JSONlines or NDJ library, or just parse it like this:For writing a JSONlines file, again, you can use a JSONlines library, or you can just make sure that each JSON text has no embedded newlines—which actually happens by default, if you don't specify non-default  or  parameters—and just write out  for each value.But hundreds of JSON texts that each take up multiple lines isn't a valid anything file.This is actually explained in the  module docs:Note Unlike  and , JSON is not a framed protocol, so trying to serialize multiple objects with repeated calls to  using the same fp will result in an invalid JSON file.What "not a framed protocol" means is basically that the format would be ambiguous. For example, if you did a , and then a , what you'd get in your file is . Which is the same thing you get from .If you can fix your file to be something valid, like JSONlines, that's the easy solution.If you can't…Well, pre-standardization, there was a concept of "JSON document", which meant basically a JSON text that's either an Array or Object. And a stream of JSON documents is not ambiguous.Since this isn't a standard format, you're probably not going to find a parser for it, so you'll have to write one yourself.One way you can do that is by using the  method in the  module. This will try to decode a JSON text, possibly with extra stuff after it, and also return the index to that extra stuff. Which, in your case, is the next JSON document.Since hundreds of objects of that size isn't too big, it's probably simpler to just read the whole file into memory and then parse it, so we don't have to worry about buffering:Remember that this will only work if your file is a stream of JSON documents—that is, the top-level values are always Array or Object. Also, if you're editing these files by hand, unlike JSONlines, which can skip one bad text and continue to parse the rest, there's now way to recover from an error here, because you have no idea where the next document starts.


Answer URL
https://docs.python.org/3/library/json.html#json.dumps
https://docs.python.org/3/library/json.html#json.JSONDecoder.raw_decode
