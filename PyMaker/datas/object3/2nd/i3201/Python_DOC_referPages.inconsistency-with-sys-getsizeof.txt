Natural Text
Why is  larger for a Python  of length 1 than for a string of length 2?  (For length > 2, the relationship seems to increase monotonically as expected.)Example:It seems it has to do with , but I don't know C well enough at all to dig into what's going on in this case.
When you , it does a whole ton of NumPy stuff, including calling  on all the single-ASCII-letter strings, and presumably somewhere else doing something similar on the single-ASCII-digit strings.That NumPy function calls the deprecated C API .When you call that in CPython 3.3+, that caches the  representation on the string's internal struct, in its  member, as the two wchar_t values  and , which takes 8 bytes on a 32-bit- build of Python. And  takes that into account.So, all of the single-character interned strings for ASCII letters and digits—but nothing else—end up 8 bytes larger.First, we know that it's apparently something that happens on  (per Brad Solomon's answer.) It may happen on  (miradulo posted, but then deleted, a comment to that effect on ShadowRanger's answer), but definitely not on .Second, we know that it happens to , but what about other single-character strings?To verify the former, and to test the latter, I ran this code:On multiple CPython installations (but all 64-bit CPython 3.4 or later on Linux or macOS), I got the same results:So,  changes nothing, and so does  (presumably why miradulo deleted the comment…), but  does.And it apparently affects ASCII digits and letters, but nothing else.Also, if you change all of the s to , so the strings never get encoded for output, you get the same results, which implies that either it's not about caching the UTF-8, or it is but that's always happening even if we don't force it.The obvious possibility is that whatever Pandas is calling is using one of the legacy  API to generate single-character strings for all of the ASCII digits and letters. So these strings end up not in compact-ASCII format, but in legacy-ready format, right? (For details on what that means, see the comments in the source.)Nope. Using the code from my , we can see that it's still in compact-ascii format:We can see that Pandas changes the size from 50 to 58, but the fields are still:… in other words, it's , length 1, mortal-interned, ASCII, compact, and ready.But, if you look at , before Pandas it's a null pointer, while after Pandas it's a pointer to the  string . And  takes that  size into account.So, the question is, how do you end up with an ascii-compact string that has a  value?Simple: you call  on it (or one of the other deprecated functions or macros that accesses the 3.2-style native  internal storage. That native internal storage doesn't actually exist in 3.3+. So, for backward compatibility, those calls are handled by creating that storage on the fly, sticking it on the  member, and calling the appropriate  function to decode to that storage. (Unless you're dealing with a compact string whose kind happens to match the  width, in which case  is just a pointer to the native storage after all.)You'd expect  to ideally include that extra storage, and from the source, you can see that it does.Let's verify that:Tada, our 50 goes to 58.So, how do you work out where this gets called?There are actually a ton of calls to , and the  macro, and other functions that call them, throughout Pandas and Numpy. So I ran Python in lldb and attached a breakpoint to , with a script that skips if the calling stack frame is the same as last time.The first few calls involve datetime formats. Then there's one with a single letter. And the stack frame is:… and above  it's pure Python all the way up to the . So, if you want to know exactly where Pandas is calling this function, you'd need to debug in , which I haven't done yet. But I think we've got enough info now.
Python 3.3+'s  is quite a complicated structure, and can end up storing the underlying data in up to three different ways, depending on which APIs have been used with the string and the code points represented by the string. The most common alternate representation case is a cached UTF-8 representation, but that only applies to non-ASCII strings so it doesn't apply here.In this case, I suspect the single character string (which, as an implementation detail, is a singleton) was used in a way that triggered the creation of the legacy  representation (extensions using the legacy  APIs can cause this), and your Python build uses a four byte , leading to the string being eight bytes bigger than it otherwise would be (four for the  itself, four more for the  terminator). The fact that it's a singleton means that even though you may never have triggered such a legacy API call, any extension which retrieved a reference to the singleton would affect the observed size for everyone by using it with the legacy API.Personally, I don't reproduce at all on my Linux 3.6.5 install (the sizes increase smoothly), indicating no  representation was created, and on my Windows 3.6.3 install,  is only 54 bytes, not 58 (which matched Windows' native two byte ). In both cases I'm running with ; it's possible different  dependencies with different versions are responsible for your (and my) inconsistent observations.To be clear, this extra cost is fairly immaterial; since the single character string is a singleton, the incremental cost of use is really just 4-8 bytes (depending on pointer width). You're not going to break the bank on memory if a handful of strings ended up used with the legacy APIs.
This appears to be related to a single Pandas import in an IPython startup file.I can reproduce the behavior in a plain Python session also:


Answer URL
https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_AsUnicode
https://docs.python.org/3/c-api/unicode.html#deprecated-py-unicode-apis
https://docs.python.org/3/c-api/unicode.html#c.PyUnicode_AsUnicode
https://docs.python.org/3/c-api/unicode.html#deprecated-py-unicode-apis
