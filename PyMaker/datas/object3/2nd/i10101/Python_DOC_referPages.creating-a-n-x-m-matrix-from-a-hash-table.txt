Natural Text
Imagine I have a dictionary / hashtable of pairs of strings (keys) and their respective probabilities (values):The  hashtable will look something like this:Imagine that this is the input hashtable that I'll read from CSV file with the first and second column being the word pairs (keys) of the hashtable and the third column the probabilitiesIf I were to put the probabilities into some sort of  matrix, I would have to do this from the hashtable:Is there another way to get the  into the |N| * |M| matrix from the hashtable without doing a nested loop through the m_vocab and n_vocab?(Note: I'm creating random words and random probabilities here but imagine I have read the hash table from a file and it's read into that hashtable structure)Assume both scenarios, where:The hashtable is from a  file (@bunji's answer resolves this)The hashtable comes from a pickled dictionary. Or that the hashtable was computed some other way before reaching the part where converting it into a matrix is necessary. It is important that the final matrix needs to be queryable, the following isn't desirable:The resulting matrix/dataframe should be queryable, i.e. is able to do something like:
I'm not sure if there is a way to completely avoid looping but I imagine it could be optimized by using :
If your end goal is to read in your data from a .csv file, it might be easier to read the file directly using pandas. this reads your data from the csv, makes the first two columns into a multi-index which corresponds to your two sets of words. It then unstacks the multi-index so that you have one set of words as the column labels and another as the index labels.  This gives your your |N|*|M| matrix which can then be converted into a numpy array with the  function. This doesn't really resolve your question about changing your  dictionary into a numpy array but given your intentions, this will allow you to avoid the need to create that dictionary altogether. Also, if you're going to be reading in the csv anyway, reading it using pandas in the first place is going to be faster than using the builtin  module anyway: see these benchmark tests hereEDITIn order to query a specific value in your DataFrame based on the row and column labels, :where  is your word in your row label and  is your column label. Also check out  and  for other ways of querying specific cells in your DataFrame. 
[a short extension of the answer of dr-xorile]Most solution look good to me. Depends a little if you need speed or convenience. I agree that you have basically a matrix in coo sparse format. You might want to look at https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.htmlOnly problem is that matrices need integer indices. So as long as you hashes are small enough to be quickly expressed as a  that should work. And the sparse format should allow an $O(1)$ access to all elements.(Sorry for the brevity!)rough outlineThis could potentially be fast but is kind of hacky.get the data in sparse representation. I think you should pick  to just hold your 2D hash map. a. load the CSV using  and use e.g. datatype  to treat the hashes as string representations of unsigned 8byte integer numbers. If that does not work you might load strings and use numpy to convert it. Finally you have three tables of size N * M like your hash table and use these with the scipy sparse matrix representation of your choice. b. if you have the object already in memory you might be able to use the sparse constructor directlyTo access you need to parse your strings again
It seems a bit inefficient to go through the entire n_vocab x m_vocab space for a sparse matrix! You could loop over the original hashes table. It would be good to know a couple of things first, of course:Do you know the size of n_vocab and m_vocab upfront? Or are you going to figure that out as you build it?  Do you know if there are any repetitions in your hash table, and if so how will you handle it? It looks like hash is a dictionary, in which case, obviously the keys are unique. In practice, that probably means you're over-writing each time, and so the last value is what will stand.In any event, here's a comparison of the two options:The output of probs1 and probs2 is, of course, the same:And, of course, your code for probs1 is very terse. However, the size of the loops is substantially different, and it could make a big difference to the run time
I tried to reduce sample size to quickly compare different codes. I coded dataframe method, which might still use for loop in pandas function, and compared to the original code and itertools code provided by Tadhg McDonald-Jensen. The fastest code is itertools.This is the code I used to compare.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.product
