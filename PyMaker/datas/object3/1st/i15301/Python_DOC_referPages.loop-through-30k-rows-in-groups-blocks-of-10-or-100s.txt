Natural Text
I have a Python script which takes as input a list of integers, which I need to work with four integers at a time.  Unfortunately, I don't have control of the input, or I'd have it passed in as a list of four-element tuples.  Currently, I'm iterating over it this way:It looks a lot like "C-think", though, which makes me suspect there's a more pythonic way of dealing with this situation.  The list is discarded after iterating, so it needn't be preserved.  Perhaps something like this would be better?Still doesn't quite "feel" right, though.  :-/Related question: How do you split a list into evenly sized chunks in Python?
Modified from the recipes section of Python's itertools docs:ExampleIn pseudocode to keep the example terse.Note: on Python 2 use izip_longest instead of zip_longest.
Simple. Easy. Fast. Works with any sequence:
I'm a fan of 
Another way:

I needed a solution that would also work with sets and generators. I couldn't come up with anything very short and pretty, but it's quite readable at least.List:Set:Generator:
The ideal solution for this problem works with iterators (not just sequences). It should also be fast.This is the solution provided by the documentation for itertools:Using ipython's %timeit on my mac book air, I get 47.5 us per loop.However, this really doesn't work for me since the results are padded to be even sized groups. A solution without the padding is slightly more complicated. The most naive solution might be:Simple, but pretty slow: 693 us per loopThe best solution I could come up with uses islice for the inner loop:With the same dataset, I get 305 us per loop.Unable to get a pure solution any faster than that, I provide the following solution with an important caveat: If your input data has instances of filldata in it, you could get wrong answer.I really don't like this answer, but it is significantly faster. 124 us per loop
Similar to other proposals, but not exactly identical, I like doing it this way, because it's simple and easy to read:This way you won't get the last partial chunk. If you want to get (9, None, None, None) as last chunk, just use izip_longest from itertools.
Using map() instead of zip() fixes the padding issue in J.F. Sebastian's answer:Example:
Since nobody's mentioned it yet here's a zip() solution:It works only if your sequence's length is always divisible by the chunk size or you don't care about a trailing chunk if it isn't.Example:Or using itertools.izip to return an iterator instead of a list:Padding can be fixed using @ΤΖΩΤΖΙΟΥ's answer:
If you don't mind using an external package you could use iteration_utilities.grouper from iteration_utilties 1. It supports all iterables (not just sequences):which prints:In case the length isn't a multiple of the groupsize it also supports filling (the incomplete last group) or truncating (discarding the incomplete last group) the last one:1 Disclaimer: I'm the author of that package.
If the list is large, the highest-performing way to do this will be to use a generator:
Using little functions and things really doesn't appeal to me; I prefer to just use slices:
Another approach would be to use the two-argument form of iter: This can be adapted easily to use padding (this is similar to Markus Jarderot’s answer):These can even be combined for optional padding:
With NumPy it's simple:output:
In your second method, I would advance to the next group of 4 by doing this:However, I haven't done any performance measurement so I don't know which one might be more efficient.Having said that, I would usually choose the first method. It's not pretty, but that's often a consequence of interfacing with the outside world.
Yet another answer, the advantages of which are:1) Easily understandable2) Works on any iterable, not just sequences (some of the above answers will choke on filehandles)3) Does not load the chunk into memory all at once4) Does not make a chunk-long list of references to the same iterator in memory5) No padding of fill values at the end of the listThat being said, I haven't timed it so it might be slower than some of the more clever methods, and some of the advantages may be irrelevant given the use case.Update:A couple of drawbacks due to the fact the inner and outer loops are pulling values from the same iterator:1) continue doesn't work as expected in the outer loop - it just continues on to the next item rather than skipping a chunk. However, this doesn't seem like a problem as there's nothing to test in the outer loop.2) break doesn't work as expected in the inner loop - control will wind up in the inner loop again with the next item in the iterator. To skip whole chunks, either wrap the inner iterator (ii above) in a tuple, e.g. for c in tuple(ii), or set a flag and exhaust the iterator.

You can use partition or chunks function from funcy library:These functions also has iterator versions ipartition and ichunks, which will be more efficient in this case.You can also peek at their implementation.
To avoid all conversions to a list import itertools and:Produces:I checked groupby and it doesn't convert to list or use len so I (think) this will delay resolution of each value until it is actually used.  Sadly none of the available answers (at this time) seemed to offer this variation.Obviously if you need to handle each item in turn nest a for loop over g:My specific interest in this was the need to consume a generator to submit changes in batches of up to 1000 to the gmail API:
About solution gave by J.F. Sebastian here:It's clever, but has one disadvantage - always return tuple. How to get string instead?Of course you can write ''.join(chunker(...)), but the temporary tuple is constructed anyway.You can get rid of the temporary tuple by writing own zip, like this:ThenExample usage:
I like this approach. It feels simple and not magical and supports all iterable types and doesn't require imports.
I never want my chunks padded, so that requirement is essential.  I find that the ability to work on any iterable is also requirement.  Given that, I decided to extend on the accepted answer, https://stackoverflow.com/a/434411/1074659.  Performance takes a slight hit in this approach if padding is not wanted due to the need to compare and filter the padded values.  However, for large chunk sizes, this utility is very performant.

Here is a chunker without imports that supports generators:Example of use:
There doesn't seem to be a pretty way to do this.  Here is a page that has a number of methods, including:
If the lists are the same size, you can combine them into lists of 4-tuples with zip(). For example:Here's what the zip() function produces:If the lists are large, and you don't want to combine them into a bigger list, use itertools.izip(), which produces an iterator, rather than a list.
One-liner, adhoc solution to iterate over a list x in chunks of size 4 -
At first, I designed it to split strings into substrings to parse string containing hex.Today I turned it into complex, but still simple generator.Arguments:Obvious onesiterable is any iterable / iterator / generator containg / generating / iterating over input data,size is, of course, size of chunk you want get,More interestingreductor is a callable, which receives generator iterating over content of chunk.I'd expect it to return sequence or string, but I don't demand that.You can pass as this argument for example list, tuple, set, frozenset,or anything fancier. I'd pass this function, returning string(provided that iterable contains / generates / iterates over strings):Note that reductor can cause closing generator by raising exception.condition is a callable which receives anything what reductor returned.It decides to approve & yield it (by returning anything evaluating to True),or to decline it & finish generator's work (by returning anything other or raising exception).When number of elements in iterable is not divisible by size, when it gets exhausted, reductor will receive generator generating less elements than size.Let's call these elements lasts elements.I invited two functions to pass as this argument:  lambda x:x - the lasts elements will be yielded.lambda x: len(x)==<size> - the lasts elements will be rejected.replace <size> using number equal to size
It is easy to make itertools.groupby work for you to get an iterable of iterables, without creating any temporary lists:Don't get put off by the nested lambdas, outer lambda runs just once to put count() generator and the constant 100 into the scope of the inner lambda.I use this to send chunks of rows to mysql.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.zip_longest
https://docs.python.org/3/library/itertools.html#itertools.groupby
