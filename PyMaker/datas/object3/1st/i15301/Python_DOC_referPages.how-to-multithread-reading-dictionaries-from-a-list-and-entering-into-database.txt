Natural Text
I am trying to multithread the following code and I just can't seem to get it working. The following code (of which I removed most of the code just for illustrative purposes) currently works smoothly, but slowly (approximately 5 minutes for a list of 3600 tweets).As such, I was looking for an option to do this multithreaded. However, I have not yet found a way in which I can:Multithread the extraction process;Print a counter per 100, 500 or 1000 tweets;Going through this tutorial hasn't given me the understanding to do this yet: the concepts of a class for each thread, what I need to customize in the class and implementing a Queue at the moment is a lot for me to grasp at the moment; I'm only semi-starting out.Could someone provide hints as to how I would incorporate the script above utilizing multiple threads? How many threads should I use? Python currently uses ~1% of my CPU while running the script and ~ 10% of RAM (my system specs)How do I take care of incrementing a counter (using the Lock()?), and printing upon hitting counter % 100?EDIT: As requested: here are the big shots from the profiler result (with dataset.upsert):Here is a second try with 'dataset.insert' instead of 'dataset.upsert':Last (and definitely not least), here's the timing when running raw psycopg2 code. Concluding, don't use dataset for performance (though writing the psycopg2 code took me 10 minutes which is >> the 10 seconds for a dataset.upsert)Now, as for the original question. Will I be able to reduce the ~ 2 second per file even more by multithreading? How?The full code can be found here
Several things that can be improved: Run the whole batch on a single transaction. Using transaction means the database are not required to actually commit (write the data to disk) on every single writes, but rather it can buffer uncommitted data on memory. This usually leads to more efficient resource usage.Add a unique index over tweet_id. Without a unique index, you may be forcing the database to do a sequential scan on every upserts, which leads bulk upsert to scale by O(n**2).Split the insert and updates, use .insert_many() whenever you can rather than .upsert(). Before doing the bulk upsert, you do a preflight query to find out the list of tweet_ids that exists in both the database and your list of tweets. Use .insert_many() to insert items that don't already exists in the database and plain .update() for those that already exists.
I dont know if you'll be able to improve performance. But as for how I think you'll want concurrent.futures.Executor.map. ProcessPoolExecutor rather than ThreadPoolExecutor should be what you want, although I'm no expert.https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.mapIf you want to show progress have a look at concurrent.futures.as_completed from the same module.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.map
