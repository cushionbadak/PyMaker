Natural Text
I have a large input file I need to read from so I don't want to use enumerate or fo.readlines().for line in fo: in the traditional way won't work and I'll state why, but I feel some modification to that is what I need right now. Consider the following file:What I need is to be able to read variable chunks of lines, pair the coordinates in  tuple, add tuple to a list of cases and move back to reading a new case from the file.I thought of this:Then couple the two lists into a tuple. I feel there must be a cleaner way to do this. Any suggestions?EDIT: The y-coordinates will have to have a separate for loop to read. They are x and y coordinates are n lines apart. So Read line i; Read line (i+n); Repeat n times - for each case.
This might not be the shortest possible solution but I believe it is “pretty optimal”.It will eagerly parse all coordinates of a single test but each test will only be parsed lazily as you ask for it.You can use it like this to iterate over the tests:Better function names might be desired to better distinguish eager from lazy functions. I'm experiencing a lack of naming creativity right now.
how about this with the grouper recipehere I use grouper to read N lines at the time getting a list of tuples with all the x and all the y, then use zip to pair all those together
It sounds like you're not really trying to "read a file line by line."  It sounds like you want to skip around the file, treating it like a large list/array but without triggering excessive memory consumption.Have you looked at the mmap module?With that you can use methods like .find() to find newlines, optionally starting at some offset (such as just past your current test header) and .seek() to move the file pointer to the nth item you've found and then .readline() and so on.An mmap object shares some methods and properties of a string or byte array and some from file like objects.  So you can use a mixture of methods like .find() (normal for strings and byte arrays) and .seek() (for files).Additionally the Python memory mapping uses your operating system's features for mapping files into memory.  (On Linux and similar systems this is the same mechanism by which your shared libraries are mapped into the address space for all of your running processes, for example).  The key point is that you memory is only used as a cache for the contents of the file, and the operating system will transparently perform the necessary I/O for loading and release memory buffers with the file's contents.I don't see a method for finding the "nth" occurrence of some character or string ... so there's no way to skip to some line.  As far as I can tell you'll have to loop over .find() but then you can skip back to any such line using Python's slice notation.  You could write a utility class/object to scan for 1000 line terminators at a time, storing them in an index/list.  Then you can use values from that in slices of the memory mapping.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools-recipes
