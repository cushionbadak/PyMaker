Natural Text
I am using the following code in which I have a dictionary file,  Dictionary.txt, and a search text file, SearchText.csv, and I am using regex to find and store the matching keywords and count them.I have a problem: some of the files are thousands or hundreds of thousands of keywords and it takes too much time to process. I run the code on one dictionary which has 300,000 keywords and after an hour it hasn't written a single row.So, what should I do to reduce the running time of this process?Here is an example of some rows from Dictionary.txt:
Your biggest time waster if this line:You are searching the whole description for each eachcity. That's a lot of searching. Consider pre-splitting description into words with nltk.word_tokenize(), converting it to a set, converting allCities into a set as well, and taking a set intersect. Something like this:No inner loop required.
Perform operations which only need to be executed once only once:Instead of andin the loop dooutside of the loop, and convert description to lowercase.You can remove matches += 1 as well, (it's the same as len(citiesFound)), but that will not give much improvement.If you do not know where your bottleneck really is, look at the tips here and here. Also, run a profiler on your code to find the real culprit. There is also a SO question regarding profiling which is very useful.Another possibility is to use C or languages which are more optimized for text handling, like awk or sed.
Use databases instead of the file system.In your case I'd probably use Elasticsearch or MongoDB.Those systems are made for handling large amounts of data.
In addition to Jan Christoph Terasa answer1. allCities - are candidate for setSo: and even more:2. Use set of precompiled regular expressions


Answer URL
https://docs.python.org/3/library/profile.html
