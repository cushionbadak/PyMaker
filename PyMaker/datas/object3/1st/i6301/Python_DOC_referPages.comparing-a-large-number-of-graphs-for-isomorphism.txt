Natural Text
I am comparing a large set of networkx graphs for isomorphism, where most of the graphs should not be isomorphic (Lets say 0-20% are isomorphic to something in the list, for example).I have tried the following approach.This let me get a much faster reduced set, but I still find it too slow for ideal use.  Is there some faster algorithm to handle this type of problem (comparing pairs of transitive commutative properties) or a way to extend this algorithm to a multicore setup (running on a 20 core machine).I am already filtering these sets of data based on the number of nodes / edges, we can assume that the nx.is_isomorphic function cannot be made faster by any filtering types of operations.  I also cannot change tools easily right now, so using a compiled package is not an option.Additional Information:Graphs tend to be roughly 16-20 nodes with 24-48 edges total,  there is a lot of interconnection so each node has roughly 8 edges.  Each edge is labeled as well, but there are only 2-3 types of edges ever used.
As others have mentioned, if you want to stay in Python + Networkx, you could use could_be_isomorphic to filter your graphs.The problem is that this method expects 2 graphs as an input, not millions. If you compare every pair of graphs with this method, it would take an awfully long time.Looking at the sourcecode of could_be_isomorphic, it compares degree, triangle, and number of cliques sequences for both graphs. If they're not equal, the graphs cannot be isomorphic.You could pack this fingerprint in a function, sort your graphs according to this fingerprint and group them with itertools.groupby. There will be a huge majority of lone graphs. The few graphs that have the same fingerprints can then be checked for isomorphism.Using a list of 100 000 random graphs:There were only 500 fingerprints that were shared by at least 2 graphs. If you add edge types information, there will be even fewer common fingerprints.Here's an example with 3000 graphs, each having between 10 and 14 nodes:It finds 4 isomorphic pairs in less than 1s:Here are the 2 last isomorphic graphs. "IDOCCY@GG":and "IOGC@\dS?":Here are 2 graphs which have the same fingerprint but aren't isomorphic:The fingerprinting could be done in parallel. Sorting and grouping would have to happen on 1 CPU, but the isomorphism check for each group could be done in distinct CPUs.
Can you use nauty (http://users.cecs.anu.edu.au/~bdm/nauty/, available in linux distributions)?  That has a canonical label algorithm that is fast and might work for your problem.  A canonical labeling makes isomorphic graphs identical (canonization).  For example using graph6 format output from a set of random graphs gives the following count of isomorphic graphsThose are the 11 graphs of 4 nodes -It would be pretty easy to parallelize this approach if it runs too slow.
You can try your code on PyPy which provides just-in-time compilation for pure Python code. For possible performance boost they say it......depends greatly on the type of task being performed. The geometric average of all benchmarks is 0.13 or 7.5 times faster than CPython  If your workload is CPU-bound (which seems like so) and Python process is long-running (so JIT compilation can be performed) then performance boost can be significant. NetworkX is pure Python (it has optional dependencies like numpy, but they are needed for extra functionality) and specifically isomorph module. I tried PyPy 5.7.1 and networkx/algorithms/isomorphism/tests/test_isomorphism.py passes. The the suite in general has a few failures:On Python 2.7.12 it's:


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.groupby
