Natural Text
I got a huge list of object: (about 500k elts).My program maps the list and generates .pkl files with a specific name for each element of the list.Some of the elements from the list were already computed, OR, a permutation of this element was computed, thus I would like to delete them before mapping.That is what I come up with. It is fairly not efficient, and I'd be glad to see your proposition to improve this!Thanks!N.B: This is a fairly simplified version of my program. The objects are far heavier (10+ parameters).
First, if you have so many of your objects you may want to consider slots, namedtuples, pandas dataframes or numpy ndarrays. This would reduce the cost of each item considerably, removing per-object dictionaries or even per-row object metadata. Second, removing items from an array is a costly operation involving moving all the items following it. This applies to Python's lists when using del or remove; even worse with the latter, it has to find the item first, so you're reading the whole array and rewriting part of it for every item you remove. At that point it's better to build a copy containing the items you keep instead. Another option is to replace the irrelevant items with placeholders such as None, an operation that doesn't require moving other entries. Third, it is frequently more efficient to not build your collections at all. Consider: In this code, you construct a (likely large) list named possibilities, grow it by consuming a permutations iterator in a for loop and calling file_namer for each item (not even passing that item!), then build another list of whether each possibility was already in files, and finally apply any() to that list for a result. That's at least two passes over the entire collection of possibilities for an answer that might have only needed to inspect one. I'm not sure the first loop even needs to exist, and the list comprehension should certainly be a generator expression to allow the any function to shortcut. So, assuming there are no side effects hidden in file_namer etc, we could simplify the entire function to:But if file_namer(s) should really be file_namer(elt), as I would expect, it should be:Another concern, since we're looking at repeated in tests for files, is that we should probably make sure that's a set, dict or other type with quick membership tests. This would be the point where Dan D's suggestion of sorting instead of repeatedly generating permutations applies. For instance, you could have an index of the lowest-valued (sorted) permutation to the actual object stored in a dictionary. If for some reason you can't make the keys hashable, you might be able to use binary searches if they're sortable. That's what came to mind at the moment. I haven't read thoroughly. 
I would suggest that you don't remove from the list. Build another:Then I would look at your is_computed function and see if one can avoid building the list of possibilities:The test: name in files would be faster if files is a set.Better still:Parse each of the filenames such as this one:Back into the tuple:Then you can avoid the permutations call by sorting (1,2,3) and (2,3,1): Into the same order and then comparing the sorted versions. So to extend this to a is_computed replacement, files is turned from ['F(1, 2, 3).pkl'] into {(1, 2, 3):'F(1, 2, 3).pkl'} then is_computed becomes:


Answer URL
https://docs.python.org/3/reference/datamodel.html#slots
https://docs.python.org/3/library/collections.html#collections.namedtuple
https://docs.python.org/3/reference/expressions.html#list-displays
https://docs.python.org/3/reference/expressions.html#generator-expressions
https://docs.python.org/3/library/functions.html#any
https://docs.python.org/3/reference/datamodel.html#object.__contains__
https://docs.python.org/3/library/bisect.html
