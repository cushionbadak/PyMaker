Natural Text
Pretty much I need to write a program to check if a list has any duplicates and if it does it removes them and returns a new list with the items that werent duplicated/removed. This is what I have but to be honest I do not know what to do.
The common approach to get a unique collection of items is to use a set. Sets are unordered collections of distinct objects. To create a set from any iterable, you can simply pass it to the built-in set() function. If you later need a real list again, you can similarly pass the set to the list() function.The following example should cover whatever you are trying to do:As you can see from the example result, the original order is not maintained. As mentioned above, sets themselves are unordered collections, so the order is lost. When converting a set back to a list, an arbitrary order is created.If order is important to you, then you will have to use a different mechanism. A very common solution for this is to rely on OrderedDict to keep the order of keys during insertion:Note that this has the overhead of creating a dictionary first, and then creating a list from it. So if you don’t actually need to preserve the order, you’re better off using a set. Check out this question for more details and alternative ways to preserve the order when removing duplicates.Finally note that both the set as well as the OrderedDict solution require your items to be hashable. This usually means that they have to be immutable. If you have to deal with items that are not hashable (e.g. list objects), then you will have to use a slow approach in which you will basically have to compare every item with every other item in a nested loop.
In Python 2.7, the new way of removing duplicates from an iterable while keeping it in the original order is:In Python 3.5, the OrderedDict has a C implementation. My timings show that this is now both the fastest and shortest of the various approaches for Python 3.5.In Python 3.6, the regular dict became both ordered and compact.  (This feature is holds for CPython and PyPy but may not present in other implementations).  That gives us a new fastest way of deduping while retaining order:In Python 3.7, the regular dict is guaranteed to both ordered across all implementations.  So, the shortest and fastest solution is:
It's a one-liner: list(set(source_list)) will do the trick.A set is something that can't possibly have duplicates.Update: an order-preserving approach is two lines:Here we use the fact that OrderedDict remembers the insertion order of keys, and does not change it when a value at a particular key is updated. We insert True as values, but we could insert anything, values are just not used. (set works a lot like a dict with ignored values, too.)

If you don't care about the order, just do this:A set is guaranteed to not have duplicates.
To make a new list  retaining the order of first elements of duplicates in Lnewlist=[ii for n,ii in enumerate(L) if ii not in L[:n]]for example if L=[1, 2, 2, 3, 4, 2, 4, 3, 5] then newlist will be [1,2,3,4,5]This checks each new element has not appeared previously in the list before adding it. Also it does not need imports. 
A colleague have sent the accepted answer as part of his code to me for a codereview today.While I certainly admire the elegance of the answer in question, I am not happy with the performance.I have tried this solution (I use set to reduce lookup time)To compare efficiency, I used a random sample of 100 integers - 62 were uniqueHere are the results of the measurementsWell, what happens if set is removed from the solution?The result is not as bad as with the OrderedDict, but still more than 3 times of the original solution
Another way of doing:
There are also solutions using Pandas and Numpy. They both return numpy array so you have to use the function .tolist() if you want a list.Pandas solutionUsing Pandas function unique():Numpy solutionUsing numpy function unique().Note that numpy.unique() also sort the values. So the list t2 is returned sorted. If you want to have the order preserved use as in this answer:The solution is not so elegant compared to the others, however, compared to pandas.unique(), numpy.unique() allows you also to check if nested arrays are unique along one selected axis.
Simple and easy:Output:
I had a dict in my list, so I could not use the above approach. I got the error:So if you care about order and/or some items are unhashable. Then you might find this useful:Some may consider list comprehension with a side effect to not be a good solution. Here's an alternative:
Try using sets:
You could also do this:The reason that above works is that index method returns only the first index of an element. Duplicate elements have higher indices. Refer to here:list.index(x[, start[, end]])  Return zero-based index in the list of  the first item whose value is x.    Raises a ValueError if there is no  such item.
All the order-preserving approaches I've seen here so far either use naive  comparison (with O(n^2) time-complexity at best) or heavy-weight OrderedDicts/set+list combinations that are limited to hashable inputs. Here is a hash-independent O(nlogn) solution:Update added the key argument, documentation and Python 3 compatibility.
Reduce variant with ordering preserve:Assume that we have list:Reduce variant (unefficient):5 x faster but more sophisticatedExplanation:
Best approach of removing duplicates from a list is using set() function, available in python, again converting that set into list
Without using set 
You can use the following function: Example: Usage:['this', 'is', 'a', 'list', 'with', 'dupicates', 'in', 'the']
This one cares about the order without too much hassle (OrderdDict & others). Probably not the most Pythonic way, nor shortest way, but does the trick:
below code is simple for removing duplicate in listit returns [1,2,3,4]
There are many other answers suggesting different ways to do this, but they're all batch operations, and some of them throw away the original order. That might be okay depending on what you need, but if you want to iterate over the values in the order of the first instance of each value, and you want to remove the duplicates on-the-fly versus all at once, you could use this generator:This returns a generator/iterator, so you can use it anywhere that you can use an iterator.Output:If you do want a list, you can do this:Output:
One more better approach could be,and the order remains preserved.
Here's the fastest pythonic solution comaring to others listed in replies.Using implementation details of short-circuit evaluation allows to use list comprehension, which is fast enough. visited.add(item) always returns None as a result, which is evaluated as False, so the right-side of or would always be the result of such an expression.Time it yourself
Using set :Using unique :
Very simple way in Python 3:
Here is an example, returning list without repetiotions preserving order. Does not need any external imports.
Check this if you want to remove duplicates (in-place edit rather than returning new list) without using inbuilt set, dict.keys, uniqify, counter
I think converting to set is the easiest way to remove duplicate:
You can use set to remove duplicates:But note the results will be unordered. If that's an issue:
To remove the duplicates, make it a SET and then again make it a LIST and print/use it.A set is guaranteed to have unique elements. For example : The output will be as follows (checked in python 2.7)


Answer URL
https://docs.python.org/3/library/collections.html#collections.OrderedDict
https://docs.python.org/3/tutorial/datastructures.html#more-on-lists
