Natural Text
I'm writing a function that returns the number of times appeared of a word that appeared the most in the list of words.For example:But this function is slow when dealing with large amount of words in a list, for example, a list of 250,000 words takes over 4 minutes for this function to present the output. So I'm seeking help to tweak this.I don't want to import anything.
While I fully agree with the comments related to your I don't want to import anything statement, I found your question amusing, so let's try it.You don't need to build a set. Just go directly with words.
To prevent multiple passes of your list for each unique word, you can simply iterate over it once and update dictionary values for each count. Outputs:That being said, this can be done much nicer using a defaultdict instead of get or using collections.Counter... limiting yourself to no imports in Python is never really a good idea if you have the choice. For instance, using collections.Counter:
You can try this code which is about 760% faster.
Data size similar to OP'sLet's start with a list of wordsand compone at random these words to form a textDifferent methods are possibleFrom the text we can have a list of words in the text (note, wl is very different from words...)from this list we want to extract a dictionary, or a dictionary-like object, with a count of the occurrences, we have many options.First option, we build a dictionary containing all the different words in wl and we set the value, for each key, equal to zero and later we do another loop on the list of words to count the occurrencesSecond option, we start with an empty dictionary and use the get() method that allows for a default valueThird and last option, we use a component of the standard libraryIs one method better than the others?Which is best? the one you like the most... at any rate, here are the respective timingsAs expected, the fastest procedure is the one that uses collections.Counter, but I was a little surprised in noticing that the first option, that makes TWO passes over the data, is faster than the second one... my guess (I mean: guess) is that all the testing for new values is done while instantiating the dictionary, probably inside some C code.


Answer URL
https://docs.python.org/3/library/collections.html#collections.Counter
