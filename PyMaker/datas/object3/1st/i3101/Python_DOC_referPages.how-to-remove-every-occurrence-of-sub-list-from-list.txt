Natural Text
I have two lists:I want to remove all sub_list occurrences in big_list.result should be [2, 3, 4]For strings you could use this:But AFAIK this does not work for lists.This is not a duplicate of Removing a sublist from a list since I want to remove all sub-lists from the big-list. In the other question the result should be [5,6,7,1,2,3,4].Update: For simplicity I took integers in this example. But list items could be arbitrary objects.Update2:if big_list = [1, 2, 1, 2, 1] and sub_list = [1, 2, 1], I want the result to be [2, 1] (like '12121'.replace('121', ''))Update3:I don't like copy+pasting source code from StackOverflow into my code. That's why I created second question at software-recommendations: https://softwarerecs.stackexchange.com/questions/51273/library-to-remove-every-occurrence-of-sub-list-from-list-pythonUpdate4: if you know a library to make this one method call, please write it as answer, since this is my preferred solution. The test should pass this test:
You'd have to implement it yourself. Here is the basic idea:This steps along every element of the original list and adds it to an output list if it isn't a member of the subset. This version is not very efficient, but it works like the string example you provided, in the sense that it creates a new list not containing your subset. It also works for arbitrary element types as long as they support ==. Removing [1,1,1] from [1,1,1,1] will correctly result in [1], as for a string.Here is an IDEOne link showing off the result of
Try del and slicing. The worst time complexity is O(N^2).result:
A recursive approach:This outputs:
A improved version to check whether lst[i:i+len(sub)] < len(lst)
How about this:Performance:UpdateMy first solution had a bug. Was able to fix it (updated my code above) but the method looks way more complicated now. In terms of performance it still does better than the solution from Mad Physicist on my local machine.
Use itertools.zip_longest to create n element tuples (where n is length of sub_list) and then filter the current element and next n-1 elements when one of the element matched the sub_listTo improve the efficiency, you can calculate tuple(sub_list) and len(sub_list) before hand you start filtering
Update: The more_itertools library has released  more_itertool.replace, a tool that solves this particular problem (see Option 3).  First, here are some other options that work on generic iterables (lists, strings, iterators, etc.):CodeOption 1 - without libraries:Option 2 - with more_itertoolsDemo Option 3 - with more_itertools.replace
More readable than any above and with no additional memory footprint:This is for onliner if you wanted a function from library, let it be this
Kinda different approach in Python 2.x!Note that you need to delete them in reverse order so that you don't throw off the subsequent indexes.In Python3, signature of locate() will differ.
(For final approach, see last code snippet)I'd have thought a simple string conversion would be sufficient:I'm essentially doing a find/replace with the string equivalents of the lists. I'm mapping them to integers afterwards so that the original types of the variables are retained. This will work for any size of the big and sub lists.However, it's likely that this won't work if you're calling it on arbitrary objects if they don't have a textual representation. Moreover, this method results in only the textual version of the objects being retained; this is a problem if the original data types need to be maintained.For this, I've composed a solution with a different approach:Essentially, I'm removing every duplicate of the sub_list when I find them and am appending to the new_list when I find an element which isn't part of a duplicate. When the new_list and big_list are equal, all of the duplicates have been found, which is when I stop. I haven't used a try-except as I don't think there should be any indexing errors.This is similar to @MadPhysicist's answer and is of roughly the same efficiency, but mine consumes less memory.This second approach will work for any type of object with any size of lists and therefore is much more flexible than the first approach. However, the first approach is quicker if your lists are just integers.However, I'm not done yet! I've concocted a one-liner list comprehension which has the same functionality as the second approach!Initially, this seems daunting, but I assure you it's quite simple! First, i create a list of the indices where the first element of the sublist has occured. Next, for each of these indices, I check if the following elements form the sublist. If they do, the range of indices which form the duplicate of the sublist is added to another list. Afterwards, I use a function from itertools to flatten the resulting list of lists. Every element in this flattened list is an index which is in a duplicate of the sublist. Finally, I create a new_list which consists of every element of the big_list which has an index not found in the flattened list.I don't think that this method is in any of the other answers. I like it the most as it's quite neat once you realise how it works and is very efficient (due to the nature of list comprehensions).
You can use recursion with a generator:Output:
Just for fun, here is the closest approximation to a one-liner:Don't trust that it works? Check it on IDEone!Of course it's far from efficient and is disgustfully cryptic, however it should help to convince the OP to accept @Mad Physicist's answer.
What you are trying to achieve can be done by converting it into list of strings and after replacing again convert it to integer type.In a single line you can do it like thisInputOutput[2, 1]InputOuput[2, 3, 4]
As compact as I can:Same approach in R:I think R is easier. [big_list[x] for x in [x for x in range(len(big_list)) if x not in a]] is equivalent to big_list[-a] 


Answer URL
https://docs.python.org/3/library/itertools.html
https://docs.python.org/3/library/itertools.html#itertools-recipes
