Natural Text
I was writing a code in python to find factor pairs for an integer. But making pairs resulted in reverse pairs as well. I want to eliminate those reverse pairs using a simple method without importing any modules.for eg.[[1, 200], [2, 100], [4, 50], [5, 40], [8, 25], [10, 20], [20, 10], [25, 8], [40, 5], [50, 4], [100, 2], [200, 1]]the output should be:This is what I've got so far:

Using set to remove duplicates.Ex:Output:
If the input is large, there is a significant performance advantage by using sets instead of lists.The inner sets must be frozenset because regular sets are mutable, and sets can only contain immutable children.To convert back to a list of lists.Sets are iterable, so depending on your usage, this step might not be needed.Here's a function that does both steps and returns the result as a nested list.Note that converting a list into a set will convert a list containing an identical pairs (for example [20,20]) to a single element set ({20}). So if your input can contain identical pairs, you might want to do an extra final step to expand singletons back to pairs.This will work with bot twin pairs and mixed pairs.
You can keep a set to keep track of what has been seen, and use a frozenset() to hash the lists into the seen set:Which Outputs:If you later want to use libraries, you could use a collections.OrderedDict():

You can use toolz.unique to maintain ordering at the outer level. If you don't have access to the 3rd party toolz library, you can use the unique_everseen recipe from the official docs.Tuple conversion is required since unique uses hashing; and tuples are hashable while lists are not. If it's important you have a list of lists, you can apply an extra conversion:At this stage, it's not particularly readable, so I suggest you split into a few steps:
I am expecting downvote because my answer seems abit off topic.In the first place, you only have to check value up to int((prod+1)**0.5)+1 which ensures no duplicates.Result:
I think in this case you can use some domain knowledge here. Specifically, you will get the pairs [x, y] (I recommend you make this a tuple (specifically a 2-tuple, otherwise known as a pair) not an array), and [y,x] except when x=y where you get it only once. So you can write a simple function:
It's actually quite tricky to get that right in a general way.It essentially boils down to two basic problems:Checking if two lists contain the same elementsRemove all lists that contain the same elementsI'll tackle these separately.Check if two lists contain the same elementsI best to refer to Raymond Hettingers answer from here:O(n):  The Counter() method is best (if your objects are hashable):O(n log n):  The sorted() method is next best (if your objects are orderable):O(n * n): If the objects are neither hashable, nor orderable, you can use equality:In your case you don't want any imports so you could replace collections.Counter with:Just in case the items are hashable and you don't care about the count of the items (for example [1,1,2] should be interpreted as equal to [1,2,2]) or they will always be unique then you could also use sets:So that way you can check if two sublists contain the same elements. There are possible optimizations in case you might have lists of different lengths, then it could be worthwhile to add a:At the beginning of each of these functions.Removing duplicates from the listThat also depends on assumptions about the result (should the non-duplicates keep their relative order or not) and the contents (again, can you hash the contents or can they be ordered).If the items are hashable (or could be converted to something hashable) you could use a set call to remove duplicates. If you care about the order you can still use a set but only for lookups, for example the recipe from the itertools documentation unique_everseen:You mentioned no imports but fortunately we don't need the key is None part anyway (see below) so you can simple use:Note that the approaches to compare the inner lists use sets, dictionaries and lists which are unhashable. But all of them can be converted to a hashable collections, like frozensets or tuples:However the last approach (if the items are unhashable and cannot be ordered) can't be used with this approach so let's ignore it for now.Basically you could then use unique_everseen like this:Or if you don't care about the duplicates (or there will be no duplicates) inside your sublists:Or if they are not hashable but can be ordered:Just the approach in case the items in your sublist are not hashable and not orderable cannot be done using that fast unique_everseen approach. You'll have to use a slower variant:The else clause belongs to the for loop and is only entered when there was no break. You could instead also check for any to avoid this for-else:In your case it's actually very easy because the integers in the sublists are hashable and orderable. But this can become very complex for more general cases.However in your case you could even avoid creating duplicate factors by simply checking that the factors are sorted (and if not stop):For example:
Answer: Explanation: First we need to sort each list so that we can make duplicate list look similar. Then we need to convert each list into tuple so that we can use set() to eliminate duplicates.We can use use List as it is since elements of list has to be hashable to use set().this gives us the set of all the elements with out duplicates. But its a set and each element is a tuple but they should be lists.we can use list comprehension to convert the tuple elements into lists.


Answer URL
https://docs.python.org/3/library/stdtypes.html#frozenset
https://docs.python.org/3/library/collections.html#collections.OrderedDict
https://docs.python.org/3/library/itertools.html#itertools-recipes
