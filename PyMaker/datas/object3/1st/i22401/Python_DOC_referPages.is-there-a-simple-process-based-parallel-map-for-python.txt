Natural Text
I'm looking for a simple process-based parallel map for python, that is, a functionthat would run function on each element of [data] on a different process (well, on a different core, but AFAIK, the only way to run stuff on different cores in python is to start multiple interpreters), and return a list of results.Does something like this exist? I would like something simple, so a simple module would be nice. Of course, if no such thing exists, I will settle for a big library :-/
I seems like what you need is the map method in multiprocessing.Pool():map(func, iterable[, chunksize])For example, if you wanted to map this function:to range(10), you could do it using the built-in map() function:or using a multiprocessing.Pool() object's method map():
For those who looking for Python equivalent of R's mclapply(), here is my implementation. It is an improvement of the following two examples:"Parallelize Pandas map() or apply()", as mentioned by @RafaelValero.How to apply map to functions with multiple arguments.It can be apply to map functions with single or multiple arguments.
This can be done elegantly with Ray, a system that allows you to easily parallelize and distribute your Python code.To parallelize your example, you'd need to define your map function with the @ray.remote decorator, and then invoke it with .remote. This will ensure that every instance of the remote function will executed in a different process.This will print:and it will finish in approximately len(list)/p (rounded up the nearest integer) where p is number of cores on your machine. Assuming a machine with 2 cores, our example will execute in 5/2 rounded up, i.e, in approximately 3 sec.There are a number of advantages of using Ray over the multiprocessing module. In particular, the same code will run on a single machine as well as on a cluster of machines. For more advantages of Ray see this related post.


Answer URL
https://docs.python.org/3/library/multiprocessing.html
