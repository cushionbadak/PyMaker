Natural Text
EDIT: Updated with environment information (see first section)EnvironmentI'm using Python 2.7Ubuntu 16.04IssueI have an application which I've simplified into a three-stage process:Gather data from multiple data sources (HTTP requests, system info, etc)Compute metrics based on this dataOutput these metrics in various formatsEach of these stages must complete before moving on to the next stage, however each stage consists of multiple sub-tasks that can be run in parallel (I can send off 3 HTTP requests and read system logs while waiting for them to return)I've divided up the stages into modules and the sub-tasks into submodules, so my project hierarchy looks like so:app.py looks roughly like so (pseudo-code for brevity):(There's additional code wrapping the dir call to ignore system modules, there's exception handling, etc -- but this is the gist of it)Each datasource sub-module looks roughly like so:Each metric sub-module looks roughly like so:In order to parallelize the first stage (datasources) I wrote something simple like the following:This works, and afterwards I can compute any metric and the datasources.x.data attribute is populatedIn order to parallelize the second stage (metrics) because it depends less on I/O and more on CPU, I felt like simple threading wouldn't actually speed things up and I would need the multiprocessing module in order to take advantage of multiple cores. I wrote the following:This code runs for a few seconds (so I think it's working?) but then when I try:it returns [], like the module was never runSomehow by using the multiprocessing module it seems to be scoping the threads so that they no longer share the data attribute. How should I go about rewriting this so that I can compute each metric in parallel, taking advantage of multiple cores, but still have access to the data when it's done?
Updated again, per comments:Since you're in 2.7, and you're dealing with modules instead of objects, you're having problems pickling what you need.  The workaround is not pretty.  It involves passing the name of each module to your operating function.  I updated the partial section, and also updated to remove the with syntax.A few things:First, in general, it's better to multicore than thread.  With threading, you always run a risk of dealing with the Global Interpreter Lock, which can be extremely inefficient.  This becomes a non-issue if you use multicore.Second, you've got the right concept, but you make it strange by having a global-to-the-module data member.  Make your sources return the data you're interested in, and make your metrics (and outputs) take a list of data as input and output the resultant list.This would turn your pseudocode into something like this:app.py:Once you do this, your data source would look like this:And your metrics would look like this:And finally, your output could look like this:Removing the data "global" and passing it makes each piece a lot cleaner (and a lot easier to test).  This highlights making each piece completely independent.  As you can see, all I'm doing is changing what's in the list that gets passed to map, and in this case, I'm injecting all the previous calculations by passing them as a tuple and unpacking them in the function.  You don't have to use lambdas, of course.  You can define each function separately, but there's really not much to define.  However, if you do define each function, you could use partial functions to reduce the number of arguments you pass.  I use that pattern a lot, and in your more complicated code, you may need to.  Here's one example:Update, per comments:When you use map, you're guaranteed that the order of your inputs matches the order of your outputs, i.e. data_list[i] is the output for running dir(datasources)[i].refresh().  Rather than importing the datasources modules into metrics, I would make this change to app.py:And then pass data_map into each metric.  Then the metric gets the data that it wants by name, e.g.
Process and Thread behave quite differently in python. If you want to use multiprocessing you will need to use a synchronized data type to pass information around. For example you could use multiprocessing.Array, which can be shared between your processes.For detail see the docs: https://docs.python.org/2/library/multiprocessing.html#sharing-state-between-processes


Answer URL
https://docs.python.org/3/library/functools.html#functools.partial
