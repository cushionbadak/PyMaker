Natural Text
I am new to python/coding in general. I was struggling some time to produce the code below (pretty proud of it) but I am now having performance problems – don’t know how to solve them.My task is the following: Each product has a delivery hour and a production timestamp. Production time can be from 1 day 15:00 before delivery until 30 min before delivery. I want to aggregate production time to 15 min intervals for each delivery hour and perform several simple operations for produced units within each interval (not all of them are built in functions )  -Volume Weighted Average Price, Total Quantity(sum), Standard Deviation etc. My Problem: The Dataset consists of around 11 million Data points. Calculating the values for a 6 month period took me more than 24 hours. I tried to loop through the sql query and only imported 15 min periods but it was even worse My Question: Do you see any way to improve the performance of this operation?it would be amazing <3The Original Input Data looks like thís:Column StartOfDelivery EndOfDelivery     ProductionDateTime    PriceEURpMW QuantityMW  p*Q73  2017-01-03 01:00:00 2017-01-03 02:00:00 2017-01-02 19:03:00   37,4         20    74880  2017-01-03 01:00:00 2017-01-03 02:00:00 2017-01-02 19:08:00   35,9         25    897,586  2017-01-03 01:00:00 2017-01-03 02:00:00 2017-01-02 19:23:00   36,3         1     36,3 94  2017-01-03 01:00:00 2017-01-03 02:00:00 2017-01-02 19:24:00   35,3         0,4   14,12915 2017-01-03 02:00:00 2017-01-03 03:00:00 2017-01-02 23:47:00   33,7         5     168,5929 2017-01-03 02:00:00 2017-01-03 03:00:00 2017-01-02 23:50:00   32,6         0,3   9,78340 03.01.2017 02:00:00 2017-01-03 03:00:00 2017-01-02 22:17:00   34           7,9   268,6345 2017-01-03 02:00:00 2017-01-03 03:00:00 2017-01-02 22:19:00   34           0,8   27,2The Output Data looks like this:My Code so far:python 3.4, Windows 10, eclipse IDE
Welcome to python! Profiling your code would be a great starting point :)That said, as your dataset grows, every call to:Gets more expensive. If your performance is good when working on 2 days of data but drops off when you scale up to 6 months, that is probably the culprit.Try pre-sorting the data by adding order by StartOfDelivery to the end your SQL query. Then split your dataframe into a list of dataframes where each subframe contains only the records for the 15 minute increment you need inside the loop's body.You can then iterate over that list as your main loop instead of:That should cut out all the dataframe filtering and make your execution time scale linearly(ish) with data set size.


Answer URL
https://docs.python.org/3/library/profile.html
