Natural Text
I want to use python multiprocessing to conduct the following:process a long list of input files include error logging set a limit on the concurrent CPU cores in use (number of processes)The python logging cookbook has two excellent examples for multiprocessing. In the code below, I've modified the second method ("logging in the main process, in a separate thread") which uses multiprocessing.Queue. Both for myself and new users, I have added detailed notes, and created example input and output files.Where I'm stuck is that the code iterates through the number of CPU cores, NOT through the number of items in my list. How can I apply the function to all my input files, without exceeding the limit on the number of concurrent processes?Note: I've tried dividing the list of input files into chunks depending on the number of CPU cores. This processed the files, but was very slow.
I found that using python multiprocessing Pool instead of Queue allowed me to process a long list of files, and limit the number of concurrent cores.Although logging is not compatible with Pool, I found that it is possible to collect the return values. The return values can be logged after all files have been processed, assuming the code doesn't throw an exception.Maybe someone here can give me a more elegant solution, but for the moment this solves the problem.


Answer URL
https://docs.python.org/3/howto/logging-cookbook.html#logging-to-a-single-file-from-multiple-processes
