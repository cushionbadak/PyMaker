Natural Text
I have a large numpy array already allocated of a given size. For exampleThe values for the array can be generated by (mock example) This step of setting the values of the array is done many times. For example, for k in range(0,1000). I don't want to do any other allocation than the one done by the numpy.empty() at the beginning.I have considered,but this looks like there is going to be at least the allocation of the list [k * val ** 2 for val in range(0, 10000)]. Is that right?I saw also numpy.fromiter, but this seem to be for constructing the array.Is it true that there is one further allocation here?To see if numpy.fromiter allocates an array I tried the followingIn the output I the two addresses printed are different. Doesn't this mean that np.fromiter allocated a new array which got then assigned to my_array?
First make sure you understand what the variable assignment does:the second assignment replaces the first.  The object that the my_array originally referenced is free and gets garbage collected.  That's just basic Python variable handling.  To hang on to the original array (a mutable object), you have to change its values, But the process that generates <new values> will, more than likely, create a temporary buffer (or two or three).  Those values are then copied to the target.  Even x += 1 does a buffered calculation.  There are few in-place numpy operations.Generally trying to second guess numpy's memory allocation doesn't work.  Efficiency can only be measured by time tests, not by guessing  what is happening under the covers.Don't bother with 'pre-allocation' unless you need to fill it iteratively:Which is a horrible way of creating an array compared to:the fromiter approach is prettier but not faster.Some timings:fromiter takes just as long as an explicit loop, while the pure numpy solution is orders of magnitude faster.  Timewise there is little difference between np.array with a list comprehension and fromiter with the generator.Creating the array from a pre-existing list takes about 1/3 the time.Assigning a list to an existing empty array isn't faster.There are some numpy functions that take an out parameter.  You may save some time by reusing an array that way.  np.cross is one function that takes advantage of this (the code is Python and readable).Another 'vectorized' way of creating values from a scalar function:
Given the explanation in the comments, it seems that the problem is the following:A large array needs to be updated frequently, and as efficiently as possible;Source of the updates are not only other numpy arrays, but arbitrary Python objects (which can generate on-the-fly).The second item is the problem: as long as your values come from Python, putting them into a numpy array will never be really efficient. This is because you have to loop over each value in interpreted code.I was expecting to find the expression for ind, elem in enumerate(iterable): my_array[ind] = elem already packaged in a built in function. Do you know if the Python interpreter compiles that expression as a whole?CPython's virtual machine is very different from the C++ model; specifically, the compiler cannot inline the expression or interpret it as a whole in a way to make it significantly more efficient. Even if it supported a byte-code instruction that did this one specific thing in C, it would still need to call the generator's next method that produces each value as heap-allocated Python object after executing Python byte-code. In either case, interpreted code is involved for every iteration, and you really want to avoid that.The efficient way to approach your problem is to design it from the ground up to never leave numpy. As others explained in the comment, the cost of allocation (if done efficiently, by numpy) is miniscule compared to the cost of actually working with data piece by piece in Python.  I would design it as follows:Convert as much code to natively work with numpy arrays, from the ground up; make returning a numpy array part of your interface and don't worry about allocation costs. Do as many loops as possible within numpy itself, so they are done in native code. Never iterate through all values of large arrays in Python.Where it is not possible to use numpy, use numpy.fromiter to convert the iterator to the numpy array as early as possible.Use either my_array[:] = new_array[:] or my_array = new_array to introduce the new values into the array. (The former will take microscopically more time, but it makes more sense when my_array is shared in many places in the data model.)Benchmark operations you are interested in. Don't assume that "copying is slow" - it might turn out that operations that would in C++ be "slow" turn out orders of magnitude faster than the Python rendition of operations that would be efficient in C++.If after doing the above some operation is not supported by numpy, and the measurements show it to be critically inefficient, you can use the Python/C API to create an extension module that performs the computation efficiently and returns the result as a numpy array created in C.
np.fromiter doesn't do any further allocation. It just creates an array from an iterable. That's the whole essence of that function. It also accepts a count argument which allows fromiter to pre-allocate the output array, instead of resizing it on demand.Also, you don't need to use np.empty if you want to change all of the items at once.After all, if you are constructing your new array by doing some mathematical operations on a range of numbers you can simply do the operations on a Numpy array as well.Here is an example:


Answer URL
https://docs.python.org/3/extending/index.html
