Natural Text
How can I access and modify the surrounding 8 cells for a 2D numpy array in an efficient manner?I have a 2D numpy array like this:For each grid cell, I want to reduce by 10% of the center cell, the surrounding 8 cells (fewer for corner cells), but only if the surrounding cell value exceeds 0.25. I suspect that the only way to do this is using a for loop but would like to see if there are better/faster solutions.-- EDIT: For loop based soln:
This answer assumes that you really want to do exactly what you wrote in your question. Well, almost exactly, since your code crashes because indices get out of bounds. The easiest way to fix that is to add conditions, like, e.g.,The reason why the main operation cannot be vectorized using numpy or scipy is that all cells are “reduced” by some neighbor cells that have already been “reduced”. Numpy or scipy would use the unaffected values of the neighbors on each operation. In my other answer I show how to do this with numpy if you are allowed to group operations in 8 steps, each along the direction of one particular neighbor, but each using the unaffected value in that step for that neighbor. As I said, here I presume you have to proceed sequentially.Before I continue, let me swap x and y in your code. Your array has a typical screen size, where 720 is the height and 1440 the width. Images are usually stored by rows, and the rightmost index in an ndarray is, by default, the one that varies more rapidly, so everything makes sense. It's admittedly counter-intuitive, but the correct indexing is arr[y, x].The major optimization that can be applied to your code (that cuts execution time from ~9 s to ~3.9 s on my Mac) is not to assign a cell to itself when it's not necessary, coupled with in-place multiplication and with [y, x] instead of [y][x] indexing. Like this:The other optimization (that brings execution time further down to ~3.0 s on my Mac) is to avoid the boundary checks by using an array with extra boundary cells. We don't care what value the boundary contains, because it will never be used. Here is the code:For the records, if the operations could be vectorized using numpy or scipy, the speed-up with respect to this solution would be at least by a factor of 35 (measured on my Mac).N.B.: if numpy did operations on array slices sequentially, the following would yield factorials (i.e., products of positive integers up to a number) – but it does not:
Please clarify your questionIs it really intended that one loop iteration depends on the other, as mentioned by @jakevdp in the comments?If this is the case, how exactly should be border pixels be handeled? This will affect the whole result due to the dependence from one loop iteration to the othersPlease add a working reference implementation (You are getting an out of bounds error in your reference implementation)Borders untouched, dependend loop iterationsI don't see any other way than using a compiler in this way. In this example I use Numba, but you can also do quite the same in Cython if this is preverred.TimingsThis is realy easy to do an gives about a factor of 1000x. Depending on the first 3 Points there might be some more optimizations possible.
No need for loops, avoid the usual python loops, they are very slow. For greater efficiency, rely on numpy's build in matrix operation, "universal" functions, filters, masks and conditions  whenever you can. https://realpython.com/numpy-array-programmin For complicated computations vectorization is not too bad see some chart and benchmarks Most efficient way to map function over numpy array (just do not use it for simpler matrix operations, like squaring of cells, build in functions will overperform)Easy to see that each internal cell would be multiplied on .9 up to 8 times due 8 neighbors (that is reduced by .1), and additionally due to be a central cell,yet it cannot be reduced below .25/.9 = 5/18. For border and corner cell number of decreases fells to 6 and 3 times.Therefore Borders and corners are be treated in same way with neighbours = 5 and 3 respectively and different views. I guess all three cases can be joined in one formula with complicated where case, yet speed up would be moderate, as borders and corners take a small fraction of all cells.Here I used a small loop, yet it just 8 repetitions. It should be can get rid of the loop too, using power, log, integer part and max functions, resulting in a bit clumsy, but somewhat faster one-liner, something around We can also try another useful technique, vectorization.The vectorization is building a function which then can be applied to all the elements of the array.For a change, lets preset margins/thresholds to find out exact coefficient to multiply on . Here is what code to look likeRemark 1 One can try use different combinations of approaches, e.g. use precomputed margins with matrix arithmetics rather than vectorization. Perhaps there are even more tricks to slightly speed up each of above solutions or combinations of above. Remark 2 PyTorch has many similarity with Numpy functionality but can greatly benefit from GPU. If you have a decent GPU consider PyTorch. There were attempt on gpu based numpy (gluon, abandoned gnumpy, minpy) More on gpu'shttps://stsievert.com/blog/2016/07/01/numpy-gpu/
Your size of the array is a typical screen size, so I guess that cells are pixel values in the range [0, 1). Now, pixel values are never multiplied by each other. If they were, operations would depend on the range (e.g., [0, 1) or [0, 255]), but they never do. So I would assume that when you say “reduce by 10% of a cell” you mean “subtract 10% of a cell”. But even so, the operation remains dependent on the order it is applied to the cells, because the usual way of calculating the total variation of a cell first and then applying it (like in a convolution) would cause some cell values to become negative (e.g., 0.251 - 8 * 0.1 * 0.999) , which does not make sense if they are pixels.Let me assume for now that you really want to multiply cells by each other and by a factor, and that you want to do that by first having each cell affected by its neighbor number 0 (your numbering), then by its neighbor number 1, and so on for neighbors number 2, 3, 5, 7 and 8. As a rule, it's easier to define this kind of operations from the “point of view” of the target cells than from that of the source cells. Since numpy operates quickly on full arrays (or views thereof), the way to do this is to shift all neighbors in the position of the cell that is to be modified. Numpy has no shift(), but it has a roll() which for our purpose is just as good, because we don't care about the boundary cells, that, as per your comment, can be restored to the original value as a last step. Here is the code:Please note that even so, the main steps are different from what you do in your solution. But they necessarily are, because rewriting your solution in numpy would cause arrays to be read and written to in the same operation, and this is not something that numpy can do in a predictable way.If you should change your mind, and decide to subtract instead of multiplying, you only need to change the column of *s before np.roll to a column of -s. But this would only be the first step in the direction of a proper convolution (a common and important operation on 2D images), for which you would need to completely reformulate your question, though.Two notes: in your example code you indexed the array like arr[x][y], but in numpy arrays, by default, the leftmost index is the most slowly varying one, i.e., in 2D, the vertical one, so that the correct indexing is arr[y][x]. This is confirmed by the order of the sizes of your array. Secondly, in images, matrices, and in numpy, the vertical dimension is usually represented as increasing downwards. This causes your numbering of the neighbors to differ from mine. Just multiply the vertical shifts by -1 if necessary.EDITHere is an alternative implementation that yields exactly the same results. It is slightly faster, but modifies the array in place:
EDIT: ah, I see that when you say "reduce" you mean multiply, not subtract. I also failed to recognize that you want reductions to compound, which this solution does not do. So it's incorrect, but I'll leave it up in case it's helpful.You can do this in a vectorized manner using scipy.signal.convolve2d:This comes from thinking about your problem the other way around: each square should have 0.1 times all the surrounding values subtracted from it. The conv array encodes this, and we slide it over the mask array using scipy.signal.convolve2d to accumulate the values that should be subtracted.
We can do this using linear indices. As described your implementation depends on how you iterate through the array. So I assume we want to fix the array, work out what to multiply each element by, then simply apply the multiplication. So it doesnt matter how we go through the array.How much to multiply each element is given by:so we will first go through the whole array, and get the 8 neighbours of each element, multiply them together, with a factor of 0.1^8, and then apply a conditional elementwise multiplication of those values with a. To do this we will use linear indexing, and offseting them. So for an array with m rows, n columns, the i,jth element has linear index in + j. To move down a row we can just add n as the (i+1),jth element has linear index (i+1)n + j = (in + j) + n. This arithmatic provides a good way to get the neighbours of every point, as the neighbours are all fixed offsets from each point.
Try using pandas
It's not possible to avoid the loop because the reduction is performed sequentially, not in parallel.Here's my implementation. For each (i,j) create 3x3 block-view of a centered at a[i,j] (the value of which I set temporarily to 0 so that it is below the threshold, since we don't want to reduce it). For the (i,j) at the boundary, the block is 2x2 at the corners and 2x3 or 3x2 elsewhere. Then the block is masked by the threshold and the unmasked elements are multiplied by a_ij*0.1.Note that the reduction is also performed from the boundary cells on the cells surrounding the them, i.e the loop starts from the first corner of the array, a[0, 0] which has 3 neighbors: a[0,1], a[1,0] and a[1,1], which are reduced by a[0,0]*0.1 if they are > 0.25. Then it goes to the cell a[0,1] which has 5 neighbors etc. If you want to operate strictly on cells that have 8 neighbors, i.e. window of size 3x3, the loop should go from a[1,1] to a[-2, -2], and the function should be modified as follows:Example:       Another example for 3x2 array:
By analyzing the problem to smaller ones, we see, that actully @jakevdp solution does the job, but forgets about checking the term mask<0.25 after convolution with the mask so that some values may drop later behind 0.25 (there maybe 8 tests for every pixel), so there must be a for loop, unless there's a built-in function for that I didn't heard of..Here's my proposal:


Answer URL
https://docs.python.org/3/reference/datamodel.html#object.__imul__
