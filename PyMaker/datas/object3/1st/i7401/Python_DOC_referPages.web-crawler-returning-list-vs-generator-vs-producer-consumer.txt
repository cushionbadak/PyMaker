Natural Text
I want to recursively crawl a web-server that hosts thousands of files and then check if they are different from what's in the local repository (this is a part of checking the delivery infrastructure for bugs).So far I've been playing around with various prototypes and here is what I noticed. If I do a straightforward recursion and put all the files into a list, the operation completes in around 230 seconds. Note that I make only one request per directory, so it makes sense to actually download the files I'm interested in elsewhere:I figured, if I could start processing the files I'm interested in while the crawler is still working, I could save time. So the next thing I tried was a generator that could be further used as a coroutine. The generator took 260 seconds, slightly more, but still acceptable. Here's the generator:UpdateAnswering some questions that came up in the comments section:I've got roughly 370k files, but not all of them will make it to the next step. I will check them against a set or dictionary (to get O(1) lookup) before going ahead and compare them to local repoAfter more tests it looks like sequential crawler takes less time in roughly 4 out of 5 attempts. And generator took less time once. So at this point is seems like generator is okayAt this point consumer doesn't do anything other than get an item from queue, since it's a concept. However I have flexibility in what I will do with the file URL I get from producer. I can for instance, download only first 100KB of file, calculate it's checksum while in memory and then compare to a pre-calculated local version. What's clear though is that if simply adding thread creation bumps my execution time by a factor of 4 to 5, adding work on consumer thread will not make it any faster.Finally I decided to give producer/consumer/queue a shot and a simple PoC ran 4 times longer while loading 100% of one CPU core. Here is the brief code (the crawler is the same generator-based crawler from above):So here are my questions:Are the threads created with threading library actually threads and is there a way for them to be actually distributed between various CPU cores?I believe the great deal of performance degradation comes from the producer waiting to put an item into the queue. But can this be avoided?Is the generator slower because it has to save the function context and then load it again over and over?What's the best way to start actually doing something with those files while the crawler is still populating the queue/list/whatever and thus make the whole program faster?
1) Are the threads created with threading library actually threads and is there a way for them to be actually distributed between various CPU cores?Yes, these are the threads, but to utilize multiple cores of your CPU, you need to use multiprocessing  package.2) I believe the great deal of performance degradation comes from the producer waiting to put an item into the queue. But can this be avoided?It depends on the number of threads you are created, one reason may be due to context switches, your threads are making. The optimum value for thread should be 2/3, i.e create 2/3 threads and check the performance again.3) Is the generator slower because it has to save the function context and then load it again over and over?Generators are not slow, it is rather good for the problem you are working on, as you find a url , you put that into queue.4) What's the best way to start actually doing something with those files while the crawler is still populating the queue/list/whatever and thus make the whole program faster?Create a ConsumerThread class, which fetches the data(url in your case) from the queue and start working on it.


Answer URL
https://docs.python.org/3/library/multiprocessing.html
