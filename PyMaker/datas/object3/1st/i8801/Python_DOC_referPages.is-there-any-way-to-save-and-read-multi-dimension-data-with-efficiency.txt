Natural Text
IntroductionI have a bunch of data series with 1000 stations and each station all have 4 features (e.g Temperature, Wind, CO2 concentration, solar radiation).  All the features are in time-series with hourly resolution.  I read this data in .csv files with the support of Pandas.   Now I need to save and organize them together for better re-use.  My solutionI creat columns entitled by 'sample_x, feature_y'. And each column contain the time series data of feature_y for sample_x.  This method is doable but not show efficiency. Because I had to creat like 4000 columns  with long column name. My questionIs there any better way to save multi-demensions data in Python. I want a simple solution that can help me assessing and handling with specific data directly.  Any advices or solution is appreciated!
I think you can use MultiIndex or Panel and then if necessary save data to hdf5.Also function concat have parameter keys which create MultiIndex from list of DataFrames.Sample:
You may want to use HDF, which has been specifically designed to handle huge arrays of multidimensional data.
The simplest answer may be just to create a sqlite3 database.It sounds like you have 6 pieces of data per hour (station, timestamp, feature1..feature4) times 1000 stations, times however-many hours.So that's 6000 data items (at, say, 4 bytes each = 24k), times 24 hours/day times 365 days/year (* 8760), or about 200mb, per year. Depending on how far back you're going, that's not too bad for a db file. (If you're going to do more than 10 years, then yeah, go to something bigger, or maybe compress the data or break it up by year or something...)


Answer URL
https://docs.python.org/3/library/sqlite3.html?highlight=sqlite#module-sqlite3
