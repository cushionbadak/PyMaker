Natural Text
I'd like to read multiple JSON objects from a file/stream in Python, one at a time. Unfortunately json.load() just .read()s until end-of-file; there doesn't seem to be any way to use it to read a single object or to lazily iterate over the objects.Is there any way to do this? Using the standard library would be ideal, but if there's a third-party library I'd use that instead.At the moment I'm putting each object on a separate line and using json.loads(f.readline()), but I would really prefer not to need to do this.Example Useexample.pyin.txtexample session
Sure you can do this. You just have to take to raw_decode directly. This implementation loads the whole file into memory and operates on that string (much as json.load does); if you have large files you can modify it to only read from the file as necessary without much difficulty.Usage: just as you requested, it's a generator.
JSON generally isn't very good for this sort of incremental use; there's no standard way to serialise multiple objects so that they can easily be loaded one at a time, without parsing the whole lot.The object per line solution that you're using is seen elsewhere too. Scrapy calls it 'JSON lines':http://doc.scrapy.org/topics/exporters.html#jsonlinesitemexporterhttp://www.enricozini.org/2011/tips/python-stream-json/You can do it slightly more Pythonically:I think this is about the best way - it doesn't rely on any third party libraries, and it's easy to understand what's going on. I've used it in some of my own code as well.
This is a pretty nasty problem actually because you have to stream in lines, but pattern match across multiple lines against braces, but also pattern match json. It's a sort of json-preparse followed by a json parse. Json is, in comparison to other formats, easy to parse so it's not always necessary to go for a parsing library, nevertheless, how to should we solve these conflicting issues?Generators to the rescue!The beauty of generators for a problem like this is you can stack them on top of each other gradually abstracting away the difficulty of the problem whilst maintaining laziness. I also considered using the mechanism for passing back values into a generator (send()) but fortunately found I didn't need to use that.To solve the first of the problems you need some sort of streamingfinditer, as a streaming version of re.finditer. My attempt at this below pulls in lines as needed (uncomment the debug statement to see) whilst still returning matches. I actually then modified it slightly to yield non-matched lines as well as matches (marked as 0 or 1 in the first part of the yielded tuple).With that, it's then possible to match up until braces, account each time for whether the braces are balanced, and then return either simple or compound objects as appropriate.This returns tuples as follows:Basically that's the nasty part done. We now just have to do the final level of parsing as we see fit. For example we can use Jeremy Roman's iterload function (Thanks!) to do parsing for a single line:Test it:I get these results (and if you turn on that debug line, you'll see it pulls in the lines as needed):This won't work for all situations. Due to the implementation of the json library, it is impossible to work entirely correctly without reimplementing the parser yourself.
A little late maybe, but I had this exact problem (well, more or less). My standard solution for these problems is usually to just do a regex split on some well-known root object, but in my case it was impossible. The only feasible way to do this generically is to implement a proper tokenizer.After not finding a generic-enough and reasonably well-performing solution, I ended doing this myself, writing the splitstream module. It is a pre-tokenizer that understands JSON and XML and splits a continuous stream into multiple chunks for parsing (it leaves the actual parsing up to you though). To get some kind of performance out of it, it is written as a C module.Example:
Here's a much, much simpler solution.  The secret is to try, fail, and use the information in the exception to parse correctly.  The only limitation is the file must be seekable.Edit:  just noticed that this will only work for Python >=3.5.  For earlier, failures return a ValueError, and you have to parse out the position from the string, e.g.
I'd like to provide a solution. The key thought is to "try" to decode: if it fails, give it more feed, otherwise use the offset information to prepare next decoding.However the current json module can't tolerate SPACE in head of string to be decoded, so I have to strip them off.=========================I have tested for several txt files, and it works fine.(in1.txt)(in2.txt)(in.txt, your initial)(output for Benedict's testcase)
I believe a better way of doing it would be to use a state machine. Below is a sample code that I worked out by converting a NodeJS code on below link to Python 3 (used nonlocal keyword only available in Python 3, code won't work on Python 2)Edit-1: Updated and made code compatible with Python 2Edit-2: Updated and added a Python3 only version as wellhttps://gist.github.com/creationix/5992451Python 3 only versionPython 2 compatible versionTesting itThe output of the same is
I used @wuilang's elegant solution. The simple approach -- read a byte, try to decode, read a byte, try to decode, ... -- worked, but unfortunately it was very slow.In my case, I was trying to read "pretty-printed" JSON objects of the same object type from a file. This allowed me to optimize the approach; I could read the file line-by-line, only decoding when I found a line that contained exactly "}":If you happen to be working with one-per-line compact JSON that escapes newlines in string literals, then you can safely simplify this approach even more:Obviously, these simple approaches only work for very specific kinds of JSON. However, if these assumptions hold, these solutions work correctly and quickly.
Here's mine:
If you use a json.JSONDecoder instance you can use raw_decode member function. It returns a tuple of python representation of the JSON value and an index to where the parsing stopped. This makes it easy to slice (or seek in a stream object) the remaining JSON values. I'm not so happy about the extra while loop to skip over the white space between the different JSON values in the input but it gets the job done in my opinion.The next version is much shorter and eats the part of the string that is already parsed. It seems that for some reason a second call json.JSONDecoder.raw_decode() seems to fail when the first character in the string is a whitespace, that is also the reason why I skip over the whitespace in the whileloop above ...In the documentation about the json.JSONDecoder class the method raw_decode https://docs.python.org/3/library/json.html#encoders-and-decoders contains the following:This can be used to decode a JSON document from a string that may have  extraneous data at the end.And this extraneous data can easily be another JSON value. In other words the method might be written with this purpose in mind.With the input.txt using the upper function I obtain the example output as presented in the original question.


Answer URL
https://docs.python.org/3/library/json.html#encoders-and-decoders
