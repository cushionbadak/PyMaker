Natural Text
I spent a whole day looking for the simplest possible multithreaded URL fetcher in Python, but most scripts I found are using queues or multiprocessing or complex libraries.Finally I wrote one myself, which I am reporting as an answer. Please feel free to suggest any improvement.I guess other people might have been looking for something similar.
Simplifying your original version as far as possible:The only new tricks here are:Keep track of the threads you create.Don't bother with a counter of threads if you just want to know when they're all done; join already tells you that.If you don't need any state or external API, you don't need a Thread subclass, just a target function.
multiprocessing has a thread pool that doesn't start other processes:The advantages compared to Thread-based solution:ThreadPool allows to limit the maximum number of concurrent connections (20 in the code example)the output is not garbled because all output is in the main thread errors are loggedthe code works on both Python 2 and 3 without changes (assuming from urllib.request import urlopen on Python 3).
The main example in the concurrent.futures does everything you want, a lot more simply. Plus, it can handle huge numbers of URLs by only doing 5 at a time, and it handles errors much more nicely.Of course this module is only built in with Python 3.2 or laterâ€¦ but if you're using 2.5-3.1, you can just install the backport, futures, off PyPI. All you need to change from the example code is to search-and-replace concurrent.futures with futures, and, for 2.x, urllib.request with urllib2.Here's the sample backported to 2.x, modified to use your URL list and to add the times:But you can make this even simpler. Really, all you need is:
I am now publishing a different solution, by having the worker threads not-deamon and joining them to the main thread (which means blocking the main thread until all worker threads have finished) instead of notifying the end of execution of each worker thread with a callback to a global function (as I did in the previous answer), as in some comments it was noted that such way is not thread-safe.
This script fetches the content from a set of URLs defined in an array. It spawns a thread for each URL to be fetch, so it is meant to be used for a limited set of URLs.Instead of using a queue object, each thread is notifying its end with a callback to a global function, which keeps count of the number of threads running.


Answer URL
