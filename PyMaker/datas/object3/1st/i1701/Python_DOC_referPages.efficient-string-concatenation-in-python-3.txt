Natural Text
This question already has an answer here:How slow is Python's string concatenation vs. str.join?                    5 answers                I am writing a Python 3 code where the task is to open about 550 files in a directory, read their contents and append it to a string variable 'all_text' which will be say around millions of line long as a single line.The inefficient code I was using till now is as follows-But then I read that using 'join()' method is efficient, so I tried the following code-The problem with this code is that this is removing the previously held contents of 'all_text' variable and writing the current file's content only!How do I get around this problem?Thanks for your help!
join() has a definition str.join(iterable) where iterable is a generator or a list or a set and so on. So it is helpful if you already have a list of strings read from the files and you are concatenating them using join. For exampleYou can get all lines in a file using join like ''.join(readlines(f))Now you can accomplish your task using join as follows using fileinput moduleRefer to this answer to know the most efficient way to concat files into a string.Suggestion: As you mentioned there would be millions of lines, did you consider the memory it is going to consume to store it in a variable? So it is better you do what you are planning to do on the fly while reading the lines instead of storing it in a variable.


Answer URL
https://docs.python.org/3/library/stdtypes.html?highlight=join#str.join
https://docs.python.org/3/glossary.html#term-iterable
https://docs.python.org/3/library/fileinput.html
