Natural Text
I have over a Million times snapshots files that I need to merge and create a single file/db for analysis.My attempt to do this in the code below. first, I read a small csv from a list of URLs, takes a few columns, parse date field from text to date and writes it to a sqlite database.while this code works well enough over a small subset of files, is too slow to iterate over a million CSVs.I'm not sure how to increase performance or even whether Python is the right tool for the job or not. any help in improving this code or suggestions will be much appreciated. 
IMHO python is well suited for this task and with simple modifications you can achieve your desired performance.AFAICS there could be two bottlenecks that affect performance:downloading the urlsyou download a single file at a time, if download a file takes 0.2 sec to download 1M files it'll take > 2 days!I suggest you'll parallelize the download, example code using concurrent.futures:inserting to SQLtry to take a look at how to optimize the SQL insertions at this SO answer.Further guidanceI would start with the parallel requests as it seems larger bottleneckrun profiler to get better idea where your code spends most of the time


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures
