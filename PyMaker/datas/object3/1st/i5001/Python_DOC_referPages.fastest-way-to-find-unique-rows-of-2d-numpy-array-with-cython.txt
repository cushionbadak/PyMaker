Natural Text
I have a 2D NumPy array that could be of any type, but for this example, we can assume it is integers. I am looking to find the fastest way to find all the unique rows in the array.My initial strategy was to convert each row into a tuple and add this to a set. If the length of the set increased, this would mean a unique row was found.What I don't know how to do is quickly hash each row as bytes. There is a question where an entire array is hashed here.What I tried - tuple creationThere are many ways to create a tuple, and each one impacts performance. Here is my function that I show 4 different variations:  Version 1:Version 2:Version 3:vals is a list with length equal to the number of columnsVersion 4:PerformanceAvoiding the tuple constructor (version 4) results in a nice performance gain.Using tostringFrom the linked SO question above, I can use the tostring method on each row and then hash this.This works but is very slow:Using typed memoryviewA huge part of the slowdown, I believe, is the access of each row a[i]. We can used typed memoryviews to increase performance, but I don't know how to turn elements of typed memoryviews into strings so they can be hashed.
This, surprising to me, is slower, but for whatever it's worth, here is a c++ solution that does what you were pointing at - hash each row as a set of bytes.  The 'trick' is taking the address of an element <char*>&a[i, 0] - most everything else is book-keeping.  I may be doing some obviously sub-optimal and/or performance is likely better with a different hash table impl.Edit:re: how to create a string from a rowI think the best you could do is this - construct a bytes object from the pointer.  This does necessarily involve a copy of the row see c api docs.// timing// helper.h// unique.pyx// setup.py
You can use ndarray.view() to change the dtype to byte string, and then use pandas.Series.duplicated() to find duplicated rows:the core algorithm of duplicated() is implemented in Cython. However it need to convert the original array to an object array, which maybe slow.To skip object array, you can use the khash library that used by Pandas directly, here is the C code:then wrap the duplicated() function by Cython or Ctypes or cffi.


Answer URL
https://docs.python.org/3/c-api/bytes.html#c.PyBytes_FromStringAndSize
