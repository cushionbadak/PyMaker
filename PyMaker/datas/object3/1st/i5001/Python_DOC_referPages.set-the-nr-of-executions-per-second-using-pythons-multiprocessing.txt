Natural Text
I wrote a script in Python 3.6 initially using a for loop which called an API, then putting all results into a pandas dataframe and writing them to a SQL database. (approximately 9,000 calls are made to that API every time the script runs).Realising the calls inside the for loop were processed one-by-one, I decided to use the multiprocessing module to speed things up.Therefore, I created a module level function called parallel_requests and now I call that instead of having the for loop:Side note: I use starmap instead of map only because my parallel_requests function takes multiple arguments which I need to zip.The good: this approach works and is much faster. The bad: this approach works but is too fast. By using 4 processes (I tried that because I have 4 cores), parallel_requests is getting executed too fast. More than 15 calls per second are made to the API, and I'm getting blocked by the API itself. In fact, it only works if I use 1 or 2 processes, otherwise it's too damn fast.Essentially what I want is to keep using 4 processes, but also to limit the execution of my parallel_requests function to only 15 times per second overall.Is there any parameter of multiprocessing.Pool that would help with this, or it's more complicated than that?
For this case I'd use a leaky bucket. You can have one process that fills a queue at the proscribed rate, with a maximum size that indicates how many requests you can "bank" if you don't make them at the maximum rate; the worker processes then just need to get from the queue before doing its work.
I'll look at the ideas posted here, but in the meantime I've just used a simple approach of opening and closing a Pool of 4 processes for every 15 requests and appending all the results in a list_of_lists.Admittedly, not the best approach, since it takes time/resources to open/close a Pool, but it was the most handy solution for now.PS: This solution is really not at all that fast due to the multiple opening/closing of pools. Thanks Nathan VÄ“rzemnieks for suggesting to open just one pool, it's much faster, plus your processor won't look like it's running a stress test.
One way to do is to use Queue, which can share details about api-call timestamps with other processes. Below is an example how this could work. It takes the oldest entry in queue, and if it is younger than one second, sleep functions is called for the duration of the difference. 


Answer URL
https://docs.python.org/3/library/queue.html#queue-objects
