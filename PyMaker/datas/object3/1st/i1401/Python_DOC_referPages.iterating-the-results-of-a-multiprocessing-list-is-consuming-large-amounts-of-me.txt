Natural Text
I have a large list. I want to process each item. I'd like to segment the list and process each segment on a different CPU. I'm using the pathos multiprocessing library. I've created the following function:It returns the correct result and uses all (or almost all) the CPUs. However, iterating over the returned list results in very large amounts of memory being consumed unexpectedly.At first I was returning a list comprehension. I switched that to a generator, hoping for less memory consumption, but that didn't improve anything.Update based on comments:I was unaware of imap and uimap and that they automatically chunk the input list. I gave uimap a try but saw very low CPU utilization and very long running times. One of the processes had very high CPU utilization though. What I think is happening is that there is a lot of pickling going on. The f that I'm passing in has a large object in a closure. When using the ProcessingPool methods (map, imap, uimap) this object needs to be pickled for each element in the list. I suspect that this is what the one process that is very busy is doing. The other processes are throttled by this pickling.If so, this explains why my manual segmenting is causing significant gains in CPU utilization: the large object only needs to be pickled once per segment instead of for every item.I then tried using uimap in my map_list_in_segments, hoping for a drop in memory consumption but this did not occur. Here's how the code looks that calls the method and iterates the results:My (limited) understanding of generators is that the first for loop that is looping through the segments should release each one from memory as it iterates. If so it would seem that the large memory usage is the pickling of the return values of the process_segment method. I'm not returning large amounts of data (about 1K bytes for each item) and the size of l I'm working with is 6000 items. Not sure why 5GB of memory gets consumed.
The problem with multiprocessing is that communication between processes is expensive.  If your result is equivalent in size to your input, you're probably going to spend most of your time pickling and unpickling data rather than doing anything useful.  This depends on how expensive f is, but you might be better off not using multiprocessing here.  
Some further testing reveals that the pickling isn't the issue. The processing I was doing in the for item in seg was constructing additional objects that were consuming a large amount of memory.The insights derived from this exercise and the intelligent commenters:ProcessPool methods (map, imap, uimap) automatically chunk the list.If you are passing in a large object to f (via a closure) you might find that manually chunking the list (as above) saves on a lot of pickling and increases CPU utilization.Using imap and uimap can significantly reduce memory usage.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map
