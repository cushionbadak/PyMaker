Natural Text
I am working on a CPU intensive ML problem which is centered around an additive model. Since addition is the main operation I can divide the input data into pieces and spawn multiple models which are then merged by the overriden __add__ method.The code relating to the multiprocessing looks like this:The issue is that the memory consumption scales exponentially as the model order increases, so at order 4 each instance of the model is about 4-5 GB, which causes the threadpool to crash as the intermediate model objects are then not pickleable. I read about this a bit and it appears as even if the pickling is not an issue, it's still extremely inefficient to pass data like this, as commented to this answer.There is very little guidance as to how one can use shared memory for this purpose, however. Is it possible to avoid this problem without having to change the internals of the model object? 
Use files!No, really, use files -- they are are efficient (OS will cache the content), and allow you to work on much larger problems (data set doesn't have to fit into RAM).Use any of https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.io.html to dump/load numpy arrays to/from files and only pass file names between the processes.P.S. benchmark serialisation methods, depending on the intermediate array size, the fastest could be "raw" (no conversion overhead) or "compressed" (if file ends up being written to disk) or something else. IIRC loading "raw" files may require knowing data format (dimensions, sizes) in advance.
Check out the ray project which is a distributed execution framework that makes use of apache arrow for serialization. It's especially great if you're working with numpy arrays and hence is a great tool for ML workflows.Here's a snippet from the docs on object serializationIn Ray, we optimize for numpy arrays by using the Apache Arrow data  format. When we deserialize a list of numpy arrays from the object  store, we still create a Python list of numpy array objects. However,  rather than copy each numpy array, each numpy array object holds a  pointer to the relevant array held in shared memory. There are some  advantages to this form of serialization.Deserialization can be very fast. Memory is shared between processes  so worker processes can all read the same data without having to copy  it.In my opinion it's even easier to use than the multiprocessing library for parallel execution especially when looking to use shared memory, intro to usage in the tutorial.
You should use Manager proxy object for shared editable objects: https://docs.python.org/3/library/multiprocessing.html#multiprocessing-managersThe access lock would be handled by that Manager proxy object.In Customized managers section there is an example, that should suit you:After that you have to connect from different processes (as shown in using a remote manager) to that manager and edit it as you wish.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing-managers
https://docs.python.org/3/library/multiprocessing.html#customized-managers
https://docs.python.org/3/library/multiprocessing.html#using-a-remote-manager
