Natural Text
The code I'm using is posted below. I'm running on Ubuntu 16.04 and my laptop has a i7 Quad-Core processor. "data" is a matrix that has ~100,000 rows and 4 columns. "eemd" is a computationally expensive function. On my machine, processing all columns take 5 minutes, regardless of whether I do each column in parallel or use Pool.map(), as shown below.I have seen other examples on this site with blocks of code that I have been able to run and successfully demonstrate Pool.map() shortening the amount of time necessary to run the code by a factor of the number of processes, but that doesn't work for me here and I can't figure out why.The result is the same whether I use Pool.map() or Pool.imap().New Code Based on Dunes' Reply
In python 3, you can try ProcessPoolExecutor of concurrent.futures module, here is an example:
Major EditIt looks like libeemd is already multi-threaded. You will not gain significant performance increases from parallel execution in Python. You've stated you're using Ubuntu 16.04, which means you will have compiled libeemd with gcc 5.4 (which supports OpenMP). The Makefile of libeemd shows it's compiled with -fopenmp. So yes, it's already multi-threaded. That the library is already multi-threaded also explains why ProcessPoolExecutor runs into problems in the example code. That is, the library has already been used before the process pool is invoked and the default way Unix systems creates new processes (forking) is to create a pseudo-copy of the process. So the child workers are left with a library that references to threads in the parent process. If you only do the ProcessPoolExecutor on its own you will see it works fine.Original AnswerGiven that pyeemd is a wrapper for libeemd using ctypes as glue, you shouldn't need to use multi-processing -- a multi-threading solution should be enough to get a speed boost (and the best speed boost at that).Why threads?Multi-processing is often used in place of multi-threading in Python when the task is CPU bound. This is because of the Global Interpreter Lock (GIL), which is essential for performance in single-threaded Python. However, the GIL makes multi-threaded pure Python code run as if it were single threaded.However, when a thread enters a C function through the ctypes module it releases the GIL as the function will not need to execute Python code. Python types are converted into C types for the call and numpy arrays are wrappers around C buffers (which are guaranteed to exist for the duration of the function). So the Python interpreter and its GIL just aren't needed.Multi-processing is a good way to get a speed boost if using pure Python, but one of its pitfalls is the need to send data to the child workers and return the result to the parent. If either of these take up significant amounts of memory then this adds a large overhead of pushing the data backwards and forwards. So, why use multi-processing if you don't need to.ExampleHere we're going test how long it takes to complete a long running C function  4 times. This will be done once in serial, once using two worker threads and once using two worker processes. This will show that multi-threading is just as good (if not better) than multi-processing when the bulk of the work is done in a C library. lengthy.c is just an example, any deterministic, but expensive, function called with identical arguments will do.lengthy.cTurn the code into a library that can be loaded by ctypes time_lengthy.pyWhich, when run gives:We can see the the two threads/processes running in parallel were nearly twice as fast as the single thread running serially. However, the threads won't suffer the overhead of sending data back and forth between parent and child workers. So, you might as well use threads as the pyeemd source shows it doesn't do any significant work in pure Python.


Answer URL
https://docs.python.org/3/library/ctypes.html
