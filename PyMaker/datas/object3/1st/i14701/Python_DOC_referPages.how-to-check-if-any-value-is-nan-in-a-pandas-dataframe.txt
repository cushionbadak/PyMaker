Natural Text
In Python Pandas, what's the best way to check whether a DataFrame has one (or more) NaN values?I know about the function pd.isnan, but this returns a DataFrame of booleans for each element. This post right here doesn't exactly answer my question either.
jwilner's response is spot on. I was exploring to see if there's a faster option, since in my experience, summing flat arrays is (strangely) faster than counting. This code seems faster:For example:df.isnull().sum().sum() is a bit slower, but of course, has additional information -- the number of NaNs.
You have a couple of options. Now the data frame looks something like this:Option 1: df.isnull().any().any() - This returns a boolean valueYou know of the isnull() which would return a dataframe like this:If you make it df.isnull().any(), you can find just the columns that have NaN values:One more .any() will tell you if any of the above are TrueOption 2: df.isnull().sum().sum() - This returns an integer of the total number of NaN values:This operates the same way as the .any().any() does, by first giving a summation of the number of NaN values in a column, then the summation of those values:Finally, to get the total number of NaN values in the DataFrame:
To find out which rows have NaNs in a specific column:
If you need to know how many rows there are with "one or more NaNs":Or if you need to pull out these rows and examine them:
df.isnull().any().any() should do it.
Adding to Hobs brilliant answer, I am very new to Python and Pandas so please point out if I am wrong.To find out which rows have NaNs:would perform the same operation without the need for transposing by specifying the axis of any() as 1 to check if 'True' is present in rows. 
Since none have mentioned, there is just another variable called hasnans. df[i].hasnans will output to True if one or more of the values in the pandas Series is NaN, False if not. Note that its not a function.pandas version '0.19.2' and '0.20.2'
Since pandas has to find this out for DataFrame.dropna(), I took a look to see how they implement it and discovered that they made use of DataFrame.count(), which counts all non-null values in the DataFrame. Cf. pandas source code. I haven't benchmarked this technique, but I figure the authors of the library are likely to have made a wise choice for how to do it.
Just usingmath.isnan(x), Return True if x is a NaN (not a number), and False otherwise.
Here is another interesting way of finding null and replacing with a calculated value
Starting from v0.23.2, you can use DataFrame.isna + DataFrame.any(axis=None) where axis=None specifies logical reduction over the entire DataFrame.Another performant option you can use is numpy.isnan:Alternatively, check the sum:You can also iteratively call Series.hasnans. For example, to check if a single column has NaNs, And to check if any column has NaNs, you can use a comprehension with any (which is a short-circuiting operation).This is actually very fast.
Or you can use .info() on the DF such as :df.info(null_counts=True) which returns the number of non_null rows in a columns such as:
Will check for each column if it contains Nan or not.
Depending on the type of data you're dealing with, you could also just get the value counts of each column while performing your EDA by setting dropna to False. Works well for categorical variables, not so much when you have many unique values.


Answer URL
https://docs.python.org/3/library/math.html#math.isnan
