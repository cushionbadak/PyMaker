Natural Text
In tensorflow tutorials, I see both codes like tf.add(tf.matmul(X, W), b) and tf.matmul(X, W) + b, what is the difference between using the math function tf.add(), tf.assign(), etc and the operators + and =, etc, in precision or other aspects? 
There's no difference in precision between a+b and tf.add(a, b). The former translates to a.__add__(b) which gets mapped to tf.add by means of following line in math_ops.py _OverrideBinaryOperatorHelper(gen_math_ops.add, "add")The only difference is that node name in the underlying Graph is add instead of Add. You can generally compare things by looking at the underlying Graph representation like thisYou could also see this directly by inspecting the __add__ method. There's an extra level of indirection because it's a closure, but you can get the underlying function as followsAnd you'll see output below which means that they call same underlying functionYou can see from tf.Tensor.OVERLOADABLE_OPERATORS that following Python special methods are potentially overloaded by appropriate TensorFlow versionsThose methods are described in Python reference 3.3.7: emulating numeric types. Note that Python data model does not provide a way to overload assignment operator = so assignment always uses native Python implementation.
Yaroslav nicely explained that there is no real difference. I will just add when using tf.add is beneficial.tf.add has one important parameter which is name. It allows you to name the operation in a graph which will be visible in tensorboard. So my rule of thumb, if it will be beneficial to name an operation in tensorboard, I use tf. equivalent, otherwise I go for brevity and use overloaded version.
Now, the value of p printed will be [2,2,2,2] and simple a+b printed will be [1,1,1,1,1,1,1,1].


Answer URL
https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types
