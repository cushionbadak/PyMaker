Natural Text
I have a list with numbers that are an integer: candidates = [1, 2 ,3, 4 , 5, 16, 20]. This list can contain > 1 million items.I have a dictionary number_ranges that has as key an integer, with a list as value that contains object with a minimum and maximum range. This dictionary consists now of about 500k keys.I am now looping through the list:where I check if there is a match of a number of candidates in the ranges of number_ranges, and if so, I return the key which will be used further on. When I run this, I see that it takes about 40 seconds to process 1000 numbers from the list. This means that if I have 1 million numbers, I need more than 11 hours to process.The expected output is returning the keys from number_ranges that are matching within the range and the candidate number used to find that key, i.e. return {"key": number_range_key, "candidate": candidate} in function search_in_range. What are the recommended ways in Python to optimize this algorithm?
Your list of candidates is sorted, so do the opposite: Loop the dictionaries in number_ranges and use bisect to binary-search the matching candidates. This will reduce the complexity from O(n*m) to O(n*logm*k) for n dictionaries, m candidates, and k matching candidates on average.(Note: I changed the format of your number_ranges from a set of dict with just a single element each to just a dict, which makes much more sense.)Output:If the candidates are not sorted in reality, or if you want the results to be sorted by candidate instead of by dictionary, you can just sort either as a pre- or post-processing step.
With a little bit of reorganisation, your code becomes a classic interval tree problem.Have a look at this package https://pypi.org/project/intervaltree/The only divergence from a normal interval tree is that you have some items that cover multiple intervals, however it would be easy enough to break them into individual intervals, e.g. {16.1: {"start": 15, "end": 20}, 16.2: {"start": 16, "end": 18}}By using the intervaltree package, a balanced binary search tree is created which is much more efficient than using nested for loops. This solution is O(logn) for searching each candidate, whereas a for loop is O(n). If there are 1MM+ candidates, the intervaltree package is going to be considerably faster than the accepted nested for loop answer.
Even though this question has an accepted answer, i would add for the sake of others that this type of scenario really justifies creating a reverse lookup. It is a 1 time headache that will save a lot of practical time as candidate list grows longer. Dictionary lookups are O(1) and if you need to perform multiple lookups, you should consider creating a reverse mapping as well.Output:


Answer URL
https://docs.python.org/3/library/bisect.html
