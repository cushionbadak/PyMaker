Natural Text
I am trying to read several thousands of hours of wav files in python and get their duration. This essentially requires opening the wav file, getting the number of frames and factoring in the sampling rate. Below is the code for that:Doing this sequentially will take too long. My understanding was that multi-threading should help here since it is essentially an IO task. Hence, I do just that:To my dismay, I however get the following results (in seconds):Is this expected? Is this a CPU intensive task? Will Multi-Processing help? How can I speed things up? I am reading files from the local drive and this is running on a Jupyter notebook. Python 3.5.EDIT: I am aware of GIL. I just assumed that opening and closing a file is essentially IO. People's analysis have shown that in IO cases, it might be counter productive to use multi-processing. Hence I decided to use multi-processing instead.I guess the question now is: Is this task IO bound?EDIT EDIT: For those wondering, I think it was CPU bound (a core was maxing out to 100%). Lesson here is to  not make assumptions about the task and check it for yourself.
Some things to check by category:CodeHow efficient is wave.open ?  Is it loading the entire file into memory when it could simply be reading header information?Why is max_workers set to 1 ?Have you tried using cProfile or even timeit to get an idea of what particular part of code is taking more time?HardwareRe-run your existing setup with some hard disk activity, memory usage and CPU monitoring to confirm that hardware is not your limiting factor.  If you see your hard disk running at maximum IO, your memory getting full or all CPU cores at 100% - one of those could be at its limit.Global Interpreter Lock (GIL)If there are no obvious hardware limitations, you are most likely running into problems with Python's Global Interpreter Lock (GIL), as described well in this answer.  This behavior is to be expected if your code has been limited to running on a single core or there is no effective concurrency in running threads.  In this case, I'd most certainly change to multiprocessing, starting by creating one process per CPU core, run that and then compare hardware monitoring results with the previous run.


Answer URL
https://docs.python.org/3/library/profile.html#module-cProfile
https://docs.python.org/3/library/multiprocessing.html
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
