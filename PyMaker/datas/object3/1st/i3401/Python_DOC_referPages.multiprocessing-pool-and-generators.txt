Natural Text
First look at the following code:Essentially I'm retrieving data from a generator, collecting in into a list and then spawning a process that consumes the batch of data.This may look fine but when the consumers (aka the pool processes) are slower than the producer (aka the generator) memory usage of the main process grows until the generator stops or... the system runs out of memory.How can I avoid this problem?
You might want to use a limited-size queue in this case. When used with max. size, this will provide you with the necessary counting and block the thread that is calling q.put() when it is full, so you could never post more than a certain number of work items on it and thus limit the memory needed to store the pending items.Alternatively, you could use a counting semaphore (e.g., multiprocessing.BoundedSemaphore(maxSize)). Acquire it each time you get a work item from the generator and release it in your work function (my_fun) once the item is processed. This way, the maximum number of work items waiting to be processed will never exceed the initial value of the semaphore.
Use the grouper itertools recipe to chunk the data from your generator.  Use the infrastructure in concurrent futures to handle task submission and retrieval with the processes.You couldsubmit a group of tasks; wait for them to finish; then submit another group, orkeep the pipeline full by submitting a new task each time one completes.Setup (attempt to simulate your process):Wait for groups of tasks to completeKeep the process pool full.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html
