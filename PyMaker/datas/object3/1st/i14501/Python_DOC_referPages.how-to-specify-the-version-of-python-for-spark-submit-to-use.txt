Natural Text
I have two versions of Python. When I launch a spark application using spark-submit, the application uses the default version of Python. But, I want to use the other one.How to specify the version of Python for spark-submit to use?
You can set the PYSPARK_PYTHON variable in conf/spark-env.sh (in Spark's installation directory) to the absolute path of the desired Python executable.Spark distribution contains spark-env.sh.template (spark-env.cmd.template on Windows) by default. It must be renamed to spark-env.sh (spark-env.cmd) first.For example, if Python executable is installed under /opt/anaconda3/bin/python3:Check out the configuration documentation for more information.
In my environment I simply usedIt worked for me
You can either specify the version of Python by listing the path to your install in a shebang line in your script:myfile.py:or by calling it on the command line without a shebang line in your script:However, I'd recommend looking into Python's excellent virtual environments that will allow you to create separate "environments" for each version of Python.  Virtual environments more or less work by handling all the path specification after you activate them, alllowing you to just type python myfile.py without worrying about conflicting dependencies or knowing the full path to a specific version of python.Click here for an excellent guide to getting started with Virtual Environments or [here] for the Python3 official documentation.If you do not have access to the nodes and you're running this using PySpark, you can specify the Python version in your spark-env.sh:Spark_Install_Dir/conf/spark-env.sh:


Answer URL
https://docs.python.org/3/library/venv.html
