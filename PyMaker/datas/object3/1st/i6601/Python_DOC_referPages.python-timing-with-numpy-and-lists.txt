Natural Text
I'm doing an assignment for school and I'm supposed to time a python implementation of an integration function alongside a numpy implementation of the same. I would normally expect numpy to perform better than the python implementation, but that is not the case. I'm getting roughly 4x the speed using list comprehension in python compared to numpy arrays. This holds true from N=1000 up to N=100 000. Haven't tested anything higher as N=100 000 is about where I need to be to get an acceptable error in my estimation.Regular python code:    f is the function being integrated, a and b are limits and N is number of points to integrate overNumpy code:It may be that I'm doing something horribly wrong with my numpy implementation, or is the regular list comprehension just that much faster at this scale?
First of all, there is a mistake in your code, it only gives the right result for a=0. You need to replace a + (i*b)/N by a + i*(b - a)/N.As COLDSPEED said in the comments, you are not making the most of numpy capabilities by using a for loop. Here is a more efficient version using array slices:This version was fifty time faster than your intergrate function when I tested it with the square function, a = 1, b = 2 and N = 100000.EDIT: I have noticed that it is even faster to define x_val with: Remark: this answer works only if the function f can be applied on an array.
It depends a bit on your f, if that allows to process arrays then you could use:This is using vectorized and broadcasting operations to make it (much) faster.If your f can't process arrays element-wise you should either make it process arrays or you need to create another array there:Note that you can also make your list-approach faster. That's because iterating over the elements is usually faster than iterating over the elements and you don't need a list-comprehension for sum, a generator will do just as nicely (and is more memory-efficient):


Answer URL
