Natural Text
I'm trying to figure out a good way to use the multiprocessing package in Python 3.6 to run a set of around 100 tasks, with a maximum of 4 of them running simultaneously.  I also want to:repeatedly reap the next completed task from the pool and process its return value, until all tasks have either succeeded or failed;make exceptions thrown in any given task non-fatal, so I can still access the results from the other tasks.I don't need to maintain the order of tasks submitted to the pool (i.e. I don't need a queue).  The total number of tasks ("100" above) isn't prohibitively huge, e.g. I don't mind submitting them all at once and letting them be queued until workers are available.I thought that multiprocessing.Pool would be a good fit for this, but I can't seem to find a "get next result" method that I can call iteratively.Is this something I'm going to have to roll myself from process management primitives?  Or can Pool (or another thing I'm missing) support this workflow?For context, I'm using each worker to invoke a remote process that could take a few minutes, and that has capacity to handle N jobs simultaneously ("4" in my concretized example above).
I came up with the following pattern (shown using 2 workers & 6 jobs, instead of 4 & 100):Seems to work pretty well:I'll see whether I can make a general utility to manage the queueing for me.The main shortcoming I think it has is that completed jobs can go unnoticed for a while, while uncompleted jobs are polled and possibly time out.  Avoiding that would probably require using callbacks - if it becomes a big enough problem, I'll probably add that to my app.


Answer URL
https://docs.python.org/3/library/multiprocessing.html
