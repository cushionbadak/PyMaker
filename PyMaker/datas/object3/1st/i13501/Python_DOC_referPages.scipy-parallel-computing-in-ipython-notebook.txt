Natural Text
I'm doing a kernel density estimation of a dataset (a collection of points). The estimation process is ok, the problem is that, when I'm trying to get the density value for each point, the speed is very slow:The sample is consist of 300,000 (x,y) points. I'm wondering if it's possible to make it run parallely, so the speed would be quicker? For example, maybe I can divide the sample in to smaller sets and run the score_samples for each set at the same time? Specifically:I'm not familliar with parallel computing at all. So I'm wondering if it's applicable in my case?If this can really speed up the process, what should I do? I'm just running the script in ipython notebook, and have no prior expereince in this, is there any good and simple example for my case?I'm reading http://ipython.org/ipython-doc/dev/parallel/parallel_intro.html now.UPDATE:
Here is a simple example of parallelization using multiprocessing built-in module :As you can see from code above, multiprocessing.Pool allows you to map a pool of worker processes executing kde.score_samples on a subset of your samples.The speedup will be significant if your processor have enough cores.


Answer URL
https://docs.python.org/3/library/profile.html#module-profile
