Natural Text
I'd like to create a set of processes with the following structure:main, which dequeues requests from an external source. main generates a variable number of worker processes.worker which does some preliminary processing on job requests, then sends data to gpuProc. gpuProc, which accepts job requests from worker processes. When it has received enough requests, it sends the batch to a process that runs on the GPU. After getting the results back, it has to then send back the completed batch of requests back to the worker processes such that the worker that requested it receives it back One could envision doing this with a number of queues. Since the number of worker processes is variable, it would be ideal if gpuProc had a single input queue into which workers put their job request and their specific return queue as a tuple. However, this isn't possible--you can only share vanilla queues in python via inheritance, and manager.Queues() fail with:Is there a pythonic way to do this without invoking some external library?
multiprocessing.Queue is implemented with a pipe, a deque and a thread.When you call queue.put() the objects ends up in the deque and the thread takes care of pushing it into the pipe.You cannot share threads within processes for obvious reasons. Therefore you need to use something else.Regular pipes and sockets can be easily shared.Nevertheless I'd rather use a different architecture for your program.The main process would act as an orchestrator routing the tasks to two different Pools of processes, one for CPU bound jobs and the other to GPU bound ones. This would imply you need to share more information within the workers but it's way more robust and scalable.Here you get a draft:


Answer URL
https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled
https://docs.python.org/3/library/multiprocessing.html#exchanging-objects-between-processes
