Natural Text
Is there a scipy function or numpy function or module for python that calculates the running mean of a 1D array given a specific window?
For a short, fast solution that does the whole thing in one loop, without dependencies, the code below works great.
UPD: more efficient solutions have been proposed by Alleo and jasaarim.You can use np.convolve for that:  ExplanationThe running mean is a case of the mathematical operation of convolution. For the running mean, you slide a window along the input and compute the mean of the window's contents. For discrete 1D signals, convolution is the same thing, except instead of the mean you compute an arbitrary linear combination, i.e. multiply each element by a corresponding coefficient and add up the results. Those coefficients, one for each position in the window, are sometimes called the convolution kernel. Now, the arithmetic mean of N values is (x_1 + x_2 + ... + x_N) / N, so the corresponding kernel is (1/N, 1/N, ..., 1/N), and that's exactly what we get by using np.ones((N,))/N.EdgesThe mode argument of np.convolve specifies how to handle the edges. I chose the valid mode here because I think that's how most people expect the running mean to work, but you may have other priorities. Here is a plot that illustrates the difference between the modes:
Efficient solutionConvolution is much better than straightforward approach, but (I guess) it uses FFT and thus quite slow. However specially for computing the running mean the following approach works fineThe code to checkNote that numpy.allclose(result1, result2) is True, two methods are equivalent.The greater N, the greater difference in time.
Update: The example below shows the old pandas.rolling_mean function which has been removed in recent versions of pandas. A modern equivalent of the function call below would bepandas is more suitable for this than NumPy or SciPy.  Its function rolling_mean does the job conveniently.  It also returns a NumPy array when the input is an array.It is difficult to beat rolling_mean in performance with any custom pure Python implementation.  Here is an example performance against two of the proposed solutions:  There are also nice options as to how to deal with the edge values.
You can calculate a running mean with:But it's slow.Fortunately, numpy includes a convolve function which we can use to speed things up. The running mean is equivalent to convolving x with a vector that is N long, with all members equal to 1/N. The numpy implementation of convolve includes the starting transient, so you have to remove the first N-1 points:On my machine, the fast version is 20-30 times faster, depending on the length of the input vector and size of the averaging window.Note that convolve does include a 'same' mode which seems like it should address the starting transient issue,  but it splits it between the beginning and end.
or module for python that calculatesin my tests at Tradewave.net TA-lib always wins:results:
For a ready-to-use solution, see https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html.It provides running average with the flat window type. Note that this is a bit more sophisticated than the simple do-it-yourself convolve-method, since it tries to handle the problems at the beginning and the end of the data by reflecting it (which may or may not work in your case...).To start with, you could try:
I know this is an old question, but here is a solution that doesn't use any extra data structures or libraries. It is linear in the number of elements of the input list and I cannot think of any other way to make it more efficient (actually if anyone knows of a better way to allocate the result, please let me know).NOTE: this would be much faster using a numpy array instead of a list, but I wanted to eliminate all dependencies. It would also be possible to improve performance by multi-threaded executionThe function assumes that the input list is one dimensional, so be careful.
If it is important to keep the dimensions of the input (instead of restricting the output to the 'valid' area of a convolution), you can use scipy.ndimage.filters.uniform_filter1d:uniform_filter1d allows multiple ways to handle the border where 'reflect' is the default, but in my case, I rather wanted 'nearest'.It is also rather quick (nearly 50 times faster than np.convolve):
I haven't yet checked how fast this is, but you could try:
A bit late to the party, but I've made my own little function that does NOT wrap around the ends or pads with zeroes that are then used to find the average as well. As a further treat is, that it also re-samples the signal at linearly spaced points. Customize the code at will to get other features.The method is a simple matrix multiplication with a normalized Gaussian kernel.A simple usage on a sinusoidal signal with added normal distributed noise:
Instead of numpy or scipy, I would recommend pandas to do this more swiftly:This takes the moving average (MA) of 3 periods of the column "data". You can also calculate the shifted versions, for example the one that excludes the current cell (shifted one back) can be calculated easily as:
Another approach to find moving average without using numpy, pandawill print [2.0, 4.0, 6.0, 6.5, 7.4, 7.833333333333333]
This question is now even older than when NeXuS wrote about it last month, BUT I like how his code deals with edge cases. However, because it is a "simple moving average," its results lag behind the data they apply to. I thought that dealing with edge cases in a more satisfying way than NumPy's modes valid, same, and full could be achieved by applying a similar approach to a convolution() based method.My contribution uses a central running average to align its results with their data. When there are too few points available for the full-sized window to be used, running averages are computed from successively smaller windows at the edges of the array. [Actually, from successively larger windows, but that's an implementation detail.]It's relatively slow because it uses convolve(), and could likely be spruced up quite a lot by a true Pythonista, however, I believe that the idea stands.
There are many answers above about calculating a running mean. My answer adds two extra features:ignores nan values calculates the mean for the N neighboring values NOT including the value of interest itselfThis second feature is particularly useful for determining which values differ from the general trend by a certain amount.I use numpy.cumsum since it is the most time-efficient method (see Alleo's answer above). This code works for even Ns only. It can be adjusted for odd numbers by changing the np.insert of padded_x and n_nan.Example output (raw in black, movavg in blue):This code can be easily adapted to remove all moving average values calculated from fewer than cutoff = 3 non-nan values.
This answer contains solutions using the Python standard library for three different scenarios.Running average with itertools.accumulateThis is a memory efficient Python 3.2+ solution computing the running average over an iterable of values by leveraging itertools.accumulate.Note that values can be any iterable, including generators or any other object that produces values on the fly.First, lazily construct the cumulative sum of the values.Next, enumerate the cumulative sum (starting at 1) and construct a generator that yields the fraction of accumulated values and the current enumeration index.You can issue means = list(rolling_avg) if you need all the values in memory at once or call next incrementally.(Of course, you can also iterate over rolling_avg with a for loop, which will call next implicitly.)This solution can be written as a function as follows.A coroutine to which you can send values at any timeThis coroutine consumes values you send it and keeps a running average of the values seen so far.It is useful when you don’t have an iterable of values but aquire the values to be averaged one by one at different times throughout your program’s life.The coroutine works like this:Computing the average over a sliding window of size NThis generator-function takes an iterable and a window size N  and yields the average over the current values inside the window.  It uses a deque, which is a datastructure similar to a list, but optimized for fast modifications (pop, append) at both endpoints.Here is the function in action:
Although there are solutions for this question here, please take a look at my solution. It is very simple and working well. 
Use Only Python Stadnard Library (Memory Efficient)Just give another version of using the standard library deque only. It's quite surprise to me that most of the answers are using pandas or numpy.Actaully I found another implementation in python docsHowever the implementation seems to me is a bit more complex than it should be. But it must be in the standard python docs for a reason, could someone comment on the implementation of mine and the standard doc?
From reading the other answers I don't think this is what the question asked for, but I got here with the need of keeping a running average of a list of values that was growing in size.So if you want to keep a list of values that you are acquiring from somewhere (a site, a measuring device, etc.) and the average of the last n values updated, you can use the code bellow, that minimizes the effort of adding new elements:And you can test it with, for example:Which gives:
There is a comment by mab buried in one of the answers above which has this method.  bottleneck has move_mean which is a simple moving average:min_count is a handy parameter that will basically take the moving average up to that point in your array.  If you don't set min_count, it will equal window, and everything up to window points will be nan.
I feel this can be elegantly solved using bottleneckSee basic sample below:"mm" is the moving mean for "a". "window" is the max number of entries to consider for moving mean. "min_count" is min number of entries to consider for moving mean (e.g. for first few elements or if the array has nan values).The good part is Bottleneck helps to deal with nan values and it's also very efficient.
How about a moving average filter? It is also a one-liner and has the advantage, that you can easily manipulate the window type if you need something else than the rectangle, ie. a N-long simple moving average of an array a:And with the triangular window applied:
Another solution just using a standard library and deque:
For educational purposes, let me add two more Numpy solutions (which are slower than the cumsum solution):Functions used: as_strided, add.reduceat
If you do choose to roll your own, rather than use an existing library, please be conscious of floating point error and try to minimize its effects:If all your values are roughly the same order of magnitude, then this will help to preserve precision by always adding values of roughly similar magnitudes.


Answer URL
https://docs.python.org/3/library/collections.html#deque-recipes
