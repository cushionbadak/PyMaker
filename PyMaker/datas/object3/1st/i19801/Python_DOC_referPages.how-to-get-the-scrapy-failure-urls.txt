Natural Text
I'm a newbie of scrapy and it's amazing crawler framework i have known! In my project, I sent more than 90, 000 requests, but there are some of them failed. I set the log level to be INFO, and i just can see some statistics but no details. Is there any way to get more detail report? For example, show those failed URLs. Thanks!
Yes, this is possible. I added a failed_urls list to my spider class and appended urls to it if the response's status was 404 (this will need to be extended to cover other error statuses). Then I added a handle that joins the list into a single string and add it to the stats when the spider is closed.Based on your comments, it's possible to track Twisted errors.Output (the downloader/exception_count* stats will only appear if exceptions are actually thrown - I simulated them by trying to run the spider after I'd turned off my wireless adapter):
Here's another example how to handle and collect 404 errors (checking github help pages):Just run scrapy runspider with -o output.json and see list of items in the output.json file.
The answers from @Talvalin and @alecxe helped me a great deal, but they do not seem to capture downloader events that do not generate a response object (for instance, twisted.internet.error.TimeoutError and twisted.web.http.PotentialDataLoss). These errors show up in the stats dump at the end of the run, but without any meta info. As I found out here, the errors are tracked by the stats.py middleware, captured in the DownloaderStats class' process_exception method, and specifically in the ex_class variable, which increments each error type as necessary, and then dumps the counts at the end of the run.  To match such errors with information from the corresponding request object, you can add a unique id to each request (via request.meta), then pull it into the process_exception method of stats.py:That will generate a unique string for each downloader-based error not accompanied by a response. You can then save the altered stats.py as something else (e.g. my_stats.py), add it to the downloadermiddlewares (with the right precedence), and disable the stock stats.py:The output at the end of the run looks like this (here using meta info where each request url is mapped to a group_id and member_id separated by a slash, like '0/14'):This answer deals with non-downloader-based errors.
Scrapy ignore 404 by default and do not parse. To handle 404 error do this.This is very easy , if you are getting error code 404 in response ,you can handle this is with very easy way.....in settings write and then handle the response status code in your parse function.in settings and get response in parse function
As of scrapy 0.24.6, the method suggested by alecxe won't catch errors with the start URLs. To record errors with the start URLs you need to override parse_start_urls. Adapting alexce's answer for this purpose, you'd get:
This is an update on this question. I ran in to a similar problem and needed to use the scrapy signals to call a function in my pipeline. I have edited @Talvalin's code, but wanted to make an answer just for some more clarity. Basically, you should add in self as an argument for handle_spider_closed.You should also call the dispatcher in init so that you can pass the spider instance (self) to the handleing method.  I hope this helps anyone with the same problem in the future.
In addition to some of these answers, if you want to track Twisted errors, I would take a look at using the Request object's errback parameter, on which you can set a callback function to be called with the Twisted Failure on a request failure.In addition to the url, this method can allow you to track the type of failure.You can then log the urls by using: failure.request.url (where failure is the Twisted Failure object passed into errback).The Scrapy docs give a full example of how this can be done, except that the calls to the Scrapy logger are now depreciated, so I've adapted my example to use Python's built in logging):https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks
You can capture failed urls in two ways.Define scrapy request with errbackUse signals.item_dropped[!Notice] Scrapy request with errback can not catch some auto retry failure, like connection error, RETRY_HTTP_CODES in settings.
Basically Scrapy Ignores 404 Error by Default, It was defined in httperror middleware.So, Add HTTPERROR_ALLOW_ALL = True to your settings file.After this you can access response.status through your parse function.You can handle it like this.


Answer URL
https://docs.python.org/3/library/logging.html
