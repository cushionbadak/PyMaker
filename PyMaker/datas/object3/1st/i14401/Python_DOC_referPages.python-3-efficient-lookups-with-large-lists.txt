Natural Text
I'm making my own rhymer is Python using NLTK. The CMU-dict has over 130,000 entries in a format like this:These are word, pron(unciation) pairs. I manipulate the prons (maybe switch a 'G' for a 'T') and check if that's a word. I do this by using this function:all_word_prons is a Python Pickle file that is 10mb and contains all 130k entriesIf I perform this look up 1000 times, it will take around 23 seconds, which isn't completely bad considering the task but there has to be a better algorithm. I've seen people recommend sets on bisects on other topics but those apply to simple string lookup. This is more or less checking to see if a list is equal, not a string.I did implment some tree-like structure that contains data like this (using the example from above):This for some reason takes even longer than iterating through it simply. Perhaps my implementation is wrong. If you're curious:TL:DRWhat is the fastest way to do this sort of lookup (matching a list) in Python 3 in the year 2015?
If I understand correctly, you want to check to see whether some pronunciation is in your data set. From your first code block, it doesn't seem like you care what word the match came from.Therefore, I think we could do:We construct pron_set from tuples because lists are not hashable (and can't be used as set members).Set lookup should be much faster than iterating through the list. I would recommend being familiar with the Python data structures; you never know when a deque might save you lots of time.
Have you considered using Python List Comprehensions as outlined here?https://docs.python.org/3/tutorial/datastructures.htmlIn certain cases, list comprehensions can be faster than plain for-loops however it still executes a byte-code level loop.  If you're not certain what I mean, checkout this thread:  Are list-comprehensions and functional functions faster than "for loops"?It may be worth a shot to see if this would be faster.


Answer URL
https://docs.python.org/3/tutorial/datastructures.html
