Natural Text
I want to perform a number of operations on a postgresql database. These operations carry out a select on a table and then insert the resulting rows into a new table which has primary keys, ignoring rows which violate the primary key constraints. There are a large number of large tables in the database to be processed, and it seems that this sort of task should be run asynchronously. It strikes me that one way to go about this would be to use the subprocess module in Python to run bash scripts which perform these operations, using something like subprocess.Popen. I can open many terminal sessions and execute queries in parallel and to my understanding this approach is imitating this. To borrow an example from here:My questions are:Are there any obvious issues with calling many postgres queries using subprocess? Under what circumstances might I instead consider using asyncio? Does it provide any advantages to the method discussed above?
Note, that asyncio itself is about controlling execution flow in the first place. It means, for example, you can flexibly manage subprocesses using asyncio. So your question is actually about using processes vs. PostgreSQL async driver.First of all you probably don't need processes: if your bash scripts don't contain much calculations you can use threads, they're cheaper.When it come to asyncio vs. threads both solve main performance bottleneck - network I/O. You probably won't see any performance difference unless you're spawning thousands of threads (see this question and answer for an example).


Answer URL
https://docs.python.org/3/library/asyncio-subprocess.html#subprocesses
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
