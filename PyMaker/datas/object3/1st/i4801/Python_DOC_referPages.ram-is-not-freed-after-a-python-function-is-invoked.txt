Natural Text
I'm using an in-house Python library for scientific computing. I need to consecutively copy an object, modify it, and then delete it. The object is huge which causes my machine to run out of memory after a few cycles.The first problem is that I use python's del to delete the object, which apparently only dereferences the object, rather than freeing up RAM.The second problem is that even when I encapsulate the whole process in a function, after the function is invoked, the RAM is still not freed up. Here's a code snippet to better explain the issue.My question is how do I completely delete a Python object?Thanks!PS. In the code snippet, each time ws.copy_project is called, a copy of proj is stored in ws dictionary.
There are some really smart python people on here.  They may be able to tell you better ways to keep your memory clear, but I have used leaky libraries before, and found one (so-far) foolproof way to guarantee that your memory gets cleared after use: execute the memory hog in another process.To do this, you'd need to arrange for an easy way to make your long calculation be executable separately.  I have done this by adding special flags to my existing python script that tells it just to run that function; you may find it easier to put that function in a separate .py file, e.g.:do_something_with.pyYou can do this using any of the python tools that handle subprocesses.  In python 3.5+, the recommended way to do this is subprocess.run.  You could change your bigger function to something like this:You'll obviously need to tailor this to fit your own situation, but by running in a subprocess, you're guaranteed to not have to worry about the memory getting cleaned up.  As an added bonus, you could potentially use multiprocess.Pool.map to use multiple processors at one time.  (I deliberately coded this to use map to make such a transition simple.  You could still use your for loop if you prefer, and then you don't need the invoke... function.)  Multiprocessing could speed up your processing, but since you're already worried about memory, is almost certainly a bad idea - with multiple processes of the big memory hog, your system itself will likely quickly run out of memory and kill your process.Your example is fairly vague, so I've written this at a high level.  I can answer some questions if you need.


Answer URL
https://docs.python.org/3/library/subprocess.html#subprocess.run
