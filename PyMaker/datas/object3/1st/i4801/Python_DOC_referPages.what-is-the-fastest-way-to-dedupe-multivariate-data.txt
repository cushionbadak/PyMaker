Natural Text
Let's assume a very simple data structure. In the below example, IDs are unique. "date" and "id" are strings, and "amount" is an integer.If date1 == date2 and id1 == id2, I'd like to merge the two entries into one and basically add up amount1 and amount2 so that data becomes:There are many duplicates.As data is very big (over 100,000 entries), I'd like to do this as efficiently as possible. What I did is a created a new "common" field that is basically date + id combined into one string with metadata allowing me to split it later (date + id + "_" + str(len(date)).In terms of complexity, I have four loops:Parse and load data from external source (it doesn't come in lists) | O(n)Loop over data and create and store "common" string (date + id + metadata) - I call this "prepared data" where "common" is my encoded field | O(n)Use the Counter() object to dedupe "prepared data" | O(n)Decode "common" | O(n)I don't care about memory here, I only care about speed. I could make a nested loop and avoid steps 2, 3 and 4 but that would be a time-complexity disaster (O(nÂ²)).What is the fastest way to do this?
Using pandas makes this really easy:For more control you can use agg instead of sum():Depending on what you are doing with the data, it may be easier/faster to go this way just because so much of pandas is built on compiled C extensions and optimized routines that make it super easy to transform and manipulate.
Consider a defaultdict for aggregating data by a unique key:GivenSome random dataOutputCodeOutputDetailsA defaultdict is a dictionary that calls a default factory (a specified function) for any missing keys.  It this case, every date + id combination is uniquely added to the dict.  The amounts are added to values if existing keys are found.  Otherwise an integer (0) initializes a new entry to the dict.  For illustration, you can visualize the aggregated values using a list as the default factory.OutputWe see three occurrences of duplicate keys where the values were appropriately summed.  Regarding efficiency, notice:keys are made with format(), which should be a bit better the string concatenation and calling str()every key and value is computed in the same iteration
You could import the data into a structure that prevents duplicates and than convert it to a list.The program's skeleton:


Answer URL
https://docs.python.org/3/library/collections.html#collections.defaultdict
https://docs.python.org/3/library/collections.html#collections.defaultdict.default_factory
