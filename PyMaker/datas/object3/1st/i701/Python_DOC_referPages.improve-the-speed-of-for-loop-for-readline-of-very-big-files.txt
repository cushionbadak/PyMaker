Natural Text
I am trying to improve the speed of my for loop for reading lines from very big files. I have two files, I am taking information from first file line by line in a for loop and matching these each to lines from a second file through an if statement. Since both the files have millions of lines, this is taking too long.I am posting my code here; How can I improve the loop statement to increase the speed of execution?Between 0 and 100 lines of the f2 file match the (str(line2[5])=="CG") and (str(line2[2])=="+") filter.
You have a O(N * M) loop over file I/O, that is very slow indeed. You can improve per-line processing by using the csv module to do parse each line into a list for you in C code, and drop the redundant str() calls (you already have strings), but your real problem is the nested loop.You can easily avoid that loop. There may be millions of rows in your second file, but you already filter those rows to a much smaller number, between 0 and 100. That can be trivially held in memory and accessed per s0 value in next to no time.Store the information from each row in a dictionary; pre-parse the 2nd column integer, and store the whole row for output to the output file in the lt list:After building that dictionary, you can look up matches against s0 in O(1) time, so your loop over f1 is a straightforward loop with a cheap operation for each row. When you find a match in the report_map dictionary, you only need to loop over the associated list to filter on the row[1] integer values:I strongly recommend against storing the whole line from the BS_Forward.fastq_bismark_pe.CX_report.txt file, certainly not as a Python printable representation. I don't know how you plan to use that data, but at least consider using JSON to serialise the lt list to a string representation. JSON is readable by other platforms and faster to parse back into a suitable Python data structure.


Answer URL
https://docs.python.org/3/library/functions.html#repr
