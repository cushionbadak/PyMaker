Natural Text
I am tasked with calculating hamming distances between 1D binary arrays in two groups - a group of 3000 arrays and a group of 10000 arrays, and every array is 100 items(bits) long. So thats 3000x10000 HD calculations on 100 bit long objects.And all that must be done in at most a dozen minutesHere's the best of what I came up withAnd it's still going to take 1-1.5 hours for it to finish. How do I go about making this faster?
You should be able to dramatically improve the summing speed by using numpy to perform it, rather than using a list comprehension and the built-in sum function (that takes no advantage of numpy vectorized operations).Just replace:with:which, for a 2D array, will return a new 1D array where each value is the sum of the corresponding row (thanks to specifying it should operate on axis 1). For example:Since it performs all the work at the C layer in a single operation without type conversions (instead of your original approach that requires a Python level loop that operates on each row, then an implicit loop that, while at the C layer, must still implicitly convert each numpy value one by one from np.bool to Python level ints just to sum them), this should run substantially faster for the array scales you're describing.Side-note: While not the source of your performance problems, there is no reason to manually maintain your index value; enumerate can do that more quickly and easily. Simply replace:with:and you'll get the same behavior, but slightly faster, more concise and cleaner in general.
IIUC, you can use np.logical_xor and list comprehension:The whole operation runs in 7 seconds in my machine.
Just in case you are not limited to using Python, this is a solution in C++ using bitset:On my machine, the whole operation (3000 x 10000) takes about 300 ms.You could put this into a function, compile it into a library and call it from Python. Another option is to store the distances to a file and then read them in Python.EDIT: I had the wrong size for the hd vector. Reserving the proper amount of memory reduces the operation to about 190 ms because relocations are avoided.


Answer URL
https://docs.python.org/3/library/functions.html#enumerate
