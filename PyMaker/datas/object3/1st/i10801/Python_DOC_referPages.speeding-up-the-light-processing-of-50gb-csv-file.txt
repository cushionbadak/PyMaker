Natural Text
I have a ~50GB csv file with which I have toTake several subsets of the columns of the CSVApply a different format string specification to each subset of columns of the CSV. Output a new CSV for each subset with its own format specification.  I opted to use Pandas, and have a general approach of iterating over chunks of a handy chunk-size (of just over half a million lines) to produce a DataFrame, and appending the chunk to each output CSV. So something like this:My problem is that this is really slow. Each chunk takes about a minute to generate append to the CSV files for, and thus I'm looking at almost 2 hours for the task to complete. I have tried to place a few optimizations by only using the union of the column subsets when reading in the CSV, as well as setting na_filter=False, but it still isn't acceptable. I was wondering if there is a faster way to do this light processing of a CSV file in Python, either by means of an optimization or correction to my approach or perhaps simply there is a better tool suited for this kind of job then Pandas... to me (an inexperienced Pandas user) this looks like it is as fast as it could get with Pandas, but I may very well be mistaken. 
I don't think you're getting any advantage from a Panda's dataframe, so it is just adding overhead.  Instead, you can use python's own CSV module that is easy to use and nicely optimized in C.Consider reading much larger chunks into memory (perhaps 10MB at a time), then writing-out each of the reformatted column subsets before advancing to the next chunk.  That way, the input file only gets read and parsed once.One other approach you could try is to preprocess the data with the Unix cut command to extract only the relevant columns (so that Python doesn't have to create objects and allocate memory for data in the unused columns): cut -d, -f1,3,5 somedata.csvLastly, try running the code under PyPy so that the CPU bound portion of your script gets optimized through their tracing JIT.
I would try using the python csv module and generators.I've found generators much faster than other approaches for parsing huge server logs and such.This is just for reading a transforming a single input csv into a single output csv, but you could write the formatter and writer to output several files.(I now see that this question is a month old - not sure if you've solved your problem already -  if not and if you want more detailed explanations/examples let me know.)
CPU is faster than disk access. One trick is to gzip your file and read from that.


Answer URL
https://docs.python.org/3/library/csv.html
