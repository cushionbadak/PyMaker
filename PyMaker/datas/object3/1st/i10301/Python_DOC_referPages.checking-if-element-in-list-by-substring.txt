Natural Text
I have a list of urls (unicode), and there is a lot of repetition.For example, urls http://www.myurlnumber1.com and http://www.myurlnumber1.com/foo+%bar%baz%qux lead to the same place.So I need to weed out all of those duplicates.My first idea was to check if the element's substring is in the list, like so:However, it tries to mach literal url[:30] to a list element and obviously returns all of them, since there is no element that exactly matches url[:30]. Is there an easy way to solve this problem?EDIT:Often the host and path in the urls stays the same, but the parameters are different. For my purposes, a url with the same hostname and path, but different parameters are still the same url and constitute a duplicate.
If you consider any netloc's to be the same you can parse with urllib.parseWhich would give you:So to get unique netlocs you could do something like:If you wanted to keep the url scheme:Presuming they all have schemes and you don't have http and https for the same netloc and consider them to be the same.If you also want to add the path:The table of attributes is listed in the docs:You just need to use whatever you consider to be the unique parts.
You can try adding another for loop, if you are fine with that.Something like:That will compare every word with every other word to check for sameness. That's just an example, I'm sure you could make it more robust.


Answer URL
https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse
