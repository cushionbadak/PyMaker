Natural Text
This question already has an answer here:Lazy Method for Reading Big File in Python?                    11 answers                I'm trying to work with a (very) large 45gb .txt file that cannot be opened using normal text editors. Data within each row is separated by a spacing, although there are also spaces within each parameter. For example, 1 row looks somewhat like this:University of Cambridge CB2 1TQ 0001234567 2011-01-25 12345 11.12345  12.12345 13.12345 14.1234 16.2716)What I would want to is essentially to clean it up, filter some data, sum up some cells/parameters, append some additional data to each row, and perform clustering analysis (probably with python or C# since I'm starting to pick it up).
Well having it in a database sounds like a good start, rather than trying to do all your data munging from a text file. If you know you can filter the data right now then that might make life easier too.Now your data is in a database and you can execute arbitrary SQL queries against it in a more performant way, get aggregate stats like sums, and append values. You can extract samples from it for your ML ops or use fit generator methods that do queries. I've used the clustering implementations in Python scikit-learn and I enjoyed working with them, but I don't recall there being any out-of-memory implementations. (I can't speak to c# libraries since I haven't used them.)Python has DB APIs in stdlib like https://docs.python.org/3/library/sqlite3.htmlPython has ORM with https://www.sqlalchemy.org/Python also has public cloud database SDKs like https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.htmland https://cloud.google.com/python/getting-started/using-cloud-sql


Answer URL
https://docs.python.org/3/library/sqlite3.html
