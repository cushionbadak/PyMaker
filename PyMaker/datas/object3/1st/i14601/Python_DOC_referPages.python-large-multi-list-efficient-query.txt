Natural Text
I'm trying to create examples on how to manipulate massive databases composed of CSV tables using only Python.I'd like to find out a way to emulate efficient indexed queries in tables spread through some list()The example below takes 24 seconds in a 3.2Ghz Core i5For this dataset.A more efficient, or more pythonic way would be greatly appreciated.
You can itertools.islice instead of reading all rows and use itertools.ifilter:Not quite sure what filter(lambda x: x[0]==i[2],players)[0] is doing, you seem to be searching the whole players list each time and just keeping the first element. It might pay to sort the list once with the first element as the key and use bisection search or build a dict with the first element as the key and the row as the value then simply do lookups.What default value you use or indeed if any is needed you will have to decide.If you have repeating elements at the start of each row but just want to return the first occurrence:Output:Timing for the code on ten players shows ifilter to be the fastest but we will see the dict winning when we increase rankings and just how badly your code scales:Now with 100 players you will see the dict is as fast as it was for 10. The cost of building the dict has been offset by constant time lookups:For 250 players:The final test looping over the whole rankings:So you can see as we loop over more rankings  the dict option is by far the most efficient as far as runtime goes and will scale extremely well. 
Consider putting your data in an SQLite database. This meets your requirement of using only Python, since it is built into the standard Python library and supported in (almost) all Python interpreters. SQLite is a database library that allows you to do processing on data using the SQL syntax. It gives you features like indexing and foreign key relationships.If you need to do multiple queries on the data, doing some pre-computation (i.e. indexes and data normalization) is the most sensible route.
This code doesn't take that much time to run. So I'm going to assume that you were really running through more of the rankings that just 10.  When I run through them all it takes a long time.  If that is what you are interested in doing, then a dictionary would shorten the search time.  For a bit of overhead to setup the dictionary, you can search it very fast.  Here's how I've modified your for loop:With this code you can process all the rankings instantaneously.


Answer URL
https://docs.python.org/3/library/sqlite3.html
