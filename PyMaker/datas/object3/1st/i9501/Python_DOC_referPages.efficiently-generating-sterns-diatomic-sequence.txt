Natural Text
Stern's Diatomic Sequence can be read about in more details over here; however, for my purpose I will define it now.Definition of Stern's Diatomic SequenceLet n be a number to generate the fusc function out of. Denoted fusc(n).If n is 0 then the returned value is 0.If n is 1 then the returned value is 1.If n is even then the returned value is fusc(n / 2).If n is odd then the returned value is fusc((n - 1) / 2) + fusc((n + 1) / 2).Currently, my Python code brute forces through most of the generation, other than the dividing by two part since it will always yield no change.However, my code must be able to handle digits in the magnitude of 1000s millions of bits, and recursively running through the function thousands millions of times does not seem very efficient or practical.Is there any way I could algorithmically improve my code such that massive numbers can be passed through without having to recursively call the function so many times?
lru_cache works wonders in your case. make sure maxsize is a power of 2. may need to fiddle a bit with that size for your application. cache_info() will help with that.also use // instead of / for integer division.and yes, this is just meomization as proposed by Filip Malczak.you might gain an additional tiny speedup using bit-operations in the while loop:UPDATE:here is a simple way of doing meomzation 'by hand':UPDATEafter reading a short article by dijkstra himself a minor update.the article states, that f(n) = f(m) if the fist and last bit of m are the same as those of n and the bits in between are inverted. the idea is to get n as small as possible.that is what the bitmask (1<<n.bit_length()-1)-2 is for (first and last bits are 0; those in the middle 1; xoring n with that gives m as described above).i was only able to do small benchmarks; i'm interested if this is any help at all for the magitude of your input... this will reduce the memory for the cache and hopefully bring some speedup.i had to increase the recursion limit:benchmarking gave strange results; using the code below and making sure that i always started a fresh interperter (having an empty _mem) i sometimes got significantly better runtimes; on other occasions the new code was slower...benchmarking code:and these are three random results i got:that is where i stopped...
With memoization for a million bits, the recursion stack would be extremely large. We can first try to look at a sufficiently large number which we can work by hand, fusc(71) in this case:fusc(71) = fusc(35) + fusc(36)fusc(35) = fusc(17) + fusc(18)    fusc(36) = fusc(18)  fusc(71) = 1 * fusc(17) + 2 * fusc(18)  fusc(17) = fusc(8) + fusc(9)    fusc(18) = fusc(9)fusc(71) = 1 * fusc(8) + 3 * fusc(9)fusc(8) = fusc(4)    fusc(9) = fusc(4) + fusc(5)fusc(71) = 4 * fusc(4) + 3 * fusc(5)fusc(4) = fusc(2)    fusc(3) = fusc(1) + fusc(2)fusc(71) = 7 * fusc(2) + 3 * fusc(3)fusc(2) = fusc(1)    fusc(3) = fusc(1) + fusc(2)fusc(71) = 11 * fusc(1) + 3 * fusc(2)fusc(2) = fusc(1)fusc(71) = 14 * fusc(1) = 14We realize that we can avoid recursion completely in this case as we can always express fusc(n) in the form a * fusc(m) + b * fusc(m+1) while reducing the value of m to 0. From the example above, you may find the following pattern:if m is odd:a * fusc(m) + b * fusc(m+1) = a * fusc((m-1)/2) + (b+a) * fusc((m+1)/2)  if m is even:a * fusc(m) + b * fusc(m+1) = (a+b) * fusc(m/2) + b * fusc((m/2)+1)Therefore, you may use a simple loop function to solve the problem in O(lg(n)) time


Answer URL
https://docs.python.org/3/library/functools.html#functools.lru_cache
