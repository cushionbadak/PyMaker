Natural Text
I'm programming a genetic algorithm in Python, however, my operator (MMX) takes too long (10 seconds) to execute for individuals with 3 million weights (each individual is a list of 3.000.000 elements).This is the code for the operator:rec1, rec2 and phiC are parameters that determine how the crossover is done, you shouldn't bother about them. They have the same value all across the algorithm.poblacion is a list of lists, lets say it's shape is [7,3000000].Individual() is a custom class. It is bassicaly inheriting "list" and adding some attributes to store the fitness value.Doing numpy.amax and numpy.amin separately seems like doing extra work. Also, there's probably a more pythonic way to do the "calc_gen()" loop.PD: "gen1" depends on "gen2": gen1 obtained randomly within a range, and then gen2 is obtained looking for the simetrical point.PD2: A more detailed explanation on MMX operator can be found on the  original paper, however, you can assume the code is okey and does what it has to do. The doi is https://doi.org/10.1007/3-540-44522-6_73PD: the enumerate() and the i are there from the old code, forgot to remove them!EDIT: Reduced 20% time with Dillon Davis's solution. Its a pretty clean solution which will work with any custom list building function, provided you obtain each value of the list by executing one function:EDIT 2: As Dillon Davis suggested I implemented it in pure numpy, reducing the time to 3,5 seconds! (65% time save)NOTE: In case you want to reuse, Individual inherits from listNOTE: if g=phiC then rec1[0] * g_pop + rec1[1]=0, allways, rec1[0] and rec1[1] guarantee that! so maybe it is better to do the math instead of a triple option?
Try replacing your for loop in cxMMX() with something like:and drop the .tolist() from your numpy.amin() and numpy.amax().This will vectorize your your calc_gen function, avoid a zip and the function overhead from several .append() calls, and should overall be quite a bit faster.Edit:Also consider converting calc_gen() to work directly on the numpy arrays. Replace calls to random.uniform() with numpy.random.uniform(), min() or max() with numpy.minimum() or numpy.maximum(), and then eliminate the for loop / map + vectorize completely. This would ultimately be the fastest option.
Have you tried using a multiprocessing.Pool?You'd need to make a wrapper for calc_gen first:Then instead of the for loop you'd do something like:P.S. You don't need enumerate in your original code since you don't seem to be using i


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool
