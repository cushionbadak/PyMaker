Natural Text
The data that I process is ~ 6 Million and it takes a lot of time to write to a file. How do I improve it ?The following are the two approaches that I tried:Time: 56.6293179989 SecTime: 115.468323946 SecI am dealing with atleast 100 files like this and the total time is a lot, please help. Also, I am not writing in .npy or compressed format as I need to read them in matlab and do further processing.
Have you considered h5py?Here's a cursory single-run time comparison:
save will almost always be hugely faster than savetxt. It just dumps the raw bytes, without having to format them as text. It also writes smaller files, which means less I/O. And you'll get equal benefits at load time: less I/O, and no text parsing.Everything else below is basically a variant on top of the benefits of save. And if you look at the times at the end, all of them are within an order of magnitude of each other, but all around two orders of magnitude faster than savetxt. So, you may just be happy with the 200:1 speedup and not care about trying to tweak things any farther. But, if you do need to optimize further, read on.savez_compressed saves the array with DEFLATE compression. This means you waste a bunch of CPU, but save some I/O. If it's a slow disk that's slowing you down, that's a win. Note that with smallish arrays, the constant overhead will probably hurt more than the compression speedup will help, and if you have a random array there's little to no compression possible.savez_compressed is also a multi-array save. That may seem unnecessary here, but if you chunk a huge array into, say, 20 smaller ones, this can sometimes go significantly faster. (Even though I'm not sure why.) The cost is that if you just load ip the .npz and stack the arrays back together, you don't get contiguous storage, so if that matters, you have to write more complicated code.Notice that my test below uses a random array, so the compression is just wasted overhead. But testing against zeros or arange would be just as misleading in the opposite direction, so… this is something to test on your real data.Also, I'm on a computer with a pretty fast SSD, so the tradeoff between CPU and I/O may not be as imbalanced as on whatever machine you're running on.numpy.memmap, or an array allocated into a stdlib mmap.mmap, is backed to disk with a write-through cache. This shouldn't reduce the total I/O time, but it means that the I/O doesn't happen all at once at the end, but is instead spread around throughout your computation—which often means it can happen in parallel with your heavy CPU work. So, instead of spending 50 minutes calculating and then 10 minutes saving, you spend 55 minutes calculating-and-saving.This one is hard to test in any sensible way with a program that isn't actually doing any computation, so I didn't bother.pickle or one of its alternatives like dill or cloudpickle. There's really no good reason a pickle should be faster than a raw array dump, but occasionally it seems to be. For a simple contiguous array like the one in my tests, the pickle is just a small wrapper around the exact same bytes as the binary dump, so it's just pure overhead.For comparison, here's how I'm testing each one:Notice the use of %timeit there. If you're not using IPython, use the timeit module in the stdlib to do the same thing a little verbosely. Testing with time has all kinds of problems (as described in the timeit docs, but the biggest is that you're only doing a single rep. And for I/O-based benchmarks, that's especially bad.Here's the results for each—but, given the caveats above, you should really only consider the first two meaningful.savetxt: 9.95s, 300MBsave: 45.8 ms, 96MBsavez_compressed: 360ms, 90MBpickle: 287ms, 96MB
How about using pickle? I found that it is more fast.the output is 


Answer URL
https://docs.python.org/3/library/mmap.html
https://docs.python.org/3/library/pickle.html
