Natural Text
Let's say we have 100k directories and 1M files whose structure is stored in lists like this:Now I'd like to search for files for which their directory name contains AB. The only method I see here is the following.(1) First get the index of DIRS which contain AB:We only loop once on DIRS, which is 100k, this is ok.(2) Now we need to loop on both I (which can be 1000 for example) and  FILES (which is 1 million), and this is too much because 1000 * 1M = 1 billion operations:This is too much operations! How to have a more efficient research, while keeping the DIRS / FILES data structures? (If 100% totally impossible, which other structure should I consider?)Note: this alternative for (2) doesn't speed up anything I think:
The time complexity of the alternative approach can reduce to O(n) (where n is the length of FILES) if you make I a set, as opposed to the original O(n*m) (where m is the length of I):One of the important uses of a set is fast membership lookup; O(1).You can also gain some significant CPU time by using a list comprehension to build the final FOUND_FILES list:If you're building the list of files by reading the entire content of the parent directory, using say os.listdir, you should apply glob.glob instead, to build a list from your pattern directly.
like this?


Answer URL
https://docs.python.org/3/library/glob.html
