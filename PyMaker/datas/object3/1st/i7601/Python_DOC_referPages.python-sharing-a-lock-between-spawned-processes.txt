Natural Text
The end goal is to execute a method in background, but not in parallel : when multiple objects are calling this method, each should wait for their turn to proceed. To achieve running in background, I have to run the method in a subprocess (not a thread), and I need to start it using spawn (not fork). To prevent parallel executions, the obvious solution is to have a global lock shared between processes.When processes are forked, which is the default on Unix, it is easy to achieve, as highlighted in both of the following codes.We can share it as a class variable :Or we can pass it to the method :Both of these codes have the appropriate behaviour of printing "hello" at one second of interval.However, when changing the start method to 'spawn', they become broken.The first one (1) prints both "hello"s at the same time. This is because the internal state of a class is not pickled, so they do not have the same lock.The second one (2) fails with FileNotFoundError at runtime. I think it has to do with the fact that locks cannot be pickled : see Python sharing a lock between processes.In this answer, two fixes are suggested (side note : I cannot use a pool because I want to randomly create an arbitrary number of processes).I haven't found a way to adapt the second fix, but I tried to implement the first one :This fails with AttributeError and FileNotFoundError (3). In fact it also fails (BrokenPipe) when the fork method is used (4).What is the proper way of sharing a lock between spawned processes ?A quick explanation of the four fails I numbered would be nice, too.I'm running Python 3.6 under Archlinux.
Congratulations, you got yourself 90% of the way there.  The last step is actually not very hard to do.Yes, your final code block fails with an AttributeError, but what specifically is the error?  "Can't get attribute 'OneAtATime' on ".  This is very similar to a problem you've already encountered - it's not pickling the class OneAtATime.I made the following change and it worked as you'd like:file ooat.py:interactive shell:You may notice, I didn't really do anything - just split your code into two separate files.  Try it out, you'll see it works fine.  (At least, it did for me, using python 3.5 on ubuntu.)
The last code snippet works, provided the script does not exit prematurely. Joining processes is enough :More info on the error it was causing here https://stackoverflow.com/a/25456494/8194503.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
