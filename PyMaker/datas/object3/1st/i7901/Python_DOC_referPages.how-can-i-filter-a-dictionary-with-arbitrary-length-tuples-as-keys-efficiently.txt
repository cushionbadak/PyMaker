Natural Text
TL;DRWhat is the most efficient way to implement a filter function for a dictionary with keys of variable dimensions? The filter should take a tuple of the same dimensions as the dictionary's keys and output all keys in the dictionary which match the filter such that filter[i] is None or filter[i] == key[i] for all dimensions i.In my current project, I need to handle dictionaries with a lot of data. The general structure of the dictionary is such that it contains tuples with 2 to 4 integers as keys and integers as values. All keys in a dictionary have the same dimensions. To illustrate, the following are examples of dictionaries I need to handle:These dictionaries contain a lot of entries, with the largest ones at about 20 000 entries. I frequently need to filter these entries, but often only looking at certain indices of the key tuples. Ideally, I want to have a function to which I can supply a filter tuple. The function should then return all keys which match the filter tuple. If the filter tuple contains a None entry, then this will match any value in the dictionary's key tuple at this index.Example of what the function should do for a dictionary with 2-dimensional keys:As my dictionaries have different dimensions of their tuples, I have tried solving this problem by writing a generator expression which takes the dimensions of the tuple into account:Unfortunately, this is quite slow compared to writing out condition completely by hand ((match[0] is None or match[0] === x[0]) and (match[1] is None or match[1] == x[1]); for 4 dimensions this is about 10 times slower. This is a problem for me as I need to do this filtering quite often.Following code demonstrates the performance issue. Code is just supplied to illustrate the problem and enable reproduction of the tests. You can skip the code part, results are below.Results:I would like to avoid having to create three different functions my_filter_fn2, my_filter_fn3, and my_filter_fn4 to cover all possible dimensions of my dictionaries and then use static dimensions filtering. I am aware that filtering for variable dimensions will always be slower than filtering for fixed dimensions, but was hoping that it would not be almost 10 times slower. As I am not a Python expert, I was hoping that there is a clever way in which my variable dimensions generator expression could be reformulated to give me better performance.What is the most efficient way to filter a huge dictionary in the way I described?
Thanks for the opportunity to think about tuples in sets and dictionaries. It's a very useful and powerful corner of Python.Python is interpreted, so if you've come from a compiled language, one good rule of thumb is to avoid complex nested iterations where you can. If you're writing complicated for loops or comprehensions it's always worth wondering if there's a better way to do it.List subscripts (stuff[i]) and range (len(stuff)) are inefficient and long-winded in Python, and rarely necessary. It's more efficient (and more natural) to iterate:The following code is fast because it uses some of the strengths of Python: comprehensions, dictionaries, sets and tuple unpacking.There are iterations, but they're simple and shallow. There's only one if statement in the whole of the code, and that's executed only 4 times per filter operation. That also helps performance-- and makes code easier to read.An explanation of the method...Each key from the original data:is indexed by position and value:(Python numbers elements from zero.)Indexes are collated into one big lookup dictionary composed of sets of tuples:Once this lookup is built (and it is built very efficiently) filtering is just set intersection and dictionary lookup, both of which are lightning-fast. Filtering takes microseconds on even a large dictionary.The method handles data with tuples of arity 2, 3 or 4 (or any other) but arity_filtered() returns only keys with the same number of members as the filter tuple. So this class gives you the option of filtering all data together, or handling the different sizes of tuple separately, with little to choose between them as regards performance.Timing results for the large random dataset (11,500 tuples) were 0.30s to build the lookup, 0.007 seconds for 100 lookups.
I've made some modifications:you don't need to use dict.keys method to iterate through keys, iterating through dict object itself will give us its keys,created separate modules, it helps to read and modify:preparations.py with helpers for generating test data:functions.py for tested functions,main.py for benchmarks.passing arguments to function instead of getting them from global scope, so given static & variable length versions becomeusing min on results of timeit.repeat instead of timeit.timeit to get most representable results (more in this answer),changing entries_keys elements count from 10 to 100 (including ends) with step 10,changing all_entries elements count from 10000 to 15000 (including ends) with step 500.But getting back to the point.ImprovementsWe can improve filtration by skipping checks for indexes with None values in keysNext suggestion is to use numpy:BenchmarksContents of main.py:which on my machine with Python 3.6.1 gives:ResumeAs we can see numpy version isn't so good as expected and it seems to be not numpy's fault.If we remove converting filtered array records to tuples with map and just leavethen it will be extremely fast (faster than static length version), so the problem is in conversion from numpy.ndarray objects to tuple.Filtering out None entry keys gives us about 50% speed gain, so feel free to add it.
I don't have a beautiful answer, but this sort of optimisation often makes code harder to read.  But if you just need more speed here are two things you can do.Firstly we can straightforwardly eliminate a repeated computation from inside the loop.  You say that all the entries in each dictionary have the same length so you can compute that once, rather than repeatedly in the loop.  This shaves off about 20% for me:Not pretty, I agree.  But we can make it much faster (and even uglier!) by building the fixed length function using eval.  Like this:For me, this is nearly as fast as the static version.
Let's say you have a dictionary - dFirst you can get dictionary keys-Now let's define a function is_match which can decide for given two tuples, if they are equal or not based on your conditions-is_match((1,7),(1,None)), is_match((1,5),(None,5)) and is_match((1,4),(1,4)) will return True while is_match((1,7),(1,8)), is_match((4,7),(6,12)) will return False.


Answer URL
https://docs.python.org/3/library/stdtypes.html#dict.keys
