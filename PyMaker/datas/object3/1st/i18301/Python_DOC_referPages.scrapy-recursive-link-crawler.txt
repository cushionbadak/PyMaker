Natural Text
It starts with a url on the web (ex: http://python.org), fetches the web-page corresponding to that url, and parses all the links on that page into a repository of links. Next, it fetches the contents of any of the url from the repository just created, parses the links from this new content into the repository and continues this process for all links in the repository until stopped or after a given number of links are fetched.How can i do that using python and scrapy?. I am able to scrape all links in a webpage but how to perform it recursively in depth
Several remarks :you don't need Scrapy for such a simple task. Urllib (or Requests) and a html parser (Beautiful soup, etc.) can do the jobI don't recall where I've heard it, but I think it's better to crawl using BFS algorithms. You can easily avoid circular references.Below a simple implementation : it does not fetcch internal links (only absolute formed hyperlinks) nor does it have any Error handling (403,404,no links,...), and it is abysmally slow ( the multiprocessing module can help a lot in this case). Output: 
Here is the main crawl method written to scrap links recursively from a webpage. This method will crawl a URL and put all the crawled URLs into a buffer. Now multiple threads will be waiting to pop URLs from this global buffer and again call this crawl method.The complete source code and instructions are available at https://github.com/tarunbansal/crawler


Answer URL
