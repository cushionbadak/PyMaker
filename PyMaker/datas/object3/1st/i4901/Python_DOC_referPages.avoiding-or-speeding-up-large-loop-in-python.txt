Natural Text
I'm using SageMath to perform some mathematical calculations, and at one point I have a for loop that looks like this:So basically, I want to create very large number of random integers in the given range, check whether the number was already in the dictionary, if yes increment its count, if not initialize it to 1. But, the loop takes such a long time that it doesn't finish in a reasonable amount of time, and not because the operations inside the loop are complicated, but instead because there are a huge number of iterations to be performed. Therefore, I want to ask whether there is any way to avoid (or speed up) this kind of loops in Python?
Biggest speedup you can make without low-level magic is using defaultdict, i.e.If you're using python2, change range to xrange.Except this - I'm pretty sure that its somewhere near limit for python. Loop is generating random integer (optimized as much as possible without static typing)calculating hashupdating dict. With defaultdict if-else branches is factored to more optimized codefrom time to time, malloc calls to resize dict - this is fast (considering inablity to preallocate memory for dict)
I profiled your code (use cProfile for this) and the vast majority of the time spent, is spend within the randint function that is called for each iteration of the loop.I recommend you vectorize the loop using numpy random number generation libraries, and then a single call to the Counter class to extract frequency counts.For a loop of 1000000 iterations (smaller than the one you suggest) I observed a reduction from 6 seconds to about 1 second. So even with this you cannot expect more than an order of magnitude reduction in terms of computation time.You may think that keeping an array of all the values in memory is inefficient, and may lead to memory exhaustion before the computation ends. However, due to the small value of "end" compared with the range of the random integers the rate at which you will be recording collisions is low, and therefore the memory cost of a full array is not significantly larger than storing the dictionary.  However, if this becomes and issue you may wish to perform the computation in batches. In that spirit you may also want to use the multiprocessing facilities to distribute computations across many CPUs or even many machines (but lookout for network costs if you chose that).


Answer URL
https://docs.python.org/3/library/collections.html#collections.defaultdict
