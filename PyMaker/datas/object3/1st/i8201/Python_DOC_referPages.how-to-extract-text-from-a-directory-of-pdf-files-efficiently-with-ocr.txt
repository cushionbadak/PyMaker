Natural Text
I have a large directory with PDF files (images), how can I extract efficiently the text from all the files inside the directory?. So far I tried to:However, it is not working... it takes a lot of time (I have some documents that have 600 pages). Additionally: a) I do not know how to handle efficiently the directory transformation part. b) I would like to add a page separator, let's say: <start/age = 1> ... page content ... <end/page = 1>, but I have no idea of how to do this.Thus, how can I apply the extract_txt function to all the elements of a directory that end with .pdf and return the same files in another directory but in a .txt format, and add a page separator with OCR text extraction?.Also, I was curios about using google docs to make this task, is it possible to programmatically use google docs to solve the aforementioned text extracting problem?.UPDATERegarding the "adding a page separator" issue (<start/age = 1> ... page content ... <end/page = 1>) after reading Roland Smith's answer I tried to:However, I still have issues with the print() part, since instead of printing, it would be more useful to save into a file all the output. Thus, I tried to redirect the output to a a file:Any idea of how to make the page extraction/separator trick and saving everything into a file?...
In your code, you are extracting the text, but you don't do anything with it.Try something like this:This writes the text to file that has the same name but a .txt extension.It also returns the path of the original file to let the parent know that this file is done.So I would change the mapping code to:You don't need to give an argument when creating a Pool. By default it will create as many workers as there are cpu-cores.Using imap_unordered creates an iterator that starts yielding values as soon as they are available.Because the worker function returned the filename, you can print it to let the user know that this file is done.Edit 1:The additional question is if it is possible to mark page boundaries. I think it is.A method that would surely work is to split the PDF file into pages before the OCR. You could use e.g. pdfinfo from the poppler-utils package to find out the number of pages in a document. And then you could use e.g. pdfseparate from the same poppler-utils package to convert that one pdf file of N pages into N pdf files of one page. You could then OCR the single page PDF files separately. That would give you the text on each page separately.Alternatively you could OCR the whole document and then search for page breaks. This will only work if the document has a constant or predictable header or footer on every page. It is probably not as reliable as the abovementioned method.Edit 2:If you need a file, write a file:


Answer URL
https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files
