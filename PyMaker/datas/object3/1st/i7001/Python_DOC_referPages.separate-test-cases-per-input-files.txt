Natural Text
Most test frameworks assume that "1 test = 1 Python method/function",and consider a test as passed when the function executes withoutraising assertions.I'm testing a compiler-like program (a program that reads *.foofiles and process their contents), for which I want to execute the same test on many input (*.foo) files. IOW, my test looks like:My current code usesunittest fromPython's standard library, i.e. one_file uses self.assert...(...)statements to check whether the test passes.This works, in the sense that I do get a program which succeeds/failswhen my code is OK/buggy, but I'm loosing a lot of the advantages ofthe testing framework:I don't get relevant reporting like "X failures out of Y tests" northe list of passed/failed tests. (I'm planning to use such systemnot only to test my own development but also to grade student's codeas a teacher, so reporting is important for me)I don't get test independence. The second test runs on theenvironment left by the first, and so on. The first failure stopsthe testsuite: testcases coming after a failure are not ran at all.I get the feeling that I'm abusing my test framework: there's onlyone test function so automatic test discovery of unittest soundsoverkill for example. The same code could (should?) be written inplain Python with a basic assert.An obvious alternative is to change my code to something likeThen I get all the advantages of unittest back, but:It's a lot more code to write.It's easy to "forget" a testcase, i.e. create a test file intests/ and forget to add it to the Python test.I can imagine a solution where I would generate one method per testcase dynamically (along the lines of setattr(self, 'test_file' + str(n), ...)), to generate the code for the second solution without having to write it by hand. But that sounds really overkill for a use-case which doesn't seem so complex.How could I get the best of both, i.e.automatic testcase discovery (list tests/*.foo files), testindependence and proper reporting?
If you can use pytest as your test runner, then this is actually pretty straightforward using the parametrize decorator:This will also automatically name the tests in a useful way, so that you can see which files have failed:
Here is a solution, although it might be considered not very beautiful... The idea is to dynamically create new functions, add them to the test class, and use the function names as arguments (e.g., filenames):This correctly runs all four tests and returns:


Answer URL
https://docs.python.org/3/library/unittest.html
