Natural Text
I have a list of around 131000 arrays, each of length 300. I am using pythonI want to check which of the arrays are repeating in this list. I am trying this by comparing each array with others. like :this is running very slowly, It might take hours to finish, how can I do this efficiently ?
You can use collections.Counter to count the frequency of each sub listWe need to cast the sublist to tuples since list is unhashable i.e. it cannot be used as a key in dict. This will give you result like this:The key of Counter object here is the list and value is the number of times this list occurs. Next you can filter the resulting Counter object to only yield elements where value is > 1:Timeit results:
You can remove duplicate comparisons by usingYou could look in to pypy for general purpose speed ups.It might also be worth looking into hashing the arrays somehow.Here's a question on the speeding up np array comparison. Do the order of the elements matter to you?
You can use set and tuple to find duplicated arrays inside another array. Create a new list contains tuples, we use tuples because lists are unhashable type. And then filter new list with using set.
maybe you can reduce the initial list to unique hashes, or non-unique sums,and go over the hashes first - which may be a faster way to compare elements 
I suggest you first sort the list (might also be helpful for further processing) and then compare. The advantage is that you only need to compare every array element to the previous one:If N is the length of word embedding and n is the length of the inner array, then your approach was to do O(N*N*n) comparisons. When reducing the comparisons as in con--'s answer, then you still have O(N*N*n/2) comparisons.Sorting will take O(N*log(N)*n) time and the subsequent step of counting only takes O(N*n) time which all in all is shorter than O(N*N*n/2)


Answer URL
https://docs.python.org/3/library/collections.html#collections.Counter
