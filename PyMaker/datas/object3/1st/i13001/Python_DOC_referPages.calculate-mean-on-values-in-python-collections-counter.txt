Natural Text
I'm profiling some numeric time measurements that cluster extremely closely. I would like to obtain mean, standard deviation, etc. Some inputs are large, so I thought I could avoid creating lists of millions of numbers and insteaduse Python collections.Counter objects as a compact representation. Example: one of my small inputs yields a collection.Counter like [(48, 4082), (49, 1146)] which means 4,082 occurrences of the value 48 and 1,146 occurrences of the value 49. For this data set I manually calculate the mean to be something like 48.2192042846.Of course if I had a simple list of 4,082 + 1,146 = 5,228 integers I would just feed it to numpy.mean().My question: how can I calculate descriptive statistics from the values in a collections.Counter object just as if I had a list of numbers? Do I have to create the full list or is there a shortcut?
While you can offload everything to numpy after making a list of values, this will be slower than needed. Instead, you can use the actual definitions of what you need. The mean is just the sum of all numbers divided by their count, so that's very simple:Standard deviation is a bit more complex. It's the square root of variance, and variance in turn is defined as "mean of squares minus the square of the mean" for your collection. Soooo...A little bit more manual work, but should also be much faster if the number sets have a lot of repetition.
collections.Counter() is a subclass of dict. Just use Counter().values() to get a list of the counts:Note that I did not call Counter.most_common() here, which would produce the list of (key, count) tuples you posted in your question.If you must use the output of Counter.most_common() you can filter out just the counts with a list comprehension:If you are using Python 3 (where dict.values() returns a dictionary view), you could either pass in list(counts.values()), or use the standard library staticstics.mean() function, which takes an iterable (including dict.values() dictionary view).If you meant to calculate the mean key value as weighted by their counts, you'd do your own calculations directly from the counter values. In Python 2 that'd be:The from __future__ import should be at the top of your module and ensures that you won't run into overflow issues with large floating point numbers. In Python 3 that'd be simplified to:The median could be calculated with bisection; sort the (key, count) pairs by key, sum the counts, and bisect the half-way point into a accumulated sum of the counts. The index for the insertion point points to the median key in the sorted keys list.
Unless you want to write your own statistic functions there is no prêt-à-porter solution (as far as I know).So at the end you need to create lists, and the fastest way is to use numpy. One way to do it is:UPDATE: Create elements from an existing collections.Counter() object


Answer URL
https://docs.python.org/3/library/statistics.html#statistics.mean
