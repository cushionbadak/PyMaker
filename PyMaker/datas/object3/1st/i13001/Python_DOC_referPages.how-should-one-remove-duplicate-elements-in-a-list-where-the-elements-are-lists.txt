Natural Text
I have a list of lists in Python:And I want to remove duplicate elements from it. Was if it a normal list not of lists I could used set. But unfortunate that list is not hashable and can't make set of lists. Only of tuples. So I can turn all lists to tuples then use set and back to lists. But this isn't fast.How can this done in the most efficient way?The result of above list should be:I don't care about preserve order.Note: this question is similar but not quite what I need. Searched SO but didn't find exact duplicate.Benchmarking:"loop in" (quadratic method) fastest of all for short lists. For long lists it's faster then everyone except groupby method. Does this make sense?For short list (the one in the code), 100000 iterations:For longer list (the one in the code duplicated 5 times):
itertools often offers the fastest and most powerful solutions to this kind of problems, and is well worth getting intimately familiar with!-)Edit: as I mention in a comment, normal optimization efforts are focused on large inputs (the big-O approach) because it's so much easier that it offers good returns on efforts. But sometimes (essentially for "tragically crucial bottlenecks" in deep inner loops of code that's pushing the boundaries of performance limits) one may need to go into much more detail, providing probability distributions, deciding which performance measures to optimize (maybe the upper bound or the 90th centile is more important than an average or median, depending on one's apps), performing possibly-heuristic checks at the start to pick different algorithms depending on input data characteristics, and so forth.Careful measurements of "point" performance (code A vs code B for a specific input) are a part of this extremely costly process, and standard library module timeit helps here. However, it's easier to use it at a shell prompt.  For example, here's a short module to showcase the general approach for this problem, save it as nodup.py:Note the sanity check (performed when you just do python nodup.py) and the basic hoisting technique (make constant global names local to each function for speed) to put things on equal footing.Now we can run checks on the tiny example list:confirming that the quadratic approach has small-enough constants to make it attractive for tiny lists with few duplicated values.  With a short list without duplicates:the quadratic approach isn't bad, but the sort and groupby ones are better.  Etc, etc.If (as the obsession with performance suggests) this operation is at a core inner loop of your pushing-the-boundaries application, it's worth trying the same set of tests on other representative input samples, possibly detecting some simple measure that could heuristically let you pick one or the other approach (but the measure must be fast, of course).It's also well worth considering keeping a different representation for k -- why does it have to be a list of lists rather than a set of tuples in the first place?  If the duplicate removal task is frequent, and profiling shows it to be the program's performance bottleneck, keeping a set of tuples all the time and getting a list of lists from it only if and where needed, might be faster overall, for example.
I don't know if it's necessarily faster, but you don't have to use to tuples and sets. 
Doing it manually, creating a new k list and adding entries not found so far:Simple to comprehend, and you preserve the order of the first occurrence of each element should that be useful, but I guess it's quadratic in complexity as you're searching the whole of new_k for each element.
Even your "long" list is pretty short. Also, did you choose them to match the actual data? Performance will vary with what these data actually look like. For example, you have a short list repeated over and over to make a longer list. This means that the quadratic solution is linear in your benchmarks, but not in reality.For actually-large lists, the set code is your best betâ€”it's linear (although space-hungry). The sort and groupby methods are O(n log n) and the loop in method is obviously quadratic, so you know how these will scale as n gets really big. If this is the real size of the data you are analyzing, then who cares? It's tiny.Incidentally, I'm seeing a noticeable speedup if I don't form an intermediate list to make the set, that is to say if I replacewithThe real solution may depend on more information: Are you sure that a list of lists is really the representation you need?
List of tuple and {} can be used to remove duplicates
All the set-related solutions to this problem thus far require creating an entire set before iteration.It is possible to make this lazy, and at the same time preserve order, by iterating the list of lists and adding to a "seen" set. Then only yield a list if it is not found in this tracker set.This unique_everseen recipe is available in the itertools docs. It's also available in the 3rd party toolz library:Note that tuple conversion is necessary because lists are not hashable.
This should work.
Another probably more generic and simpler solution is to create a dictionary keyed by the string version of the objects and getting the values() at the end:The catch is that this only works for objects whose string representation is a good-enough unique key (which is true for most native objects).
Create a dictionary with tuple as the key, and print the keys.create dictionary with tuple as key and index as valueprint list of keys of dictionary
Strangely, the answers above removes the 'duplicates' but what if I want to remove the duplicated value also??The following should be useful and does not create a new object in memory!and the o/p is:


Answer URL
https://docs.python.org/3/library/itertools.html#itertools-recipes
