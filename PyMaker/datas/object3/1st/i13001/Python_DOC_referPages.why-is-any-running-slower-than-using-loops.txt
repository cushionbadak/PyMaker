Natural Text
I've been working in a project that manages big lists of words and pass them trough a lot of tests to validate or not each word of the list. The funny thing is that each time that I've used "faster" tools like the itertools module, they seem to be slower.Finally I decided to ask the question because it is possible that I be doing something wrong. The following code will try to test the performance of the any() function versus the use of loops.The previous code is pretty representative of the kind of tests I do, and if we take a look to the results:I find kinda amazing that using loops is half faster than using any(). What would be the explanation for that? Am I doing something wrong?(I used python3.4 under GNU-Linux)
Actually the any() function is equal to following function :which is like your second function, but since the any() returns a boolean value by itself, you don't need to check for the result and then return a new value, So the difference of performance is because of that you are actually use a redundant return and if conditions,also calling the any inside another function.So the advantage of any here is that you don't need to wrap it with another function because it does all the things for you.Also as @interjay mentioned in comment it seems that the most important reason which I missed is that you are passing a generator expression to any() which doesn't provide the results at once and since it produce the result on demand it does an extra job.Based on PEP 0289 -- Generator ExpressionsThe semantics of a generator expression are equivalent to creating an anonymous generator function and calling it. For example:is equivalent to:So as you can see each time that python want to access the next item it calls the iter function and the next method of a generator.And finally the result is that it's overkill to use any() in such cases.
Since your true question is answered, I'll take a shot at the implied question:You can get a free speed boost by just doing unallowed_combinations = sorted(set(unallowed_combinations)), since it contains duplicates.Given that, the fastest way I know of doing this isWith CPython 3.5 I get, for some test data with a line length of 60 characters,where the third is the regex version, and on PyPy3 I getFWIW, this is competitive with Rust (a low-level language, like C++) and actually noticeably wins out on the regex side. Shorter strings favour PyPy over CPython a lot more (eg. 4x CPython for a line length of 10) since overhead is more important then.Since only about a third of CPython's regex runtime is loop overhead, we conclude that PyPy's regex implementation is better optimized for this use-case. I'd recommend looking to see if there is a CPython regex implementation that makes this competitive with PyPy.


Answer URL
https://docs.python.org/3/library/functions.html?highlight=enumerate#any
