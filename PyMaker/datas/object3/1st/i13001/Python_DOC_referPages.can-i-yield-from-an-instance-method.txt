Natural Text
Is it okay to use the yield statement in an instance method of a class?  For example,Python doesn't complain about this, and simple cases seem to work.  However, I've only seen examples with yield from regular functions.I start having problems when I try to use it with itertools functions.  For example, suppose I have two large data streams X and Y that are stored across multiple files, and I want to compute their sum and difference with only one loop through the data.  I could use itertools.tee and itertools.izip like in the following diagramIn code it would be something like this (sorry, it's long)But this fails unless I swap itertools.izip out for itertools.izip_longest even though the iterators have the same length.  It's the last assert that gets hit, with output likeEdit: I guess it's not obvious from the example I wrote, but the input data X and Y are only available in blocks (in my real problem they're chunked in files).  This is important because I need to maintain state between blocks.  In the toy example above, this means Nth needs to yield the equivalent ofNOT the equivalent ofI could use itertools.chain to join the blocks ahead of time and then make one call to Nth.itervalues, but I'd like to understand what's wrong with maintaining state in the Nth class between calls (my real app is image processing involving more saved state, not simple Nth/add/subtract).I don't understand how my Nth instances end up in different states when their lengths are the same.  For example, if I give izip two strings of equal lengthI get a result of the same length; how come my Nth.itervalues generators seem to be getting unequal numbers of next() calls even though each one yields the same number of results?
 Gist repo with revisions  |  Quick link to solution Quick answerYou never reset self.i and self.nout in class Nth. Also, you should have used something like this:but since you don't even need nout, you should use this:Long answerYour code had an off-by-one smell that led me to this line in NthSumDiff.itervalues():If you swap gen_sum and gen_diff, you'll see that gen_diff will always be the one with nout greater by one. This is because izip() pulls from gen_sum before pulling from gen_diff. gen_sum raises a StopIteration exception before gen_diff is even tried in the last iteration.For example, say you pick N samples where N % step == 7. At the end of each iteration, self.i for the Nth instances should equal 0. But on the very last iteration, self.i in gen_sum will increment up to 7 and then there will be no more elements in x. It will raise StopIteration. gen_diff is still sitting at self.i equal to 0, though.If you add self.i = 0 and self.nout = 0 to the beginning of Nth.itervalues(), the problem goes away.LessonYou only had this problem because your code is too complicated and not Pythonic. If you find yourself using lots of counters and indexes in loops, that's a good sign (in Python) to take a step back and see if you can simplify your code. I have a long history of C programming, and consequently, I still catch myself doing the same thing from time to time in Python. Simpler implementationPutting my money where my mouth is...More explanation of the problemIn response to Brian's comment:This doesn't do the same thing. Not resetting i and nout is  intentional. I've basically got a continuous data stream X that's  split across several files. Slicing the blocks gives a different  result than slicing the concatenated stream (I commented earlier about  possibly using itertools.chain). Also my actual program is more  complicated than mere slicing; it's just a working example. I don't  understand the explanation about the order of StopIteration. If  izip('ABCD','abcd') --> Aa Bb Cc Dd then it seems like equal-length  generators should get an equal number of next calls, no? â€“ Brian  Hawkins 6 hours agoYour problem was so long that I missed the part about the stream coming from multiple files. Let's just look at the code itself. First, we need to be really clear about how itervalues(x) actually works.In itervalues(x) above, for every next() call, it internally increments self.i by self.n and then yields OR it increments self.i by the number of objects remaining in x and then exits the for loop and then exits the generator (itervalues() is a generator because it yields). When the itervalues() generator exits, Python raises a StopIteration exception.So, for every instance of class Nth initialized with N, the value of self.i after exhausting all elements in itervalues(X) will be:Now when you iterate over izip(Nth_1, Nth_2), it will do something like this:So, imagine N=10 and len(X)=13. On the very last next() call to izip(),both A and B have self.i==0 as their state. A.next() is called, increments self.i += 3, runs out of elements in X, exits the for loop, returns, and then Python raises StopIteration. Now, inside izip() we go directly to the exception block skipping B.next() entirely. So, A.i==3 and B.i==0 at the end.Second try at simplification (with correct requirements)Here's another simplified version that treats all file data as one continuous stream. It uses chained, small, re-usable generators. I would highly, highly recommend watching this PyCon '14 talk about generators by David Beazley. Guessing from your problem description, it should be 100% applicable.
Condensing the discussion, there's nothing wrong with using yield in an instance method per se.  You get into trouble with izip if the instance state changes after the last yield because izip stops calling next() on its arguments once any of them stops yielding results.  A clearer example might bewhich hits the last assertion,In the original question, the Nth class can consume input data after its last yield, so the sum and difference streams can get out of sync with izip.  Using izip_longest would work since it will try to exhaust each iterator.  A clearer solution might be to refactor to avoid changing state after the last yield.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.islice
