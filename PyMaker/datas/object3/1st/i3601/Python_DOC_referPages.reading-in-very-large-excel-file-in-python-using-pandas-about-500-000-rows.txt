Natural Text
I have a 14MB Excel file with five worksheets that I'm reading into a Pandas dataframe, and although the code below works, it takes 9 minutes!Does anyone have suggestions for speeding it up?
As others have suggested, csv reading is faster.  So if you are on windows and have Excel, you could call a vbscript to convert the Excel to csv and then read the csv.  I tried the script below and it took about 30 seconds.Here's a little snippet of python to create the ExcelToCsv.vbs script:This answer benefited from Convert XLS to CSV on command line and  csv & xlsx files import to pandas data frame: speed issue
If you have less than 65536 rows (in each sheet) you can try xls (instead of xlsx. In my experience xls is faster than xlsx. It is difficult to compare to csv because it depends on the number of sheets.Although this is not an ideal solution (xls is a binary old privative format), I have found it is useful if you have too many sheets, internal formulas with values that are often updated, or for whatever reason you would really like to keep the excel multisheet functionality.
I know this is old but in case anyone else is looking for an answer that doesn't involve VB. Pandas read_csv() is faster but you don't need a VB script to get a csv file. Open your Excel file and save as *.csv (comma separated value) format. Under tools you can select Web Options and under the Encoding tab you can change the encoding to whatever works for your data. I ended up using Windows, Western European because Windows UTF encoding is "special" but there's lots of ways to accomplish the same thing. Then use the encoding argument in pd.read_csv() to specify your encoding. Encoding options are listed here


Answer URL
https://docs.python.org/3/library/codecs.html#standard-encodings
