Natural Text
I have a question about topic modeling (lda).I do not fully understand the principles of topic modeling, so the question may seem strange.Is it at the end that this phrase is random, is it a high frequency (probability)?What is the exact meaning of this phrase?My code fetched as many topics as the number of documents (I heard it was not possible to reduce it more than the number of documents). I extract only a part of it, some say the representative, some say the frequency is high, I do not know the principle of.
ranking[:5] is known as a slice. It is a copy of a sublist of ranking. It is equivalent to ranking[0:5] and takes the first 5 elements of the list. This is explained in more detail here. (Look for it in the table and especially see footnote 4.)


Answer URL
https://docs.python.org/3/library/stdtypes.html#common-sequence-operations
