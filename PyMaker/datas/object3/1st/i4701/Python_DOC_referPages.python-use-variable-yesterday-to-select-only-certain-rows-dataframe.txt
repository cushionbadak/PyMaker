Natural Text
How to select rows from a DataFrame based on values in some column in pandas?In SQL I would use: I tried to look at pandas documentation but did not immediately find the answer.
To select rows whose column value equals a scalar, some_value, use ==:To select rows whose column value is in an iterable, some_values, use isin:Combine multiple conditions with &: Note the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses is parsed as which results in a Truth value of a Series is ambiguous error.To select rows whose column value does not equal some_value, use !=:isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:For example,yieldsIf you have multiple values you want to include, put them in alist (or more generally, any iterable) and use isin:yieldsNote, however, that if you wish to do this many times, it is more efficient tomake an index first, and then use df.loc:yieldsor, to include multiple values from the index use df.index.isin:yields
tl;drThe pandas equivalent to isMultiple conditions:orCode exampleIn the above code it is the line df[df.foo == 222] that gives the rows based on the column value, 222 in this case.Multiple conditions are also possible:But at that point I would recommend using the query function, since it's less verbose and yields the same result:
There are a few basic ways to select rows from a pandas data frame.Boolean indexingPositional indexingLabel indexingAPIFor each base type, we can keep things simple by restricting ourselves to the pandas API or we can venture outside the API, usually into numpy, and speed things up.I'll show you examples of each and guide you as to when to use certain techniques.SetupThe first thing we'll need is to identify a condition that will act as our criterion for selecting rows.  The OP offers up column_name == some_value.  We'll start there and include some other common use cases.Borrowing from @unutbu:Assume our criterion is column 'A' = 'foo'1.Boolean indexing requires finding the true value of each row's 'A' column being equal to 'foo', then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, mask.  We'll do so here as well.We can then use this mask to slice or index the data frameThis is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the mask.2.Positional indexing has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.3.Label indexing can be very handy, but in this case, we are again doing more work for no benefit4.pd.DataFrame.query is a very elegant/intuitive way to perform this task.  But is often slower.  However, if you pay attention to the timings below, for large data, the query is very efficient.  More so than the standard approach and of similar magnitude as my best suggestion.My preference is to use the Boolean mask Actual improvements can be made by modifying how we create our Boolean mask.mask alternative 1Use the underlying numpy array and forgo the overhead of creating another pd.Series I'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the maskEvaluating the mask with the numpy array is ~ 30 times faster.  This is partly due to numpy evaluation often being faster.  It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.Next, we'll look at the timing for slicing with one mask versus the other.The performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.mask alternative 2We could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframeâ€”you must take care of the dtypes when doing so!Instead of df[mask] we will do thisIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.However, if the data frame is not of mixed type, this is a very useful way to do it.GivenVersusWe cut the time in half.mask alternative 3@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.However, as before, we can utilize numpy to improve performance while sacrificing virtually nothing.  We'll use np.in1dTimingI'll include other concepts mentioned in other posts as well for reference.Code Below Each Column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.You'll notice that fastest times seem to be shared between mask_with_values and mask_with_in1dFunctions Testing Special TimingLooking at the special case when we have a single non-object dtype for the entire data frame.Code Below Turns out, reconstruction isn't worth it past a few hundred rows.Functions Testing 
I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the query() method in v0.13 and I much prefer it. For your question, you could do df.query('col == val')Reproduced from http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-queryYou can also access variables in the environment by prepending an @.
Faster results can be achieved using numpy.where. For example, with unubtu's setup -Timing comparisons:
Here is a simple example  
I just tried editing this, but I wasn't logged in, so I'm not sure where my edit went. I was trying to incorporate multiple selection. So I think a better answer is:For a single value, the most straightforward (human readable) is probably:For lists of values you can also use:For example,yieldsIf you have multiple criteria you want to select against, you can put them in a list and use 'isin':yieldsNote, however, that if you wish to do this many times, it is more efficient to make A the index first, and then use df.loc:yields
If you finding rows based on some integer in a column, thenIf you are finding value based on stringIf based on both

To append to this famous question (though a bit too late): You can also do df.groupby('column_name').get_group('column_desired_value').reset_index() to make a new data frame with specified column having a particular value. E.g.Run this gives:
For selecting only specific columns out of multiple columns for a given value in pandas:Options:or 
If you came here looking to select rows from a dataframe by including those whose column's value is NOT any of a list of values, here's how to flip around unutbu's answer for a list of values above:(To not include a single value, of course, you just use the regular not equals operator, !=.)Example:gives usTo subset to just those rows that AREN'T one or three in column B:yields
You can also use .apply:It actually works row-wise (i.e., applies the function to each row).The output is The results is the same as using as mentioned by @unutbu



Answer URL
https://docs.python.org/3/reference/expressions.html#operator-precedence
