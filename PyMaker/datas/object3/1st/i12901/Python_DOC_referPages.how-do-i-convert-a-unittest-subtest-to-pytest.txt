Natural Text
I have some kind of test data and want to create a unit test for each item. My first idea was to do it like this:The downside of this is that it handles all data in one test. I would like to generate one test for each item on the fly. Any suggestions?
i use something like this:The parameterized package can be used to automate this process:Which will generate the tests:
Using unittest (since 3.4)Since Python 3.4, the standard library unittest package has the subTest context manager.See the documentation:26.4.7. Distinguishing test iterations using subtestssubTestExample:You can also specify a custom message and parameter values to subTest():Using noseThe nose testing framework supports this. Example (the code below is the entire contents of the file containing the test):The output of the nosetests command:
This can be solved elegantly using Metaclasses:
As of Python 3.4 subtests have been introduced to unittest for this purpose. See the documentation for details. TestCase.subTest is a context manager which allows one to isolate asserts in a test so that a failure will be reported with parameter information but does not stop the test execution. Here's the example from the documentation:The output of a test run would be:This is also part of unittest2, so it is available for earlier versions of Python.
load_tests is a little known mechanism introduced in 2.7 to dynamically create a TestSuite. With it, you can easily create parametrized tests.For example:That code will run all the TestCases in the TestSuite returned by load_tests. No other tests are automatically run by the discovery mechanism.Alternatively, you can also use inheritance as shown in this ticket: http://bugs.python.org/msg151444
It can be done by using pytest. Just write the file test_me.py with content:And run your test with command py.test --tb=short test_me.py. Then the output will be looks like:It simple!. Also pytest has more features like fixtures, mark, assert, etc ...
Use the ddt library. It adds simple decorators for the test methods:This library can be installed with pip. It doesn't require nose, and works excellent with the standard library unittest module.
You would benefit from trying the TestScenarios library.testscenarios provides clean dependency injection for python unittest style tests. This can be used for interface testing (testing many implementations via a single test suite) or for classic dependency injection (provide tests with dependencies externally to the test code itself, allowing easy testing in different situations).
There's also Hypothesis which adds fuzz or property based testing: https://pypi.python.org/pypi/hypothesisThis is a very powerful testing method.
You can use nose-ittr plugin (pip install nose-ittr).It's very easy to integrate with existing tests, minimal changes (if any) are required. It also supports nose multiprocessing plugin.Not that you can also have a customize setup function per test.It is also possible to pass nosetest parameters like with their build-in plugin attrib, this way you can run only a specific test with specific parameter:
I came across ParamUnittest the other day when looking at the source code to radon (example usage on the github repo). It should work with other frameworks that extend TestCase (like Nose).Here is an example:
I use metaclasses and decorators for generate tests. You can check my implementation python_wrap_cases. This library doesn't require any test frameworks.Your example:Console output:Also you may use generators. For example this code generate all possible combinations of tests with arguments a__list and b__listConsole output:
Just use metaclasses, as seen here;Output:
RESULT:
You can use TestSuite and custom TestCase classes. 
I'd been having trouble with a very particular style of parameterized tests.  All our Selenium tests can run locally, but they also should be able to be run remotely against several platforms on SauceLabs.  Basically, I wanted to take a large amount of already-written test cases and parameterize them with the fewest changes to code possible.  Furthermore, I needed to be able to pass the parameters into the setUp method, something which I haven't seen any solutions for elsewhere.Here's what I've come up with:With this, all I had to do was add a simple decorator @sauce_labs() to each regular old TestCase, and now when running them, they're wrapped up and rewritten, so that all the test methods are parameterized and renamed.  LoginTests.test_login(self) runs as LoginTests.test_login_internet_explorer_10.0(self), LoginTests.test_login_internet_explorer_11.0(self), and LoginTests.test_login_firefox_43.0(self), and each one has the parameter self.platform to decide what browser/platform to run against, even in LoginTests.setUp, which is crucial for my task since that's where the connection to SauceLabs is initialized.Anyway, I hope this might be of help to someone looking to do a similar "global" parameterization of their tests!
This solution works with unittest and nose:
The metaclass-based answers still work in Python3, but instead of the __metaclass__ attribute one has to use the metaclass parameter, as in:
Meta-programming is fun, but can get on the way. Most solutions here make it difficult to:selectively launch a testpoint back to the code given test's nameSo, my first suggestion is to follow the simple/explicit path (works with any test runner):Since we shouldn't repeat ourselves, my second suggestion builds on @Javier's answer: embrace property based testing. Hypothesis library:is "more relentlessly devious about test case generation than us mere humans"will provide simple count-examplesworks with any test runnerhas many more interesting features (statistics, additional test output, ...)class TestSequence(unittest.TestCase):To test your specific examples, just add:To run only one particular example, you can comment out the other examples (provided example will be run first). You may want to use @given(st.nothing()). Another option is to replace the whole block by:Ok, you don't have distinct test names. But maybe you just need:a descriptive name of the property under test.which input leads to failure (falsifying example).Funnier example
Super late to the party, but I had trouble making these work for setUpClass.Here's a version of @Javier's answer that gives setUpClass access to dynamically allocated attributes.Outputs
Besides using setattr, we can use load_tests since python 3.2. Please refer to blog post blog.livreuro.com/en/coding/python/how-to-generate-discoverable-unit-tests-in-python-dynamically/
Following is my solution. I find this useful when:1. Should work for unittest.Testcase and unittest discover2. Have a set of tests to be run for different parameter settings.3. Very simple no dependency on other packages        import unittest


Answer URL
https://docs.python.org/3/library/unittest.html#distinguishing-test-iterations-using-subtests
https://docs.python.org/3/library/unittest.html#unittest.TestCase.subTest
https://docs.python.org/3/library/unittest.html#distinguishing-test-iterations-using-subtests
