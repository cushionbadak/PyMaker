Natural Text
I have a folder with around 590,035 json files. Each file is a document that has to be indexed. If I index each document using python then it is taking more than 30 hours. How do I index these documents quickly?Note - I've seen bulk api but that requires merging all the files into one which takes similar amount of time as above.Please tell me how to improve the speed. Thank You.
If you're sure that I/O is your bottleneck, use threads to read files, i.e. with ThreadPoolExecutor, and either accumulate for bulk request, or save one by one. ES will have no issues whatsoever, until you're using either unique or internal IDs.Bulk will work faster, just by saving you time on HTTP overhead, saving 1 by 1 is a little bit easier to code.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
