Natural Text
string.split() returns a list instance. Is there a version that returns a generator instead? Are there any reasons against having a generator version? 
It is highly probable that re.finditer uses fairly minimal memory overhead.Demo:edit: I have just confirmed that this takes constant memory in python 3.2.1, assuming my testing methodology was correct. I created a string of very large size (1GB or so), then iterated through the iterable with a for loop (NOT a list comprehension, which would have generated extra memory). This did not result in a noticeable growth of memory (that is, if there was a growth in memory, it was far far less than the 1GB string).
The most efficient way I can think of it to write one using the offset parameter of the str.find() method. This avoids lots of memory use, and relying on the overhead of a regexp when it's not needed.  [edit 2016-8-2: updated this to optionally support regex separators]This can be used like you want...While there is a little bit of cost seeking within the string each time find() or slicing is performed, this should be minimal since strings are represented as continguous arrays in memory.
This is generator version of split() implemented via re.search() that does not have the problem of allocating too many substrings.EDIT: Corrected handling of surrounding whitespace if no separator chars are given.
Here is my implementation, which is much, much faster and more complete than the other answers here. It has 4 separate subfunctions for different cases.I'll just copy the docstring of the main str_split function:Split the string s by the rest of the arguments, possibly omittingempty parts (empty keyword argument is responsible for that).This is a generator function.When only one delimiter is supplied, the string is simply split by it.empty is then True by default.When multiple delimiters are supplied, the string is split by longestpossible sequences of those delimiters by default, or, if empty is set toTrue, empty strings between the delimiters are also included. Note thatthe delimiters in this case may only be single characters.When no delimiters are supplied, string.whitespace is used, so the effectis the same as str.split(), except this function is a generator.This function works in Python 3, and an easy, though quite ugly, fix can be applied to make it work in both 2 and 3 versions. The first lines of the function should be changed to:
Did some performance testing on the various methods proposed (I won't repeat them here). Some results:str.split (default    = 0.3461570239996945manual search (by character) (one of Dave Webb's answer's) = 0.8260340550004912re.finditer (ninjagecko's answer)     = 0.698872097000276str.find (one of Eli Collins's answers)      = 0.7230395330007013itertools.takewhile (Ignacio Vazquez-Abrams's answer) = 2.023023967998597str.split(..., maxsplit=1) recursion = N/A††The recursion answers (string.split with maxsplit = 1) fail to complete in a reasonable time, given string.splits speed they may work better on shorter strings, but then I can't see the use-case for short strings where memory isn't an issue anyway.Tested using timeit on:This raises another question as to why string.split is so much faster despite its memory usage.
No, but it should be easy enough to write one using itertools.takewhile().EDIT:Very simple, half-broken implementation:
I don't see any obvious benefit to a generator version of split().  The generator object is going to have to contain the whole string to iterate over so you're not going to save any memory by having a generator.If you wanted to write one it would be fairly easy though:
I wrote a version of @ninjagecko's answer that behaves more like string.split (i.e. whitespace delimited by default and you can specify a delimiter).Here are the tests I used (in both python 3 and python 2):python's regex module says that it does "the right thing" for unicode whitespace, but I haven't actually tested it.Also available as a gist.
If you would also like to be able to read an iterator (as well as return one) try this:Usage
I wanted to show how to use the find_iter solution to return a generator for given delimiters and then use the pairwise recipe from itertools to build a previous next iteration which will get the actual words as in the original split method.note: I use prev & curr instead of prev & next because overriding next in python is a very bad ideaThis is quite efficient
more_itertools.spit_at offers an analog to str.split for iterators.more_itertools is a third-party package.

Need is for me, at least, with files used as generators.This is version I did in preparation to some huge files with empty line separated blocks of text (this would need to be thoroughly tested for corner cases in case you would use it in production system):
here is a simple response


Answer URL
https://docs.python.org/3/library/re.html
