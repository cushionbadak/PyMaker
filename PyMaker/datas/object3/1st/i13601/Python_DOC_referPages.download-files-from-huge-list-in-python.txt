Natural Text
I have a list with 2M+ file links and would like to download them. My current approach is using urllib.urlretrieve, but it does it serially and really slow. How could I speed it up? Is it possible to do it asyncronously (and how)? 
If you are running Python 3.4+, you can use the stdlib asyncio module to write asynchronous code. See aiohttp for an async web client. There are countless examples of making parallel requests using aiohttp, so use those as a starting point.
It seems you have a large list of tasks (retrieve file and store somewhere) which are independent of each other and can be run in parallel.I would suggest looking at something like Celery. http://www.celeryproject.org/ Read what it's about and see if you agree its a good fit for your problem domain.


Answer URL
https://docs.python.org/3/library/asyncio.html
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
