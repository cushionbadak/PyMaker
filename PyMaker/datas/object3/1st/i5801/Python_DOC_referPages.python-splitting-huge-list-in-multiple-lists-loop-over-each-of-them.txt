Natural Text
In reference to this question of mine, I realized that the corpus is too big and needs be split up in multiple mini-lists before going through the levenshtein calculations. The following code is my simple attempt, but I was wondering if there's a more elegant way to do it:For each of the above lists, I then compute the following loop:Again, everything works, but I was wondering there's a more elegant way to code a single loop for each of the mini-lists.
There are a few problems with your code, and other points you could improve:instead of having six different variables for verbs and output each, use two lists; this way you can more easily adapt the "splitting points" or number of sublists, and you do not have to copy-paste the code block for comparing the words six times; just use another loopthe sublist words[13081:9811] is empty, and also any others where the second index is smaller than the firstwith verbs1 = words[:3269] and verbs2 = words[3269:13080], words[3269] will be in neither of the sublists, as the second index is exclusive; same for the following listsjust in case this was your intention, splitting the list will not reduce the complexity or the running time, as you still have to compare each of the words; a*x + b*x + c*x is the same as (a+b+c) * xinstead of checking a < b and dismissing half of the product, use combinations instead (but this only works when not splitting the list)if you are only interested in pairs with an edit distance <= 5, you could do a few other checks first, like comparing the length of the two words, or the set difference of contained characters; both of those checks will be faster then the actual edit distance check, which is O(nÂ²), and might rule out many combinationsfor the same reason, do not calculate the edit distance twice, once in the check and again for writing it to the file, but just once and store it in a temporary variableif you split the file so that the output file does not become too large for Excel to handle (as I understood one of your comments), your approach might not work, as the size of the output file can vary drastically, depending on how many matches there are in that sublistCombining the above, you could try something like this (not tested):Here, chunks is a function to split an iterator into chunks and might_be_close is a helper function comparing e.g. the length, or sets of contained letters, as described above. The size of the output file might still vary, but will never exceed max_count.Or try this, to get output files with exactly max_count entries:Here, the filter_matches generator pre-filters the combinations and we are chunking those to the right size.
Put the verbs in a list:And then use the length of that list to create a loop with same length. By using the index we can create the path and access the correct element in verbs.


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.combinations
