Natural Text
I have two csv files. train_all.csv :It has 1963891 records, but only 1082190 unique records. Thats why i sorted unique msno-s with pandas. The other csv is about 30GB, cointaining more than 900 million records with user logs. I would like to iterate through the unique msno-s in train_all.csv and search for 5 user_log records in user_logs.csv for each msno. My code stops after about 20 minutes with only 104 records in the results.csv - desired output file with matched msno - user_logs.
Probable reason why it stops without finding everything:the inner loop advances the position in the file every time it reads a row.when you break out of the inner loop the the file counter/pointer/position(?) remains at the place it left offduring the next iteration of the outer loop, the inner loop starts at the last file position instead of at the beginning. So it cannot find any valid msno's in the previous lines/rows of the file. You can verify this by printing the file's position (file.tell()) just before the inner loop (remember ctrl-c to break out of it.)You can fix this by sending the file back to the beginning after the inner loop has completed (whether by StopIteration or a break) so that the inner loop can search through the whole file looking for the msno. Be prepared for a much longer wait.unique_msnos has 1.1E6 records, reader has 9E8 records.  For each record in unique_msnos you will iterate 9e8 times - that would come to 1e15-ish  iterations.  You have to be careful with nested loops.When checking for membership you should use a set.You want to keep track of the number of records you find so maybe make unique_msnos a dictionary with the value holding the count.Then iterate over reader check for membership and adding logic to see if there are already 5:This should alleviate it somewhat - 9e8 iterations.  There are probably other optimizations.  If the msno-s are guaranteed to all be the same length you may get an incremental improvement using slicing instead of using a csv.Reader.  I really don't know how fast csv readers are but slicing is pretty fast.Something likeIt won't reduce the iteration count but if if you get (a_small_improvement * 9e8) improvement it might be noticeable.Looks like you are making a DataFrame then throwing it away. It may be faster to just make unique_msnos directly from the file:You would need to try it to see if it is faster.You may want to make yourself some smaller files to work with while you are trying to get it to work and making optimizations. Just do it once for each file then use the small files for debuging.  Using cprofile or timeit to try and figure out where the bottlenecks are


Answer URL
https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset
https://docs.python.org/3/library/profile.html#the-python-profilers
https://docs.python.org/3/library/timeit.html
