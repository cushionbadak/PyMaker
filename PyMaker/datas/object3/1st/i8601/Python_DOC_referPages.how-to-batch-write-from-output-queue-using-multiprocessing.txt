Natural Text
Suppose I have the following multiprocessing structure:My question: How would I 'batch' write my results to a single file while the working_queue is still 'working' (or at least, not finished)?Note: My actual data structure is not sensitive to unordered results relative to inputs (despite my example using integers).Also, I think that batch/set writing from the output queue is best practice rather than from the growing results bank object. However, I am open to solutions relying on either approach. I am new to multiprocessing so unsure of best practice or most efficient solution(s) to this question.
If you wish to use mp.Processes and mp.Queues, here is a way to process the results in batches. The main idea is in the writer function, below:When passed two arguments, iter expects a callable and a sentinel:iter(callable, sentinel). The callable (i.e. a function) gets called repeatedly until it returns a value equal to the sentinel. Sodefines items to be an iterable which, when iterated over, will return items from output_queueuntil output_queue.get() returns SENTINEL. The for-loop:calls the lambda function repeatedly until an empty list is returned. When called, the lambda function returns a list of up to threshold number of items from the iterable items. Thus, this is an idiom for "grouping by n items without padding". See this post for more on this idiom.Note that it is not a good practice to test working_q.empty(). It could lead to a race condition. For example, suppose we have the 2 worker processes on these lines when the working_q has only 1 item left in it:Suppose Process-1 calls working_queue.empty() while there is still one item in the queue. So it returns False. Then Process-2 calls working_queue.get() and obtains the last item. Then Process-1 gets to line picked = working_queue.get() and hangs because there are no more items in the queue. Therefore, use sentinels (as shown above) to concretely signal when a for-loopor while-loop should stop instead of checking queue.empty().
There is no operation like "batch q.get". But it is a good practice to put/pop a batch of items instead of items one by one.Which is exactly what multiprocessing.Pool.map is doing with its parameter chunksize :)For writing output as soon as possible there is Pool.imap_unordered which returns an iterable instead of list.


Answer URL
https://docs.python.org/3/library/functions.html#iter
