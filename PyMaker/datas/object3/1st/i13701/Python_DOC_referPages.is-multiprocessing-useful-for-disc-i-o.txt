Natural Text
Imagine a simple walk done on a storage device (HDD, SSD, DVD etc). The task is to index all filenames in to an index-file. We are doing a top-down walk and this, I believe, can be parallelized to improve performance. This thread discusses the same issue, but with multithreading rather than multiprocessing.However, the trouble is finding how to start such a process. Since directories have a tree structure, we can't predict which branch will be more lengthy than others.Take a look at this directory tree: If I start from the project folder and make two processes to walk through each subfolder, one of the process will stop after walking through the sec1 folder. It is just an empty folder, while the other one is heavily branched out. This is not at all beneficial. Are there any ways to overcome such issues with disc I/O multiprocessing. Could you illustrate it with an testable example code?
Since you do not know the balance of the tree, there is no reasonable way for you to split the task between the processors.The more important point is that traversing I/O is not a CPU-bound task, it is I/O bound.  Therefore, adding more CPU horsepower is not going to have that much of an effect on the end result.Imagine you have the most power super-giga-peta-hertz 16-CPU computer at your disposal, and further walk has been enhanced for multi-processors.Now you attach a 5400 RPM 1TB hard drive over USB to this device, and do a walk.It is obvious that walk can only walk as fast as the disk can spin, because that's how fast the filesystem (and underlying subsystems) can read the partition table to figure out the directory structure.However, if you were doing a CPU-bound task on each file (like image processing for example), then this portion of your program would benefit from increased CPU performance but it would be waiting to launch till the file system can provide it something to work on - and now you are back to the slow IO issue.


Answer URL
https://docs.python.org/3/library/os.html#os.walk
