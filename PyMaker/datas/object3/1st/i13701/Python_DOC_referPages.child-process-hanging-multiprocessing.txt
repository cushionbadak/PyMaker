Natural Text
I am having a problem where child processes are hanging in my python application, only 4/16 processes have finished all of these processes are adding to a multiprocessing queue. https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues According to python docs:WarningAs mentioned above, if a child process has put items on a queue (and  it has not used JoinableQueue.cancel_join_thread), then that process  will not terminate until all buffered items have been flushed to the  pipe.This means that if you try joining that process you may get a deadlock  unless you are sure that all items which have been put on the queue  have been consumed. Similarly, if the child process is non-daemonic  then the parent process may hang on exit when it tries to join all its  non-daemonic children.Note that a queue created using a manager does not have this issue.  See Programming guidelines.I believe this may be my problem, however I do a get() off the queue before I join.  I am not sure what other alternatives I can take.
The issue with your code is that while not output.empty() is not reliable (see empty). You might also run into the scenario where the interpreter hits while not output.empty() before the processes you created finished their initialization (thus having the Queue actually empty).Since you know exactly how much items will be put in the queue (i.e. len(dictionnary)) you can read that number of items from the queue:If at some point you're modifying your script and don't know anymore howmuch items will be produced, you can use Queue.get with a reasonnable timeout:You might need to adjust the timeout depending on the actual time of the computation in your CreateScript.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.empty
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.get
