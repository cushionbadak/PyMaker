Natural Text
Down below you see a blue print of my crawler. I thought I could speed it up with multithreading but I can't. Often times when I load a page the webserver is slow and then it would be nice to crawl another webpage that loads faster with multithreading. But it isn't faster. Why?Ok I tested if the threads run parallel and they are not :/ What am I doing wrong?I know this because the output is always the same:
you need to provide pool.map() an iterableat the moment you're running start_it() which basically runs all your calls one after another. I don't know what implementation of ThreadPool you are using but you probably need to do something like:
Not to digress, but Asynchronous IO is also a good candidate for your problem. You can use an amazing library called asyncio which has been recently added to python 3.4. For older versions you can use trollius or Twisted.
If the code you posted actualy runs, you shouldn't do pool.map(start_it()), as that calls start_it before passing the result to pool.map. Instead you must pass start_it without any (), as in pool.map(start_it). You pprobably need another argument as well (values to pass to start_it).You can try the example below, which seems to work for me.You could also use multiprocess.Process, e.g.:Example output:Everything under the multiprocessing module uses processes instead of threads, which are truly parallel. Just note that there might be some issues with that (versus running them as threads under the same process).
I think this way it is running truely parallel. I experienced a significant speed up of the crawling. Awesome ;)


Answer URL
https://docs.python.org/3/library/asyncio.html
