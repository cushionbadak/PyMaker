Natural Text
I have a 50K list of strings (city names) and I need a the smallest list of character tri-grams (prefarably n-grams) where every string is at least hit once by one tri-gram. Consider the following list:    ['amsterdam', 'rotterdam', 'haarlem', 'utrecht', 'groningen']the list of identifying trigrams is 4 long and should be (alternatives possible):I thought my solution finds the correct right answer but it gave the wrong answers when used on other lists. I got two answers so far, but both of them have flaws. The one from Rupesh is good for lists that are smaller then 10 items. My lists have over 50K items. The one from mujjiga does come up with a solution albeit not the perfect one.A bounty for the Python Ninja who comes up with a perfect solution that scales.Bonus kuddos if it performs well and gives same solution every time it runs!
Here's a theoretical analysis of @mujjiga answer:You can create classes of words that share the same ngram. You want to pick the smallest number of those classes (that is the smallest number of ngrams) that covers the whole set of words. This is the set cover problem. Unfortunately, this problem is NP-hard (not NP-complete , thanks @mujjiga). (EDIT: Hence, there is no known solution that will give you the expected result in a reasonable time.) The greedy algorithm is almost the best solution (see https://cs.stackexchange.com/questions/49777/is-greedy-algorithm-the-best-algorithm-for-set-cover-problem).Note that even the greedy algorithm may give weird results. Take the sets {a, b}, {b, c}, {c, d} and the superset {a, b, c, d}. The three subsets are maxmimum. If you take {b, c} first, you need the two other subsets to cover the superset. If you take {a, b} or {c, d}, two subsets are enough.Let's use the greedy algorithm, and consider the implementation. The code to create the dictionary that maps ngrams to words is pretty straightforward:The setdefault is equivalent to get if the key ngram exists, and create an empty set otherwise. This is O(|all_words|*|len max word|) complexity.Now, we want to take the ngram with the most words and remove those words from the dictionary. Repeat until you get the words you want.Here's the simple version:Output:This second step has a O(|all_words|*|ngrams|) complexity because of the loop to find the max and the update of the dictionary. Hence, the overall complexity is O(|all_words|*|ngrams|)It is possible to reduce the complexity with a priority queue. Retrieving the best ngram has a cost of O(1), but updating the len of the words mapped to a ngram has a priority O(lg |ngrams|):With this code, the overall priority falls to O(|all_words|*|lg ngrams|). That being said, I would be curious to know if this is faster than the naive previous version with you 50k items.
Below is an implementation of the greedy algorithm for set cover. It runs in about half a second on 50,000 English dictionary words on my machine. The output isn't always optimal, but it's often close in practice. You could probably solve your instances to optimality with an external library for integer programming, but I don't know if you want to go in that direction.The code below dynamically maintains the bipartite graph of ngrams and uncovered texts. The one subtle bit is that, since Python lacks an intrusive heap in its standard library, I've exploited the fact that the keys only increase to fake one. Every ngram is in the heap with a score less than or equal to what it should be. When we pull the minimum, if it is less than it should be, we put it back with the updated value. Otherwise, we know that it's the true minimum.This code should produce deterministic output. At each step it chooses the lexicographically minimum ngram that covers the maximum number of uncovered texts.
Above solution is failing because, Counter is returning trigrams in a not ordered manner, so if you run your solution multiple times, you will get the needed solution also randomlyAnd you are returning the combination as soon as you find it, you are neither going in the order of length nor finding the best combination among all the combinations which satisfies the problemHere I'm going in the order of least to highest elements contained trigram list, Then returning as soon as I found the solution.
Output:['ter', 'utr', 'gro', 'lem'] ['wol', 'dam'] ['dam'] ['dam']Create a dictionary of structure {'ngram': list of cities which contain this ngram }Find the ngram (say x) which is covered in most cities (greedy approach) and remove this ngram and add  it to solution Now we dont have to worry about cities covered by the above selected ngram x, so we go trough the dictionary and remove the cities covered by x. Repeat from step 1 till you find no more ngrams Why is the above solution not always optimal : As mentioned by others the above algorithm is greedy and this problem can be reduced to set-cover which has no  deterministic polynomial time solution. So unless you want to win  $1 million prize it is futile to solve for a polynomial time algorithm which gives optimal solution.  So the next best solution is greedy. Lets look at how bad the greedy solution will be compared to optimal solutionHow bad is greedy: If there are X cities and if the best solution is c (i.e you will need c ngrams to cover all the X cities then the greedy solution cannot be worst then c*ln m. So if you have 50K cities then the greedy solution will be off by maximum of 10.8197782844 times the optimal. 


Answer URL
