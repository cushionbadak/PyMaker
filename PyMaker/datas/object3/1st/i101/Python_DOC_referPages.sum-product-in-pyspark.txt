Natural Text
I have a pyspark dataframe like thisI have Another dataframe like thisI want to divide every column in df1 by the respective value in df2. Hence df3 will look likeUltimately, I want to add colA and colB to get the final df4 per ID
The idea is to join both the DataFrames together and then apply the division operation. Since, df2 contains the column names and the respective value, so we need to pivot() it first and then join with the main table df1. (Pivoting is an expensive operation, but it should be fine as long as the DataFrame is small.)The code is fairly generic, so that we need not need to specify the column names on our own. We find the column names we need to operate on. Except ID we need all.Pivoting the df2, which we will join to df1.We can change the column names, so that we don't have a duplicate name for every column. We do so, by adding a suffix _x on all the names.Next we join the tables with a Cartesian join. (Note that you may run into memory issues if df2 is large.)Finally adding the columns by dividing them with the corresponding value first. reduce() applies function add() of two arguments, cumulatively, to the items of the sequence.Note: OP has to be careful with the division with 0. The snippet just above can be altered to take this condition into account.


Answer URL
https://docs.python.org/3/library/functools.html#functools.reduce
