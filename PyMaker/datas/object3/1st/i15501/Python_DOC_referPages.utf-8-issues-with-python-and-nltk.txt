Natural Text
I have this little script, which is basically just a test I'm doing for a larger program. I have a problem with the encoding. When I try to write to file the utf-8 characters, such as øæå, are not encoded properly. Why is that, and how can I solve this issue?
The thing is nltk.wordpunct_tokenize doesn't work with non-ascii data. It is better to use PunktWordTokenizer from nltk.tokenize.punkt. So import is as:and replace:with:
There are any number of reasons why the encoding isn't working properly. Unicode is a vast and varied mess. The Python HOWTO on Unicode is somewhat helpful for background info: https://docs.python.org/3/howto/unicode.htmlWhen I just need stuffy to work, I've had success forcing encodings into unicode by using ftfy,  available on PyPi: https://pypi.python.org/pypi/ftfy/3.3.0Example usage: 


Answer URL
https://docs.python.org/3/howto/unicode.html
