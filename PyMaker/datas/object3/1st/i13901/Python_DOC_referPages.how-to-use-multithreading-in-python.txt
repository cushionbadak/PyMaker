Natural Text
I need to check whether all the url's are responding or not.If some url(s) is not responding I need to display that.Here I don't want to waitfor the one by one checking and display.For this reason I want to use Multi threading concept.Here's how to use Multi-threading to make use of my code in an efficient way.
Python is not designed to be multithreaded. In fact, there is a Global Interpreter Lock (GIL) baked into Python which makes true multithreading difficult with the vanilla libraries.That is not to say it is completely impossible though; you can use other libraries that work around the GIL. The easiest (and most applicable) for your situation would be Gevent. I don't know what your exact performance requirements are and I don't have any benchmarks at hand to recommend a Gevent approach to follow but you can check them out on your own:You can monkey patch your script. Monkey patching makes the vanilla libraries work with Gevent. Takes the least effort.You can rewrite your script using Gevent-based networking/HTTP libraries.Again, I've no data to tell which is better but this is what I'd do given your situation.
The tool which I'm using to unshorten URLs is concurrent.futures.Take a look right here: concurrent, maybe it can help. Unfortunately, similarly to the answer skytreader gave you, I can't tell you which is better or faster.
You should take a look at the multiprocessing library if you want to do real multi-threading to get around the Global Interpreter Lock. Also, I'd recommend looking at the requests library to see if that does a lot of what you're trying to implement already.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example
