Natural Text
I have genomic data from 16 nuclei. The first column represents the nucleus, the next two columns represent the scaffold (section of genome) and the position on the scaffold respectively, and the last two columns represent the nucleotide and coverage respectively. There can be equal scaffolds and positions in different nuclei.Using input for start and end positions (scaffold and position of each), I'm supposed to output a csv file which shows the data (nucleotide and coverage) of each nucleus within the range from start to end. I was thinking of doing this by having 16 columns (one for each nucleus), and then showing the data from top to bottom. The leftmost region would be a reference genome in that range, which I accessed by creating a dictionary for each of its scaffolds.In my code, I have a defaultdict of lists, so the key is a string which combines the scaffold and the location, while the data is an array of lists, so that for each nucleus, the data can be appended to the same location, and in the end each location has data from every nucleus.Of course, this is very slow. How should I be doing it instead?Code:Files:https://drive.google.com/file/d/0Bz7WGValdVR-bTdOcmdfRXpUYUE/view?usp=sharinghttps://drive.google.com/file/d/0Bz7WGValdVR-aFdVVUtTbnI2WHM/view?usp=sharing
(Only focusing on the CSV section in the middle of your code)The example csv file you supplied is over 2GB and 77,822,354 lines. Of those lines, you seem to only be focused on 26,804,253 lines or about 1/3.As a general suggestion, you can speed thing up by:Avoid processing the data you are not interested in (2/3 of the file);Speed up identifying the data you are interested in;Avoid the things that repeated millions of times that tend to be slower (processing each line as csv, reassembling a string, etc);Avoid reading all data when you can break it up into blocks or lines (memory will get tight)Use faster tools like numpy, pandas and pypyYou data is block oriented, so you can use a FlipFlop type object to sense if you are in a block or not.The first column of your csv is numeric, so rather than splitting the line apart and reassembling two columns, you can use the faster Python in operator to find the start and end of the blocks:Prints:That runs in about 9 seconds in PyPy and 46 seconds in Python 2.7. You can then take the portion that reads the source csv file and turn that into a generator so you only need to deal with one block of data at a time.(Certainly not correct, since I spent no time trying to understand your files overall..):Or, if you need to combine all the blocks into one dict:That executes in about 1 minute on my (oldish) Mac in PyPy and about 3 minutes in cPython 2.7. Best


Answer URL
https://docs.python.org/3/library/sqlite3.html
