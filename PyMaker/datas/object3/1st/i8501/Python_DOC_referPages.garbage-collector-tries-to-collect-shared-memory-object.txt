Natural Text
I've got two Python scripts that both should do essentially the same thing: grab a large object in memory, then fork a bunch of children. The first script uses bare os.fork:The second script uses multiprocessing module. I'm running both on Linux (Ubuntu 14.04), so it should use os.fork under the hood too, as documentation states:The difference between those two scripts is the following: when I kill a child (sending a SIGTERM), bare-fork script tries to garbagecollect the shared dictionary, despite the fact that it is still referenced by parent process and isn't actually copied into child's memory (because of copy-on-write)(perf record -e page-faults -g -p <pid> output:)While multiprocessing-based script does no such thing:(perf record -e page-faults -g -p <pid> output:)I can also force the same behavior on multiprocessing-based script by explicitly calling gc.collect() before raising GracefulExit. Curiously enough, the reverse is not true: calling gc.disable(); gc.set_threshold(0) in bare-fork script doesn't help to get rid of PyInt_ClearFreeList calls.To the actual questions:Why is this happening? I sort of understand why python would like to free all the allocated memory on process exit, ignoring the fact that the child process doesn't physically own it, but how come multiprocessing module doesn't do the same?I'd like to achieve second-script-like behavior (i.e.: not trying to free the memory which has been allocated by a parent process) with bare-fork solution (mainly because I use a third-party process manager library which doesn't use multiprocessing); how could I possibly do that?
Couple thingsIn python, multiple python processes means multiple interpreters with their own GIL, GC et alThe d dictionary is not passed in as an argument to the process, it is a globally shared variable. The reason it gets collected is because each process thinks its the only one holding a reference to it which, strictly speaking, is true as it's a single globally shared object reference to the dictionary. When Python GC checks it, it checks the ref counter for that object. Since there is only the one shared reference, removing that would mean ref count == 0, so it gets collected.To resolve the issue, d should be passed into each forked process, making each process hold its own reference to it.
Multiprocessing behaves differently because it uses os._exit which doesn't call exit handler, which, apparently, involves garbage collection (more on the topic). Explicitly calling os._exit in bare-fork version of the script achieves the same result.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
