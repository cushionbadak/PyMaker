Natural Text
Testing Environment:Python Version: 3.5.1OS Platform: Ubuntu 16.04IDE: PyCharm Community Edition 2016.3.2I write a simple program to test process-safe. I find that subprocess2 won't run until subprocess1 finished. It seems that the instance variable self.count is process-safe.How the process share this variable? Does they share self directly?Another question is when I use Queue, I have to use multiprocessing.Manager to guarantees process safety manually, or the program won't run as expected.(If you uncomment self.queue = multiprocessing.Queue(), this program won't run normally, but using self.queue = multiprocessing.Manager().Queue() is OK.)The last question is why the final result is 900? I think it should be 102.Sorry for asking so many questions, but I'm indeed curious about these things. Thanks a lot!Code:Output:
The underlying details are rather tricky (see the Python3 documentation for more, and note that the details are slightly different for Python2), but essentially, when you pass self.subprocess1 or self.subprocess2 as an argument to self.pool.apply_async, Python ends up calling:in the main process—the initial one on Linux before forking, or the one invoked as __main__ on Windows—and then, eventually, pickle.loads() of the resulting byte-string in the pool process.1  The pickle.dumps code winds up calling your own __getstate__ function; that function's job is to return something that can be serialized to a byte-string.2  The subsequent pickle.loads creates a blank instance of the appropriate type, does not call its __init__, and then uses its __setstate__ function to fill in the object (instead of __init__ing it).Your __getstate__ returns the dictionary holding the state of self, minus the pool object, for good reason:Since pool objects refuse to be pickled (serialized), we must avoid even attempting to do that.In any case, all of this means that the pool process has its own copy of self, which has its own copy of self.count (and is missing self.pool entirely).  These items are not shared in any way so it is safe to modify self.count there.I find the simplest mental model of this is to give each worker process a name: Alice, Bob, Carol, and so on, if you like.  You can then think of the main process as "you": you copy something and give the copy to Alice, then copy it and give that one to Bob, and so on.  Function calls, such as apply or apply_async, copy all of their arguments—including the implied self for bound methods.When using a multiprocessing.Queue, you get something that knows how to work between the various processes, sharing data as needed, with appropriate synchronization.  This lets you pass copies of data back and forth.  However, like a pool instance, a multiprocessing.Queue instance cannot be copied.  The multiprocessing routines do let you copy a multiprocessing.Manager().Queue() instance, which is good if you want a copied and otherwise private Queue() instance.  (The internal details of this are complicated.3)The final result you get is just 900 because you are looking only at the original self object.Note that each applied functions (from apply or apply_async) returns a result.  This result is copied back, from the worker process to the main process.  With apply_async, you may choose to get called back as soon as the result is ready.  If you want this result you should save it somewhere, or use the get function (as shown in that same answer) to wait for it when you need it.1We can say "the" pool process here without worrying about which one, as you limited yourself to just one.  In any case, though, there is a simple byte-oriented, two-way communications stream, managed by the multiprocessing code, connecting each worker process with the parent process that invoked it.  If you create two such pool processes, each one has its own byte-stream connecting to the main process.  This means it would not matter if there were two or more: the behavior would be the same.2This "something" is often a dictionary, but see Simple example of use of __setstate__ and __getstate__ for details.3The output of pickle.dumps on such an instance is:I did a little trickiness to split this at newlines and then manually added the parentheses, just to keep the long line from being super-long.  The arguments will vary on different systems; this particular one uses a file system object that is a listener socket, that allows cooperating Python processes to establish a new byte stream between themselves.
Question: ... why the final result is 900? I think it should be 102.  The result should be 106, range are 0 based, you get 3 iterations.You can get the expected output, for instance:Output:  Task 1 in Process 5601 has been started - start=0  Task 1 in Process 5601, count = 0  Task 2 in Process 5602 has been started - start=100  Task 2 in Process 5602, count = 100  Task 1 in Process 5601, count = 1  Task 2 in Process 5602, count = 101  Task 1 in Process 5601, count = 2  Task 2 in Process 5602, count = 102  Task 1 in Process 5601 has been completed - count=3  Task 2 in Process 5602 has been completed - count=103  sum(count) = 106  Tested with Python:3.4.2 


Answer URL
https://docs.python.org/3/howto/descriptor.html#functions-and-methods
