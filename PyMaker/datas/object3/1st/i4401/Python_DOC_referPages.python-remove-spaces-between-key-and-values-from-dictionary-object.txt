Natural Text
I write python script that read JSON from file, transform it and at the end write modified JSON to file. JSON that I read doesn't have white characters, because I need minimal size. The problem is when I read JSON from file and call json.loads(json_content) I get dictionary object and python adds spaces between key and values. JSON in file:{"metadata":{"info":"important info"},"timestamp":"2018-04-06T12:19:38.611Z","content":{"id":"1","name":"name test","objects":[{"id":"1","url":"http://example.com","properties":[{"id":"1","value":"1"}]}]}}My script:And the output of this piece of code is:Explanation:Python reads JSON from file as string that is the same in the file. Length of contents is 206 and size is 255 because empty string in python has 49 bytes. Calling json.loads(contents) returns dict object that has size 240. JSON represented as dict object has added spaces between key names and values. And this is the cause why after call string_data = json.dumps(data) size of string_data is equal to 274 and length is equal to 224. Therefore I tried to remove white spaces from string_data using function .replace(" ", ""), but I forget that some values have white spaces and after this operation JSON is not correct.I need to read JSON as dict object to convenient transformations and at the same time I have to keep minimal size of JSON.Is there any way to remove unnecessary white spaces from dict object?
All you need to care about is the length of a string, so use len() to compare results. If you care about generated JSON sizes, you can trivially tell json.dumps() to not use any spaces:Demo:With separators specified, the output length matches the original input. The separators option is documented as:If specified, separators should be an (item_separator, key_separator) tuple. The default is (', ', ': ') if indent is None and (',', ': ') otherwise. To get the most compact JSON representation, you should specify (',', ':') to eliminate whitespace.(Bold emphasis mine).sys.getsizeof() is the wrong tool to make comparisons here. The function gives you the memory footprint of an object (without recursing), and this memory footprint is a implementation detail of your current Python implementation and operating system specific data type sizes.The dict object uses 240 bytes on your OS, but this is a over-allocated hash table of references. On a different OS with different size pointers, the memory size would be different, and either way, you didn't include the size of the referenced string objects. For string objects, the memory footprint depends heavily on the highest Unicode codepoint; the string 'a' has a different footprint from 'ìêë', even both have length 1, because the latter contains a non-BMP codepoint so 4 bytes are needed to store just that one character, plus Python object overhead.Next, all you saved was 19 spaces. Compression would have saved you much more, and spaces don't matter much when compressing:Compression saved 50 bytes there, while using the compact JSON separators saved another single byte.
The json.dumps function accepts several arguments that let you specify how you want the data formatted when you write your dictionary back out to a JSON string. From it's docstring:If indent is a non-negative integer, then JSON array elements and      object members will be pretty-printed with that indent level. An indent      level of 0 will only insert newlines. None is the most compact      representation.If specified, separators should be an (item_separator, key_separator)      tuple.  The default is (', ', ': ') if indent is None and      (',', ': ') otherwise.  To get the most compact JSON representation,      you should specify (',', ':') to eliminate whitespace.So try string_data = json.dumps(data, indent=None, separators=(',', ':')) for the most compact representation.


Answer URL
https://docs.python.org/3/library/archiving.html
