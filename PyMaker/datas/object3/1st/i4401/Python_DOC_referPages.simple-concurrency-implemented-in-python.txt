Natural Text
Purpose of the question: learn more about ways to implement concurrency in Python / experimenting.Context: I want to count all of the words in all of the files that match a particular pattern. The idea is that I can invoke the function count_words('/foo/bar/*.txt') and all of the words (i.e., strings separated by one or more whitespace characters) will be counted.In the implementation I am looking for ways to implement count_words using concurrency. So far I managed to use multiprocessing and asyncio. Do you see alternative approaches to do the same task?I did not use threading as I noticed the performance improvement was not that impressive due to the limitation of the Python GIL.For simulating large quantity of files, I downloaded some books from Project Gutenberg (http://gutenberg.org/) and used the following command to create several duplicates of the same file.
async def doesn't make function calls concurrent magically, in asyncio you need to explicitly give up execution to allow other coroutines run concurrently by using await on awaitables. That said, your current count_words_for_file is still executed sequentially.You may want to introduce aiofiles to defer the blocking file I/O into threads, allowing concurrent file I/O in different coroutines. Even with that, the piece of CPU-bound code that calculates the number of words still run sequentially in the same main thread. To parallelize that, you still need multiple processes and multiple CPUs (or multiple computers, check Celery).Besides, there is an issue in your asyncio code - for ... run_until_complete again make the function calls run sequentially. You'll need loop.create_task() to start them concurrently, and aysncio.wait() to join the results.


Answer URL
https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop.create_task
https://docs.python.org/3/library/asyncio-task.html#asyncio.wait
