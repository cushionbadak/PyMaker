Natural Text
So I have task A which is copying some unkown number of files into a folder.Task B runs on each of those files in the folder. I have no way of knowing the number of files beforehand as they keep changing. Is there a way to make this work in airflow. I am getting name of all the files in the directory and then running the operator for them, but airflow doesn't work that way as it needs to know the number beforehand. 
I don't expect Airflow to modify a DAG while DagRun is active, so I wouldn't bet money on getting files and then appending tasks in the same DAG. That being said, Airflow regenerates DAGs every few seconds. You could have one DAG that gets the files and another DAG that processes those files. After getting the files, first DAG would have to wait a minute to make sure Airflow noticed and then kick off the second DAG with a TriggerDagRunOperator.DAG1:DAG2:More hack than a solution, but something like this should work. There are issues with external triggers and dynamic tasks though. I typically stumble into scheduler problems when I have to use depends_on_past=True.
I've got this type of thing to work by making separate pipelines instead of separate tasks.  
Have you tried using glob module and modifying your pipeline to to process all files in given directory?
Regarding my blog post on Creating a Dynamic Workflow using Apache Airflow, you can test the following code:With this code, you need to already have some files (you can also create a Python function which check if there is some files, otherwise create a DummyOperator just to have the entire workflow working) in your /tmp/filetoprocess folder ; otherwise, the Airflow scheduler will have issue to generate a proper DAG.I have test it with the new Apache Airflow release (v.1.10) and it seems to work perfectly.Dynamic tasks on Airflow DAG


Answer URL
https://docs.python.org/3/library/glob.html
