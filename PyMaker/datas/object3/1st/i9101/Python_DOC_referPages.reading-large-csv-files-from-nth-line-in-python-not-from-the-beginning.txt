Natural Text
I have 3 huge CSV files containing climate data, each about 5GB. The first cell in each line is the meteorological station's number (from 0 to about 100,000) each station contains from 1 to 800 lines in each file, which is not necessarily equal in all files. For example, Station 11 has 600, 500, and 200 lines in file1, file2, and file3 respectively. I want to read all the lines of each station, do some operations on them, then write results to another file, then the next station, etc.The files are too large to load at once in memory, so I tried some solutions to read them with minimal memory load, like this post and this post which include this method:The problem with this method that it reads the file from the beginning every time, while I want to read files as follows:The problem is that each time I start over to take next station, the for loop would start from the beginning, while I want it to start from where the 'Break' occurs at the nth line, i.e. to continue reading the file.How can I do it?Thanks in advanceNotes About the solutions below:As I mentioned below at the time I posted my answer, I implemented the answer of @DerFaizio but I found it very slow in processing. After I had tried the generator-based answer submitted by @PM_2Ring I found it very very fast. Maybe because it depends on Generators.The difference between the two solutions can be noticed by the numbers of processed stations per minutes which are 2500 st/min for the generator based solution, and 45 st/min for the Pandas based solution. where the Generator based solution is >55 times faster.I will keep both implementations below for reference. Many thanks to all contributors, especially @PM_2Ring.
The code below iterates over the files line by line, grabbing the lines for each station from each file in turn and appending them to a list for further processing.The heart of this code is a generator file_buff that yields the lines of a file but which allows us to push a line back for later reading. When we read a line for the next station we can send it back to file_buff so that we can re-read it when it's time to process the lines for that station.To test this code, I created some simple fake station data using create_data.outputThis code can cope if station data is missing for a particular station from one or two of the files, but not if it's missing from all three files, since it breaks the main processing loop when the station_lines list is empty, but that shouldn't be a problem for your data.For details on generators and the generator.send method, please see 6.2.9. Yield expressions in the docs.This code was developed using Python 3, but it will also run on Python 2.6+ (you just need to include from __future__ import print_function at the top of the script).If there may be station ids missing from all 3 files we can easily handle that. Just use a simple range loop instead of the infinite str_count generator.output
I would suggest to use pandas.read_csv. You can specify the rows to skip using skiprows and also use a reasonable number of rows to load depending on your filesize using nrowsHere is a link to the documentation:http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
I posted the code below before @PM-2Ring posted his solution.I would like to leave both solutions active:The #1 solution that depends on Pandas library (by @DerFaizio). :This solution finished 5450 stations in 120 minutes (about 45 stations/minute)The following solution is the Generator based solution (by @PM-2Ring)This solution finished 30000 stations in 12 minutes (about 2500 stations/minute)Thanks for all.
Going with the built-in csv module, you could do something like:Where n is the number of lines you want to skip.  


Answer URL
https://docs.python.org/3/reference/expressions.html#yield-expressions
