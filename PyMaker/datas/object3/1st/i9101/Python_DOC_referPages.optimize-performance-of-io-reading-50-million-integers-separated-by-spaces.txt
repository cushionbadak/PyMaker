Natural Text
If using interpreted Python 2.7.6, and trying to read about 50 million integers (signed, 32 bits) from a file linked to stdin, what's the fastest (performance) way to do this if they come in a single line (no \n at the end), space separated?, or perhaps comma separated? Preferably using generators and/or reading in chunks so that the whole file is not read into memory at once, or a list of all 50M integers stored at once. The list should be reduced to sum of all adjacent element xors (A[0]^A[1] + A[1]^A[2] + ... ), the numbers are very close to each other so the reduction does not break 32 bits signed integer.An initial line can be added to have either the number of integers (n), and/or the length of the line (L).I am not proficient on python, and I get unacceptable results (>30 seconds). For a tenth of the limits I do about 6 seconds, so I feel I need to improve this much much more.It seems to me if they had come separated with line breaks this might have been possible. Is there a way to tell python to use a different delimiter for readline()?Tried:for ch in stdin.read(), it takes 3 seconds to loop all ch, but building the integers with multiplications and then doing the reduction manually takes too long.read(n), reading in chunks, then storing the incomplete tail for later, using split and map int, for xrange and reduce on the chunk sequentially to build the reduction list, but again seems to take too long.I have done this on faster languages already thanks, looking for interpreted python answers.This is my best code, runs in 18 seconds in SOME cases, in others it is too slow. But it is faster than the version where I built the integers with multiplications on an accumulator. It is also faster than reading byte per byte: read(1).I can see it could (maybe) be improved if it was possible to initialize b only once and then not using append but actually accessing the index, but when I tried b = [None]*12 I got an RTE during join cant join None, need a join over a range, so I dropped the idea for the moment. Also faster functions to do what I already do.Update:This is 3 times faster (10 million can be done in 6 seconds, but 50 take over 30), for 50 million, it is still too slow, IO seems not to be the main bottleneck, but the data processing.Instead of the deque a regular list can be used, calling pop(0) instead of popleft. It is also possible not to call len(b) on every loop, as you have n at the beginning and can subtract instead, but besides that this seems the fastest so far.
Read a stream of bytes until EOF. Once you hit a space, convert a list of "digit" bytes to an integer, do your XOR, and reset the list. Or just keep appending digits to a list until you do hit a space. Something like the following untested code:You could probably optimize this by reading in chunks of bytes, instead of one byte at a time. Another way to optimize this, perhaps, is to keep a kind of "nul" terminator for the list, an index that keeps the "length" of the list. Instead of wiping it clear on every loop, you do your map operation on a start-/end-indexed subset of bytes. But hopefully this demonstrates the principle.Short of this, you could perhaps use a Unix utility like sed to replace spaces with newlines and pipe the output of sed to a Python script, and have Python read from the stdin stream, using its (perhaps optimized) ability to read a line at a time.(But, really, Python is probably the wrong answer for anything that needs speedy I/O.)
I ran this code:And got these results:That's 56 million (small) numbers, on a 5 year old MacBook Pro, in 64 seconds or so - about 1m numbers per second. Can you give us your timings, and what you expect to get?
I would be surprised if you could find a much faster implementation than numpy.fromfileHowever, parsing ints from a text files is much slower than just reading binary data. Here's some quick and dirty benchmarks using two files with the same ~50M integers. The first is text format, the other is binary (written using numpy.ndarray.tofile)
how about thisinstead of going 1 character at the time, use a module that specialize it character manipulation like re


Answer URL
https://docs.python.org/3/library/array.html#array.array.fromfile
