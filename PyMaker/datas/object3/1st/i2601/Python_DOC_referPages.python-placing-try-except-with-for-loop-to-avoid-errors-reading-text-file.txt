Natural Text
I am trying to clean a huge (8gb) JSON file from Yelp. I want to convert the data either into a pandas data frame or write to a CSV. Goal: I want to essentially skip any lines in the JSON file that are problematic, but add any I can to my pandas data frame. Note: I wrote a function flatten_json which returns a dictionary where each key is the column name and each value is the row. Attempt 1But this code fails because for some reason the for loop is unable to even process the line from the file, which I don't understand. Attempt 2Since the code was having trouble processing lines from the text, I tried to use a try-except to screen for error-prone lines from the file. But this doesn't work either because, when the errors arise, it just skips the entire rest of the loop. Attempt 3The problem with this approach is that, I don't know how many lines are in the file. I tried setting it to some large number like 15,000,000, but it never terminatedQuestion: Where can I place the try-except such that it skip lines with errors and also so the for loop is structured so it will go  through every line in the file? 
Your attempt 2 is close. You just need to move the try inside the for, so it only skips that one loop iteration (that one line), rather than the whole loop (the whole file).But there's no reason to rewrite the for around manually calling next, as in your attempt 3—you're not trying to deal with errors in reading the line from the file, only errors in decoding bad UTF-8 or parsing JSON.In fact, you generally want to make your try as narrow as possible, not as wide as possible, so you don't accidentally swallow errors you weren't expecting and hoping to swallow. And, for the same reason, you almost never want a bare except: statement.Handling the JSON errors is easy, but how do you handle the encoding errors?One option is to just do the decoding explicitly, so you can try it narrowly:But, even more simply: loads can accept UTF-8 bytes directly:(If you're not using Python 3.6 or later, see the docs for your version of loads instead of the 3.6 docs—this same line should work, but the details for why it works are different…)The problem with this approach is that, I don't know how many lines are in the file. I tried setting it to some large number like 15,000,000, but it never terminated.As explained above, you don't need to do this.But in case you ever do, I'll explain what's wrong, and what to do about it.When you reach the end of the file, next(myfile) will raise StopIteration. But you catch that in your bare except: and just go on to the next line. Which will again raise StopIteration. And so on. So, if you've got 1 million lines, you'll have to go through 14 million except: loops after reaching the end of the file.This is exactly why you don't want a bare except:. And one option is to just change that, so StopIteration isn't caught there. You can catch it at separately, and use it to break out of the loop:A different alternative is to use file.readline() instead of next(file). The readline method will return an empty string at EOF, but will never return an empty string otherwise (a blank line is still '\n'). So:Either way, of course, you no longer need to guess at the length; instead of for i in range(15000000):, just do while True:.But then you've just got a while True: around a line = next(file) with an except StopIteration: break, which is exactly what for line in file: does in the first place, so… just write that.Finally: Are you sure you really want to silently ignore all non-UTF-8 lines?It may just be that the data is garbage—each JSON text is in a different encoding, with most of them in UTF-8 but some in others, and the encodings aren't specified anywhere in-band or out-of-band, so there's really no good answer. (Although even then, you might want to try using chardet or unicodedammit or another heuristic guesser when UTF-8 fails…)But if your data is in, say, Latin-1, what you're doing is ignoring anything that isn't in English. It would be much more useful to find out that the data is in Latin-1, and decode it as such.That should be documented by your source. If it isn't, a library like chardet or unicodedammit might help you guess (they're even better for manual guessing than automated, of course). If you can't figure it out, instead of just silently discarding errors, maybe log them (e.g., log the repr of both the exception and the line), and then come back to Stack Overflow and ask for help with the info in your logs.
You have to actually fix the problem, which is completely unrelated to json decoding.As you can see in your error traceback:Your error happens in the for line, before even json.loads!The error UnicodeDecodeError means that the file contents are not utf-8 as you specified. You can try specifying another encoding, or you can pass the ignore parameter when opening the file to ignore those errors:That will remove the unknown byte when decoding, so it will be missing but no errors will raise.Attempts 2 and 3 have the line decoding inside the try clauseso they will shadow the real error which is text decoding.


Answer URL
https://docs.python.org/3/library/json.html#json.loads
