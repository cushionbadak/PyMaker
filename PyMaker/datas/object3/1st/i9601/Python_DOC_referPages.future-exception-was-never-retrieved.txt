Natural Text
I have a scraper (based on Python 3.4.2 and asyncio/aiohttp libs) and bunch of links (> 10K) to retrive some small amount of data.Part of scraper code:Whan it trying to make requests to malformed URL's I get messages like this:The error occures when trying to yield response from session.get. As I understand the exception was never consumed by asyncio and so it wasn't "babble up".First I tryed to simply wrap request by try/except:This doesn't work.Then I read here about chaining coroutines to catch exception but this didn't work for me either. I still get those messages and script crashes after certain amount of time.So my question - how I can handle this exception in proper way?
not an answer to your question, but perhaps a solution to your problem either way depending on if you just want to get the code working or not.I would validate the URLS before i request them. i've had alot of headaches with this kind of stuff trying to harvest some data, so i decided to fix them upfront ,and report malformed urls to a log.You can use django's regex or other code to do this as it's publicly availible.In this question a person gives the validation regex for django.Python - How to validate a url in python ? (Malformed or not) 


Answer URL
https://docs.python.org/3/library/asyncio-dev.html#detect-exceptions-never-consumed
