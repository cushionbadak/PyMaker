Natural Text
I'm trying to execute a function on every line of a CSV file as fast as possible. My code works, but I know it could be faster if I make better use of the multiprocessing library.I'm thinking I should put the tasks into a Queue and process them with a Pool but all the examples make it seem like Queue doesn't work the way I assume, and that I can't map a Pool to an ever expanding Queue.
I've done something similar using a Pool of workers.  However, as pvg commented under your question, you might want to consider how to batch your data.  Going row by row may not the the right level of granularity.  You might also want to profile/test to figure out the bottle-neck.  For example, if disk access is limiting you, you might not benefit from parallelizing.  mulprocessing.Queue is a means to exchanging objects among the processes, so it's not something you'd put a task into.  
For me it looks like you are actually trying to speed upwhich can be done withCreating processes takes some time (especially on windows) so in most cases using threads via multiprocessing.dummy is faster (and also multiprocessing is not totally trivial - see Guidelines).


Answer URL
https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy
https://docs.python.org/3/library/multiprocessing.html#programming-guidelines
