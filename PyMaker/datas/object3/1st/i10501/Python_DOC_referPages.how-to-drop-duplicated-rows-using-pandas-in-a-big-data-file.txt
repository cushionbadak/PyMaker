Natural Text
I have a csv file that too big to load to memory.I need to drop duplicated rows of the file.So I follow this way:But if duplicated rows distribute in different chunk seems like above script can't get the expected results.Is there any better wayï¼Ÿ
You could try something like this.First, create your chunker.Now create a set of ids:Now iterate over the chunks:However, now, within the body of the loop, drop also ids already in the set of known ids:Finally, still within the body of the loop, add the new idsIf ids is too large to fit into main memory, you might need to use some disk-based database.


Answer URL
https://docs.python.org/3/library/shelve.html
