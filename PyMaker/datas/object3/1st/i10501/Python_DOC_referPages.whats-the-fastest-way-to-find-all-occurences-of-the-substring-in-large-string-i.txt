Natural Text
Edited to clarify input/output. I think this will be somewhat slow no matter what, but up until now I haven't really considered speed in my python scripts, and I'm trying to figure out ways to speed up operations like these.My input is pickled dictionaries of the genome sequences. I'm currently working with two genomes, the budding yeast genome (11.5 MB on disk), and the human genome (2.8 GB on disk). These dictionaries have the form:I want to find all single-base instances of a nucleotide(s) in both strands of the genome. Where the + strand refers to the sequence in the above dictionary, and the - strand is the reverse complement of the sequences. My output is a nested dictionary, where the top level keys are + or -, the nested keys are chromosome names, and the values are lists of 0-indexed positions:test_d defines a set of positions to examine in a large Illumina sequencing dataset later in the script.My first attempt uses enumerate, and iteration.The output for yeast:The output for human:I tried using numpy arrays rather than enumerate() and iteration:The output for yeast:The output for human:Why does the numpy version lose its performance advantage for the larger human genome? Is there a faster way to do this? I tried implementing a version using multiprocessing.Pool, but that's slower than either version:I haven't run this on the human genome, but the yeast version is slower:
To be honest, I think you've been doing the right things.There are a few more tweaks you can make to your code, though.  In general, when performance is key, only do the bare minimum in your innermost loops.  Looking through your code, there are still some quick optimizations left on this front:Use if...elif instead of if...if.Don't use lists where you don't have to - e.g. just a single string is sufficient for nts and the reverse.Don't evaluate the same result multiple times - e.g. the reverse lookup.I'm guessing that your problem with multi-processing is down to the serialization of these very large strings, offsetting any performance gain you might have from running in parallel.  However, there may be another way to do this - see Parallelizing a Numpy vector operation.  I can't verify as I am having difficulty installing numexpr.Putting them together and trying out some of the other suggestions in this trail, I get the following results:My code is as follows.
Use built-insInstead of manually iterating through your long string, try str.find or str.index. Don't slice the string yourself, use these methods' built-in slicing.This also ditches enumerate-ing, although that shouldn't be costly anyway.Also, you could use set to store indices, not list - additions could be faster.You would have to do it twice, though, to find both your nucleotide and its complement. Of course, look up the complement outside of the loop.Try regular expressionsYou could also try regular expressions to do the same thing (if you are going to try this, try both 2 regex (for "T" and "A") and one for "T|A").Also, instead of doingYou could doWhich has little to do with performance, but makes the code more readable.
Finds the longest chromosome string, and creates an empty array with one row per chromosome, and columns up to the longest one in the dictionary. Then it puts each chromosome into its own row, where you can call np.where on the whole arrayUsing this method, it's slightly different for strings of > 1 nucleotide, but I'm sure it's achievable in only a couple more steps.


Answer URL
https://docs.python.org/3/library/stdtypes.html#str.find
https://docs.python.org/3/library/stdtypes.html#str.index
https://docs.python.org/3/library/re.html
