Natural Text
I am trying to compare each record with all other records, while comparing i am returning the index of the elements which are different in both records. Initially i thought, reading the file to python takes time and so i tried to execute my code with only reading part, it took only seconds. But when i add this comparison part, its taking lot of memory as it has to compare each record and each element in it. I could see my computer memory usage goes high and reaches the limit (8GB). And my CPU usage is within 50%, so i guess i can assume that the computation is not that intense and so not required to parallelize the process (Correct me if i am wrong).Is there any way, i can optimize this?Added after the commentHere is my data.. And this is my output.{frozenset({'Age', 'Workclass', 'Fnlwgt'}), frozenset({'Age', 'Workclass', 'Education-num', 'Fnlwgt', 'Education'})}{frozenset({'Age', 'Workclass', 'Fnlwgt'})}
The only massive amount of memory you're using here is the list that you're trying to build.the data is 3.5mb, it has 35000 records and each record has 15 elements.If mytuples is a list of 35000 15-tuples, then combinations is going to iterate over 612,517,500 pairs of 15-tuples.The "comparison part", where you unzip that pair of 15-tuples into an iterator over 15 2-tuples, is not going to run out of memory. That's a few KB at worst, not 8GB.But the fact that you're trying to store a list of a few hundred million single-element lists whose elemnets are integers… well, in 64-bit CPython 3.4, each integer (up to 1<<62) is 28 bytes, and a list takes 8 bytes per element plus a 64-byte header, so you're talking 100 bytes per value, so as soon as you get to around 80 million, that's 8GB.Your updated version is instead storing a giant list of generators (why?!); generators are at least 64 bytes, or more depending on how much state they have, so it's going to be in the same ballpark.You can reduce that by storing them in a more compact object. An array.array('I') or a numpy np.ndarray('I4') will only use 4 bytes per value instead of 36, so you can get to 2 billion (more than you have) before you run out of memory.Of course this will only work to store an array of integers, not an array of lists of integers, or an array of generators that yield lists of integers. If you really need lists of integers, you can do that with a 2D array in numpy, but not in array.array. If you really need generators of lists of integers, neither one works.But I think you can eliminate the top level entirely. What do you need new for?I need my results to be in a set of sets. I am converting them after this using skt = set(frozenset(temp) for temp in new)If the only thing you're ever doing with new is iterating over it once, you can just use an iterator instead of a list. The easiest way to do that is to change the list comprehension into a generator expression (that is, change those outer square brackets […] into parentheses (…)). Then you won't be using any memory, except for the memory for the current value and a bit of iterator state.Given that in the latest versions, you're just storing a bunch of generators, which can only each be iterated once, I can't imagine why you'd need to iterate the collection of them more than once or access them in random order.But if you can't do this for some reason, and can restructure things, just write the appropriate generator expression and pass it to the array.array constructor or the np.fromiter function. (If you want, array.array can be appended to just like a list, so you could write an explicit for statement, but I don't think you need to.)So, maybe this is what you want:Or maybe one of these:Or maybe the last one reshaped to be a 2D Nx1 array instead of a 1D array.


Answer URL
https://docs.python.org/3/library/array.html
