Natural Text
I have a python 3.4 script fetching multiple web pages. At first, I used requests library to fetch pages:Above code gives an average speed of 4.6 requests per second.To increase speed I rewrote function to use sockets library:And average speed fell to 4.04 requests per second. I was not hoping for drammatic speed boost, but was hoping for slight increase, as socket is more low level. Is this library issue or I'm doing something wrong?
requests uses urllib3, which handles HTTP connections very efficiently. Connections to the same server are re-used wherever possible, saving you the socket connection and teardown costs:Re-use the same socket connection for multiple requests, with optional client-side certificate verification. See: HTTPConnectionPool and HTTPSConnectionPoolIn addition, urllib3 and requests advertise to the server that they can handle compressed responses; with compression you can transfer more data in the same amount of time, leading to more requests per second.Supports gzip and deflate decoding. See: decode_gzip() and decode_deflate()urllib3 uses sockets too (albeit via the http.client module); there is little point in reinventing this wheel. Perhaps you should think about fetching URLs in parallel instead, using threading or multiprocessing, or eventlets; the requests author has a gevents-requests integration package that can help there.
The slowness is probably simply because you are doing HTTP wrong: You issue a HTTP/1.1 request and even explicitly specify connection keep-alive (not even needed because this is implicit with HTTP/1.1). But then you just read from the socket and expect the server to close the connection after the request is done. But the server will not do that, it will instead wait for more requests from you because of keep-alive and only close the connection after some time of inactivity, which depends on the server configuration. You are lucky to connect to a server with a very short timeout where you are still getting 4.04 requests per seconds, with  others servers it would be only few requests per minute with your code.If you want to make a simple HTTP request with a plain socket use HTTP/1.0 and do not use keep-alive. Then you could read just until the server closes and you also don't have to deal with chunked transfer encoding which was introduced with HTTP/1.1. You also don't have to deal with compressed encoding because you don't specifically accept them (but some broken servers will send them anyway).  But, while this will make your code faster than it is now, it will not be as fast as requests, because all this keep-alive, compression etc was added to improve speed. To re-implement all of this correctly is not that easy, so I recommend you stay with the requests library.


Answer URL
https://docs.python.org/3/library/http.client.html
