Natural Text
I have created a dictionary in python and dumped into pickle. Its size went to 300MB. Now, I want to load the same pickle.Loading this pickle takes around 15 seconds. How can I reduce this time?Hardware Specification: Ubuntu 14.04, 4GB RAMThe code bellow shows how much time takes to dump or load a file using json, pickle, cPickle.After dumping, file size would be around 300MB. Output :I have seen that cPickle takes less time to dump and load but loading a file still takes a long time. 
Try using the json library instead of pickle. This should be an option in your case because you're dealing with a dictionary which is a relatively simple object.According to this website,JSON is 25 times faster in reading (loads) and 15 times faster in  writing (dumps).Also see this question: What is faster - Loading a pickled dictionary object or Loading a JSON file - to a dictionary?Upgrading Python or using the marshal module with a fixed Python version also helps boost speed (code adapted from here):Results:Python 3.4 uses pickle protocol 3 as default, which gave no difference compared to protocol 4. Python 2 has protocol 2 as highest pickle protocol (selected if negative value is provided to dump), which is twice as slow as protocol 3.
I've had nice results in reading huge files (e.g: ~750 MB igraph object - a binary pickle file) using cPickle itself. This was achieved by simply wrapping up the pickle load call as mentioned hereExample snippet in your case would be something like:Surely, there might be more apt ways to get the task done, however, this workaround does reduce the time required drastically.(For me, it reduced from 843.04s to 41.28s, around 20x)
If you are trying to store the dictionary to a single file, it's the load time for the large file that is slowing you down.  One of the easiest things you can do is to write the dictionary to a directory on disk, with each dictionary entry being an individual file.  Then you can have the files pickled and unpickled in multiple threads (or using multiprocessing).  For a very large dictionary, this should be much faster than reading to and from a single file, regardless of the serializer you choose.  There are some packages like klepto and joblib that already do much (if not all of the above) for you.  I'd check those packages out.  (Note: I am the klepto author. See https://github.com/uqfoundation/klepto).


Answer URL
https://docs.python.org/3/library/pickle.html#data-stream-format
