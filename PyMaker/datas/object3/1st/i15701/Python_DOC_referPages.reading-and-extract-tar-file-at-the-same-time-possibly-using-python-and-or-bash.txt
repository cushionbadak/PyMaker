Natural Text
There is a very large tar file on the web (1.2 TB) which contains many many hi res images(possibly in subtared files) and some text files. I need all the images but only in a lower resolution, also I need the text files. But I don't have enough space to download the whole thing. Also the large tar file supports download resume.So I want to do a script that downloads just one part of the file, extracts the contained file and process it. Then do the next part and so on. Possibly python should be the easiest way, no? or maybe a bash script? how can I do this?
You can do this in python, but it's not straightforward. At all.You can use tarfile.open and supply the fileobj argument.You technically could supply it something straight out of urllib.urlopen. The main problem is that, since you're processing over a terabyte, the transfer will fail. As you said, you'll need to retry the transfer on demand. Your best bet is to to craft a file-like object that resiliently reads from a URL, handling disconnections and timeouts.Apparently, urllib3 will do this automatically, so you don't need to reinvent the wheel.Another problem is that (normal) tar files have no index. You can't really list the files inside without processing the whole tar first - so you'll need to extract them as they appear. There doesn't seem to be a built-in way to do this and regain flow control after each file is extracted (i.e.:a callback), so you'll have to write it yourself. Take a look into TarFile.extractall's source code to see how it's done (print inspect.getsource(tarfile.TarFile.extractall))
My own partial answer, so as to kick start Ideas, unfortunately it seems I am not proficient enough in python or bash to know the most elegant and straight forward way, but here is what I have found:Python has this tarmodule:https://docs.python.org/3/library/tarfile.html, and also there is this file resuming download script:http://code.activestate.com/recipes/83208-resuming-download-of-a-file/But I don't know how to stick them together. Also I can download and untar at the same time using bash, but how can I then do this recursively (remember there may be other tar files that we need to go into or text files that we have to process accordingly), also is this resumable?http://www.howtogeek.com/howto/uncategorized/linux-quicktip-downloading-and-un-tarring-in-one-step/One Idea is to use a Frankenstein of bash and python. That is to use curl and untar to get the files individually and then pass the file to my own script to process, then I can have all the checks in the script:curl can support resume:http://www.cyberciti.biz/faq/curl-command-resume-broken-download/But then we'd have the problem of: is tar resumable!


Answer URL
https://docs.python.org/3/library/tarfile.html
