Natural Text
I have just written a script that is intended to be run 24/7 to update some files. However, if it takes 3 minutes to update one file, then it would take 300 minutes to update 100 files. Is it possible to run n instances of the script to manage n separate files to speed up the turnaround time? 
Yes it is possible. Use the multiprocessing module to start several concurrent processes. This has the advantage that you do not run into problems because of the Global Interpreter Lock and threads as is explained in the manual page. The manual page includes all the examples you will need to make your script execute in parallel. Of course this works best if the processes do not have to interact, which your example suggests.
I suggest you first find out if there is any way to reduce the 3 minutes in a single thread.The method I use to discover speedup opportunities is demonstrated here.That will also tell you if you are purely I/O bound.If you are completely I/O bound, and all files are on a single disk, parallelism won't help.In that case, possibly storing the files on a solid-state drive would help.On the other hand, if you are CPU bound, parallelism will help, as @hochl said.Regardless, find the speedup opportunities and fix them.I've never seen any good-size program that didn't have one or several of them.That will give you one speedup factor, and parallelism will give you another, and the total speedup will be the product of those two factors.


Answer URL
