Natural Text
We've built a large python repo that uses lots of libraries (numpy, scipy, tensor flow, ...) And have managed these dependencies through a conda environment. Basically we have lots of developers contributing and anytime someone needs a new library for something they are working on they 'conda install' it. Fast forward to today and now we need to deploy some applications that use our repo. We are deploying using docker, but are finding that these images are really large and causing some issues, e.g. 10+ GB. However each individual application only uses a subset of all the dependencies in the environment.yml. Is there some easy strategy for dealing with this problem? In a sense I need to know the dependencies for each application, but I'm not sure how to do this in an automated way. Any help here would be great. I'm new to this whole AWS, Docker, and python deployment thing... We're really a bunch of engineers and scientists who need to scale up our software. We have something that works, it just seems like there has to be a better way üòÅ.
First see if there are easy wins to shrink the image, like using Alpine Linux and being very careful about what gets installed with the OS package manager, and ensuring you only allow installing dependencies or recommended items when truly required, and that you clean up and delete artifacts like package lists, big things you may not need like Java, etc.The base Anaconda/Ubuntu image is ~ 3.5GB in size, so it's not crazy that with a lot of extra installations of heavy third-party packages, you could get up to 10GB. In production image processing applications, I routinely worked with Docker images in the range of 3GB to 6GB, and those sizes were after we had heavily optimized the container.To your question about splitting dependencies, you should provide each different application with its own package definition, basically a setup.py script and some other details, including dependencies listed in some mix of requirements.txt for pip and/or environment.yaml for conda.If you have Project A in some folder / repo and Project B in another, you want people to easily be able to do something like pip install <GitHub URL to a version tag of Project A> or conda env create -f ProjectB_environment.yml or something, and voila, that application is installed.Then when you deploy a specific application, have some CI tool like Jenkins build the container for that application using a FROM line to start from your thin Alpine / whatever container, and only perform conda install or pip install for the dependency file for that project, and not all the others.This also has the benefit that multiple different projects can declare different version dependencies even among the same set of libraries. Maybe Project A is ready to upgrade to the latest and greatest pandas version, but Project B needs some refactoring before the team wants to test that upgrade. This way, when CI builds the container for Project B, it will have a Python dependency file with one set of versions, while in Project A's folder or repo of source code, it might have something different.
There are many ways to tackle this problem:Lean docker images - start with a very simple base image; and layer your images. See best practices for building images.Specify individual app requirements using requirements.txt files (make sure you pin your versions) and see specific instructions for conda. Build and install "on demand"; when you do docker build, only install those requirements for the specific applications and not one giant image for every possible eventuality.
I‚Äôd recommend:Make use of standard Python application packaging; your individual applications should be well-behaved Python packages that declare their own dependencies.Each application becomes its own Docker image and manages its own dependencies.Don‚Äôt try to maintain a giant all-the-dependencies base image.Developers don‚Äôt have to use Docker for local development if they don‚Äôt want to.Python has good built-in support for applications declaring their own dependencies, and for running an application in an environment with the specific library dependencies it needs.  Your developers should set up Python virtual environments for each project they‚Äôre working on (or if they prefer one shared one for their local development).  There are two paths for a Python application to declare its dependencies, and each project should declare its dependencies in its project-specific setup.py and/or requirements.txt files.All of this is totally independent of Docker.  Many people do use Docker as part of their core development flow, but if you maintain good practices around packages declaring their dependencies correctly, this both helps building containers and helps a purely local development process.When you get to deploying a Docker image, each application should have its own image.  The Dockerfile should be checked into source control, typically in the root directory of the project alongside the setup.py and/or requirements.txt files.  (It can be pretty boilerplate; the example in the Docker Hub python image documentation will actually work fine for most applications.)One giant base image with every conceivable dependency will cause problems.  Practically, I‚Äôve run into network errors with images and layers in the gigabyte+ range; longer-term, you‚Äôll run into trouble when two applications need incompatible versions of the same library.  I‚Äôd abandon this approach.You might need to use some of the common tricks like multi-stage builds if you‚Äôre doing things like compiling numpy from source.  Your runtime images don‚Äôt need full C and FORTRAN toolchains and you don‚Äôt need to carry around the intermediate build artifacts.  Build the package in a base layer (maybe into a wheel, maybe installing it) and copy it into your final layer.


Answer URL
https://docs.python.org/3/library/venv.html
