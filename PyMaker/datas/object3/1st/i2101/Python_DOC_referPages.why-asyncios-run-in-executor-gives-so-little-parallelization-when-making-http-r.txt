Natural Text
I've written a benchmark utility that batch queries a REST endpoint. It does it in three ways:sequentially, using the requests library,concurrently, using the requests library, but wrapping each request with loop.run_in_executor(),concurrently, using the aiohttp library.Below are the results with different batch sizes:batch_size=16batch_size=4As the results show, the aiohttp routine is consistently more parallel by a wide margin. What's more, with a small batch size (4), the 2nd approach of using loop.run_in_executor ("concur_times" column) achieves only a speed up of 1/9 v. the sequential method.Why is that? Is my code at fault? I'm including it below.I've tried swapping out the network IO for sleep and asyncio.sleep and that produced expected results of methods 2 and 3 being equally fast and method 1 being batch_size times slower.Code:The benchmark is run like this: bench(n_iters=50,batch_size=4). Then pass the output through lambda output: pandas.DataFrame(output).describe() to produce the tables.
The default executor for asyncio's run_in_executor is ThreadPoolExecutor, which uses Python threads. So it is also affected by the GIL, as described in this thread.In your case only one thread with asynchronous job runs at a time, resulting aiohttp to show better performance.


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
