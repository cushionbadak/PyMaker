Natural Text
I've been trying to write an interactive wrapper (for use in ipython) for a library that controls some hardware.  Some calls are heavy on the IO so it makes sense to carry out the tasks in parallel. Using a ThreadPool (almost) works nicely:The problem comes if the user wants to stop the process or there is an IO error in do_other_stuff_for_a_bit(). Pressing Ctrl+C stops the main process but the worker threads carry on running until their current task is complete.Is there some way to stop these threads without having to rewrite the library or have the user exit python?  pool.terminate() and pool.join() that I have seen used in other examples do not seem to do the job.The actual routine (instead of the simplified version above) uses logging and although all the worker threads are shut down at some point, I can see the processes that they started running carry on until complete (and being hardware I can see their effect by looking across the room).This is in python 2.7. UPDATE:The solution seems to be to switch to using multiprocessing.Process instead of a thread pool.  The test code I tried is to run foo_pulse:If you try running this using ThreadPool then ctrl-C does not stop foo_pulse from running (even though it does kill the threads right away, the print statements keep on coming:However a version using multiprocessing.Process works as expected:Where I have defined a wrapper for the library foo (so that it did not need to be re-written).  If the return value is not needed the neither is this wrapper :From the documentation I see no reason why a pool would not work (other than a bug).
This is a very interesting use of parallelism. However, if you are using multiprocessing, the goal is to have many processes running in parallel, as opposed to one process running many threads. Consider these few changes to implement it using multiprocessing:You have these functions that will run in parallel:Let's create and start the processes, say 4:The processes are running in parallel, presumably in a separate cpu core, but that is to the OS to decide. You can check in your system monitor.In the meantime you run a process that will break, and you want to stop the running processes, not leaving them orphan:If it doesn't make sense to continue with the main process when one or all of the subprocesses have terminated, you should handle the exit of the main program.Hope it helps, and you can adapt bits of this for your library.
In answer to the question of why pool did not work then this is due to (as quoted in the Documentation) then main needs to be importable by the child processes and due to the nature of this project interactive python is being used.  At the same time it was not clear why ThreadPool would - although the clue is right there in the name.  ThreadPool creates its pool of worker processes using multiprocessing.dummy which as noted here is just a wrapper around the Threading module.  Pool uses the multiprocessing.Process.  This can be seen by this test:As threads do not have a terminate method the worker threads carry on running until they have completed their current task.  Killing threads is messy (which is why I tried to use the multiprocessing module) but solutions are here.The one warning about the solution using the above:is that changes to attributes inside the instance of the object are not passed back up to the main program.  As an example the class foo above can also have methods such as: def addIP(newIP):self.hardwareIP=newIPA call to r=mp.Process(target=a.addIP,args=(127.0.0.1)) does not update a.The only way round this for a complex object seems to be shared memory using a custom manager which can give access to both the methods and attributes of object a For a very large complex object based on a library this may be best done using dir(foo) to populate the manager.  If I can figure out how I'll update this answer with an example (for my future self as much as others).


Answer URL
https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy
