Natural Text
I have problem with displaying national characters from “ENGLISH_UNITED KINGDOM.US7ASCII” Oracle 11 database using Python 3.3 cx_Oracle 5.1.2 and "NLS_LANG" environment variable.Db table column type is "VARCHAR2(2000 BYTE)"How to display string "£aÀÁÂÃÄÅÆÇÈ" from Oracle US7ASCII in Python? This will be some sort of hack.The hank works in every other scripting language Perl, PHP, PL/SQL and in Python 2.7, but it does not work in Python 3.3.In Oracle 11 Database I created SECURITY_HINTS.ANSWER="£aÀÁÂÃÄÅÆÇÈ". ANSWER column type is "VARCHAR2(2000 BYTE)".Now when using cx_Oracle and default NLS_LANG, I get "¿a¿¿¿¿¿¿¿¿¿"and when using NLS_LANG="ENGLISH_UNITED KINGDOM.US7ASCII" I getUpdate1I made some progress. When switching to Python 2.7 and cx_Oracle 5.1.2 for Python 2.7 the problem goes away (I get all >127 characters from db). In Python 2 strings are represented as bytes and in Python 3+ strings are represented as unicode. I still need best possible solution for Python 3.3. Update2One possible solution to the problem is to used rawtohex(utl_raw.cast_to_raw see code below.source code of my script is below or at GitHub  and GitHub Sollution see log output below.I am trying to Display it in Django Web page. But each character comes as character with code 191 or 65533.I looked atchoosing NLS_LANG for Oracle and Importing from Oracle using the correct encoding with PythonCannot Insert Unicode Using cx-Oracle
If you want to get unchanged ASCII string in client application, the best way is transfer it from DB in binary mode. So, first conversion must be down on server side with help of UTL_RAW package and standard rawtohex function. Your select in cursor.execute may look like that:On the client you got a string of hexadecimal characters which may be converted to a string representation with help of binascii.unhexlify function:P.S. I didn't know a Python language, so last statement may be incorrect.   
I think you should not revert to such evil trickery. NLS_LANG should simply be set to the client's default encoding. Look at more solid options:Extend the character set of the database to allow these characters in a VARCHAR column.Upgrade this particular column to NVARCHAR. You could perhaps use a new name for this column and create a VARCHAR computed column with the old name for the legacy applications to read.Keep the database as is but check the data when it gets entered and replace all non-ASCII characters with an acceptable ASCII equivalent. Which option is best depends on how common the non-ASCII characters are. If there's more tables with the same issue, I'd suggest option 1. If this is the only table, option 2. If there are only a couple non-ASCII characters in the entire table, and their loss is not that big a deal: option 3.One of the tasks of a database is to preserve the quality of your data after all, and if you cheat when forcibly inserting illegal characters into the column, it cannot do its job properly and each new client or upgrade or export will come with interesting new undefined behavior.EDIT: See Oracle's comment on an example of a similar setup in the NLS_LANG faq (my emphasis):A database is created on a UNIX system with the US7ASCII character  set. A Windows client connecting to the database works with the  WE8MSWIN1252 character set (regional settings -> Western Europe /ACP  1252) and the DBA, use the UNIX shell (ROMAN8) to work on the  database. The NLS_LANG is set to american_america.US7ASCII on the  clients and the server.Note:This is an INCORRECT setup to explain character set conversion, don't  use it in your environment!


Answer URL
