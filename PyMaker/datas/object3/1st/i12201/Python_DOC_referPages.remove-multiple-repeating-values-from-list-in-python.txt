Natural Text
I am processing large (~5000 lines) text files which have been generated by reporting software. These files have multiple header lines per page and many blank lines throughout. I have figured out a method for filtering out the data I don't need, but I am wondering if this is the best way to do this. I have this function that I use to filter the list, it is basically iterating over the list and reducing it by removing one of the filter lines each time. I feel like I'm doing more passes than necessary. Is there a better way to get this filtering done?
You can definitely do it in one go.
str.startswith() method accepts a tuple of prefix. So instead of multiple loops you can use one list comprehension and pass all the patters to one startswith() method.And as a more pythonic way you can use following generator function to return an iterator filtered object from your file:If you are not consider about memory use, as a more faster way you can use a list comprehension to filter your file object out.
You might find these methods useful:Given: We can subtract b from a as follows:which produces:or we can remove duplicates as follows:which produces:Note that in the latter method, the order is lost. If you wish to preserve the order, use collections.OrderedDict from Python's native library. Now it's just the matter of splitting your strings and manipulating them. 
Put your patterns in a list and then you can veto any given line withTo process the entire file, use filter to build an iterator
str.startswith can accept a tuple instead of a string:


Answer URL
https://docs.python.org/3/library/stdtypes.html#str.startswith
