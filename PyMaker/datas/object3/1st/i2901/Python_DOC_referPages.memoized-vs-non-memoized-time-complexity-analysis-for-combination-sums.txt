Natural Text
I'm trying to understand why using lru_cache to solve this problem yields a slower performance of the code. The question is essentially to return all combinations that add up to a certain target.I'm using the lru_cache decorator to do the memoization (docs), and this is my solution:It seems like when the lru_cache decorator is commented out, I get almost a 50% increase in the runtime speed of this algorithm. This seems a little counter intuitive as I thought the time complexity of the solution should be reduced, even with the increased overhead of function calls to retrieve results from the memoization.For the memoized solution, I believe the time complexity should be O(n^2*k*2^n) where n is the length of the array, and k being all numbers in the range from 0 to target. This is my analysis (need a little help verifying):I'm also missing some gaps in my knowledge on how to analyze the time complexity of the recursive solution, I could use some help in doing so!EDIT:I'm using range(1, 10000) as a test input, here are the benchmarks:
You didn't give both arguments, and they're both important.  I can make either version much faster than the other by picking specific pairs.  If you're passing range(1, 10000) as candidates, then each cache lookup has to (among other things) do 9999 comparisons just to determine that the candidates are always the same - and that's huge overhead.  Try, e.g.,for a case where the cached version is much faster.  After which:"Analysis" is futile if you don't account for the expense of doing a cache lookup, and you're apparently trying cases where cache lookup is extremely expensive.  Dict lookup is expected-case O(1), but the hidden constant factor can be arbitrarily large depending on how expensive equality-testing is (and for a key involving an N-element tuple, establishing equality requires at least N comparisons).Which should suggest a major improvement:  keep candidates out of the argument list.  It's invariant so there's really no need to pass it.  Then the cache just needs to store fast-to-compare (i, target) pairs.EDIT: PRACTICAL CHANGESHere's another version of the code that doesn't pass in candidates.  Forit's faster by at least a factor of 50 on my box.  There's one other material change:  don't do a recursive call when target is reduced below zero.  An enormous number of cache entries were recording empty-list results for (j, negative_integer) arguments.  This change reduced the final cache size from 449956 to 1036 in the case above - and cut the number of hits from 9444864 to 6853.
Try running the following on your resultyou should get a result of something like thisbecause your function parameters are very long so they don't match often with the cached values, I blame the target parameter here, restructuring the program might improve the hits drastically.


Answer URL
https://docs.python.org/3/library/functools.html
