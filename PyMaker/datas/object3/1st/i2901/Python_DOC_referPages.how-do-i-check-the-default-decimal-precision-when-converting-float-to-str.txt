Natural Text
When converting a float to a str, I can specify the number of decimal points I want to displayBut when simply calling str on a float in python 2.7, it seems to default to 12 decimal points maxWhere is this max # of decimal points defined/documented? Can I programmatically get this number?
The number of decimals displayed is going to vary greatly, and there won't be a way to predict how many will be displayed in pure Python.  Some libraries like numpy allow you to set precision of output.This is simply because of the limitations of float representation.The relevant parts of the link talk about how Python chooses to display floats.Python only prints a decimal approximation to the true decimal value of the binary approximation stored by the machinePython keeps the number of digits manageable by displaying a rounded value insteadNow, there is the possibility of overlap here:Interestingly, there are many different decimal numbers that share the same nearest approximate binary fractionThe method for choosing which decimal values to display was changed in Python 3.1 (But the last sentence implies this might be an implementation detail).For example, the numbers 0.1 and 0.10000000000000001 are both  approximated by 3602879701896397 / 2 ** 55. Since all of these decimal  values share the same approximation, any one of them could be  displayed while still preserving the invariant eval(repr(x)) == xHistorically, the Python prompt and built-in repr() function would  choose the one with 17 significant digits, 0.10000000000000001.  Starting with Python 3.1, Python (on most systems) is now able to  choose the shortest of these and simply display 0.1.
I do not believe this exists in the python language spec. However, the cpython implementation does specify it. The float_repr() function, which turns a float into a string, eventually calls a helper function with the 'r' formatter, which eventually calls a utility function that hardcodes the format to what comes down to format(float, '.16g'). That code can be seen here. Note that this is for python3.6.giving the maximum number of signification digits (both before and after the decimal) at 16. It appears that in the python2.7 implementation, this value was hardcoded to .12g. As for why this happened (and is somewhat lacking documentation, can be found here.)So if you are trying to get how long a number will be formatted when printed, simply get it's length with .12g.
Well, if you're looking for a pure python way of accomplishing this, you could always use something like,I couldn't find it in the documentation and will add it here if I do, but this is a work around that can at least always return the length of precision if you want to know before hand. As you said, it always seems to be 12 even when feeding bigger floating-points.From what I was able to find, this number can be highly variable and in these cases, finding it empirically seems to be the most reliable way of doing it. So, what I would do is define a simple method like this,This will return you the maximum length representation on your current system,
By looking at the output of random numbers converted, I have been unable to understand how the length of the str() is determined, e.g. under Python 3.6.6:You may opt for this code that actually simulates your real situation:Here we are testing the length of ~90 random numbers in the (.1,1) open interval after conversion (and deducing the 0. from the left, hence the -2).Python 2.7.5 on a 64bit linux gives me 12, and Python 3.4.8 and 3.6.6 give me 17.


Answer URL
https://docs.python.org/3/tutorial/floatingpoint.html
