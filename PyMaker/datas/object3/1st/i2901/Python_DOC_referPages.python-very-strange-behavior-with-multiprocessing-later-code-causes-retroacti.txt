Natural Text
I'm trying to learn how to implement multiprocessing for computing Monte Carlo simulations. I reproduced the code from this simple tutorial where the aim is to compute an integral. I also compare it to the answer from WolframAlpha and compute the error. The first part of my code has no problems and is just there to define the integral function and declare some constants:But there's some very spooky stuff that happens in the next two parts (I've labelled them because it's important). First (labelled "BLOCK 1"), I do the simulation without any multiprocessing at all, just to get a benchmark. After this (labelled "BLOCK 2"), I do the same thing but with a multiprocessing step. If you're reproducing this, you may want to adjust the num_procs variable depending on how many cores your machines has:The output is:So, the multiprocessing is slower. That's not at all unheard of; maybe the overhead from the multiprocessing is just more than the gains from the parallelization?But, that is not what is happening. Watch what happens when I merely comment out the first block:The output is:That's right -- the time to complete the multiprocessing goes down from 55 seconds to less than 7 seconds! And that's not even the weirdest part. Watch what happens when I move Block 1 to be after Block 2:The output is:We're back to the slow output again, which is completely crazy! Isn't Python supposed to be interpreted? I know that statement comes with a hundred caveats, but I took for granted that the code gets executed line-by-line, so stuff that comes afterwards (outside of functions, classes, etc) can't affect the stuff from before, because it hasn't been "looked at" yet.So, how can the stuff that gets executed after the multiprocessing step has concluded, retroactively slow down the multiprocessing code?Finally, the fast behavior is restored merely by indenting Block 1 to be inside the if __name__ == "__main__" block, because of course it does:The output is:And the fast behavior is also restored if you keep Block 1 inside the if block, but move it to above where num_procs is defined (not shown here because this question is already getting long).So, what on Earth is causing this behavior? I'm guessing it's some kind of race-condition to do with threading and process branching, but from my level of expertise it might as well be that my Python interpreter is haunted.
This is because you are using Windows. On Windows, each subprocess is generated using the 'spawn' method which essentially starts a new python interpreter and imports your module instead of forking the process. This is a problem, because all the code outside if __name__ == '__main__' is executed again. This can lead to a multiprocessing bomb if you put the multiprocessing code at the top-level, because it will start spawning processes until you run out of memory.This is actually warned about in the docsSafe importing of main moduleMake sure that the main module can be safely imported by a new Python  interpreter without causing unintended side effects (such a starting a  new process)....Instead one should protect the “entry point” of the program by using  if __name__ == '__main__'...This allows the newly spawned Python interpreter to safely import the  module...That section used to be called "Windows" in the older docs on Python 2.
Adding some detail, on Windows the module is imported "from scratch" in each worker process.  That means everything in the module is executed by each worker.  So, in your first example, each worker process first executes "BLOCK 1".But your output doesn't reflect that.  You should have gotten a line of output like1000000000 iterations on single-thread: 37.448 seconds.from each of your 8 worker processes.  But your output doesn't show that.  Perhaps you're using an IDE that suppresses output from spawned processes?  If you run it in a "DOS box" (cmd.exe window) instead, that won't suppress output, and can make what's going on clearer.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
