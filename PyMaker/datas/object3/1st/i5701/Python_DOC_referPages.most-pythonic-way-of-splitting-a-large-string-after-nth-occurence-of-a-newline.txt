Natural Text
I have a very large multiline string, and would like to split it into an array, let's say, after each 50th occurrence of the newline character (\n)What is, in your opinion, the most Pythonic and efficient way to do so?
A couple approaches that avoid storing all the lines as separate strings in memory (which any use of split/splitlines would require, involving quite a bit of overhead thanks to the fairly high per-object overhead for each string) would be to use a file-like object wrapper to get lines one-by-one, and itertools.islice (or a clever zip based trick) to batch them up.Equivalently, though a bit more obscurely, you could use zip_longest:The second approach appears to be the faster of the two. Comparing them to a non-memory sensitive approach that splitlines and joins slices of the result:the zip_longest approach and the splitlines based approach have roughly identical timings for me (for large numbers of short lines), while the islice approach takes about 40% longer. For some inputs, the zip_longest approach is slower than splitlines, though if the data has huge numbers of lines (enough to cause memory pressure when you make millions of individual strs from the lines up front), you'll gain more from reduced memory than it costs you in CPU. On my Python (64 bit Python 3.6.1 on Windows), the per str overhead (ignoring the cost of storing the actual data) for ASCII str is 49 bytes (it goes up for non-ASCII). So if data is 1 GB of data, comprising 10 million lines, holding the split lines in memory simultaneously would cost you another 1 GB for the split up data, plus another ~470 MB for the object headers associated with each str (add in the cost of the list to store them, and you're a little over 540 MB of extra overhead). If your system has 3 GB of RAM, and the OS and other applications are using 800 MB of it, the cost of that extra 540 MB will be paid in agonizing slowdowns from page thrashing. If your data is smaller than that, sure, go for the simple approach, but if you might approach system memory limits, you may want to use a lazier approach to line splitting.
you could use split and join every i:i+n lines. Not sure if it is the most pythonic way.results in 
Here's one way that should be quite efficient. I can't claim it's Pythonic, if it was it would probably be shorter. But it does use yield.
It's probably not the most pythonic, but here's my stab at the matter:It works by splitting the string into each line, then grabbing them in groups of 50. I've been told that using sum on strings is a very bad idea, so here's another take:
This is a short, pythonic version:It does not do stream processing of the string, like @ShadowRanger but it's simpler and still contained in 2 lines (without imports). It returns an iterator as well. The solution comes from the recipe for the grouper function available at https://docs.python.org/3/library/itertools.html#itertools.islice
Recursive Algorithm for Generator on Stream InputTake a recursive approach:which gives the indices of the every n-th cuts, then split around the cuts, like so:This approach gives a generator that also works well with s coming in as stream, since the algorithm does not require the whole string s, but only the head of it. Very gentle on RAM.You call the function withPretty Pythonic, at least I think so;/


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.islice
