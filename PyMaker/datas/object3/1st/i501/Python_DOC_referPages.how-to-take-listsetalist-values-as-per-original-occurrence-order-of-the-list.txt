Natural Text
Is there a built-in that removes duplicates from list in Python, whilst preserving order? I know that I can use a set to remove duplicates, but that destroys the original order. I also know that I can roll my own like this:(Thanks to unwind for that code sample.)But I'd like to avail myself of a built-in or a more Pythonic idiom if possible.Related question: In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique while preserving order?
Here you have some alternatives: http://www.peterbe.com/plog/uniqifiers-benchmarkFastest one:Why assign seen.add to seen_add instead of just calling seen.add? Python is a dynamic language, and resolving seen.add each iteration is more costly than resolving a local variable. seen.add could have changed between iterations, and the runtime isn't smart enough to rule that out. To play it safe, it has to check the object each time.If you plan on using this function a lot on the same dataset, perhaps you would be better off with an ordered set: http://code.activestate.com/recipes/528878/O(1) insertion, deletion and member-check per operation.(Small additional note: seen.add() always returns None, so the or above is there only as a way to attempt a set update, and not as an integral part of the logical test.)
Edit 2016As Raymond pointed out, in python 3.5+ where OrderedDict is implemented in C, the list comprehension approach will be slower than OrderedDict (unless you actually need the list at the end - and even then, only if the input is very short). So the best solution for 3.5+ is OrderedDict.Important Edit 2015As @abarnert notes, the more_itertools library (pip install more_itertools) contains a unique_everseen function that is built to solve this problem without any unreadable (not seen.add) mutations in list comprehensions. This is also the fastest solution too:Just one simple library import and no hacks.This comes from an implementation of the itertools recipe unique_everseen which looks like:In Python 2.7+ the accepted common idiom (which works but isn't optimized for speed, I would now use unique_everseen) for this uses collections.OrderedDict:Runtime: O(N)This looks much nicer than:and doesn't utilize the ugly hack:which relies on the fact that set.add is an in-place method that always returns None so not None evaluates to True.Note however that the hack solution is faster in raw speed though it has the same runtime complexity O(N).
In Python 2.7, the new way of removing duplicates from an iterable while keeping it in the original order is:In Python 3.5, the OrderedDict has a C implementation. My timings show that this is now both the fastest and shortest of the various approaches for Python 3.5.In Python 3.6, the regular dict became both ordered and compact.  (This feature is holds for CPython and PyPy but may not present in other implementations).  That gives us a new fastest way of deduping while retaining order:In Python 3.7, the regular dict is guaranteed to both ordered across all implementations.  So, the shortest and fastest solution is:Response to @max:  Once you move to 3.6 or 3.7 and use the regular dict instead of OrderedDict, you can't really beat the performance in any other way.  The dictionary is dense and readily converts to a list with almost no overhead.  The target list is pre-sized to len(d) which saves all the resizes that occur in a list comprehension.  Also, since the internal key list is dense, copying the pointers is about almost fast as a list copy.
unique â†’ ['1', '2', '3', '6', '4', '5']
The list doesn't even have to be sorted, the sufficient condition is that equal values are grouped together.Edit: I assumed that "preserving order" implies that the list is actually ordered. If this is not the case, then the solution from MizardX is the right one.Community edit: This is however the most elegant way to "compress duplicate consecutive elements into a single element".
I think if you wanna maintain the order,you can try this:OR similarly you can do this:You can also do this:It can also be written as this:
Not to kick a dead horse (this question is very old and already has lots of good answers), but here is a solution using pandas that is quite fast in many circumstances and is dead simple to use.  
For another very late answer to another very old question:The itertools recipes have a function that does this, using the seen set technique, but:Handles a standard key function.Uses no unseemly hacks.Optimizes the loop by pre-binding seen.add instead of looking it up N times. (f7 also does this, but some versions don't.)Optimizes the loop by using ifilterfalse, so you only have to loop over the unique elements in Python, instead of all of them. (You still iterate over all of them inside ifilterfalse, of course, but that's in C, and much faster.)Is it actually faster than f7? It depends on your data, so you'll have to test it and see. If you want a list in the end, f7 uses a listcomp, and there's no way to do that here. (You can directly append instead of yielding, or you can feed the generator into the list function, but neither one can be as fast as the LIST_APPEND inside a listcomp.) At any rate, usually, squeezing out a few microseconds is not going to be as important as having an easily-understandable, reusable, already-written function that doesn't require DSU when you want to decorate.As with all of the recipes, it's also available in more-iterools.If you just want the no-key case, you can simplify it as:
Just to add another (very performant) implementation of such a functionality from an external module1: iteration_utilities.unique_everseen:TimingsI did some timings (Python 3.6) and these show that it's faster than all other alternatives I tested, including OrderedDict.fromkeys, f7 and more_itertools.unique_everseen:And just to make sure I also did a test with more duplicates just to check if it makes a difference:And one containing only one value:In all of these cases the iteration_utilities.unique_everseen function is the fastest (on my computer).This iteration_utilities.unique_everseen function can also handle unhashable values in the input (however with an O(n*n) performance instead of the O(n) performance when the values are hashable).1 Disclaimer: I'm the author of that package.
In Python 3.7 and above, dictionaries are guaranteed to remember their key insertion order. The answer to this question summarizes the current state of affairs.The OrderedDict solution thus becomes obsolete and without any import statements we can simply issue:
For no hashable types (e.g. list of lists), based on MizardX's:
Borrowing the recursive idea used in definining Haskell's nub function for lists, this would be a recursive approach:e.g.:I tried it for growing data sizes and saw sub-linear time-complexity (not definitive, but suggests this should be fine for normal data).I also think it's interesting that this could be readily generalized to uniqueness by other operations. Like this:For example, you could pass in a function that uses the notion of rounding to the same integer as if it was "equality" for uniqueness purposes, like this:then unique(some_list, test_round) would provide the unique elements of the list where uniqueness no longer meant traditional equality (which is implied by using any sort of set-based or dict-key-based approach to this problem) but instead meant to take only the first element that rounds to K for each possible integer K that the elements might round to, e.g.:
5 x faster reduce variant but more sophisticatedExplanation:
MizardX's answer gives a good collection of multiple approaches.This is what I came up with while thinking aloud:
You can reference a list comprehension as it is being built by the symbol '_[1]'. For example, the following function unique-ifies a list of elements without changing their order by referencing its list comprehension.Demo:Output:
You could do a sort of ugly list comprehension hack.
Relatively effective approach with _sorted_ a numpy arrays:Outputs:
A generator expression that uses the O(1) look up of a set to determine whether or not to include an element in the new list.
A simple recursive solution:
If you need one liner then maybe this would help:... should work but correct me if i'm wrong
If you routinely use pandas, and aesthetics is preferred over performance, then consider the built-in function pandas.Series.drop_duplicates:Timing: 
this will preserve order and run in O(n) time. basically the idea is to create a hole wherever there is a duplicate found and sink it down to the bottom. makes use of a read and write pointer. whenever a duplicate is found only the read pointer advances and write pointer stays on the duplicate entry to overwrite it.
A solution without using imported modules or sets:Gives output:
An in-place methodThis method is quadratic, because we have a linear lookup into the list for every element of the list (to that we have to add the cost of rearranging the list because of the del s).That said, it is possible to operate in place if we start from the end of the list and proceed toward the origin removing each term that is present in the sub-list at its leftThis idea in code is simplyA simple test of the implementation


Answer URL
https://docs.python.org/3/library/itertools.html#itertools-recipes
