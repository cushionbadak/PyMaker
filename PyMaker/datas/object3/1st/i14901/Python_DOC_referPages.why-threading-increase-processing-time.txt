Natural Text
I was working on multitasking a basic 2-D DLA simulation. Diffusion Limited Aggregation (DLA) is when you have particles performing a random walk and aggregate when they touch the current aggregate.In the simulation, I have 10.000 particles walking to a random direction at each step. I use a pool of worker and a queue to feed them. I feed them with a list of particles and the worker perform the method .updatePositionAndggregate() on each particle.If I have one worker, I feed it with a list of 10.000 particles, if I have two workers, i feed them with a list of 5.000 particles each, if I have 3 workers, I feed them with a list of 3.333 particles each, etc and etc.I show you some code for the worker nowAnd in the main thread:So, I print the average time in waiting the workers to terminate their tasks. I did some experiment with different thread number.I really wonder why adding workers increase the processing time, I thought that at least having 2 worker would decrease the processing time, but it dramatically increases from .14s. to 0.23s. Can you explain me why ?EDIT:So, explanation is Python threading implementation, is there a way so I can have real multitasking ?  
This is happening because threads don't execute at the same time as Python can execute only one thread at a time due to GIL (Global Interpreter Lock). When you spawn a new thread, everything freezes except this thread. When it stops the other one is executed. Spawning threads needs lots of time.Friendly speaking, the code doesn't matter at all as any code using 100 threads is SLOWER than code using 10 threads in Python (if more threads means more efficiency and more speed, which is not always true).Here's an exact quote from the Python docs:CPython implementation detail: In CPython, due to the Global Interpreter Lock, only one thread can execute Python code at once (even though certain performance-oriented libraries might overcome this limitation). If you want your application to make better use of the computational resources of multi-core machines, you are advised to use multiprocessing or concurrent.futures.ProcessPoolExecutor. However, threading is still an appropriate model if you want to run multiple I/O-bound tasks simultaneously.Wikipedia about GILStackOverflow about GIL
Threads in python (at least in 2.7) are NOT executed simultaneously because of GIL: https://wiki.python.org/moin/GlobalInterpreterLock - they run in single process and share CPU, therefore you can't use threads for speeding your computation up.If you want to use parallel computation to speed up your calculation (at least in python2.7), use processes - package multiprocessing.
This is due to Python's global interpreter lock. Unfortunately, with the GIL in Python threads will block I/O and as such will never exceed usage of 1 CPU core. Have a look here to get you started on understanding the GIL:  https://wiki.python.org/moin/GlobalInterpreterLockCheck your running processes (Task Manager in Windows, for example) and will notice that only one core is being utilized by your Python application.I would suggest looking at multi-processing in Python, which is not hindered by the GIL: https://docs.python.org/2/library/multiprocessing.html
It takes time to actually create the other thread and start processing it. Since we don't have control of the scheduler, I'm willing to bet both of these threads get scheduled on the same core (since the work is so small), therefore you are adding the time it takes to create the thread and no parallel processing is done


Answer URL
https://docs.python.org/3/library/threading.html
