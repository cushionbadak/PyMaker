Natural Text
I am using DASK to read CSV file which sizes around 2GB.I want to write each row of it to separate 255 number CSV files based on some hash function as below.My naive solution:This way takes around 15 minutes. Is there any way we can do it faster.
Since your procedure is dominated by IO, it is very unlikely that Dask would do anything but add overhead in this case, unless your hash function is really really slow. I assume that is not the case.@zwer 's solution would look something likeHowever, your data appears to fit in memory, so you may find much better performancebecause you write to each file continuously rather than jumping between them. Depending on your IO device and buffering, the difference can be none or huge.


Answer URL
https://docs.python.org/3/library/csv.html
