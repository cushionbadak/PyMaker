Natural Text
How can I reduce the memory footprint for a dictionary containing 4M+ objects with strings?It currently consumes about 1.5 GBytes of RAM, and I need to add several million more objects to it on systems that have limited resources due to prohibitive cost (cloud-based).Here's some simplified code illustrating the gist of what I'm doing. Basically I fetch a set of about 4 million users from a database and put all the info into a local dict with all the users for quick access (I must work with a local copy of the user data for performance reasons).Simplified CodeData Typesuser_id: intname: string, each one averaging maybe about 13 characterstype: intFrom some web searching, it seems to me User.name is consuming the majority of the space due to the large amount of memory required for string objects.I already decreased the footprint from about 2GB down to 1.5GB by using __slots__, but I need to reduce it further.
If you really need the data locally, consider saving it to a SQLite DB on the host, and letting SQLite load the hot dataset into memory for you, instead of keeping all of it in memory.If you really need all that data in memory, consider configuring swap space on the host as a cheaper alternative. The OS will swap colder memory pages to this swap space.Of course, you can always compress your strings using gzip, if name is a large string. Other tricks include deduplication with an index, if there are repeated words in your names.You can also use structs instead of classes.If you know that your user IDs are contiguous, and you are using fixed length structs, you can also lookup simple array by counting byte offsets, instead of using the dict. (Numpy arrays would be useful here.)For something closer to production quality, you will want to have a data prep step that appends these structs to a file, which can later be read when you are actually using the dataHowever, I am not convinced that this is the best design for the problem you actually have. Other alternatives include:inspecting your db design, especially your indexesusing memcache/redis to cache the most frequently used records
A 13-character string's actual string storage takes only 13 bytes if it's all Latin-1, 26 bytes if it's all BMP, 52 bytes if it's got characters from all over Unicode.However, the overhead for a str object is another 52 bytes. So, assuming you've got mostly Latin-1, you're using about 5x as much storage as you need.If your strings are, once encoded to UTF-8 or UTF-16-LE or whatever is best for your data, all around the same size, you probably want to store them in a big flat array and pull them out and decode them on the fly as needed, as shown in James Lim's answer. Although I'd probably use a NumPy native structured dtype rather than use the struct module.But what if you have a few huge strings, and you don't want to waste 88 bytes for each one when most of them are only 10 bytes long?Then you want a string table. This is just a giant bytearray where all the (encoded) strings live, and you store indexes into that table instead of storing the strings themselves. Those indexes are just int32 or at worst int64 values that you can pack into an array with no problems.For example, assuming none of your strings are more than 255 characters, we can store them as "Pascal strings", with a length byte followed by the encoded bytes:So now:Except, of course, you probably still want to replace that all_users with a numpy array.
Instead of using cursor.fetchall(), storing all the data in the client side, you should use an SSCursor to leave the result set on the server side:so that you can fetch the rows one by one:And depending on what you want to do with the all_users dict, you may not need to store all user info in a dict either. If you can process each user one by one, do it directly inside the for loop above instead of building up a huge dict.
Do you actually need this cached in memory, or just on the local system?If the latter, just use a local database.Since you just want something that acts like a dict, you just want a key-value database. The simplest KV database is a dbm, which Python supports out of the box. Using a dbm from Python looks exactly like using a dict, except that the data are on disk instead of in memory.Unfortunately, dbm has two problems, but they're both solvable:Depending on the underlying implementation, a huge database might either not work, or go very slowly. You can use a modern variant like KyotoCabinet to solve that, but you'll need a third-party wrapper.dbm keys and values can only be bytes. Python's dbm module wraps things up to allow storing Unicode strings transparently, but nothing else. But Python comes with another module, shelve, which lets you transparently store any kind of value that can be pickled in a dbm.But you might want to instead use a more powerful key-value database like Dynamo or Couchbase.In fact, you might even be able to get away with just using a KV database like Redis or Memcached purely in-memory, because they'll store the same data you're storing a lot more compactly.Alternatively, you could just dump the data from the remote MySQL into a local MySQL, or even a local SQLite (and optionally throw an ORM in front of it).


Answer URL
https://docs.python.org/3/library/dbm.html
https://docs.python.org/3/library/shelve.html
