Natural Text
The following reproducible code produces an example data set that mimics my data on a much smaller scale. The following reproducible code returns the results I want, but is very inefficient for large data sets.How would I do this more efficiently?I have tried np.apply_along_axis(), np.where(), df.apply(), and df.transform(), but can't get any of them to work for this case without errors.
If this is the df:then your sel is:That is a list of arrays of differing sizes.The df columns as arrays are:both are object arrays of arrays.  But we can convert them to 2d arrays with stack (or vstack):We can simply index with the boolean mask - but the result is a 1d array:where on idx produces the the equivalent tuple of indexing arrays:We can also produce a masked array from these arrays:But to get the row by row values, we have to do some sort of iteration:And for this, the object arrays from In[7] and In[8] are just as good, if not better than the stacked 2d arrays.And your range/append loop is nearly as good (if not better).The fact that your sel arrays vary in size (or at least in theory can vary), is a pretty good indication that 'vectorized', whole-array, operations are not possible.  But do you need such a list?  If you can't generate it with a fast array operation, you can't use it with one either. Both in creating and using you'll have to iterate on 'rows'.
The premise is bad because you shouldn't store data like this. You can at least speed this up by joining your data with itertools.chain, indexing, and then splitting the result with np.array_split.
Using a list comprehension and numpy indexing:
You shouldn't really use Pandas series to store lists. However, if this is unavoidable, you can use itertools.compress with map, feeding df['vals'] and df['idx'] as separate arguments:If your df['vals'] series is genuinely a NumPy array, you can use NumPy indexing:
Apply should work fine if you unpack it a bit into a function. As for any speed increases, please report back on your use case/data as it might be rather costly to call the function over and over again:
Thank you everyone for the answers.Below is what I came up with.  I have not (yet) compared timings with the other solutions.I think this is better (though I have not tested) than the for loop I first presented in the OP, because this lambda function should be mapped (applied) to the tmp np.array in parallel and does not need to track the internal state of i.  Unless that is what python does with for loops anyway. EDIT:The for loop in the Original Post is significantly faster.  I don't have exact timings, but for my large data set the map function in this answer takes several minutes to complete, and the for loop in the OP takes seconds.@hpaulj's comment "And your range/append loop is nearly as good (if not better)" is correct. 


Answer URL
https://docs.python.org/3/library/itertools.html#itertools.compress
