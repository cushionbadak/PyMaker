Natural Text
I have a list of tuples with 3 members in each tuple as seen below:The tuples are ordered in ascending according to first element of each tuple - the hash value of each key (e.g 'test0'). I want to find a quick way of searching through these tuples using binary search of their hash values to find a specific key. Problem is the quickest way I have found is using a for loop:The function I have written above seems to be very slow at searching through a much longer list of tuples, lets say a list of 6000 different tuples. It also breaks if there are any hash collisions. I was wondering if there was a more efficient/quick way of searching the list for the correct tuple?Side note: I know using dictionaries will be a much quicker and easier way to solve my problem but I'd like to avoid using them.
You could modify  bisect  to just check the first element: replicating some collisions, it does what it should:Some timings:To get a better idea you would have to throw in collisions but to find the section that the hash may belong is pretty efficient.Just type casting a few variables and compiling with cython:Gets us almost down to 1 microsecond:
First, prehash the key, don't do it over and over. Second, you can use next with an unpacking generator expression to optimize a bit:That said, you claim you want to do a binary search, but the above is still a linear search, just optimized to avoid redundant work and to stop when the desired key is found (it checks hash first, assuming key comparison is expensive, then checks key equality only on hash match, since you complained about issues with duplicates). If the goal is binary search, and D is sorted by hash code, you'd want to use the bisect module. It's not trivial to do so (because bisect doesn't take a key argument like sorted), but if you could split D into two parts, one with just hash codes, and one with codes, keys and values, you could do:That gets you true binary search, but as noted, requires that the hash codes be separated from the complete tuples of hashcode, key, value ahead of time, before the search. Splitting the hash codes at the time of each search wouldn't be worth it since the loop that split them off could have just found your desired value directly (it would only be worth splitting if you were performing many searches at once).As Padraic notes in his answer, at the expense of giving up the C accelerator code, you could copy and modify the pure Python implementation of bisect.bisect_right and bisect.bisect_left changing each use of a[mid] to a[mid][0] which would get you bisect code that doesn't require you to maintain a separate list of hashes. The memory savings might be worth the higher lookup costs. Don't use itertools.islice to perform the slicing though, as islice with a start index iterates the whole list up to that point; true slicing only reads and copies what you care about. If you want to avoid the second bisect operation though, you could always write your own Sequence-optimized islice and combine it with itertools.takewhile to get a similar effect without having to calculate the end index up front. Code for that might be something like:Note: You could save even more work at the expense of more memory, by having Dhashes actually be (hashcode, key) pairs; assuming uniqueness, this would mean a single bisect.bisect* call, not two, and no need for a scan between indices for a key match; you either found it in the binary search or you didn't. Just for example, I generated 1000 key value pairs, storing them as either (hashcode, key, value) tuples in a list (which I sorted on the hashcode), or a dict mapping keys->values. The keys were all 65 bit ints (long enough that the hash code wasn't a trivial self-mapping). Using the linear search code I provided up top, it took ~15 microseconds to find the value located at index 321; with binary search (having copied hashes only to a separate list) it took just over 2 microseconds. Looking it up in the equivalent dict took ~55 _nano_seconds; the run time overhead even for binary search worked out to ~37x, and linear search ran ~270x higher. And that's before we get into the increased memory costs, increased code complexity, and increased overhead to maintain sorted order (assuming D is ever modified).Lastly: You say "I'd like to avoid using [dicts]", but give no explanation as to why. dicts are the correct way to solve a problem like this; assuming no self-hashing (i.e. key is an int that hashes to itself, possibly saving the cost of the hash code), the memory overhead just for the list of tuples (not including a separate list of hash codes) would be (roughly) twice that of a simple dict mapping keys to values. dict would also prevent accidentally storing duplicates, have ~O(1) insertion cost (even with bisect, insertion maintaining sorted order would have ~O(log n) lookup and O(n) memory move costs), ~O(1) lookup cost (vs. ~O(log n) with bisect), and beyond the big-O differences, would do all the work using C built-in functions that are heavily optimized, so the real savings would be greater.
Try using list comprehensions. I'm not sure if it's the most efficient way, but it's the pythonic way and quite effective!


Answer URL
https://docs.python.org/3/library/bisect.html
