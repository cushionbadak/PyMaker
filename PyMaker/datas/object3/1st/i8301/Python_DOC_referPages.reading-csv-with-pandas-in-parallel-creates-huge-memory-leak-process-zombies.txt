Natural Text
I'm reading +1000 of ~200Mb CSVs in parallel and saving the modified CSV afterwards using pandas. This creates many zombie processes that accumulate to +128Gb of RAM which devastates performance.This is my current solution. It doesn't seem to cause a problem until you process more than 80 CSVs or so.PS: Even when pool is completed ~96Gb of RAM is still occupied and you can see the python processes occupying RAM but not doing anything nor being destoryed. Moreover, I know with certainty that the function the pool is executing itself is running to completion.I hope that's descriptive enough.
Python's multiprocessing module is process-based. So it is natural that you have many processes.Worse, these processes do not share memory, but communicate through pickling/unpickling. So they are very slow if large data need to be transferred between processed, which is happening here.For this case, because the processing is I/O related, you may have better performance using multithread with threading module if I/O is the bottleneck. Threads share memory but they also 'share' 1 CPU core, so it's not guarantee to run faster, you should try it.Update: If multithread does not help, you don't have many options left. Because this case is exactly against the critical weakness of Python's parallel processing architecture. You may want to try dask (parallel pandas): http://dask.readthedocs.io/en/latest/
Question:  I tried your example code, but I'm not able to start more then one process.Your code looks extraordinary, beside this Pool(48) are to much processes.To start more than one process I have to change toPython » 3.6.1 Documentation multiprocessing.pool.Pool.starmap     starmap(func, iterable[, chunksize])      Like map() except that the elements of the iterable are expected to be iterables that are unpacked as arguments.      Hence an iterable of [(1,2), (3, 4)] results in [func(1,2), func(3,4)].As I know nothing about (a, b), doublecheck that the following don't apply to you.Python » 3.6.1 Documentation multiprocessing.html#all-start-methods      Explicitly pass resources to child processes      On Unix using the fork start method, a child process can make use of a shared resource created in a parent process using a global resource. However, it is better to pass the object as an argument to the constructor for the child process.      Apart from making the code (potentially) compatible with Windows and the other start methods this also ensures that as long as the child process is still alive the object will not be garbage collected in the parent process. This might be important if some resource is freed when the object is garbage collected in the parent process.Question:  I know with certainty that the function the pool is executing itself is running to completion.  terminate()Please explain, why do you call process_pool.terminate()?  


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.starmap
https://docs.python.org/3/library/multiprocessing.html#all-start-methods
