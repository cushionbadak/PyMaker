Natural Text
I have two files in which I store:an IP range - country lookup a list of requests coming from different IPsThe IPs are stored as integers (using inet_aton()). I tried using Spark SQL to join these pieces of data by loading both files into dataframes and registering them as temp tables.I tried using Spark SQL to join these pieces of data using a SQL statement like so -There are about 850K records in RecordsTable and about 2.5M records in GeoLocTable. The join as it exists runs for about 2 hours with about 20 executors. I have tried caching and broadcasting the GeoLocTable but it does not really seem to help. I have bumped up spark.sql.autoBroadcastJoinThreshold=300000000 and  spark.sql.shuffle.partitions=600.Spark UI shows a BroadcastNestedLoopJoin being performed. Is this the best I should be expecting? I tried searching for conditions where this type of join would be performed but the documentation seems sparse. PS - I am using PySpark to work with Spark.
The source of the problem is pretty simple. When you execute join and join condition is not equality based the only thing that Spark can do right now is expand it to Cartesian product followed by filter what is pretty much what happens inside BroadcastNestedLoopJoin. So logically you have this huge nested loop which tests all 850K * 2.5M records. This approach is obviously extremely inefficient. Since it looks like lookup table fits into memory the simplest improvement is to use local, sorted data structure instead of Spark DataFrame. Assuming your data looks like this:We can project and sort reference data by ipstart and create broadcast variable:Next we'll use an UDF and bisect module to augment records_tableand finally join both datasets:


Answer URL
