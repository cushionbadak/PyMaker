Natural Text
I have code like this.I would like to make two or three request at the same time to accelerate this. Can I use urllib2 for this, and how? If not which other library should I use? Thanks.
You can use asynchronous IO to do this.requests + gevent = grequestsGRequests allows you to use Requests with Gevent to make asynchronous HTTP Requests easily.
Take a look at gevent â€” a coroutine-based Python networking library that uses greenlet to provide a high-level synchronous API on top of libevent event loop.Example:
So, it's 2016 ðŸ˜‰ and we have Python 3.4+ with built-in asyncio module for asynchronous I/O. We can use aiohttp as HTTP client to download multiple URLs in parallel.Source: copy-pasted from http://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html
I know this question is a little old, but I thought it might be useful to promote another async solution built on the requests library.The docs are here: http://pythonhosted.org/simple-requests/
Either you figure out threads, or you use Twisted (which is asynchronous).
maybe using multiprocessing and divide you work on 2 process or so .Here is an example (it's not tested)


Answer URL
https://docs.python.org/3/library/asyncio.html
