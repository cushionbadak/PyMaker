Natural Text
I am using python multiprocessing library to process information within a set of processes. These processes also contain processes that further divide the amount of work that has to be done. There is a single Manager.Queue that accumulates the results from all of the processes that are consuming the data. In the main thread of the python script. I tried to use the join to block the main thread until we can reasonably determine if all the sub-processes were completed then write the output to a single file. However the system terminates and the file closes before all of the data is written to the file.The following code is a simplified extraction of the implementation of the above described solution.for queue in inQueues:        queue.join()The out_queue.qsize() will print an excess of 500 records available, however only 100 will be printed to the file.Also at this point I am not 100% certain if 500 records are the total generated by the system, but just the number reported at this point.How do I ensure that all the results are written to the results.csv file?
Do not wait for all processes to finish before you consume the data, but process the data at the same time and memorize which processes are still running:Do not test if processes is empty outside of the Queue.Empty case or you will have races.But probably you would be happier with a higher level function:


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map_async
