Natural Text
I have a long running task that can be parallelized. I debugged code with use of multiprocessing.dummy. It works well and I get results I expect. But when I change it to  multiprocessing, it runs _test function impossibly fast and actual output is not even touchedThe job is to fill pandas DataFrame with data up to some row count threshold. Each of longer processes in while cycle adds about 2500 rows at one run. Data acquisition is independent on other processes. The idea is, that processes pass DataFrame through Queue between each other and use lock to block access to it from other processes while they work with Dataframe. Once their work is done, they put it back and release lock.Once DataFrame is filled to required size, process can end and other processes are no longer required to finish(but Im not sure, if they are terminated once they finish without join() or just what happens with them - therefore .is_alive() check could replace .join())For this example TRAINING_DATA_LENGTH is set only to 10k but actual size will be much higherThe problem is, that when I change from multiprocessing.dummy to multiprocessing whole operation is finished in 0.7 seconds and returned X size is 0Maybe there is another way how to do it but Im not yet aware of it.Also I need it to run in separate file not __main__test_mp.pyand run it form another file with multiprocessing.dummy I get following output:with multiprocessing Its:
Adding if __name__ == '__main__': in runfile made the code execute and get "some" results. But with more testing it appears to be using only one core(or I have something wrong in code)test_mp.pywith multiprocessing.dummy I get following output:with multiprocessing Its:Solvedtime.sleep() does not stress processor, but when its switched for function like the results from both multiprocessing.dummy and multiprocessing are as expected - both returns same length, but multiprocessing N-times faster


Answer URL
https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming
