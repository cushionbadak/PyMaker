Natural Text
Given ten 1MB csv files, each with slightly different layouts, I need to combine them into a normalized single file with the same header. Empty string is fine for nulls.Examples of columns: The output would look like (although order not important, my code puts them in order discovered): So basically the fields can come in any order, fields may be missing, or new fields not seen before.  All must be included in the output file.  No joining required, in the end the count of data rows in the parts must equal the count of rows in the output.Reading all 10MB into memory is OK. Somehow using 100MB to do it would not be.  You can open all files at once if needed as well.  Lots of file hands, memory available, but it will be running against a NAS so it needs to be efficient for that (not too many NAS ops). The method I have right now is to read each file into columns lists, build new columns lists as I discover new columns then write it all out to a single file. I'm hoping someone has something a bit more clever, though, as I'm bottlenecking on this process so any relief is helpful.I have samples files here if anyone wants to try.  I'll post my current code as a possible answer.  Looking for the fastest time when I run it on my server (lots of cores, lots of memory) using local disk.
Use a two-pass approach with csv.DictReader() and csv.DictWriter() objects. Pass one collects the set of headers used across all the files, and pass two then copies across data based on the headers.Collecting headers is as simple as just accessing the fieldnames attribute on the reader objects is enough:Each reader produces a dictionary with a subset of all fields, but DictWriter will use the value of the restval argument (defaulting to '' when omitted like I did here) to fill in the value of each missing key.I assumed Python 2 here; if this is Python 3 you could use an ExitStack() to manage the open files for the readers; omit the b from the file modes and add a newline='' argument to all open calls to leave newline handling to the CSV module.The above code only ever uses a buffer to read and write rows; rows are mostly moved from one open reader to the writer one row at a time at a time.Unfortunately, we cannot use writer.writerows(reader) as the DictWriter.writerows() implementation first converts everything in reader to a list of lists before passing it on to the underlying csv.writer.writerows() method, see issue 23495 in the Python bug tracker.
Using the pandas library and the concat function
Here's a simple solution using standard library modules.  This is Python 3.  Use the alternate commented with lines for Python 2:EditPer comment, adding file name and line number:Original on my system took 8.8s.  This update took 10.6s.Also note that if you order fields before passing to DictWriter you can put the columns in the order you want.
It's not super short or anything, but basically I'm reading these into column stores then writing them all out. I'm hoping for something faster, or same speed, same i/o and less memory is good too... but faster is most important.
@MartijnPieter's answer is very helpful, but due to keeping the files open after reading the headers to re-use when reading the content, it crashes at ~255 files (I found).  I needed to combine ~32,000 files, so rewrote his code slightly to not crash.  I also chose to split it into two functions, so that I could analyse the column headers, in between.When opening a very motley assortment of CSVs (<1k - 700k; 20-60 mixed columns each; ~130 headers in the total set) the second stage is taking ~1 minute per 1000 files on a 1.4GHz MacBook Air.  Not bad, and several orders of magnitude faster than Pandas.


Answer URL
https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack
