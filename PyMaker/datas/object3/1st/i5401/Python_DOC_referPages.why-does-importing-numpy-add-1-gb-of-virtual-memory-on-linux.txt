Natural Text
I have to run python in a resource constrained environment with only a few GB of virtual memory. Worse yet, I have to fork children from my main process as part of application design, all of which receive a copy-on-write allocation of this same amount of virtual memory on fork. The result is that after forking only 1 - 2 children, the process group hits the ceiling and shuts everything down. Finally, I am not able to remove numpy as a dependency; it is a strict requirement.Any advice on how I can bring this initial memory allocation down?  e.g.  Change the default amount allocated to numpy on import?Disable the feature and force python / numpy to allocate more dynamically?  Details:Red Hat Enterprise Linux Server release 6.9 (Santiago)Python 3.6.2numpy>=1.13.3Bare Interpreter:
Thank you, skullgoblet1089, for raising questions on SO and at https://github.com/numpy/numpy/issues/10455 , and for answering.Citing your 2018-01-24 post:Reducing threads with export OMP_NUM_THREADS=4 will bring down VM allocation.


Answer URL
https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
