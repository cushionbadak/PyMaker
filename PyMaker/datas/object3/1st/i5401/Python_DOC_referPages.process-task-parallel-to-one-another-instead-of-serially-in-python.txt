Natural Text
Task1: I have a ~120Gb set of the document from which one program is continuously extracting "ids".Task2: Once all "ids" are extracted I have to process each of the "ids" and extract some data which links to each "id". I have written a python script which performs this serially, i.e Task 2 on completion of Task 1. I was wondering if there is a way in which as an when each "ids" is found in Task1, it sends it to Task2 immediately for processing while itself resuming to look for the next "ids". Basically, make both tasks work parallelly to save time. 
What you are describing reminds me of celery. Some other recommended libraries:concurrent.futures.ProcessPoolExecutormultiprocessing.Pool


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor
https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.pool
