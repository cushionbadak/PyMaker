Natural Text
I have 630,220 urls that I need to open and scrape. These urls themselves have been scraped, and scraping them was much easier because every single scraped page would return around 3,500 urls.To scrape these 630,220 urls, I'm currently doing parallel scraping in Python using threads. Using 16 threads, it takes 51 seconds to scrape 200 urls. Thus, it would take me 44 hours to scrape all 630,220 urls which seems to be an unnecessarily time consuming and grossly inefficient way to handle this problem.Assuming that the server will not be overloaded, is there a way to asynchronously send something like 1000 requests per second? That would bring down the total scraping time to about 10 minutes, which is pretty reasonable.
Use gevent. Enable monkey-patching of Python standard library, and just use your favourite scrapping library - replace the threads by 1000 greenlets doing the same thing. And you're done.


Answer URL
https://docs.python.org/3/library/asyncio.html
