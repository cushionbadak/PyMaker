Natural Text
My application reads data from a slow i/o source, does some processing and then writes it to a local file. I've implemented this with generators like so:Now I'd like to parallelize this task with the help of asyncio, but I can't seem to figure out how. The main issue for me is to directly feed data through a generator from the producer to the consumer while letting asyncio make multiple parallel requests to io_task(x). Also, this whole async def vs. @asyncio.coroutine thing is confusing me.Can someone show me how to build a minimal working example that uses asyncio from this sample code?(Note: It is not ok to just make calls to io_task(), buffer the results and then write them to a file. I need a solution that works on large data sets that can exceed the main memory, that's why I've been using generators so far. It is however safe to assume that the consumer is always faster than all producers combined)
Since python 3.6 and asynchronous generators, very few changes need be applied to make your code compatible with asyncio.The io_task function becomes a coroutine:The producer generator becomes an asynchronous generator:The consumer function becomes a coroutine and uses aiofiles, asynchronous context management and asynchronous iteration:And the main coroutine runs in an event loop:Also, you may consider using aiostream to pipeline some processing operations between the producer and the consumer.EDIT: The different I/O tasks can easily be run concurrently on the producer side by using as_completed:


Answer URL
https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.as_completed
https://docs.python.org/3/library/asyncio-task.html#asyncio.gather
