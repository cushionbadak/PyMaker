Natural Text
We have a dataset which has approx 1.5MM rows. I would like to process that in parallel. The main function of that code is to lookup master information and enrich the 1.5MM rows. The master is a two column dataset with roughly 25000 rows. However i am unable to make the multi-process work and test its scalability properly. Can some one please help.  The cut-down version of the code is as followsMethod work will have the business logic and i would like to pass partitioned data_df into work to enable parallel processing. The sample data is as followsIdeally i would like to process 6 rows in each partition.Please help.Thanks and RegardsBala
First, work is returning the output of mylist.append(data), which is None. I assume (and if not, I suggest) you want to return a processed Dataframe.To distribute the load, you could use numpy.array_split to split the large Dataframe into a list of 6-row Dataframes, which are then processed by work.
My best recommendation is for you to use the chunksize parameter in read_csv (Docs) and iterate over. This way you wont crash your ram trying to load everything plus if you want you can for example use threads to speed up the process.Im not sure if this answer your specific question but i hope it helps.


Answer URL
https://docs.python.org/3/library/threading.html
