Natural Text
My aim is to generate a list ls of numbers which starts from 1.0 and ends at 1.499 where the step between subsequent numbers is 0.001.Ultimately, I am loading a file that has a column of float numbers, e.g., 1.293, 1.101, ... all with 3 numbers past floating point, and would like to find where in ls each number read from the file occurs. My two methods for creating ls:1ls = np.arange(1.,1.5,0.001)and 2According to either method of 1 or 2, one expects for instance to find the number 1.293 in ls, but it seems not to be the case. For instance when I print ls having been created using the 2nd method, it prints: [1.0, 1.001, 1.0019999999999998, 1.0029999999999997,... as opposed to the desired [1.0, 1.001, 1.002, 1.003, ...].On the other hand, using the 1st method to create ls, and testing if we find again the number 1.293 as follows: It never prints 'Found' meaning it does not contain the number 1.293, which by construction it is expected to contain as I gave the step size when using np.arange...How am I supposed to create ls such that I really get all numbers between 1.0 and 1.5 with step size 0.001 and without having to deal with higher precision in floating points. 
Floating point numbers are not exact real numbers, only the closest numbers that can fit in a 52-bit binary fraction. For example, you can't fit 1.001 in a binary fraction; the nearest value is 1.000999999999999889865875957184471189976. If you add 0.001 to that, the nearest value to the result is 1.001999999999999779731751914368942379951. But the nearest value to 1.002 is 1.002000000000000001776356839400250464678. Those aren't equal. They only differ by one part in 2**52, but they still differ.And now you're adding .001 to a number slightly less than 1.002, so the errors can accumulate. Worst case, you can be off by one part in 2**52 500 times, so the total error can be as bad as 500 parts in 2**52, or roughly one part in 2**43.The way to deal with that is to never check whether two floating point numbers are equal; instead, work out how much error your floating-point math can introduce, and check whether they're within that error, using math.isclose or np.isclose.In cases like this, where you only wanted 3 digits of precision, and you can be sure the maximum error never gets anywhere near those 3 digits, you can simplify things and just check whether they're within, say, 0.0001. Or, even more simply, you can verify that the default tolerances used by isclose are more than good enough, and then just use them.So, instead of this:… write this:An alternative is to use floating point decimal numbers, instead of floating-point binary numbers.Of course these have the same problem—1/3 can't fit exactly in a decimal fraction any more than it can fit in a binary fraction. But if the only numbers you're dealing with are decimal fractions, like 1/1000, that's not a problem.And, even when it is a problem, being able to choose a precision rather than having to accept the hardcoded one, and choosing it in decimal digits rather than binary digits, can make the error tracking a lot easier to think about.The only problem is that decimal.Decimal values aren't "native" to your CPU. Decimal math runs roughly an order of magnitude slower, NumPy doesn't know how to deal with Decimal values except as generic Python objects so it can't give you its usual space and speed savings, etc. But if you're only dealing with 500 numbers, who cares?So:And if you think about it, it's pretty easily to simplify things to not need all of those Decimal('…') constructors, because many of your numbers are just plain integers:


Answer URL
https://docs.python.org/3/library/decimal.html
