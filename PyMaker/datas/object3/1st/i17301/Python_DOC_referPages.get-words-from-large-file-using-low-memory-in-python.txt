Natural Text
I need to iterate over the words in a file. The file could be very big (over 1TB), the lines could be very long (maybe just one line). Words are English, so reasonable in size. So I don't want to load in the whole file or even a whole line.I have some code that works, but may explode if lines are to long (longer than ~3GB on my machine).Can you tell be how I can, simply, rewrite this iterator function so that it does not hold more than needed in memory?
Don't read line by line, read in buffered chunks instead:I'm using the callable-and-sentinel version of the iter() function to handle reading from the file until file.read() returns an empty string; I prefer this form over a while loop.If you are using Python 3.3 or newer, you can use generator delegation here:Demo using a small chunk size to demonstrate this all works as expected:


Answer URL
