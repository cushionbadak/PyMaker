Natural Text
Given the Unicode non spacing marks list - https://www.fileformat.info/info/unicode/category/Mn/list.htm NOTE.Please note that we have both \U000XXXXX and \uXXXX representations here.I want to count the Unicode input text like this Hindi string "अब यहां से कहा जाएँ हम" or just a token word like "समझा", excluding the non spacing characters.My implementation looks likeThanks to the help provided in the answers below I have put all together in this github. There is also a mark codepoints list ready to be used in JavaScript / Node.js - https://github.com/loretoparisi/unicode_marks
Fastest way I came up with.  len was slightly faster than sum.  I built a set of all combining mark types in the setup.test.py:Output:Timings:It's worth noting that there is a grapheme 3rd party library.  grapheme.length(s) == 16, but it was much slower (118us).  The full grapheme-detecting algorithm is more complicated than skipping the modifier category.  Consider the combining emojis for families and skin colors.See also Unicode Text Segmentation.
This might be a better alternative:
How about using a dictionary to look up the values and if not present, increment the count? It should be faster than the former approach because the time complexity to check the presence of the character reduces to O(1).The implementation should look somewhat like this:Create a dict and populate it:lookup_dict = {}for alpha in UNICODE_NSM:  lookup_dict[alpha] = 1Look it up while looping through the string:def countNonSpacingCharString(str):  count = 0;  for char in str:    start_time = time.time()    if not lookup_dict.get(char):      count = count + 1    print("--- %s seconds ---" % (time.time() - start_time))  return count
I must note that using str, as variable name in Python is bad idea, as it is name of built-in function. Anyway I would implement your function following way:in Python 2in Python 3Inspecting my function using dis.dis showed that it produced less bytecode than your version with count, thus suggesting it might be faster, though this need further investigation.EDIT: I tested my code in Python 2, but not Python 3 - version for Python 3 added, using Mohammad Banisaeid answer from this topic.EDIT 2: If you uses UNICODE_NSM only for that, you might try to use set instead of list, which should boost in operator, though again this need further investigation. For discussion about list vs set performance see this thread.
Perhaps the easiest way to do this is to use the unicodedata module. In part, because it will be more rigorously tested. Indeed, I found your list appeared to be including categories other than Mn. That is, it includes Unicode points from Mc (Mark, spacing combining) as well, but you said you only wanted to exclude Unicode points from Mn (Mark, Nonspacing).eg.This appears to be about 60 times faster according to timeit.You might get a TypeError, if your version of Python and therefore unicodedata is not up-to-date, and so not aware of recent additions to Unicode. You can get around this by installing unicodedata2 and using that instead.
From your comments it looks like you're really after counting "user perceived characters". This is a complicated process with a number of edge cases. If you can then you should to install regex on your environment (that would be micropython?). You can then do:Which splits your string into "user perceived characters", and then you can work on this list of strings to get what you need.Failing that, your current solution of just ignoring Mark code points is an 80/20 solution (gets you most of the way their for the least amount of effort). You will have to revise what your list of Unicode marks though. My tests showed that your list was missing 113 code points across all the Indo-European and Dravidian scripts in Unicode (Devanagari, Bengali, Gurmukhi, Gujarati, Oriya, Tamil, Telugu, Kannada, Malayalam, and Sinhala).I extracted these characters by downloading and parsing: https://www.unicode.org/Public/11.0.0/ucd/UnicodeData.txt with the following code:
Mark Tolonens answer is the fastest, because it uses a set for comparison. If you have a text of length n and m whitespace-characters to compare with, then your worst-case runtime using two lists is O(nm). Using a set for the whitespace characters reduces that to O(n).Using unicodedata.category is just nicer because it is shorter and less prone to human error.Performance comparisonYou can clearly see that the markset_count and the category_count are way faster than the generator_count and the loop_count. Also the speed of the latter two varies way more. Interestingly, the generator_count is slower than the loop_count.The markset_count is a bit faster than the category_count. I think that is the case because looking up the category and doing the string comparison also takes a bit of time. The difference is way more clear when you only plot the two and increase the text length:


Answer URL
https://docs.python.org/3/library/unicodedata.html
