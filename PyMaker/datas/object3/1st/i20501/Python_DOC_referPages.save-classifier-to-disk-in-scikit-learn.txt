Natural Text
How do I save a trained Naive Bayes classifier to disk and use it to predict data?I have the following sample program from the scikit-learn website:
Classifiers are just objects that can be pickled and dumped like any other. To continue your example:
You can also use joblib.dump and joblib.load which is much more efficient at handling numerical arrays than the default python pickler.Joblib is included in scikit-learn:
What you are looking for is called Model persistence in sklearn words and it is documented in introduction and in model persistence sections.So you have initialized your classifier and trained it for a long time withAfter this you have two options:1) Using Pickle2) Using JoblibOne more time it is helpful to read the above-mentioned links 
In many cases, particularly with text classification it is not enough just to store the classifier but you'll need to store the vectorizer as well so that you can vectorize your input in future.future use case:Before dumping the vectorizer, one can delete the stop_words_ property of vectorizer by:to make dumping more efficient.Also if your classifier parameters is sparse (as in most text classification examples) you can convert the parameters from dense to sparse which will make a huge difference in terms of memory consumption, loading and dumping. Sparsify the model by:Which will automatically work for SGDClassifier but in case you know your model is sparse (lots of zeros in clf.coef_) then you can manually convert clf.coef_ into a csr scipy sparse matrix by:and then you can store it more efficiently.
sklearn estimators implement methods to make it easy for you to save relevant trained properties of an estimator. Some estimators implement __getstate__ methods themselves, but others, like the GMM just use the base implementation which simply saves the objects inner dictionary:The recommended method to save your model to disc is to use the pickle module:However, you should save additional data so you can retrain your model in the future, or suffer dire consequences (such as being locked into an old version of sklearn).From the documentation:In order to rebuild a similar model with future versions of  scikit-learn, additional metadata should be saved along the pickled  model: The training data, e.g. a reference to a immutable snapshot The python source code used to generate the model The versions of scikit-learn and its dependencies The cross validation score obtained on the training dataThis is especially true for Ensemble estimators that rely on the tree.pyx module written in Cython(such as IsolationForest), since it creates a coupling to the implementation, which is not guaranteed to be stable between versions of sklearn. It has seen backwards incompatible changes in the past.If your models become very large and loading becomes a nuisance, you can also use the more efficient joblib. From the documentation:In the specific case of the scikit, it may be more interesting to use  joblibâ€™s replacement of pickle (joblib.dump & joblib.load), which is  more efficient on objects that carry large numpy arrays internally as  is often the case for fitted scikit-learn estimators, but can only  pickle to the disk and not to a string:


Answer URL
https://docs.python.org/3/library/pickle.html
