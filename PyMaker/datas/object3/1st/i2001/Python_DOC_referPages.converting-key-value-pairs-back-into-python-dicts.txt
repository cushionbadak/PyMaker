Natural Text
There's a logfile with text in the form of space-separated key=value pairs, and each line was originally serialized from data in a Python dict, something like:The keys are always just strings. The values could be anything that ast.literal_eval can successfully parse, no more no less.  How to process this logfile and turn the lines back into Python dicts? Example:Here is some extra context about the data:Keys are valid namesInput lines are well-formed (e.g. no dangling brackets)The data is trusted (unsafe functions such as  eval, exec, yaml.load are OK to use)Order is not important. Performance is not important. Correctness is important.Edit:  As requested in the comments, here is an MCVE and an example code that didn't work correctly
Your input can't be conveniently parsed by something like ast.literal_eval, but it can be tokenized as a series of Python tokens. This makes things a bit easier than they might otherwise be.The only place = tokens can appear in your input is as key-value separators; at least for now, ast.literal_eval doesn't accept anything with = tokens in it. We can use the = tokens to determine where the key-value pairs start and end, and most of the rest of the work can be handled by ast.literal_eval. Using the tokenize module also avoids problems with = or backslash escapes in string literals.This behaves correctly on your example inputs, as well as on an example with backslashes:Incidentally, we probably could look for token type NAME instead of = tokens, but that'll break if they ever add set() support to literal_eval. Looking for = could also break in the future, but it doesn't seem as likely to break as looking for NAME tokens.
Regex replacement functions to the rescueI'm not rewriting a ast-like parser for you, but one trick that works pretty well is to use regular expressions to replace the quoted strings and replace them by "variables" (I've chosen __token(number)__), a bit like you're offuscating some code.Make a note of the strings you're replacing (that should take care of the spaces), replace space by comma (protecting against symbols before like : allows to pass last test) and replace by strings again.prints:The key is to extract the strings (quoted/double quoted) using non-greedy regex and replace them by non-strings (like if those were string variables not literals) in the expression. The regex has been tuned so it can accept escaped quotes and double escape at the end of string (custom solution)The replacement function is an inner function so it can make use of the nonlocal dictionary & counter and track the replaced text, so it can be restored once the spaces have been taken care of.When replacing the spaces by commas, you have to be careful not to do it after a colon (last test) or all things considered after a alphanum/underscore (hence the \w protection in the replacement regex for comma)If we uncomment the debug print code just before the original strings are put back that prints:The strings have been pwned, and the replacement of spaces has worked properly. With some more effort, it should probably be possible to quote the keys and replace k1= by "k1": so ast.literal_eval can be used instead of eval (more risky, and not required here)I'm sure some super-complex expressions can break my code (I've even heard that there are very few json parsers able to parse 100% of the valid json files), but for the tests you submitted, it'll work (of course if some funny guy tries to put __tokenxx__ idents in the original strings, that'll fail, maybe it could be replaced by some otherwise invalid-as-variable placeholders). I have built an Ada lexer using this technique some time ago to be able to avoid spaces in strings and that worked pretty well.
You can find all the occurrences of = characters, and then find the maximum runs of characters which give a valid ast.literal_eval result. Those characters can then be parsed for the value, associated with a key found by a string slice between the last successful parse and the index of the current =:Output:Disclaimer:This solution is not as elegant as @Jean-Fran√ßoisFabre's, and I am not sure if it can parse 100% of what is passed to to_dict, but it may give you inspiration for your own version.


Answer URL
https://docs.python.org/3/library/ast.html#ast.literal_eval
https://docs.python.org/3/reference/lexical_analysis.html#identifiers
https://docs.python.org/3/library/functions.html#eval
https://docs.python.org/3/library/functions.html#exec
https://docs.python.org/3/library/tokenize.html
