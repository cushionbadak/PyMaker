Natural Text
I have a csv file which is ~40gb and 1800000 lines. I want to randomly sample 10,000 lines and print them to a new file.Right now, my approach is to use sed as:Where $vars is a randomly generated list of lines. (Eg: 1p;14p;1700p;...;10203p)While this works, it takes about 5 minutes per execution. It's not a huge time, but I was wondering if anybody had ideas on how to make it quicker?
The biggest advantage to having lines of the same length is that you don't need to find newlines to know where each line starts. With a file size of ~40GB containing ~1.8M lines, you have a line length of ~20KB/line. If you want to sample 10K lines, you have ~40MB between lines. This is almost certainly around three orders of magnitude larger than the size of a block on your disk. Therefore, seeking to the next read location is much much more efficient than reading every byte in the file.Seeking will work with files that have unequal line lenghs (e.g., non-ascii characters in UTF-8 encoding), but will require minor modifications to the method. If you have unequal lines, you can seek to an estimated location, then scan to the start of the next line. This is still quite efficient because you will be skipping ~40MB for every ~20KB you need to read. Your sampling uniformity will be compromised slightly since you will select byte locations instead of line locations, and you won't know which line number you are reading for sure.You can implement your solution directly with the Python code that generates your line numbers. Here is a sample of how to deal with lines that all have the same number of bytes (usually ascii encoding):If you are willing to sacrifice a (very) small amount of uniformity in the distribution of line numbers, you can easily apply a similar technique to files with unequal line lengths. You just generate random byte offsets, and skip to the next full line after the offset. In the following implementation, it is assumed that you know for a fact that no line is longer than 40KB in length. You would have to do something like this if your CSV had non-ascii unicode characters encoded in UTF-8, because even if the lines all contained the same number of characters, they would contain different numbers of bytes. In this case, you would have to open the file in binary mode, since otherwise you might run into decoding errors when you skip to a random byte, if that byte happens to be mid-character:All code here is Python 3.
In case all lines have the same length, you could do it without the need to parse the whole file or load it to memory, using dd.You have to know the lines number, having already executed wc -l, and the precise byte length of each line, and of course to have test and ensure all lines really have the same length. Even wc will be slow as it will read the whole file.For example, if every line is 20000 bytesThis way we loop and run 10K processes, I am not sure if it could be done at once, so although dd is faster, using a language, like Python and seek() method, (as @tripleee says and @Mad Physicist hinted at with comments) will have the advantage of one process.save some more seconds, if output is small enough, you can keep it in a bytearray and write it at once at the end.
If indeed your lines are all the same length, your Python script can randomly seek() ahead in the file, and you know which index exactly to seek to in order to land exactly on the character after a newline.The Python script which generates the random indices for your sed script should be easy to adapt to this approach. Basically, when you generate 123p to feed into sed, instead seek to 122*line length and read the line you land at.A complication is that Python 3 prohibits random seeks in files which are opened in text mode (because it needs to know where enooded characters start and end). For a quick and dirty script, simply reading and writing bytes should be fine (in general the recommendation is to decode bytes into Unicode, then encode again before writing; but since you aren't processing the lines in Python at all, this is unnecessary).
For testing purposes, let's create a file of 1,800,000 lines:Assuming you don't know the number of lines in that file, the fastest way to get the total number of lines is with the POSIX utility wc:So to get the total line count of a text file with 1,800,000 lines is pretty fast.Now that you know the total number of lines, you can use awk to print a random sample of those lines:That runs in about 200 ms on my older iMac. Note that the total is close to 10,000 but likely less since you will hit the end of the file often before you hit 10,000 lines. If you want exactly 10,000 at a penalty of true randomness, you can do:Or, alternatively, generate 10,000 unique numbers between 1 and the number of lines:
You will want to insert the data into a database (such as sqlite or mysql), and then repeat your idea in SQLYou can also read up how to select a random sample from this excellent tutorial http://jan.kneschke.de/projects/mysql/order-by-rand/ and There is no way devise a shell script that would run significantly faster, as your code ultimately relies on the way filesystems fundamentally work. That is, for good performance you want to access the disk sequentially and in chunks. Databases are designed to solve this issue by storing how the data is laid out in the harddrive in a separate file called the index. It works much in the same way as an index of a book. This is a rich topic and requires some learning. If you're new to database programming, 40 gb dataset is a good starting point, though.
Yet another idea, borrowed from the world of Monte Carlo simulations, would be to loop over the lines and generate a random number in each iteration. Now, if you want 10k lines from a set of 180k lines, you'd reason as follows. There is a 10/180 change that you want to include the row in question. If the random number is less than or equal to 10/180, you accept the row. Otherwise you reject it or break the loop if the desired amount of rows has been collected.The downside of this approach that there is no guarantee that exactly 10k lines are sampled. I am also suspicious that there are biases in this approach and that it will not be random enough.


Answer URL
https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects
