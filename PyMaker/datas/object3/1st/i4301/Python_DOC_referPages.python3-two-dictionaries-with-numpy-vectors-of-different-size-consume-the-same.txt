Natural Text
I have two python dictionaries {word: np.array(float)}, in the first dictionary I use 300-dimensional numpy vectors, in the second (keys are the same) - 150-dimensional. File size of the first one is 4.3 GB, of the second one - 2.2 GB.When I check loaded objects with sys.getsizeof() I get:For big dictionary:Linux top command shows 6.22GB:For small dictionary:But when I look at the python3 process with linux top command I see 6.17GB:Both dictionaries were saved using pickle.HIGHEST_PROTOCOL in Python3, I do not want to use json because of possible errors with encoding and slow loading. Also, using numpy arrays is important for me, as I compute np.dot for these vectors.How can I shrink RAM for the dictionary with smaller vectors in it?More precise memory measurement:EDIT: Thanks to @etene's hint, I've managed to save and load my model using hdf5:Saving:Which produces keyphrases list with length 3713680 and vectors array with the shape (3713680, 150).Loading:Now I indeed have only 3GB of RAM consumed:@etene, you can write your comment as answer, I will choose it.The only problem left is that loading now takes considerable time (5min), perhaps because of lookup made in hdf5 file for each position in numpy array. If I can iterate hdf5 by the second coordinate somehow, without loading into RAM, that will be great.EDIT2: Following @hpaulj 's suggestion, I loaded file in chunks, and it is now as fast as pickle or even quicker (4s) when chunk in 10k is used:Thanks everyone !!!
Summarizing what we found out in the comments:sys.getsizeof returning the same value for two dicts with the same keys is normal behavior. From the docs: "Only the memory consumption directly attributed to the object is accounted for, not the memory consumption of objects it refers to."Deserializing all your data at once is what eats up so much RAM; this Numpy discussion thread mentions the HDF5 file format as a solution to read data in smaller batches, reducing memory usage.However, reading in smaller batches can also have an impact on performance due to the disk i/o. Thanks to @hpaulj, @slowpoke was able to determine a larger batch size that worked for him.TL;DR for future readers: If it's really large, don't deserialize your whole dataset at once, that can take unpredictable amounts of RAM. Use a specialized format such as HDF5 and chop your data in reasonably-sized batches, keeping in mind that smaller reads = more disk i/o and larger reads = more memory usage.


Answer URL
https://docs.python.org/3/library/sys.html#sys.getsizeof
