Natural Text
I currently have a program which uses a while loop to receive messages as a subscriber from a zmq_socket. Every time I get a new message, I need to make an HTTP request to a server, which will then send a response. This call takes around 1 second to make. I would like to have a way to call the HTTP request and then loop again, get another message from zmq_socket, and call another HTTP request without waiting for the first one.The pseudo-code would look something like this:Is there any way to do this in Python?I've looked into libraries like tornado and asyncio as well as looking at multithreading and I haven't figured out a solution.
Yes, there are several ways to do this in Python :Their main difference is the cost-of-operations and performance.A )The simplest and cheapest ever is to use Tkinter-native infrastructure of independent .mainloop()-orchestrated processing, where independent serial-processing cheaply takes place in a co-orchestrated manner, with additional soft-real-time benefits, if one may wish to use them. Given this option, one may soft-schedule the callback using either an .after() or even .after_idle() ( as your code regularly goes into a blocking-mode of the .recv() receiver ) scheduling methods.B )Another, way smarter Tkinter-native infrastructure tool may do a similar job more efficiently, given the message would actually become a Tkinter's StringVar-instance, that has been equipped with a <aVarTRACER>:<aTracedVarEventHANDLER>-monitoring-tool. This way any value-change of the message will auto-trigger a callback, without the code taking any further steps ( but the correct setup of such StringVar monitor ). I love this Tkinter-tool, indeed for many powerful Live-GUI designs.C )Last, but not least, one may design a scalable performance ZeroMQ workflow from inside of your while-loop, where any such received message gets marshalled into a pool of remote-workers over another zmq_task_to_pool_of_workers_socket. This approach can help easily "mask" the worker-side processing latency by adding more remote-workers into the pool of workers, besides the elementary trick of getting the async .post()-method and other associated work done "outside" of the said while-loop. The almost linear performance scaling + latency masking of this option may go beyond the shared GIL-lock stepping, so if these are the core design-features, this is the way to go.


Answer URL
https://docs.python.org/3/library/queue.html#queue.Queue
https://docs.python.org/3/library/threading.html
