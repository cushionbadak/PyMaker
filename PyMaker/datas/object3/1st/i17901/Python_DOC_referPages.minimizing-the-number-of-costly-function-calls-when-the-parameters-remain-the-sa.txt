Natural Text
Suppose that there is a function costly_function_a(x) such that: it is very costly in terms of execution time;it returns the same output whenever the same x is fed to it; and it does not perform "additional tasks" besides returning an output.In these conditions, instead of calling the function twice in a row with the same x, we can store the result in a temporary variable, then use that variable to do these computations.Now suppose that there are some functions (f(x), g(x) and h(x) in the example below) that call costly_function_a(x), and that some of these functions may call each others (in the example below, g(x)and h(x) both call f(x)). In this case, using the simple method mentioned above still results in repeated calls to costly_function_a(x) with the same x (see OkayVersion below). I did found a way to minimize the number of calls, but it is "ugly" (see FastVersion below). Any ideas of a better way to do this?The output of this code is:Note that I do not want to "store" the results for all values of x used in the past (because this would require too much memory). Moreover, I do not want to have a function returning a tuple of the form (f,g,h) because there are cases where I only want f (so there is no need to evaluate costly_function_b).
What you are looking for is a LRU cache; only most-recently used items are cached, limiting memory usage to balance invocation cost with memory requirements.As your costly function is invoked with different values for x, up to a number of return values (per unique x value) is cached, with the least-recently used cache results discarded when the cache is full.As of Python 3.2, the standard library comes with a decorator implementation: @functools.lru_cache():A backport is available for earlier versions, or pick one of the other available libraries that can handle LRU caches avaible on PyPI.If you only ever need to cache one most recent item, create your own decorator:This simpler decorator has less overhead than the more featureful functools.lru_cache().
I am accepting @MartijnPieters' solution because it is probably the right way to do it for 99% of the people that will get a problem similar to mine. However, in my very particular case, I only need a "cache of 1", so the fancy @lru_cache(1) decorator is slightly overkill. I ended up writing my own decorator (thanks to this awesome stackoverflow answer), which I provide below. Be warned that I am new to Python, so this code may not be perfect.


Answer URL
