Natural Text
I'm looking for ways to make the code more efficient (runtime and memory complexity)Should I use something like a Max-Heap? Is the bad performance due to the string concatenation or sorting the dictionary not in-place or something else?Edit: I replaced the dictionary/map object to applying a Counter method on a list of all retrieved names (with duplicates) minimal request:  script should take less then 30 seconds current runtime: it takes 54 seconds
You are calling an endpoint of an API that generates dummy information one person at a time - that takes considerable amount of time. The rest of the code is taking almost no time. Change the endpoint you are using (there is no bulk-name-gathering on the one you use) or use built-in dummy data provided by python modules.You can clearly see that "counting and processing names" is not the bottleneck here:Output for 10000 names:inGeneral advise for code optimization: measure first then optimize the bottlenecks. If you need a codereview you can check https://codereview.stackexchange.com/help/on-topic and see if your code fits with the requirements for the codereview stackexchange site. As with SO some effort should be put into the question first - i.e. analyzing where the majority of your time is being spent. Edit - with performance measurements:Output:You can make the parsing part better .. I did not, because it does not matter. It is better to use timeit for performance testing (it calls code multiple times and averages, smoothing artifacts due to caching/lag/...) (thx @bruno desthuilliers ) - in this case I did not use timeit because I do not want to call API 100000 times to average results 


Answer URL
https://docs.python.org/3/library/timeit.html
