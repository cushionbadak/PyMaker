Natural Text
I'm writing on a script that reads all lines from multiple files, reads in a number  at the beginning of each block and puts that number in front of every line of the block until the next number and so on. Afterwards it writes all read lines into a single .csv file.The files I am reading look like this:And the output file should look like this:Currently my script is this:It has a runtime of about 135 Seconds (The 4 files that are read are each 500MB big and the output file has 2.3GB). Running the script takes about 10GB of RAM. I think this might be a problem.The biggest amount of time is needed to create the list of all lines, I think.I would like to reduce the runtime of this program, but I am new to python and not sure how to do this. Can you give me some advice?ThanksEdit:I measured the times for the following commands in cmd (I have only Windows installed on my Computer, so I used hopefully equivalent cmd-Commands):sequential writing to NULsequential writing to fileparallel
In this case you're not gaining anything by using asyncio for two reasons:asyncio is single-threaded and doesn't parallelize processing (and, in Python, neither can threads)the IO calls access the file system, which asyncio doesn't cover - it is primarily about network IOThe giveaway that you're not using asyncio correctly is the fact that your read_one coroutine doesn't contain a single await. That means that it never suspends execution, and that it will run to completion before ever yielding to another coroutine. Making it an ordinary function (and dropping asyncio altogether) would have the exact same result.Here is a rewritten version of the script with the following changes:byte IO throughout, for efficiencyiterates through the file rather than loading all at oncesequential codeOn my machine and Python 3.7, this version performs at approximately 22 s/GiB, tested on four randomly generated files, of 550 MiB each. It has a negligible memory footprint because it never loads the whole file into memory.The script runs on Python 2.7 unchanged, where it clocks at 27 s/GiB. Pypy (6.0.0) runs it much faster, taking only 11 s/GiB.Using concurrent.futures in theory ought to allow processing in one thread while another is waiting for IO, but the result ends up being significantly slower than the simplest sequential approach.
You want to read 2 GiB and write 2 GiB with low elapsed time and low memory consumption.Parallelism, for core and for spindle, matters.Ideally you would tend to keep all of them busy.I assume you have at least four cores available.Chunking your I/O matters, to avoid excessive malloc'ing.Start with the simplest possible thing.Please make some measurements and update your question to include them.sequentialPlease make sequential timing measurements ofandI assume you will have low CPU utilization, and thus will be measuring read & write I/O rates.parallelPlease make parallel I/O measurements:This will let you know if overlapping reads offers a possibility for speedup.For example, putting the files on four different physical filesystems might allow this -- you'd be keeping four spindles busy.asyncBased on these timings, you may choose to ditch async I/O, and instead fork off four separate python interpreters.logicThis is where much of your large memory footprint comes from.Rather than slurping in the whole file at once, consider reading by lines, or in chunks.A generator might offer you a convenient API for that.EDIT:compressionIt appears that you are I/O bound -- you have idle cycles while waiting on the disk.If the final consumer of your output file is willing to do decompression, thenconsider using gzip, xz/lzma, or snappy.The idea is that most of the elapsed time is spent on I/O, so you want to manipulate smaller files to do less I/O.This benefits your script when writing 2 GiB of output,and may also benefit the code that consumes that output.As a separate item, you might possibly arrange for the code that produces the four input files to produce compressed versions of them.
I have tried to solve your problem. I think this is very easy and simple way if don't have any prior knowledge of any special library.I just took 2 input files named input.txt & input2.txt with following contents.Note: All files are in same directory.input.txtinput2.txtI have written the code in modular way so that you could easily import and use it in your project. Once you run the below code from terminal using python3 csv_writer.py, it will read all the files provided in list file_names and generate output.csv will the result that you're looking for.csv_writer.pyoutput.csv


Answer URL
https://docs.python.org/3/library/gzip.html
https://docs.python.org/3/library/lzma.html
