Natural Text
Given a list of 50k websites urls, I've been tasked to find out which of them are up/reachable. The idea is just to send a HEAD request to each URL and look at the status response. From what I hear an asynchronous approach is the way to go and for now I'm using asyncio with aiohttp.I came up with the following code but the speed is pretty abysmal. 1000 URLs takes approximately 200 seconds on my 10mbit connection. I don't know what speeds to expect but I'm new to asynchronous programming in Python so I figured I've stepped wrong somewhere. As you can see I've tried increasing the number of allowed simultaneous connections to 1000 (up from the default of 100) and the duration for which DNS resolves are kept in the cache; neither to any great effect. The environment has Python 3.6 and  aiohttp 3.5.4.Code review unrelated to the question is also appreciated.
Right now you're launching all your requests at once. Thus probably bottleneck appeared somewhere. To avoid this situation semaphore can be used:I tested it following way:And got:Although it requests a single host, it shows that asynchronous approach does the job: 13 sec. < 2000 sec.Several more things can be done:You should play semaphore value to achieve better performancefor your concrete environment and task.Try to lower timeout from 20 to, let's say, 5seconds: since you're just doing head request it shouldn't take muchtime. If request hangs for 5 seconds there are good chances it won'tbe successful at all.Monitoring your system resources (network/CPU/RAM) while script runningcan help to find out if bottleneck is still present.By the way, did you install aiodns (as doc suggests)?Does disabling ssl change anything?Try to enable debug level of logging to see if there is any useful info thereTry to setup client tracing and especially measure time for each request step to see which ones take most timeIt's difficult to say more without fully reproducible situation.


Answer URL
https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore
