<div class="post-text" itemprop="text">
<p>I am beginner to Python and trying to add few lines of code to convert json to csv and back to json. Have thousands of files (size 300 MB) to be converted and processed. With current program (using 1 CPU), i am not able to use 16 CPUs of server and need suggestions to fine tune the program for multiprocessing. Below is my code with python 3.7 version.</p>
<pre><code>import json
import csv
import os

os.chdir('/stagingData/Scripts/test')

for JsonFile in os.listdir(os.getcwd()):
    PartialFileName = JsonFile.split('.')[0]

    j = 1
    with open(PartialFileName +".csv", 'w', newline='') as Output_File:

        with open(JsonFile) as fileHandle:
            i = 1
            for Line in fileHandle:
                try:
                    data = json.loads(Line, parse_float=str)
                except:
                    print("Can't load line {}".format(i))
                if i == 1:
                    header = data.keys()
                    output = csv.writer(Output_File)
                    output.writerow(header) #Writes header row
                i += 1
                output.writerow(data.values()) #writes values row
        j += 1
</code></pre>
<p>Appreciate suggestions on multiprocessing logic</p>
</div>
<div class="post-text" itemprop="text">
<p>If you have a single big file that you want to process more effectively I suggest the following:</p>
<ol>
<li><p>Split file into chunks</p></li>
<li><p>Create a process to process each chunk </p></li>
<li><p>(if necessary) merge the processed chunks back into a single file</p></li>
</ol>
<p>Something like this:</p>
<pre><code>import csv
import json
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor

source_big_file = Path('/path/to/file')

def chunk_file_by_line(source_filepath: Path, chunk_size: int = 10_000):
    chunk_line_size = 10_000
    intermediate_file_handlers = {}
    last_chunk_filepath = None
    with source_big_file.open('r', encoding='utf8') as big:
        for line_number, line in big:
            group = line_number - (line_number % chunk_line_size)
            chunk_filename = f'{source_big_file.stem}.g{group}{source_big_file.suffix}'
            chunk_filepath = source_big_file.parent / chunk_filename
            if chunk_filepath not in intermediate_file_handlers:
                file_handler = chuck_filepath.open('w', encoding='utf8')
                intermediate_file_handlers[chunk_filepath] = file_handler
                if last_chunk_filepath:
                    last_file_hanlder = intermediate_file_handlers[last_chunk_filepath]
                    last_file_handler.close()
                    yield last_chunk_filepath
            else:
                file_handler = intermediate_file_handlers[chunk_filepath]
            file_handler.write(line)
            last_chunk_filepath  = chunk_filepath
    # output last one
    yield last_chunk_filepath


def json_to_csv(json_filepath: Path) -&gt; Path:
    csv_filename = f'{json_filepath.stem}.csv'
    csv_filepath = json_filepath.parent / csv_filename
    with csv_filepath.open('w', encoding='utf8') as csv_out, json_filepath.open('r', encoding='utf8') as json_in:
        dwriter = csv.DictWriter(csv_out)
        headers_written = False
        for json_line in json_in:
            data = json.loads(json_line)
            if not headers_written:
                # create header record
                headers = {k:k for k in data.keys()}
                dwriter.writeline(headers)                
                headers_written = True
            dwriter.writeline(data)
    return csv_filepath


with ProcessPoolExecutor() as pool:
    futures = []
    for chunk_filepath in chuck_file_by_line(source_big_file):
        future = pool.submit(json_to_csv, chunk_filepath)
        futures.append(future)

    # wait for all to finish
    for future in futures:
        csv_filepath = future.result(timeout=None)  # waits until complete
        print(f'conversion complete&gt; csv filepath: {csv_filepath}')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Since you have many files, the simplest multiprocessing example from the documentation should work for you. <a href="https://docs.python.org/3.4/library/multiprocessing.html?highlight=process" rel="nofollow noreferrer">https://docs.python.org/3.4/library/multiprocessing.html?highlight=process</a></p>
<pre><code>f(JsonFile):
    # open input, output files and convert

with Pool(16) as p:
    p.map(f, os.listdir(os.getcwd()))
</code></pre>
<p>You could also try replacing <code>listdir</code> with <a href="https://docs.python.org/3/library/os.html#os.scandir" rel="nofollow noreferrer"><code>os.scandir()</code></a>, which doesn't have to return all directory entries before starting.</p>
</div>
<span class="comment-copy">I'd suggest reading up on Threads in Python.  Or maybe the Python 3 has actual coprocessing ops, which are like lighter weight threads.  I'm back at Python 2.7, so I don't know what cool new stuff is in Python 3.  I would think that simple threads in Python would be simpler either way, since you aren't going to want to have too many threads running at once (certainly not one for each file)</span>
<span class="comment-copy">Look at the <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer"><code>concurrent.futures</code></a> module. You want to run one thread per file. Define a function that takes the filename/fileobject and does what you are doing here and then use <code>Pool.map</code> or similar. I don't know if the <code>json</code> module releases the GIL while parsing so try to use the <code>ThreadPoolExecutor</code> and if you don't see more than one core being used change it to <code>ProcessPoolExecutor</code>.</span>
<span class="comment-copy">Thanks restricting might add bit complexity to handle multiple threads. Any other suggestions to handle single file/processor. As these files will be processed further in downstream processing, so want to avoid the errors. Appreciate your feedback. Thanks</span>
<span class="comment-copy">So you want to merge back to a single file?</span>
<span class="comment-copy">No. Each file will be processed and converted separately. Having thousands of files, want to distribute the load across 16 CPUs</span>
<span class="comment-copy">Tried couple of executions.</span>
<span class="comment-copy">Tried couple of executions. without multiprocessing - execution completes in 85-90 secs where 11 files (3 GB) are converted With multiprocessing - its taking more time. Completes in 150 secs  Not sure, what is happening?</span>
<span class="comment-copy">I tried with pandas data frames, but performance was very bad. Like, execution was taking 5 mins, where in current code will finish it in 15 secs.</span>
<span class="comment-copy">added few more lines for multiprocessing, but disappointed with output. I was able to process in 180 secs with single CPU and its taking 150 secs with 16 CPU. Any better way to do it? file conversion is separate function &amp; passing the arguments here.   if <b>name__=='__main</b>':     worker_count = 16     processes = []     for i in range(worker_count):         p = multiprocessing.Process(target=fileConversion, args=('/stagingData/Scripts/test',))         processes.append(p)         p.start()     for process in processes:         process.join()</span>
<span class="comment-copy">It could be that most of the time is spent reading and writing to storage and that you would only see a dramatic speedup if your conversion task was more CPU intensive.</span>
<span class="comment-copy">While running program, i looked at the CPU usage which is showing more than 95% utilization for all 16 CPUs. So, that concludes program is CPU intensive</span>
