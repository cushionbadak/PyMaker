<div class="post-text" itemprop="text">
<p>I have a 50K list of strings (city names) and I need a the smallest list of character tri-grams (prefarably n-grams) where every string is at least hit once by one tri-gram. Consider the following list:
    ['amsterdam', 'rotterdam', 'haarlem', 'utrecht', 'groningen']</p>
<p>the list of identifying trigrams is 4 long and should be (alternatives possible):</p>
<pre><code>['ter', 'haa', 'utr', 'gro']
</code></pre>
<p>I thought my solution finds the correct right answer but it gave the wrong answers when used on other lists. </p>
<pre><code>from collections import Counter

def identifying_grams(list, n=3):

    def f7(seq):
        seen = set()
        seen_add = seen.add
        return [x for x in seq if not (x in seen or seen_add(x))]

    def ngrams(text, n=3):
        return [text[i:i + n] for i in range(len(text) - n + 1)]

    hits = []
    trigrams = []
    for item in list:
      #  trigrams += ngrams(item)
        trigrams += f7(ngrams(item))

    counts = Counter(trigrams).most_common()

    for trigram, count in counts:
        items = []
        for item in list:
            if trigram in item:
                hits.append(trigram)
                items.append(item)
        for i in items:
            list.remove(i)

    return(f7(hits))

list1 = ['amsterdam','rotterdam','haarlem','utrecht','groningen']
print(identifying_grams(list1))
# Good, we get: ['ter', 'haa', 'utr', 'gro']

list2 = ['amsterdam','schiedam']
print(identifying_grams(list2))
# Good, we get: ['dam']

list3 = ['amsterdam','schiedam','terwolde','wolstad']
print(identifying_grams(list3))
# Ouch, we get: ['ter', 'dam', 'wol']
# this should be ['dam', 'wol'] as this is only 2 trigrams that identify the list...
</code></pre>
<p>I got two answers so far, but both of them have flaws. The one from Rupesh is good for lists that are smaller then 10 items. My lists have over 50K items. The one from mujjiga does come up with a solution albeit not the perfect one.</p>
<p>A bounty for the Python Ninja who comes up with a perfect solution that scales.
Bonus kuddos if it performs well and gives same solution every time it runs!</p>
</div>
<div class="post-text" itemprop="text">
<p>Here's a theoretical analysis of @mujjiga answer:</p>
<p>You can create classes of words that share the same ngram. You want to pick the smallest number of those classes (that is the smallest number of ngrams) that covers the whole set of words. This is the <a href="https://en.wikipedia.org/wiki/Set_cover_problem" rel="nofollow noreferrer">set cover problem</a>. Unfortunately, this problem is NP-hard (<em>not</em> NP-complete <em>, thanks @mujjiga</em>). (<em>EDIT: Hence, there is no known solution that will give you the expected result in a reasonable time.</em>) The greedy algorithm is almost the best solution (see <a href="https://cs.stackexchange.com/questions/49777/is-greedy-algorithm-the-best-algorithm-for-set-cover-problem">https://cs.stackexchange.com/questions/49777/is-greedy-algorithm-the-best-algorithm-for-set-cover-problem</a>).</p>
<p>Note that even the greedy algorithm may give weird results. Take the sets <code>{a, b}, {b, c}, {c, d}</code> and the superset <code>{a, b, c, d}</code>. The three subsets are maxmimum. If you take <code>{b, c}</code> first, you need the two other subsets to cover the superset. If you take <code>{a, b}</code> or <code>{c, d}</code>, two subsets are enough.</p>
<p>Let's use the greedy algorithm, and consider the implementation. The code to create the dictionary that maps ngrams to words is pretty straightforward:</p>
<pre><code>all_words= ['amsterdam','schiedam','werkendam','amstelveen','schiebroek','werkstad','den haag','rotjeknor','gouda']
n=3
words_by_ngram = {}
for word in all_words:
    for ngram in (word[i:i+n] for i in range(0, len(word)-n+1)):
        words_by_ngram.setdefault(ngram, set()).add(word)
</code></pre>
<p>The <code>setdefault</code> is equivalent to <code>get</code> if the key <code>ngram</code> exists, and create an empty set otherwise. This is <code>O(|all_words|*|len max word|)</code> complexity.</p>
<p>Now, we want to take the ngram with the most words and remove those words from the dictionary. Repeat until you get the words you want.</p>
<p>Here's the simple version:</p>
<pre><code>s = set(all_words) # the target
gs = set()
d = words_by_ngram.copy() # for the display
while s:
    # take the the best ngram
    ngram, words = max(d.items(), key=lambda i: len(i[1])) # sort on word count
    # remove the words from the dictionary and delete the ngrams whose words have been already found
    d = {k:v for k, v in ((k, v - words) for k, v in d.items()) if len(v)}
    gs.add(ngram) # add the ngram to the result
    s -= words # remove the words from the target

# check
assert set().union(*[words_by_ngram[g] for g in gs]) == set(all_words)
# display
for g in gs:
    print("{} -&gt; {}".format(g, words_by_ngram[g]))
</code></pre>
<p>Output:</p>
<pre><code>ams -&gt; {'amstelveen', 'amsterdam'}
gou -&gt; {'gouda'}
wer -&gt; {'werkstad', 'werkendam'}
rot -&gt; {'rotjeknor'}
dam -&gt; {'amsterdam', 'werkendam', 'schiedam'}
sch -&gt; {'schiebroek', 'schiedam'}
den -&gt; {'den haag'}
</code></pre>
<p>This second step has a <code>O(|all_words|*|ngrams|)</code> complexity because of the loop to find the max and the update of the dictionary. Hence, the overall complexity is <code>O(|all_words|*|ngrams|)</code></p>
<p>It is possible to reduce the complexity with a priority queue. Retrieving the best ngram has a cost of <code>O(1)</code>, but updating the <code>len</code> of the words mapped to a ngram has a priority <code>O(lg |ngrams|)</code>:</p>
<pre><code>import heapq
class PriorityQueue:
    """Adapted from https://docs.python.org/3/library/heapq.html#priority-queue-implementation-notes
    A prority of 1 invalidates the entries
    """
    def __init__(self, words_by_ngram):
        self._d = {ngram:[-len(words), (ngram, words)] for ngram, words in words_by_ngram.items()}
        self._pq = list(self._d.values())
        heapq.heapify(self._pq)

    def pop(self):
        """get the ngram, words tuple with the max word count"""
        minus_len, (ngram, words) = heapq.heappop(self._pq)
        while minus_len == 1: # entry is not valid
            minus_len, (ngram, words) = heapq.heappop(self._pq)
        return ngram, words

    def update(self, ngram, words_to_remove):
        """remove the words from the sets and update priorities"""
        del self._d[ngram]
        ngrams_to_inspect = set(word[i:i+n] for i in range(0, len(word)-n+1)
                        for word in words_to_remove)
        for ngram in ngrams_to_inspect:
            if ngram not in self._d: continue
            self._d[ngram][0] = 1 # use the reference to invalidate the entry
            [L, (ngram, words)] = self._d[ngram]
            words -= words_to_remove
            if words:
                self._d[ngram] = [-len(words), (ngram, words)] # new entry
                heapq.heappush(self._pq, self._d[ngram]) # add to the pq (O(lg ngrams))
            else: # nothing left: remove it from dict
                del self._d[ngram]


pq = PriorityQueue(words_by_ngram)
gs = set()
s = set(all_words) # the target
while s:
    # take the the best ngram
    ngram, words = pq.pop()
    gs.add(ngram) # add the ngram to the result
    s -= words # remove the words from the target
    # remove the words from the dictionary and update priorities
    pq.update(ngram, words)
</code></pre>
<p>With this code, the overall priority falls to <code>O(|all_words|*|lg ngrams|)</code>. That being said, I would be curious to know if this is faster than the naive previous version with you 50k items.</p>
</div>
<div class="post-text" itemprop="text">
<p>Below is an implementation of the greedy algorithm for set cover. It runs in about half a second on 50,000 English dictionary words on my machine. The output isn't always optimal, but it's often close in practice. You could probably solve your instances to optimality with an external library for integer programming, but I don't know if you want to go in that direction.</p>
<p>The code below dynamically maintains the bipartite graph of ngrams and uncovered texts. The one subtle bit is that, since Python lacks an intrusive heap in its standard library, I've exploited the fact that the keys only increase to fake one. Every ngram is in the heap with a score less than or equal to what it should be. When we pull the minimum, if it is less than it should be, we put it back with the updated value. Otherwise, we know that it's the true minimum.</p>
<p>This code should produce deterministic output. At each step it chooses the lexicographically minimum ngram that covers the maximum number of uncovered texts.</p>
<pre><code>import collections
import heapq


def ngrams_from_text(text, n):
    return {text[i:i + n] for i in range(len(text) - n + 1)}


def greedy_ngram_cover(texts, n):
    neighbors_of_text = {text: ngrams_from_text(text, n) for text in texts}
    neighbors_of_ngram = collections.defaultdict(set)
    for text, ngrams in neighbors_of_text.items():
        for ngram in ngrams:
            neighbors_of_ngram[ngram].add(text)
    heap = [(-len(neighbors), ngram)
            for (ngram, neighbors) in neighbors_of_ngram.items()]
    heapq.heapify(heap)
    cover = []
    while neighbors_of_text:
        score, ngram = heapq.heappop(heap)
        neighbors = neighbors_of_ngram[ngram]
        if score != -len(neighbors):
            heapq.heappush(heap, (-len(neighbors), ngram))
            continue
        cover.append(ngram)
        for neighbor in list(neighbors):
            for neighbor_of_neighbor in neighbors_of_text[neighbor]:
                neighbors_of_ngram[neighbor_of_neighbor].remove(neighbor)
            del neighbors_of_text[neighbor]
    return cover


print(
    greedy_ngram_cover(
        ['amsterdam', 'rotterdam', 'haarlem', 'utrecht', 'groningen'], 3))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Above solution is failing because, </p>
<ul>
<li><code>Counter</code> is returning trigrams in a not ordered manner, so if you run your solution multiple times, you will get the needed solution also randomly</li>
<li>And you are returning the combination as soon as you find it, you are neither going in the order of length nor finding the best combination among all the combinations which satisfies the problem</li>
</ul>
<p>Here I'm going in the order of least to highest elements contained trigram list, Then returning as soon as I found the solution.</p>
<pre><code>from itertools import permutations

def checkTrigramsPresentInList(trigrams_list,input_list):
    for input_string in input_list:
        flag = False
        for trigram in trigrams_list:
            if trigram in input_string:
                flag = True
        if not flag:
            return False
    return True

def ngrams(text, n=3):
        return [text[i:i + n] for i in range(len(text) - n + 1)]

def identifying_grams(input_list, n=3):
    trigrams = []
    for item in input_list:
        trigrams += ngrams(item)
    len_of_trigrams = len(trigrams)
    trigrams_unique = list(set(trigrams))
    idx =1
    correct_tri_lists = []
    unique_trigrams_list = []
    while idx &lt;= len_of_trigrams:
        trigrams_lists = permutations(trigrams_unique,idx)

        for trigrams_list in trigrams_lists:
            print(trigrams_list)
            if not trigrams_list in unique_trigrams_list:
                if checkTrigramsPresentInList(list(trigrams_list),input_list):
                    correct_tri_lists.append(list(trigrams_list))
            ##Uncomment below lines if only one combination is needed
                if correct_tri_lists:
                    return correct_tri_lists
                    unique_trigrams_list.append(trigrams_list)
        idx = idx+1


list1 = ['amsterdam','rotterdam','haarlem','utrecht','groningen']
print(identifying_grams(list1))
# # Good, we get: ['ter', 'haa', 'utr', 'gro']

list2 = ['amsterdam','schiedam']
print(identifying_grams(list2))
# # Good, we get: ['dam']

list3 = ['amsterdam','schiedam','terwolde','wolstad']
print(identifying_grams(list3))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>from nltk.util import ngrams

def load_dictonary(cities, n=3):
    ngram2cities = {}
    for city in cities:
        grams = [''.join(x) for x in ngrams(city,n)]
        for g in grams:
            if g in ngram2cities and city not in ngram2cities[g]:
                ngram2cities[g].append(city)
            else:
                ngram2cities[g] = [city]                
    return ngram2cities

def get_max(the_dict):
    n = 0
    the_max_key = None
    for key in the_dict :
        size = len(the_dict[key])
        if size &gt; n:
            n = size
            the_max_key = key
    return the_max_key

def get_min_ngrams(cities, n=3):
    selected_ngrams = list()
    ngram2cities = load_dictonary(cities, n)
    ngram = get_max(ngram2cities)
    while ngram is not None:
        cities_covered = ngram2cities[ngram]
        ngram2cities.pop(ngram)
        selected_ngrams.append(ngram)        
        for city in cities_covered:
            for n in ngram2cities:
                if city in ngram2cities[n]:
                    ngram2cities[n].remove(city)
        ngram = get_max(ngram2cities)
    return selected_ngrams

cities_1 = ['amsterdam','rotterdam','haarlem','utrecht','groningen']
cities_2 = ['amsterdam','schiedam','terwolde','wolstad']
cities_3 = ['amsterdam','schiedam']
cities_4 = ['amsterdam','walwalwalwaldam']

print (get_min_ngrams(cities_1))
print (get_min_ngrams(cities_2))
print (get_min_ngrams(cities_3))
print (get_min_ngrams(cities_4))
</code></pre>
<p>Output:</p>
<p><code>['ter', 'utr', 'gro', 'lem']
 ['wol', 'dam']
 ['dam']
 ['dam']</code></p>
<ol>
<li>Create a dictionary of structure {'ngram': list of cities which contain this ngram }</li>
<li><ol>
<li>Find the ngram (say x) which is covered in most cities (greedy approach) and remove this ngram and add  it to solution </li>
</ol></li>
<li><ol start="2">
<li>Now we dont have to worry about cities covered by the above selected ngram x, so we go trough the dictionary and remove the cities covered by x. </li>
</ol></li>
<li>Repeat from step 1 till you find no more ngrams </li>
</ol>
<p><strong>Why is the above solution not always optimal</strong> : As mentioned by others the above algorithm is greedy and this problem can be reduced to set-cover which has no  deterministic polynomial time solution. So unless you want to win  $1 million prize it is futile to solve for a polynomial time algorithm which gives optimal solution.  So the next best solution is greedy. Lets look at how bad the greedy solution will be compared to optimal solution</p>
<p><strong>How bad is greedy</strong>: If there are X cities and if the best solution is <code>c</code> (i.e you will need <code>c</code> ngrams to cover all the X cities then the greedy solution cannot be worst then <code>c*ln m</code>. So if you have <code>50K</code> cities then the greedy solution will be off by maximum of <code>10.8197782844</code> times the optimal. </p>
</div>
<span class="comment-copy"><code>I need a the smallest list of character tri-grams (prefarably n-grams) so that every string is at least once hit by every tri-gram</code> how is 'ter' a solution if it is not there in <code>haarlem</code></span>
<span class="comment-copy">"haa" matches "haarlem", "ter" matches "rotterdam" and "amsterdam"</span>
<span class="comment-copy">I need the smallest possible list of tri-grams that hits every item in the cities list at least once.</span>
<span class="comment-copy">I propose you change your wording from <i>so that every string is at least once hit by every tri-gram</i> <b>to</b> <i>so that every string is at least once hit by <b>a</b> trigram</i></span>
<span class="comment-copy">good point. Changed the question</span>
<span class="comment-copy">Darn, I hoped you solved the problem and brought me one step closer to solving P = NP and thus 1.000.000 $. en.wikipedia.org/wiki/Millennium_Prize_Problems.</span>
<span class="comment-copy">@JordenvanForeest Sorry! I already solved the problem and took the 1.000.000 $: P = NP &lt;=&gt; P=0 or N=1.</span>
<span class="comment-copy"><code>The decision version of set covering is NP-complete, and the optimization/search version of set cover is NP-hard</code> so should not finding the minimal set cover be NP-hard problem rather then NP-complete ?</span>
<span class="comment-copy">@mujjiga You are right (see my edit). Thanks!</span>
<span class="comment-copy">The set cover approach is the one to choose imho. It has the double advantage of tackling exactly the problem (which is just a reformulation of the DS problem somehow), and of being a very classical problem for which plenty of algorithms and implementations already exist.</span>
<span class="comment-copy">The commenting out part does not seem to work.... But not required. And it seems to give the correct output, but every time a different one...</span>
<span class="comment-copy">I get memory error if I do a list of 10 items e.g.:['amsterdam','schiedam','werkendam','amstelveen','schiebroek','werkstad','den haag','rotjeknor','gouda']</span>
<span class="comment-copy">Yes it might get because we are following completely brute-force method here, We can prevent memory error by creating generator instead of <code>list(permutations(trigrams,idx))</code> and traverse over it. But I cannot guarantee the speed of execution. As there will be a lot of trigram combinations. As the number of elements increases, speed decreases.</span>
<span class="comment-copy">I get a different output then you.</span>
<span class="comment-copy">['ter', 'dam', 'wol'] on the second one.... and this is not the correct one.</span>
<span class="comment-copy">and I get ['wal','dam'] on this list: cities_4 = ['amsterdam','walwalwalwaldam']. only ['dam'] is sufficient.</span>
<span class="comment-copy">@JordenvanForeest There was a bug in the code which was adding the same repeated ngrams. I have fixed and update the output I am getting.</span>
<span class="comment-copy">@JordenvanForeest you are correct its is giving different output for ['ter', 'dam', 'wol'] when I run it on a different machine :( I know why but tricky to fix. Issue is with greedy programming</span>
