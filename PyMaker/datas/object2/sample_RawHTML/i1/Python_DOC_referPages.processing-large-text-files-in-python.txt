<div class="post-text" itemprop="text">
<p>I am working on 2 large files. The master file has 2 fields say customer name and second field is customer ID. I have a second file which is a subset of the first file and has only customer names. 
I wish to find the customer ID of the names which exist in my the subset file. </p>
<p>The first file has 30 million lines and the second file has 5 million lines.</p>
<p>I am trying to do it using dictionary but it is taking huge time.</p>
<p>Can you please suggest me a better way to do it.</p>
<p>Here is the snippet from my code and few lines from my files.</p>
<p><em>Master file</em></p>

#

<p>John 2343245</p>
<p>Karim 126754</p>
<p>Rob 6543289</p>
<p>Vijay 2247861</p>
<p>Sam 2649860</p>
<p>....</p>
<p><em>Subset second file</em></p>
<p>Sam</p>
<p>Rob</p>
<p>John</p>
<pre><code>def extract_info(sub_file,master_file):
    sub_fh = open(sub_file,'r',16777216)
    sub_inst_list = []
    for line in sub_fh:
        if line.startswith('#'):
            continue
        else:
            name = line.rstrip()
            sub_inst_list.append(name)
    sub_fh.close()


out_file = "results.rpt"
outf = open(out_file,'w')
bunchsize = 10000
bunch = []
master_fh = open(master_file,'r',16777216)
for line in master_fh:
    if line.startswith('#'):
        continue
    else:
        data = line.split()
        name = data[0]
        if str(data[1]) == "n/a":
            continue
        else:
            if name in sub_inst_list:
                id = str(data[1])
                line = "%-80s %-15s\n" % (name, id)
                bunch.append(line)
                if len(bunch) == bunchsize: 
                    outf.writelines(bunch)
                    bunch= []
                outf.writelines(bunch)
  master_fh.close()
  outf.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>A better way is to put all the data from the master file into a database and then look up the values based on the keys from the second file:</p>
<pre><code>import sqlite3

conn = sqlite3.connect(":memory:")
c = conn.cursor()
c.execute("CREATE TABLE data (Name VARCHAR(255), ID INT)")

# fill the DB
with open("master.txt") as f:
    for line in f:
        c.execute("INSERT INTO data VALUES (?, ?)", line.split())
conn.commit()

# search for data
with open("slave.txt") as f:
    for line in f:
        print(c.execute("SELECT ID FROM data WHERE Name=:search_name", {"search_name": line.strip()}).fetchall())

conn.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Another potential solution (likely slower than ForceBru's but hey it was fun to code XD) is to use threads to drastically cut down on time:</p>
<pre><code>from queue import Queue
from threading import Thread

q = Queue()
number_of_threads = 3


def get_customer_id():
    while True:
        customer_name = q.get()
        with open('Master.txt', 'r') as f:
            for line in f.readlines():
                if customer_name.strip() in line:
                    print(line.strip())
                    break
        q.task_done()


with open('Slave.txt', 'r') as f:
    for line in f.readlines():
        q.put(line)

for i in range(number_of_threads):
    new_thread = Thread(target=get_customer_id)
    new_thread.setDaemon(True)
    new_thread.start()


print('main thread waiting')
q.join()
print('done')
</code></pre>
<p>You could up the number of threads to say 100-200 maybe and let them crank away! This will be very computationally expensive though as you have a maximum number of iterations of nearly 125,000,000,000,000 worst case. That's pretty exaggerated though because the <code>break</code> statement should cut a significant amount of those iterations out. And if it's running in 100 threads then you can possibly divide the number by 100 after reducing it from the <code>break</code> already (Assuming you are not already maxing CPU usage in which case multiprocessing would be superior). Still though LOOOOTS of computations with this method.</p>
<p>This is essentially doing the same thing your initial script is doing, but running it many times faster by dividing and conquering!</p>
</div>
<span class="comment-copy">why not use a proper database? those are made for this kind of queries.</span>
<span class="comment-copy">If you are trying to improve times, why not use multi-threading or multi-processing?</span>
<span class="comment-copy">Thanks ForceBru for providing the code. Is there a way I can get both the name and ID into a file instead of writing on the shell.</span>
<span class="comment-copy"><a href="https://docs.python.org/3/library/sqlite3.html" rel="nofollow noreferrer">Of</a> <a href="https://www.w3schools.com/python/python_file_write.asp" rel="nofollow noreferrer">course</a>.</span>
<span class="comment-copy">Guys, I tried what was suggested by @ForceBru but I still see that the processing the file takes a huge run time. I reported the time for every 1000 lines searched.   debug:Writing to output file 1000 lines  Time take to write 1000 lines =18102 debug:Writing to output file 2000 lines  Time take to write 1000 lines =19825 debug:Writing to output file 3000 lines  Time take to write 1000 lines =21060. Is it expected? I am new to using database in python. Please help.</span>
<span class="comment-copy">Guys, I tried what was suggested by ForceBru but I still see that the processing the file takes a huge run time. I reported the time for every 1000 lines searched.   debug:Writing to output file 1000 lines  Time take to write 1000 lines =18102 debug:Writing to output file 2000 lines  Time take to write 1000 lines =19825 debug:Writing to output file 3000 lines  Time take to write 1000 lines =21060. Is it expected? I am new to using database in python. Please help.</span>
<span class="comment-copy">@user3770616 why not try to use the database and threaded methods combined then? I’m not sure if sql can be threaded, but if so that seems like a good solution</span>
<span class="comment-copy">@user3770616 I’m not experienced in databases either, so I’m not sure if these times are expected. You’re saying it is 18,000 seconds to compute 1000 lines as in 18 seconds per line? That may not be unreasonable for searching through 5,000,000 lines...</span>
<span class="comment-copy">At the pace at which it is progressing to search through 5 million lines it would take 3years. There should be some better way to do it. SQL with Threading might be one of it. Any other idea or thoughts? Thanks.</span>
<span class="comment-copy">@akashaya I'm sorry. Besides combining them I can't really think of anything other than upgrading your hardware. I would try combining them first though because you could potentially turn 18 seconds a line to .18 seconds per line with multiprocessing or multithreading (depending on I/O and CPU usage). If you could achieve this speed the computation would only take 10 days instead of 1000 though</span>
