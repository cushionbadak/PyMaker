<div class="post-text" itemprop="text">
<p>My program needs to spawn multiple instances of a class, each processing data that is coming from a streaming data source.</p>
<p>For example:</p>
<pre><code>parameters = [1, 2, 3]

class FakeStreamingApi:
    def __init__(self):
        pass

    def data(self):
        return 42
    pass

class DoStuff:
    def __init__(self, parameter):
        self.parameter = parameter

    def run(self):
        data = streaming_api.data()
        output = self.parameter ** 2 + data # Some CPU intensive task
        print output

streaming_api = FakeStreamingApi()

# Here's how this would work with no multiprocessing
instance_1 = DoStuff(parameters[0])
instance_1.run()
</code></pre>
<p>Once the instances are running they don't need to interact with each other, they just have to get the data as it comes in. (and print error messages, etc)</p>
<p>I am totally at a loss how to make this work with multiprocessing, since I first have to create a new instance of the class DoStuff, and then have it run.</p>
<p>This is definitely not the way to do it:</p>
<pre><code># Let's try multiprocessing
import multiprocessing

for parameter in parameters:
    processes = [ multiprocessing.Process(target = DoStuff, args = (parameter)) ]

# Hmm, this doesn't work...
</code></pre>
<p>We could try defining a function to spawn classes, but that seems ugly:</p>
<pre><code>import multiprocessing

def spawn_classes(parameter):
    instance = DoStuff(parameter)
    instance.run()

for parameter in parameters:
        processes = [ multiprocessing.Process(target = spawn_classes, args = (parameter,)) ]

# Can't tell if it works -- no output on screen?
</code></pre>
<p>Plus, I don't want to have 3 different copies of the API interface class running, I want that data to be shared between all the processes... and as far as I can tell, multiprocessing creates copies of everything for each new process.</p>
<p>Ideas?</p>
<p><strong>Edit:</strong>
I think I may have got it... is there anything wrong with this?</p>
<pre><code>import multiprocessing

parameters = [1, 2, 3]

class FakeStreamingApi:
    def __init__(self):
        pass

    def data(self):
        return 42
    pass

class Worker(multiprocessing.Process):
    def __init__(self, parameter):
        super(Worker, self).__init__()
        self.parameter = parameter

    def run(self):
        data = streaming_api.data()
        output = self.parameter ** 2 + data # Some CPU intensive task
        print output

streaming_api = FakeStreamingApi()

if __name__ == '__main__':
    jobs = []
    for parameter in parameters:
        p = Worker(parameter)
        jobs.append(p)
        p.start()
    for j in jobs:
        j.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I came to the conclusion that it would be necessary to use multiprocessing.Queues to solve this. The data source (the streaming API) needs to pass copies of the data to all the different processes, so they can consume it.</p>
<p>There's another way to solve this using the multiprocessing.Manager to create a shared dict, but I didn't explore it further, as it looks fairly inefficient and cannot propagate changes to inner values (e.g if you have a dict of lists, changes to the inner lists will not propagate).</p>
</div>
<span class="comment-copy">If you need a group of processes to split up work, you could use the pool functionality of the multiprocessing library: <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer">docs.python.org/3/library/â€¦</a></span>
<span class="comment-copy">The structure of my program is that I will have several instances of Worker() running in parallel, and each instance will run forever (more or less) after being started. I won't be starting and stopping instances during the program run.   I'm not really sure what the benefit of using Pool would be for this situation, can you explain?</span>
