<div class="post-text" itemprop="text">
<p>We are building an LSTM for modeling a physical optical procees. </p>
<p>So far, I have produced the following code in python using Keras with Tensorflow backend.</p>
<pre><code>#Define model
model = Sequential()
model.add(LSTM(128, batch_size=BATCH_SIZE, input_shape=(train_x.shape[1],train_x.shape[2]), return_sequences=True, stateful=False ))#,,return_sequences=Tru# stateful=True 
model.add(Dense(2, activation='softmax'))
opt = tf.keras.optimizers.Adam(lr=0.01, decay=1e-6)

#Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,#,
    verbose=1)

#Now I want to make sure that the we can predict the training set (using evaluate) and that it is the same result as during training
score = model.evaluate(train_x, train_y, batch_size=BATCH_SIZE, verbose=0)
print(' Train accuracy:', score[1])
</code></pre>
<p>The Output of the code is</p>
<pre><code>Epoch 1/10 5872/5872 [==============================] - 0s 81us/sample - loss: 0.6954 - acc: 0.4997
Epoch 2/10 5872/5872 [==============================] - 0s 13us/sample - loss: 0.6924 - acc: 0.5229 
Epoch 3/10 5872/5872 [==============================] - 0s 14us/sample - loss: 0.6910 - acc: 0.5256
Epoch 4/10 5872/5872 [==============================] - 0s 13us/sample - loss: 0.6906 - acc: 0.5243 
Epoch 5/10 5872/5872 [==============================] - 0s 13us/sample - loss: 0.6908 - acc: 0.5238

Train accuracy: 0.52480716
</code></pre>
<p>So the problem is that the final modeling accuracy (0.5238) should be equal (evaluation) accuracy (0.52480716) which it is not. What have I done wrong here any help highly appreciated</p>
</div>
<div class="post-text" itemprop="text">
<p><em>"Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches."</em></p>
<p><a href="https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss" rel="nofollow noreferrer">https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss</a></p>
<p>For evaluation, since the trained model is taken, the accuracy is higher.</p>
</div>
<div class="post-text" itemprop="text">
<p>Thank you
as shown below, it is not, which I dont understand</p>
<pre><code>model = Sequential()
model.add(LSTM(32, batch_size=BATCH_SIZE, input_shape=(train_x.shape[1],train_x.shape[2]), return_sequences=True, stateful=False ))#,,return_sequences=Tru# stateful=True 
model.add(Dense(2, activation='softmax'))
opt = tf.keras.optimizers.Adam(lr=0.01, decay=1e-6)

#Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=opt,
    metrics=['accuracy']
)

#Train model
model.fit(
    train_x, train_y,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    verbose=1,
    shuffle=False,
    validation_data=(validation_x, validation_y)]
)

score = model.evaluate(validation_x, validation_y, batch_size=BATCH_SIZE, verbose=0)

print(' Validation accuracy:', score[1])
</code></pre>
<p>Output</p>
<pre><code> Epoch 1/5 5872/5872 [==============================] - 3s 554us/sample - loss: 0.6923 - acc: 0.5154 - val_loss: 0.7149 - val_acc: 0.4668
 Epoch 2/5 5872/5872 [==============================] - 2s 406us/sample - loss: 0.6895 - acc: 0.4983 - val_loss: 0.7218 - val_acc: 0.4821
 Epoch 3/5 5872/5872 [==============================] - 2s 404us/sample - loss: 0.6890 - acc: 0.4940 - val_loss: 0.7230 - val_acc: 0.4821
 Epoch 4/5 5872/5872 [==============================] - 2s 406us/sample - loss: 0.6883 - acc: 0.4928 - val_loss: 0.7336 - val_acc: 0.4592
 Epoch 5/5 5872/5872 [==============================] - 2s 404us/sample - loss: 0.6881 - acc: 0.4934 - val_loss: 0.7278 - val_acc: 0.4745

 Validation accuracy: 0.45663264
</code></pre>
</div>
<span class="comment-copy">Thank you for this. But I use a single batch, keep in mind I evaluate the model on training data and not testing data. So the training data during evaluation presumable should give the same result as "during" training.</span>
<span class="comment-copy">Yes, I saw that you've used training data in both cases. The difference is because the model weights were different during training and after training.</span>
<span class="comment-copy">Thanks, but I dont understand this. Why are they different? In the end of epoch 5 (which is the last epoch and final model) this model should correspond to the evaluation. Same happens for a validation set (not shown). The validation accuracy during training is not the same as after evalutation. Notice there is no dropout that can introduce different between training and evalutaion/prediction. If this is the case then we cannot count on our model accuracy in keras once we have developed a model. Im sure Im missing out a concept, can you elaborate? Many thanks</span>
<span class="comment-copy">If the validation_data(same as train_x,train_y) was provided during training to model.fit() then the validation loss/accuracy at the end of epoch 5 would be the same as  the output of model.evaluate.</span>
<span class="comment-copy"><a href="https://stackoverflow.com/a/55183699/11204517">stackoverflow.com/a/55183699/11204517</a></span>
