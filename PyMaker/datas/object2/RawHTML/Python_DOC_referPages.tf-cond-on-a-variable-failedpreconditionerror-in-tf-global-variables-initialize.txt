<div class="post-text" itemprop="text">
<p>I am running into  FailedPreconditionError error in <code>tf.global_variables_initializer()</code>. I have zeroed-in on the following part of the code to be the culprit:</p>
<pre><code>def __init__(...):
    ...
    self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')
    ...
    step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)

def step_rampup(self, global_step, rampup_length):
    result = tf.cond(global_step &lt; rampup_length,
                     lambda: tf.constant(0.0),
                     lambda: tf.constant(1.0))
    return tf.identity(result, name="step_rampup")
session.run(tf.global_variables_initilizer())
</code></pre>
<p><code>self.global_step</code> is to be incremented by <code>1</code> by optimizer at each iteration. It's value has to change. So, that is the behavior I want.</p>
<p>Error message:</p>
<pre><code>FailedPreconditionError ...
506         with tf.Session(graph=highgraph) as session:
--&gt; 507             session.run(tf.global_variables_initializer())
...
FailedPreconditionError: Attempting to use uninitialized value global_step
 [[node global_step/read (defined at NML_U/sNeural.py:103)  = Identity[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"](global_step)]]
</code></pre>
<p>Why is that part of the code is culprit?
Because, The following code works</p>
<pre><code>def __init__(...):
    ...
    self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')
    ...
    step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)

def step_rampup(self, global_step, rampup_length):
    result = tf.cond(global_step.initialized_value() &lt; rampup_length,
                     lambda: tf.constant(0.0),
                     lambda: tf.constant(1.0))
    return tf.identity(result, name="step_rampup")
session.run(tf.global_variables_initilizer())
</code></pre>
<p>but that will evaluate the conditional with the initialized value of <code>self.global_step(=0)</code> each time which is not the intended behavior</p>
<p>Also,</p>
<p>This code works as well:</p>
<pre><code>def __init__(...):
    ...
    self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')
    self.global_step = tf.assign(self.global_step,0.)
    ...
    step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)

def step_rampup(self, global_step, rampup_length):
    result = tf.cond(global_step &lt; rampup_length,
                     lambda: tf.constant(0.0),
                     lambda: tf.constant(1.0))
    return tf.identity(result, name="step_rampup")
session.run(tf.global_variables_initilizer())
</code></pre>
<p>But (maybe) this will again not lead to the dependency on <code>global_step</code> but  instead on assign op which will keep assigning <code>0</code> to <code>self.global_step</code></p>
<p>How do I go about achieving the behavior?</p>
</div>
<div class="post-text" itemprop="text">
<p>You did not provide the full code, so I can only guess that you are perhaps calling <code>tf.global_variables_initializer()</code> <em>before</em> <code>__init__()</code>. Indeed the former will not initialize variables that are created after it has been called.</p>
</div>
<span class="comment-copy">Have you tried <code>tf.variables_initializer</code> ? Also consider adding full code</span>
<span class="comment-copy">Thank you for the answer. However, the issue was something else. Can be summarized by <a href="https://github.com/tensorflow/tensorflow/issues/8172" rel="nofollow noreferrer">github.com/tensorflow/tensorflow/issues/8172</a> . I was wrong in identifying the source of the issue. I was using result of step_rampup to compute adam_beta_2 for my adam optimizer and adam was not able to initialize its parameters because global_step was not initialized at the time of runtime trying to initialize adam's parameters and result of step_rampup is tf.cond(...) on global_step</span>
