<div class="post-text" itemprop="text">
<p>I am trying to run a program many times in parallel (with different arguments). I searched online and found that subprocess in python is a good way to do that. My code is the following:</p>
<pre><code>import subprocess
import os

models_path="~/"
procs = []
num_of_procs_running=0
i = 0
for model_name in os.listdir(models_path):
    if num_of_procs_running &gt; num_of_procs:
        for p in procs:
            p.wait()
        procs = []
        num_of_procs_running = 0
    elif model_name.endswith(".onnx"):
        name, ending = model_name.split(".")
        runner = './SOME_PROGRAM '+str(i)+'&gt; output.txt'
        i+=1
        procs.append(subprocess.Popen(runner,shell=True))
        num_of_procs_running += 1
        print("Total processes:",num_of_procs_running)
        print("\n")
for p in procs:
    p.wait()
</code></pre>
<p>I am getting Segmentation Fault(core dumped) if I am trying to run more than 56 subprocesses. My machine has 56 CPU's and and each CPU has 12 cores. How can I run more than 56 subprocesses, or maybe use threads?
Thanks.</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>1)</strong> As @danielpryden already <a href="https://stackoverflow.com/questions/52064036/segmentation-fault-core-dumped-when-trying-to-use-subprocess-in-python#comment91080917_52064036">suggested</a> don't invoke a shell:</p>
<pre><code>import subprocess

# bad (mocking your code)
p = subprocess.Popen("echo 1 &gt; output.txt", shell=True)
p.wait()

# better
fp = open("output.txt", "w")
p = subprocess.Popen(["echo", "1"], stdout=fp)

p.wait() # or do something else
fp.close() # remember to close file
</code></pre>
<p><strong>2)</strong> When reaching the maximum number of threads, it waits until <em>all</em> threads have finished, quite inefficent. What you seemingly try to achieve is exactly what <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer">multiprocessing.pool.Pool</a> does:</p>
<pre><code>from multiprocessing.pool import Pool

# mocking your code
modules = ["mod1.onnx", "mod2.onnx"]

def run(module):
  print("running", module)

# use the maximum number of threads by default
with Pool() as pool:
  pool.map(run, modules)
</code></pre>
<p><strong>3)</strong> The segmentation fault isn't caused by your Python application, but rather from whatever program you try to execute. If it's C++ have a look at <a href="https://en.cppreference.com/w/cpp/thread/thread/hardware_concurrency" rel="nofollow noreferrer">std::thread::hardware_concurrency</a>; You need to change the program code not your python script.</p>
</div>
<span class="comment-copy">Your first step in debugging this needs to be to find a way to reliably trigger the segfault when running your <code>MY_PROGRAM</code> executable directly. Then fire up a debugger (e.g. <code>gdb</code>) and see why it's crashing. Your Python code is not the problem here -- the problem lies in <code>MY_PROGRAM</code> somehow.</span>
<span class="comment-copy">As an aside: don't use <code>shell=True</code>, especially when you're assembling the arguments manually like this. Just pass a list of arguments directly.</span>
<span class="comment-copy">Is there a reason you do not use <code>name</code> and <code>ending</code>?</span>
<span class="comment-copy">@DavidCullen yes. It is being use in my original code. It's not necessary for this post.. It's one of the arguments sent to the program.</span>
<span class="comment-copy">@GuyOhayon: Mostly you should avoid passing arguments in one string. Why launch a whole separate process (the shell) just to parse a string into a list of strings for you? And if you accidentally assemble the string incorrectly, the shell could execute arbitrary commands you didn't expect, whereas if you pass a list of strings, that doesn't happen. But addressing your other comment: can you reproduce <i>exactly</i> the same command line in both cases? Can you try otherwise to reproduce the environment (match up env vars, redirect or close stdin/stdout, etc.)?</span>
<span class="comment-copy">Thank you! I am new to python and especially to parallel runs (that's actually my first python code ever) so obviously things might not be as efficient. I have two questions regarding your answer: 1) Could you please elaborate on how to use multiprocessing.pool? I don't see in your code anywhere that you call a script. Know that I have something like 500 models to run, so I will have to use a loop. 2) Why is it better to not invoke a shell? What is the theory behind?</span>
<span class="comment-copy">Ok great!! Where should I put the .wait() command? I am using pool and Popen but the program finishes and there are no processes on the backgroud which runs my program. I suppose I should put wait() somewhere?</span>
<span class="comment-copy">I put the .wait() right after I call the subprocess, but it seems that when I avoid shell=True the program don't run and my script just creates text files. When I get back to running with a shell and a long string everything seems to work fine. What might be the reason? I double checked my arg list..</span>
<span class="comment-copy">Ok, I will try using run(). Why did you pass str(idx) as an argument to the program? Like I said, suppose I have many arguments. When I try to use Popen like how you use run(), my programs isn't executed (only text files are created as stdout), and when I pass a long string containing all the arguments, everything is fine. How bad is it to use shell=True? Does it consumes a lot of performance?</span>
<span class="comment-copy">I know, I tried to pass the arguments in a list like you did, and also like I saw in many examples online, but it's still not working. Are the arguments passes in the same order? Because my program expects to get the arguments by order. For example I have an argument: -i &lt;INPUT_FILE_PATH&gt;. thanks</span>
