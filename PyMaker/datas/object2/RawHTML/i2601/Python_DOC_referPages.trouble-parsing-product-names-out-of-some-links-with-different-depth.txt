<div class="post-text" itemprop="text">
<p>I've written a script in python to reach the target page where each category has their avaiable item names in a website. My below script can get the product names from most of the links (generated through roving category links and then subcategory links).</p>
<p>The script can parse sub-category links revealed upon clicking <code>+</code> sign located right next to each category which are visible in the below image and then parse all the product names from the target page. <a href="https://www.courts.com.sg/home-appliances/large-appliances/dishwashers" rel="noreferrer">This is one of such</a> target pages.</p>
<blockquote>
<p>However, few of the links do not have the same depth as other links. For example <a href="https://www.courts.com.sg/televisions" rel="noreferrer">this link</a> and <a href="https://www.courts.com.sg/living-dining-room" rel="noreferrer">this one</a> are different from usual links like <a href="https://www.courts.com.sg/home-appliances/refrigeration/fridge" rel="noreferrer">this one</a>.</p>
</blockquote>
<p><strong><em>How can I get all the product names from all the links irrespective of their different depth?</em></strong></p>
<p>This is what I've tried so far:</p>
<pre><code>import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup

link = "https://www.courts.com.sg/"

res = requests.get(link)
soup = BeautifulSoup(res.text,"lxml")
for item in soup.select(".nav-dropdown li a"):
    if "#" in item.get("href"):continue  #kick out invalid links
    newlink = urljoin(link,item.get("href"))
    req = requests.get(newlink)
    sauce = BeautifulSoup(req.text,"lxml")
    for elem in sauce.select(".product-item-info .product-item-link"):
        print(elem.get_text(strip=True))
</code></pre>
<p><strong><em>How to find trget links:</em></strong></p>
<p><a href="https://i.stack.imgur.com/ZAl9c.jpg" rel="noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/ZAl9c.jpg"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>The site has six main product categories. Products that belong to a subcategory can also be found in a main category (for example the products in <code>/furniture/furniture/tables</code> can also be found in <code>/furniture</code>), so you only have to collect products from the main categories. You could get the categories links from the main page, but it'd be easier to use the sitemap.  </p>
<pre><code>url = 'https://www.courts.com.sg/sitemap/'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')
cats = soup.select('li.level-0.category &gt; a')[:6]
links = [i['href'] for i in cats]
</code></pre>
<p>As you've mentioned there are some links that have differend structure, like this one: <code>/televisions</code>. But, if you click the <code>View All Products</code> link on that page you will be redirected to <code>/tv-entertainment/vision/television</code>. So, you can get all the <code>/televisions</code> rpoducts from <code>/tv-entertainment</code>. Similarly, the products in links to brands can be found in the main categories. For example, the <code>/asus</code> products can be found in <code>/computing-mobile</code> and other categories.</p>
<p>The code below collects products from all the main categories, so it should collect all the products on the site. </p>
<pre><code>from bs4 import BeautifulSoup
import requests

url = 'https://www.courts.com.sg/sitemap/'
r = requests.get(url)
soup = BeautifulSoup(r.text, 'html.parser')

cats = soup.select('li.level-0.category &gt; a')[:6]
links = [i['href'] for i in cats]
products = []

for link in links:
    link += '?product_list_limit=24'
    while link:
        r = requests.get(link)
        soup = BeautifulSoup(r.text, 'html.parser')
        link = (soup.select_one('a.action.next') or {}).get('href')
        for elem in soup.select(".product-item-info .product-item-link"):
            product = elem.get_text(strip=True)
            products += [product]
            print(product)
</code></pre>
<p>I've increased the number of products per page to 24, but still this code takes a lot of time, as it collects products from all main categories and their pagination links. However, we could make it much faster with the use of <a href="https://docs.python.org/3/library/threading.html#thread-objects" rel="noreferrer">threads</a>.</p>
<pre><code>from bs4 import BeautifulSoup
import requests
from threading import Thread, Lock
from urllib.parse import urlparse, parse_qs

lock = Lock()
threads = 10
products = []

def get_products(link, products):
    soup = BeautifulSoup(requests.get(link).text, 'html.parser')
    tags = soup.select(".product-item-info .product-item-link")
    with lock:
        products += [tag.get_text(strip=True) for tag in tags]
        print('page:', link, 'items:', len(tags))

url = 'https://www.courts.com.sg/sitemap/'
soup = BeautifulSoup(requests.get(url).text, 'html.parser')
cats = soup.select('li.level-0.category &gt; a')[:6]
links = [i['href'] for i in cats]

for link in links:
    link += '?product_list_limit=24'
    soup = BeautifulSoup(requests.get(link).text, 'html.parser')
    last_page = soup.select_one('a.page.last')['href']
    last_page = int(parse_qs(urlparse(last_page).query)['p'][0])
    threads_list = []

    for i in range(1, last_page + 1):
        page = '{}&amp;p={}'.format(link, i)
        thread = Thread(target=get_products, args=(page, products))
        thread.start()
        threads_list += [thread]
        if i % threads == 0 or i == last_page:
            for t in threads_list:
                t.join()

print(len(products))
print('\n'.join(products))
</code></pre>
<p>This code collects 18,466 products from 773 pages in about 5 minutes. I'm using 10 threads because I don't want to stress the server too much, but you could use more (most servers can handle 20 threads easily). </p>
</div>
<div class="post-text" itemprop="text">
<p>I would recommend starting your scrape from the pages sitemap</p>
<p><a href="https://www.courts.com.sg/sitemap" rel="nofollow noreferrer">Found here</a></p>
<p>If they were to add products, it's likely to show up here as well.</p>
</div>
<div class="post-text" itemprop="text">
<p>Since your main issue is finding the links, here is a generator that will find all of the category and sub-category links using the sitemap krflol pointed out in his solution:</p>
<pre><code>from bs4 import BeautifulSoup
import requests


def category_urls():
    response = requests.get('https://www.courts.com.sg/sitemap')
    html_soup = BeautifulSoup(response.text, features='html.parser')
    categories_sitemap = html_soup.find(attrs={'class': 'xsitemap-categories'})

    for category_a_tag in categories_sitemap.find_all('a'):
        yield category_a_tag.attrs['href']
</code></pre>
<p>And to find the product names, simply scrape each of the yielded <code>category_urls</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>I saw the website for parsing and found that all the products are available at the bottom left side of the main page <a href="https://www.courts.com.sg/" rel="nofollow noreferrer">https://www.courts.com.sg/</a> .After clicking one of these we goes to advertisement front page of a particular category. Where we have to go in click All Products for getting it.</p>
<p>Following is the code as whole:</p>
<pre><code>import requests
from bs4 import BeautifulSoup

def parser():
    parsing_list = []
    url = 'https://www.courts.com.sg/'
    source_code = requests.get(url)
    plain_text = source_code.text
    soup = BeautifulSoup(plain_text, "html.parser")
    ul = soup.find('footer',{'class':'page-footer'}).find('ul')
    for l in ul.find_all('li'):
        nextlink = url + l.find('a').get('href')
        response = requests.get(nextlink)
        inner_soup = BeautifulSoup(response.text, "html.parser")
        parsing_list.append(url + inner_soup.find('div',{'class':'category-static-links ng-scope'}).find('a').get('href'))
return parsing_list
</code></pre>
<p>This function will return list of all products of all categories which your code didn't scrape from it. </p>
</div>
<span class="comment-copy">Why does the depth of the URL matter? What does that change?</span>
<span class="comment-copy">Why the depth matters would be clearer If you read the post @Cole. However, that doesn't let me fetch product names from those links (containing different depth) when the existing logic is applied within the scraper.</span>
<span class="comment-copy">I’ve read it and I just reread it. Given the requirements, the format of the links are besides the point of what you’re trying to accomplish. Unless I’m missing something?</span>
<span class="comment-copy">Wish to parse all the product names and that's it.</span>
<span class="comment-copy">It seems this thread is gonna solve now. It's always a pleasure to find you in the loop @t.m.adam. Where and how should I use the <code>print</code> statement within your script? Will accept it when time is right. Thanks.</span>
<span class="comment-copy">And I'm very happy to see that you're still around man! I suppose you could print each product in the loop (I've updated the code). BTW what do you think of my solution? I've searched the site a little and I think the main categories include all the products. Please let me know if you find a product that is not in a main category so I can take a look. Also, I'm trying to use multithreading because the script is too slow. I'll update the code If I can make it work.</span>
<span class="comment-copy">That still comes up with the same depth issues. Check out this links <code>https://www.courts.com.sg/klipsch</code>,<code>https://www.courts.com.sg/simmons</code> which are of different depth than this two <code>https://www.courts.com.sg/home-appliances/small-appliances</code>,<code>https://www.courts.com.sg/smart-tech/smart-gadgets</code> and so on.</span>
<span class="comment-copy">@Topto Are you parsing data from the URL itself?</span>
