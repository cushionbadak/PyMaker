<div class="post-text" itemprop="text">
<p>I am trying to split financial documents to sentences. I have ~50.000 documents containing plain English text. The total file size is ~2.6 GB. </p>
<p>I am using NLTK's <code>PunktSentenceTokenizer</code> with the standard English pickle file. I additionally tweaked it with providing additional abbreviations but the results are still not accurate enough. </p>
<p>Since NLTK PunktSentenceTokenizer bases on the unsupervised algorithm by Kiss &amp; Strunk (2006) I am trying to train the sentence tokenizer based on my documents, based on <a href="https://stackoverflow.com/questions/21160310/training-data-format-for-nltk-punkt">training data format for nltk punkt</a>.</p>
<pre><code>import nltk.tokenize.punkt
import pickle
import codecs

tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()
text = codecs.open("someplain.txt", "r", "utf8").read()
tokenizer.train(text)
out = open("someplain.pk", "wb")
pickle.dump(tokenizer, out)
out.close()
</code></pre>
<p>Unfortunately, when running the code, I got an error, that there is not sufficient memory. (Mainly because I first concatenated all the files to one big file.)</p>
<p>Now my questions are:</p>
<ol>
<li>How can I train the algorithm batchwise and would that lead to a lower memory consumption? </li>
<li>Can I use the standard English pickle file and do further training with that already trained object?</li>
</ol>
<p>I am using Python 3.6 (Anaconda 5.2) on Windows 10 on a Core I7 2600K and 16GB RAM machine.</p>
</div>
<div class="post-text" itemprop="text">
<p>As described in the <a href="https://www.nltk.org/_modules/nltk/tokenize/punkt.html" rel="nofollow noreferrer">source code</a>:</p>
<blockquote>
<p>Punkt Sentence Tokenizer</p>
<p>This tokenizer divides a text into a list of sentences
  by using an unsupervised algorithm to build a model for abbreviation
  words, collocations, and words that start sentences. <strong>It must be
  trained on a large collection of plaintext</strong> in the target language
  before it can be used.</p>
</blockquote>
<p>It is not very clear what a <strong>large collection</strong> really means. In the <a href="https://www.mitpressjournals.org/doi/abs/10.1162/coli.2006.32.4.485" rel="nofollow noreferrer">paper</a>, there are no information given about learning curves (when it is sufficiant to stop learning process, because enough data was seen). Wall Street Journal corpus is mentioned there (it has approximately 30 million words). So it is very unclear if you can simply trim your training corpus and have less memory footprints.</p>
<p>There is also an <a href="https://github.com/nltk/nltk/issues/2008" rel="nofollow noreferrer">open issue</a> on your topic saying something about 200 GB RAM and more. As you can see there, NLTK has probably not a good implementation of the algorithm presented by Kiss &amp; Strunk (2006).</p>
<p>I cannot see how to batch it, as you can see in the function signature of <code>train()</code>-method (NLTK version 3.3):</p>
<pre><code>def train(self, train_text, verbose=False):
    """
    Derives parameters from a given training text, or uses the parameters
    given. Repeated calls to this method destroy previous parameters. For
    incremental training, instantiate a separate PunktTrainer instance.
    """
</code></pre>
<p>But there are probably more issues, e.g. if you compare the signature of given version 3.3 with the git tagged version 3.3, <a href="https://github.com/nltk/nltk/blob/3.3/nltk/tokenize/punkt.py#L712" rel="nofollow noreferrer">there</a> is a new parameter <code>finalize</code> which might be helpful and indicates a possible batch-process or a possible merge with an already trained model:</p>
<pre><code>def train(self, text, verbose=False, finalize=True):
    """
    Collects training data from a given text. If finalize is True, it
    will determine all the parameters for sentence boundary detection. If
    not, this will be delayed until get_params() or finalize_training() is
    called. If verbose is True, abbreviations found will be listed.
    """
</code></pre>
<p>Anyway, I would strongly recommend not using NLTK's Punkt Sentence Tokenizer if you want to do sentence tokenization beyond playground level. Nevertheless, if you want to stick to that tokenizer, I would simply recommend using also the given models and not train new models unless you have a server with huge RAM memory.</p>
</div>
<span class="comment-copy">Besides: It's better to use a <code>with</code>-statement to open files, see <a href="https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files" rel="nofollow noreferrer">docs</a>.</span>
<span class="comment-copy">Thanks for your suggestion. I wasn't aware of the finalize opportunity. I will give it try. As you do not recommend nltk for sentence segmentation, do you have any recommendation how to accomplish this task? Can you recommend any libraries or methods?</span>
<span class="comment-copy">@JumpinMD you can take <a href="https://spacy.io/" rel="nofollow noreferrer">spaCy</a> for example if you want to have an easy solution with Python without much effort but strength and so much more features. Simply load the corpus with a (dependency) parser and then you can simply iterate over the sentences (automatically detected). That's only one of many solutions and could be a little bit overpowered for your needs. But it's fast and production ready. If you want to do sentence tokenization only as a preprocessing step, you can use e.g. Stanford's tokenizer (but there are tons of more tokenizers out there -&gt; Search engine).</span>
<span class="comment-copy">I have already tried spaCy but wasn't able to add additional abbreviations. Without that, spaCy was quite bad. Do you know a way to do that?</span>
<span class="comment-copy">This is going off topic... But I think you can adjust your <a href="https://spacy.io/api/vocab" rel="nofollow noreferrer">vocabulary</a> (inkluding abbreviations) for the <a href="https://spacy.io/usage/linguistic-features#section-tokenization" rel="nofollow noreferrer">tokenization task</a>. <a href="https://spacy.io/usage/linguistic-features#sbd-manual" rel="nofollow noreferrer">This</a> might also be interesting for you.</span>
<span class="comment-copy">@JumpinMD did you find any resolution to this?</span>
