<div class="post-text" itemprop="text">
<p>If I have about 10+ million little tasks to process in python (convert images or so), how can I create queue and <em>save progress</em> in case of crash in processing. To be clear, how can I save progress or stop process whatever I want and continue processing from the last point.</p>
<p>Also how to deal with multiple threads in that case?</p>
<p>In general question is how to save progress on processed data to file. Issue if it huge amount of very small files, saving file after each iteration will be longer than processing itself...</p>
<p>Thanks! </p>
<p>(sorry for my English if its not clear)</p>
</div>
<div class="post-text" itemprop="text">
<p>First of I would suggest not to go for multi-threading. Use <strong>multi-processing</strong> instead. Multiple threads do not work synchronously in python due to GIL when it comes to computation intensive task. </p>
<p>To solve the problem of saving result use following sequence</p>
<ol>
<li>Get the names of all the files in a list and divide the list into chunks.</li>
<li>Now assign each process one chunk.</li>
<li>Append names of processed files after every 1000 steps to some file(say monitor.txt) on system(assuming that you can process 1000 files again in case of failure).</li>
<li>In case of failure skip all the files which are saved in the monitor.txt for each process.</li>
</ol>
<p><em>You can have monitor_1.txt, monitor_2.txt ... for each process so you will not have to read the whole file for each process.</em></p>
<p>Following gist might help you. You just need to add code for the 4th point. 
<a href="https://gist.github.com/rishibarve/ccab04b9d53c0106c6c3f690089d0229" rel="nofollow noreferrer">https://gist.github.com/rishibarve/ccab04b9d53c0106c6c3f690089d0229</a></p>
</div>
<div class="post-text" itemprop="text">
<p>I/O operations like saving files are always relatively slow. If you have to process a large batch of files, you will be stuck with a long I/O time regardless of the number of threads you use. </p>
<p>The easiest is to use multithreading and not multiprocessing, and let the OS's scheduler figure it all out. <a href="https://docs.python.org/3/library/threading.html#threading.Thread" rel="nofollow noreferrer">The docs</a> have a good explanation of how to set up threads. A simple example would be</p>
<pre><code>from threading import Thread

def process_data(file_name):
    # does the processing
    print(f'processed {file_name}')

if __name__ == '__main__':
    file_names = ['file_1', 'file_2']
    processes = [Thread(target=process_data, args=(file_name,)) for file_name in file_names]

    # here you start all the processes
    for proc in processes:
        proc.start()

    # here you wait for all processes to finish
    for proc in processes:
        proc.join()
</code></pre>
<p>One solution that might be faster is to create a separate process that does the I/O. Then you use a <code>multiprocessing.Queue</code> to queue the files from the `data process thread', and let the I/O thread pick these up and process them one after the other.</p>
<p>This way the I/O never has to rest, which will be close to optimal. I don't know if this will yield a big advantage over the threading based solution, but as is generally the case with concurrency, the best way to find out is to do some benchmarks with your own application.</p>
<p>One issue to watch out for is that if the data processing is much faster, then the <code>Queue</code> can grow very big. This <em>might</em> have a performance impact, depending on your system amongst other things. A quick workaround is to pause the data processing if the queue gets to large.</p>
<p>Remember to write all multiprocessing code in Python in a script with the</p>
<pre><code>if __name__ == '__main__':
    # mp code
</code></pre>
<p>guard, and be aware that some IDEs don't play nice with concurrent Python code. The safe bet is to test your code by executing it from a terminal.</p>
</div>
<span class="comment-copy">Hi Mike, welcome to Stack Overflow. Your question is very generic. Can you make it more specific by adding a minimum code example of what you have tried and where it goes wrong?</span>
<span class="comment-copy">You are right that threads are not always suitable for concurrent computation in python, but threads are all the more suitable for concurrent I/O. There is no single best solution for all problems, and OP would do best to do some benchmarks. In this case, threading might actually be faster.</span>
<span class="comment-copy">My suggestion in the answer is not to save the progress at every step. But save the progress after processing 1k(or 10k) items. We might have to process few files that are already processed in case of failure but we can save a lot of disk IO operations. If there is an assumption that one file must be processed only once than I accept that your answer as multiple threads works well in that case.</span>
<span class="comment-copy">Yes, I think its not bad idea, but if pre-sort processed files by size (because it could be a lot of 1kb files, and some few 10Gb).</span>
