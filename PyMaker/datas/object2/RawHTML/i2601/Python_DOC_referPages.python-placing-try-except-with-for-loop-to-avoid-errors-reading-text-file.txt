<div class="post-text" itemprop="text">
<p>I am trying to clean a huge (8gb) JSON file from Yelp. I want to convert the data either into a pandas data frame or write to a CSV. </p>
<p>Goal: I want to essentially skip any lines in the JSON file that are problematic, but add any I can to my pandas data frame. </p>
<p>Note: I wrote a function <code>flatten_json</code> which returns a dictionary where each key is the column name and each value is the row. </p>
<h2>Attempt 1</h2>
<pre><code>with open(json_file, encoding='UTF-8') as myfile:
    for line in myfile:
        try:
            line_contents = json.loads(line)
            temp = pd.DataFrame.from_dict(flatten_json(line_contents), orient='index').transpose()
            for col in temp.columns:
                if col not in data.columns:        
                    data[col] = np.NaN 
            data = data.append(temp)
        except:
            continue
</code></pre>
<p>But this code fails because for some reason the for loop is unable to even process the line from the file, which I don't understand. </p>
<pre><code>---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-18-b3526001dc66&gt; in &lt;module&gt;()
      4     data = data.drop(data.index[[0]])
      5 with open(json_file, encoding='UTF-8') as myfile:
----&gt; 6     for line in myfile:
      7         try:
      8             line_contents = json.loads(line)

C:\ProgramData\Anaconda3\lib\codecs.py in decode(self, input, final)
    319         # decode input (taking the buffer into account)
    320         data = self.buffer + input
--&gt; 321         (result, consumed) = self._buffer_decode(data, self.errors, final)
    322         # keep undecoded input until the next call
    323         self.buffer = data[consumed:]

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x9b in position 3615: invalid start byte
</code></pre>
<h2>Attempt 2</h2>
<p>Since the code was having trouble processing lines from the text, I tried to use a <code>try-except</code> to screen for error-prone lines from the file. </p>
<pre><code>with open(json_file, encoding='UTF-8') as myfile:
    try:
        for line in myfile:
            line_contents = json.loads(line)
            temp = pd.DataFrame.from_dict(flatten_json(line_contents), orient='index').transpose()
            for col in temp.columns:
                if col not in data.columns:        
                    data[col] = np.NaN 
            data = data.append(temp)
    except:
        pass
</code></pre>
<p>But this doesn't work either because, when the errors arise, it just skips the entire rest of the loop. </p>
<h2>Attempt 3</h2>
<pre><code>with open(json_file, encoding='UTF-8') as myfile:
    for i in range(10000):
        try:
            line = next(myfile)
            line_contents = json.loads(line)
            temp = pd.DataFrame.from_dict(flatten_json(line_contents), orient='index').transpose()
            for col in temp.columns:
                if col not in data.columns:        
                    data[col] = np.NaN 
            data = data.append(temp)
        except:
            continue
</code></pre>
<p>The problem with this approach is that, I don't know how many lines are in the file. I tried setting it to some large number like 15,000,000, but it never terminated</p>
<p>Question: Where can I place the try-except such that it skip lines with errors and also so the for loop is structured so it will go  through every line in the file? </p>
</div>
<div class="post-text" itemprop="text">
<p>Your attempt 2 is close. You just need to move the <code>try</code> inside the <code>for</code>, so it only skips that one loop iteration (that one line), rather than the whole loop (the whole file).</p>
<p>But there's no reason to rewrite the <code>for</code> around manually calling <code>next</code>, as in your attempt 3—you're not trying to deal with errors in reading the line from the file, only errors in decoding bad UTF-8 or parsing JSON.</p>
<p>In fact, you generally want to make your <code>try</code> as narrow as possible, not as wide as possible, so you don't accidentally swallow errors you weren't expecting and hoping to swallow. And, for the same reason, you almost never want a bare <code>except:</code> statement.</p>
<p>Handling the JSON errors is easy, but how do you handle the encoding errors?One option is to just do the decoding explicitly, so you can <code>try</code> it narrowly:</p>
<pre><code>with open(json_file, mode='rb') as myfile:
    for line in myfile:
        try:
            line_contents = json.loads(line.decode())
        except (UnicodeDecodeError, JSONDecodeError):
            continue
        temp = pd.DataFrame.from_dict(flatten_json(line_contents), orient='index').transpose()
        for col in temp.columns:
            if col not in data.columns:        
                data[col] = np.NaN 
        data = data.append(temp)
</code></pre>
<p>But, even more simply: <a href="https://docs.python.org/3/library/json.html#json.loads" rel="nofollow noreferrer"><code>loads</code></a> can accept UTF-8 <code>bytes</code> directly:</p>
<pre><code>        try:
            line_contents = json.loads(line)
        except (UnicodeDecodeError, JSONDecodeError):
            continue
</code></pre>
<p>(If you're not using Python 3.6 or later, see the docs for your version of <code>loads</code> instead of the 3.6 docs—this same line should work, but the details for <em>why</em> it works are different…)</p>
<hr/>
<blockquote>
<p>The problem with this approach is that, I don't know how many lines are in the file. I tried setting it to some large number like 15,000,000, but it never terminated.</p>
</blockquote>
<p>As explained above, you don't need to do this.</p>
<p>But in case you ever do, I'll explain what's wrong, and what to do about it.</p>
<p>When you reach the end of the file, <code>next(myfile)</code> will raise <code>StopIteration</code>. But you catch that in your bare <code>except:</code> and just go on to the next line. Which will again raise <code>StopIteration</code>. And so on. So, if you've got 1 million lines, you'll have to go through 14 million <code>except:</code> loops after reaching the end of the file.</p>
<p>This is exactly why you don't want a bare <code>except:</code>. And one option is to just change that, so <code>StopIteration</code> isn't caught there. You can catch it at separately, and use it to break out of the loop:</p>
<pre><code>try:
    line = next(file)
except StopIteration:
    break
try:
    line_contents = json.loads(line)
except JSONDecodeError:
    continue
</code></pre>
<p>A different alternative is to use <code>file.readline()</code> instead of <code>next(file)</code>. The <code>readline</code> method will return an empty string at EOF, but will never return an empty string otherwise (a blank line is still <code>'\n'</code>). So:</p>
<pre><code>line = file.readline()
if notline:
    break
try:
    line_contents = json.loads(line)
except JSONDecodeError:
    continue
</code></pre>
<p>Either way, of course, you no longer need to guess at the length; instead of <code>for i in range(15000000):</code>, just do <code>while True:</code>.</p>
<p>But then you've just got a <code>while True:</code> around a <code>line = next(file)</code> with an <code>except StopIteration: break</code>, which is exactly what <code>for line in file:</code> does in the first place, so… just write that.</p>
<hr/>
<p>Finally: Are you sure you really want to silently ignore all non-UTF-8 lines?</p>
<p>It may just be that the data is garbage—each JSON text is in a different encoding, with <em>most</em> of them in UTF-8 but some in others, and the encodings aren't specified anywhere in-band or out-of-band, so there's really no good answer. (Although even then, you might want to try using <code>chardet</code> or <code>unicodedammit</code> or another heuristic guesser when UTF-8 fails…)</p>
<p>But if your data is in, say, Latin-1, what you're doing is ignoring anything that isn't in English. It would be much more useful to find out that the data is in Latin-1, and decode it as such.</p>
<p>That should be documented by your source. If it isn't, a library like <code>chardet</code> or <code>unicodedammit</code> might help you guess (they're even better for manual guessing than automated, of course). If you can't figure it out, instead of just silently discarding errors, maybe log them (e.g., log the <code>repr</code> of both the exception and the line), and then come back to Stack Overflow and ask for help with the info in your logs.</p>
</div>
<div class="post-text" itemprop="text">
<p>You have to actually fix the problem, which is completely unrelated to json decoding.</p>
<p>As you can see in your error traceback:</p>
<pre><code>      5 with open(json_file, encoding='UTF-8') as myfile:
----&gt; 6     for line in myfile:
</code></pre>
<p>Your error happens in the <code>for</code> line, before even <code>json.loads</code>!</p>
<p>The error <code>UnicodeDecodeError</code> means that the file contents are not <code>utf-8</code> as you specified. You can try specifying another encoding, or you can pass the <code>ignore</code> parameter when opening the file to ignore those errors:</p>
<pre><code>with open(json_file, encoding='UTF-8', errors='ignore') as myfile:
</code></pre>
<p>That will remove the unknown byte when decoding, so it will be missing but no errors will raise.</p>
<p>Attempts 2 and 3 have the line decoding inside the <code>try</code> clauseso they will shadow the real error which is text decoding.</p>
</div>
<span class="comment-copy">As a side not: You don't have a JSON file, you have .a JSONlines file (or maybe an RDJ file, or one of the other near-identical formats). A JSON file is a file with one JSON text, which can have newlines and all kinds of other stuff; a JSONlines file is a file with one JSON text per line (with no newlines or other line terminators between fields in a text, and everything but printable ASCII chars in strings escaped).</span>
<span class="comment-copy">Don't create so many dataframes; it'll run like molasses if you do. Read in, say, 10k records to a dataframe and write them to a CSV; read the next 10k into the same dataframe and append to the existing CSV, etc. It would also be helpful to see what a single record looks like.</span>
<span class="comment-copy">Seems like your real problem is that the text isn't UTF-8 like you expect. All of your problems are a result of interpreting non-UTF-8 text as UTF-8 text. Does the data source publish the actual encoding? For instance, if it's using <a href="https://en.wikipedia.org/wiki/Windows-1252" rel="nofollow noreferrer">cp1252</a>, that bad byte would correspond to a "close angle quote" character.</span>
<span class="comment-copy">The actual data isn't UTF-8, so this will error during iteration when it tries to block decode the bad line. Your answer is a partial solution, but the encoding issue needs to be addressed too.</span>
<span class="comment-copy">@ShadowRanger Ah, you're right; I'll edit it.</span>
<span class="comment-copy">Note: This silently ignores errors; if possible, you'd want to figure out the actual encoding of the data and use that, rather than silently dropping potentially important information.</span>
