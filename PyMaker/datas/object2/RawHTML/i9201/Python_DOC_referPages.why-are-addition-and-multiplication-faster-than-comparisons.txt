<div class="post-text" itemprop="text">
<p>I always thought comparisons were the fastest operation a computer could execute. I remember hearing it on a presentation from D. Knuth where he'd write loops in descending order "because comparison against 0 is fast". I also read that multiplications should be slower than additions <a href="https://streamcomputing.eu/blog/2012-07-16/how-expensive-is-an-operation-on-a-cpu/" rel="noreferrer">here</a>.</p>
<p>I'm surprised to see that, in both Python 2 and 3, testing under both Linux and Mac, comparisons seem to be much slower than arithmetic operations.</p>
<p>Could anyone explain why?</p>
<pre><code>%timeit 2 &gt; 0
10000000 loops, best of 3: 41.5 ns per loop

%timeit 2 * 2
10000000 loops, best of 3: 27 ns per loop

%timeit 2 * 0
10000000 loops, best of 3: 27.7 ns per loop

%timeit True != False
10000000 loops, best of 3: 75 ns per loop

%timeit True and False
10000000 loops, best of 3: 58.8 ns per loop
</code></pre>
<p>And under python 3:</p>
<pre><code>$ ipython3
Python 3.5.2 | packaged by conda-forge | (default, Sep  8 2016, 14:36:38) 
Type "copyright", "credits" or "license" for more information.

IPython 5.1.0 -- An enhanced Interactive Python.
?         -&gt; Introduction and overview of IPython's features.
%quickref -&gt; Quick reference.
help      -&gt; Python's own help system.
object?   -&gt; Details about 'object', use 'object??' for extra details.

In [1]: %timeit 2 + 2
10000000 loops, best of 3: 22.9 ns per loop

In [2]: %timeit 2 * 2
10000000 loops, best of 3: 23.7 ns per loop

In [3]: %timeit 2 &gt; 2
10000000 loops, best of 3: 45.5 ns per loop

In [4]: %timeit True and False
10000000 loops, best of 3: 62.8 ns per loop

In [5]: %timeit True != False
10000000 loops, best of 3: 92.9 ns per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This is happening due to <a href="https://en.wikipedia.org/wiki/Constant_folding" rel="noreferrer">Constant Folding</a> in the <a href="https://en.wikipedia.org/wiki/Peephole_optimization" rel="noreferrer">Peep Hole</a> <a href="http://faster-cpython.readthedocs.io/bytecode.html#cpython-peephole-optimizer" rel="noreferrer">optimizer</a> within Python compiler.</p>
<p>Using the dis module, if we break each of the statements to look inside how they are being translated at machine level, you will observe that all operators like inequality, equality etc are first loaded into memory and then evaluated. However, all expressions like multiplication, addition etc are calculated and loaded as a constant into memory.</p>
<p>Overall, this leads to a lesser number of execution steps, making the steps faster: </p>
<pre><code>&gt;&gt;&gt; import dis

&gt;&gt;&gt; def m1(): True != False
&gt;&gt;&gt; dis.dis(m1)
  1           0 LOAD_GLOBAL              0 (True)
              3 LOAD_GLOBAL              1 (False)
              6 COMPARE_OP               3 (!=)
              9 POP_TOP             
             10 LOAD_CONST               0 (None)
             13 RETURN_VALUE        

&gt;&gt;&gt; def m2(): 2 *2
&gt;&gt;&gt; dis.dis(m2)
  1           0 LOAD_CONST               2 (4)
              3 POP_TOP             
              4 LOAD_CONST               0 (None)
              7 RETURN_VALUE        

&gt;&gt;&gt; def m3(): 2*5
&gt;&gt;&gt; dis.dis(m3)
  1           0 LOAD_CONST               3 (10)
              3 POP_TOP             
              4 LOAD_CONST               0 (None)
              7 RETURN_VALUE        

&gt;&gt;&gt; def m4(): 2 &gt; 0
&gt;&gt;&gt; dis.dis(m4)
  1           0 LOAD_CONST               1 (2)
              3 LOAD_CONST               2 (0)
              6 COMPARE_OP               4 (&gt;)
              9 POP_TOP             
             10 LOAD_CONST               0 (None)
             13 RETURN_VALUE        

&gt;&gt;&gt; def m5(): True and False
&gt;&gt;&gt; dis.dis(m5)
  1           0 LOAD_GLOBAL              0 (True)
              3 JUMP_IF_FALSE_OR_POP     9
              6 LOAD_GLOBAL              1 (False)
        &gt;&gt;    9 POP_TOP             
             10 LOAD_CONST               0 (None)
             13 RETURN_VALUE        
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>As others have explained, this is because Python's peephole optimiser optimises arithmetic operations but not comparisons.</p>
<p>Having written my own peephole optimiser for a Basic compiler, I can assure you that optimising constant comparisons is just as easy as optimising constant arithmetic operations. So there is no technical reason why Python should do the latter but not the former.</p>
<p>However, each such optimisation has to be separately programmed, and comes with two costs: the time to program it, and the extra optimising code taking up space in the Python executable. So you find yourself having to do some triage: which of these optimisations is common enough to make it worth the costs?</p>
<p>It seems that the Python implementers, reasonably enough, decided to optimise the arithmetic operations first. Perhaps they will get round to comparisons in a future release.</p>
</div>
<div class="post-text" itemprop="text">
<p>A quick <a href="https://docs.python.org/3/library/dis.html" rel="nofollow noreferrer">disassambling</a> reveals that the comparison involves more operations. According to <a href="https://stackoverflow.com/questions/22983625/why-float-function-is-slower-than-multiplying-by-1-0#answer-22983997">this answer</a>, there is some precalculation done by the <a href="https://hg.python.org/cpython/file/648dcafa7e5f/Python/peephole.c#l283" rel="nofollow noreferrer">"peephole optimiser"</a> (<a href="https://en.wikipedia.org/wiki/Peephole_optimization" rel="nofollow noreferrer">wiki</a>) for multiplication, addition, etc., but not for the comparison operators:</p>
<pre><code>&gt;&gt;&gt; import dis
&gt;&gt;&gt; def a():
...   return 2*3
... 
&gt;&gt;&gt; dis.dis(a)
  2           0 LOAD_CONST               3 (6)
              3 RETURN_VALUE
&gt;&gt;&gt; def b():
...   return 2 &lt; 3
... 
&gt;&gt;&gt; dis.dis(b)
  2           0 LOAD_CONST               1 (2)
              3 LOAD_CONST               2 (3)
              6 COMPARE_OP               0 (&lt;)
              9 RETURN_VALUE
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Like others have commented - it is due to the peep hole optimizer which pre-computes the results of 2*3 (6). As the dis shows</p>
<pre><code>0 LOAD_CONST               3 (6)
</code></pre>
<p>But try this - it will disable the optimizer from pre-computing the results</p>
<pre><code>&gt;&gt;&gt; def a(a, b):
...     return a*b
...
&gt;&gt;&gt; dis.dis(a)
  2           0 LOAD_FAST                0 (a)
              3 LOAD_FAST                1 (b)
              6 BINARY_MULTIPLY
              7 RETURN_VALUE
&gt;&gt;&gt; def c(a,b):
...     return a&lt;b
...
&gt;&gt;&gt; dis.dis(c)
  2           0 LOAD_FAST                0 (a)
              3 LOAD_FAST                1 (b)
              6 COMPARE_OP               0 (&lt;)
              9 RETURN_VALUE
&gt;&gt;&gt;
</code></pre>
<p>If you time these functions the compare will be faster.</p>
</div>
<div class="post-text" itemprop="text">
<p>For python case the above answers are correct.  For machine code things a bit more complicated.  I assume we are talking about integer operations here, with floats and complex objects none of the below will apply.  Also, we assume that the values you are comparing are already loaded into registers.  If they are not fetching them from wherever they are could take 100 of times longer than the actual operations</p>
<p>Modern CPUs have several ways to compare two numbers.  Very popular ones are doing XOR a,b if you just want to see if two values are equal or CMP a,b if you want to know the relationship between the values ( less, greater, equal, etc ).  CMP operation is just a subtraction with the result thrown away because we are only interested in post-op flags.</p>
<p>Both of these operations are of depth 1, so they could be executed in a single CPU cycle.  This is as fast as you can go.   Multiplication is a form of repeated additions so the depth of the operation is usually equal to the size of your register.  There are some optimizations that could be made to reduce the depth, but generally multiplication is one of the slower operations that CPU can perform.</p>
<p>However, multiplying by 0,1 or any power of 2 could be reduced to shift operation. Which is also depth one operation.  So it takes the same time as comparing two numbers.  Think about decimal system,  you can multiply any number by 10, 100, 1000 by appending zeros at the end of the number.   Any optimizing compiler will recognize this type of multiplication and use the most efficient operation for it.  Modern CPUs are also pretty advanced, so they can perform same optimization in the hardware by counting how many bits are set in any of the operands.  And if it's just one bit the operation will be reduced to the shift.</p>
<p>So in your case multiplying by 2 is as fast as comparing two numbers.  As people above pointed out any optimizing compiler will see that you are multiplying two constants, so it will replace just replace the function with returning a constant.</p>
</div>
<div class="post-text" itemprop="text">
<p>Wow, The answer by @mu 無 blew my mind! However, it is important not to generalize when deriving your conclusions... You are checking the times for <strong>CONSTANTS</strong> not <strong>variables</strong>. For variables, multiplication seems to be slower than comparison.</p>
<p>Here is the more interesting case, in which the numbers to be compared are stored in actual variables...</p>
<pre><code>import timeit
def go():
    number=1000000000
    print
    print 'a&gt;b, internal:',timeit.timeit(setup="a=1;b=1", stmt="a&gt;b", number=number)
    print 'a*b, internal:',timeit.timeit(setup="a=1;b=1", stmt="a*b", number=number)
    print 'a&gt;b, shell   :',
    %%timeit -n 1000000000 "a=1;b=1" "a&gt;b"
    print 'a*b, shell   :',
    %%timeit -n 1000000000 "a=1;b=1" "a*b"
go()
</code></pre>
<p>The result gives:</p>
<pre><code>a&gt;b, internal: 51.9467676445
a*b, internal: 63.870462403
a&gt;b, shell   :1000000000 loops, best of 3: 19.8 ns per loop
a&gt;b, shell   :1000000000 loops, best of 3: 19.9 ns per loop
</code></pre>
<p>And order is restored in the universe ;)</p>
<p>For completeness, lets see some more cases... What about if we have one variable and one constant?</p>
<pre><code>import timeit
def go():
    print 'a&gt;2, shell   :',
    %%timeit -n 10000000 "a=42" "a&gt;2"
    print 'a*2, shell   :',
    %%timeit -n 10000000 "a=42" "a*2"
go()

a&gt;2, shell   :10000000 loops, best of 3: 18.3 ns per loop
a*2, shell   :10000000 loops, best of 3: 19.3 ns per loop
</code></pre>
<p>what happens with bools?</p>
<pre><code>import timeit
def go():
    print 
    number=1000000000
    print 'a==b    : ', timeit.timeit(setup="a=True;b=False",stmt="a==b",number=number) 
    print 'a and b : ', timeit.timeit(setup="a=True;b=False",stmt="a and b",number=number) 
    print 'boolean ==, shell   :',
    %%timeit -n 1000000000 "a=True;b=False" "a == b"
    print 'boolean and, shell   :',
    %%timeit -n 1000000000 "a=False;b=False" "a and b"
go()

a==b    :  70.8013108982
a and b :  38.0614485665
boolean ==, shell   :1000000000 loops, best of 3: 17.7 ns per loop
boolean and, shell   :1000000000 loops, best of 3: 16.4 ns per loop
</code></pre>
<p>:D <strong>Now this is interesting</strong>, it seems <em>boolean and</em> is faster than ==. However all this would be ok as the Donald Knuth would not loose his sleep, the best way to compare would be to use and...</p>
<p>In practice, we should check numpy, which may be even more significant...</p>
<pre><code>import timeit
def go():
    number=1000000 # change if you are in a hurry/ want to be more certain....
    print '====   int   ===='
    print 'a&gt;b  : ', timeit.timeit(setup="a=1;b=2",stmt="a&gt;b",number=number*100) 
    print 'a*b  : ', timeit.timeit(setup="a=1;b=2",stmt="a*b",number=number*100) 
    setup = "import numpy as np;a=np.arange(0,100);b=np.arange(100,0,-1);"
    print 'np: a&gt;b  : ', timeit.timeit(setup=setup,stmt="a&gt;b",number=number) 
    print 'np: a*b  : ', timeit.timeit(setup=setup,stmt="a*b",number=number) 
    print '====   float ===='
    print 'float a&gt;b  : ', timeit.timeit(setup="a=1.1;b=2.3",stmt="a&gt;b",number=number*100) 
    print 'float a*b  : ', timeit.timeit(setup="a=1.1;b=2.3",stmt="a*b",number=number*100) 
    setup = "import numpy as np;a=np.arange(0,100,dtype=float);b=np.arange(100,0,-1,dtype=float);"
    print 'np float a&gt;b  : ', timeit.timeit(setup=setup,stmt="a&gt;b",number=number) 
    print 'np float a*b  : ', timeit.timeit(setup=setup,stmt="a*b",number=number) 
    print '====   bool ===='
    print 'a==b    : ', timeit.timeit(setup="a=True;b=False",stmt="a==b",number=number*1000) 
    print 'a and b : ', timeit.timeit(setup="a=True;b=False",stmt="a and b",number=number*1000) 
    setup = "import numpy as np;a=np.arange(0,100)&gt;50;b=np.arange(100,0,-1)&gt;50;"
    print 'np a == b  : ', timeit.timeit(setup=setup,stmt="a == b",number=number) 
    print 'np a and b : ', timeit.timeit(setup=setup,stmt="np.logical_and(a,b)",number=number) 
    print 'np a == True  : ', timeit.timeit(setup=setup,stmt="a == True",number=number) 
    print 'np a and True : ', timeit.timeit(setup=setup,stmt="np.logical_and(a,True)",number=number) 
go()

====   int   ====
a&gt;b  :  4.5121130192
a*b  :  5.62955748632
np: a&gt;b  :  0.763992986986
np: a*b  :  0.723006032235
====   float ====
float a&gt;b  :  6.39567713272
float a*b  :  5.62149055215
np float a&gt;b  :  0.697037433398
np float a*b  :  0.847941712765
====   bool ====
a==b  :  6.91458288689
a and b  :  3.6289697892
np a == b  :  0.789666454087
np a and b :  0.724517620007
np a == True  :  1.55066706189
np a and True :  1.44293071804
</code></pre>
<p>Again, same behavior...
So I guess, one can benefit by using instead for == in general,</p>
<p>at least in Python 2 (Python 2.7.11 |Anaconda 2.4.1 (64-bit)| (default, Feb 16 2016, 09:58:36) [MSC v.1500 64 bit (AMD64)]), where I tried all these...</p>
</div>
<span class="comment-copy">I expected all of those examples to be constant folded on the way to becoming bytecode. Perhaps timeit gets in the way of this.</span>
<span class="comment-copy">more generally, <code>because comparison against 0 is fast</code> is highly relevant if programming a Z80 or the like... otherwise it's probably not at all, and it's precisely the kind of thing referred to by a certain far more famous quote of Knuth. I might guess that Knuth was talking about raw assembly language here, and if so, trying to apply that to a compiled/interpreted language is a near total non sequitur.</span>
<span class="comment-copy">When benchmarking, be wary of <a href="https://en.wikipedia.org/wiki/Constant_folding" rel="nofollow noreferrer">constant folding</a>.</span>
<span class="comment-copy">@underscore_d or he was saying it about compiled languages but considering what the generated assembly would be like… and doing so several decades ago.</span>
<span class="comment-copy">I'm surprised that the peephole optimiser doesn't reduce known conditionals, especially considering that it would allow dead stripping branches. Good use of dis</span>
<span class="comment-copy">@JonChesterfield <i>surprised that the peephole optimiser doesn't reduce known conditionals</i>: Well, so am I. But given that inequality and other such expressions can be used on any kind of objects in the Python world, maybe, leaving them out of the compiler would be more helpful in real world code examples.</span>
<span class="comment-copy">It might be worth a separate question. I can't think of a reason why 2 &lt; 2 can't be statically resolved. Perhaps Python lets the user overwrite <code>__LT__</code> for built in types, or maybe it's just a missing case in the optimiser.</span>
<span class="comment-copy">Well, looking at <a href="https://hg.python.org/cpython/file/tip/Python/peephole.c#l510" rel="nofollow noreferrer">peephole.c</a> looks like there are dfferent opcodes for different math ops but no such thing for different comparison operator, only <code>COMPARE_OP</code>. Maybe folding constants there would involve accessing information that is not easily accessible?</span>
<span class="comment-copy">The term for this is <a href="https://en.wikipedia.org/wiki/Constant_folding" rel="nofollow noreferrer">constant folding</a>.</span>
<span class="comment-copy">It's worth noting that in python 2, the <code>True != False</code> and <code>True and False</code> operations can't be optimised that way, since both <code>True</code> and <code>False</code> can be redefined, so these globals need to be looked up every time it executes. In python 3, <code>True</code> and <code>False</code> are keywords and can't be redefined.</span>
<span class="comment-copy">Thanks @Amanda, I was not aware of that. Curious, then, that the OP's Python 3 timings omit the True and False cases.</span>
<span class="comment-copy">There is also a third cost - the time taken to actually perform the optimisation pass each time the source is compiled (which in some circumstances may be every time the program is run). :)</span>
