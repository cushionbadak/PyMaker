<div class="post-text" itemprop="text">
<p>I am currently doing some coding problems on leetcode.com and am stumped by the difference in computational time between two algorithms that output the same result.</p>
<p>The problem:
Given a string array words, find the maximum value of length(word[i]) * length(word[j]) where the two words do not share common letters. You may assume that each word will contain only lower case letters. If no such two words exist, return 0.</p>
<p>At a bare minimum, one needs to compare every combination of pairs of strings in order to compute the answer. I realize bitwise is much faster/more efficient, but I am choosing to ignore it for the sake of this question.</p>
<p>You can find the test case word list I am using at the bottom of this post.</p>
<p>Algorithm 1:</p>
<pre><code>curr_max=0
for i in range(0,len(words)):
    for j in range(0,len(words)):
        if i&lt;j:
            curr_word=set(words[i])
            other_word=words[j]
            for char in curr_word:
                if char in other_word:
                    break
            else:
                curr_max=max(curr_max,len(words[i])*len(other_word))
print(curr_max)
</code></pre>
<p>This algorithm essentially is a nested for loop that compares every possible combination of words, and would include double counts and counts against itself if not for the "if i

<p>Algorithm 2:</p>
<pre><code>curr_max = 0
while words:
    curr_word = set(words[0])
    curr_len = len(words[0])
    words = words[1:]
    for word in words:
        for char in curr_word:
            if char in word:
                break
        else:
            curr_max = max(curr_max, curr_len*len(word))
print(curr_max)
</code></pre>
<p>This algorithm essentially compares the first word in the list to all the others, then deletes it from the list and repeats the process. It takes approximately 0.02 seconds, making it about 4 times faster. I am having a hard time understanding why exactly the difference is so large. Thanks in advance for all the help!</p>
<p>The input I am using to compare the times is this:</p>
<pre><code>words=["eedfdddadbcc","acdadecebafaebaec","dfde","ececbefe","bbafebbeccbddddbd","eafeffddbbbf","cbd","abadeaddfbcfbadb","ffdcacaebbeaa","fbadcfeede","bdcefdbfec","bbadfccdfebefd","dbefdfabededb","cbbccdecfbbe","abaeeecdbecebafedbfee","fbefbdfc","fffafb","bfadbdefbfedbddff","cbccbfdadbfe","bacaafecbbfaae","fcbffdbefcfbccd","fefaadfaaafdfbdaff","ecabaff","ccbdefcdcfac","bbbfafbffcefbc","edecefa","bcdfbcebabae","aaefecfcbbccfaeeaf","beaabaaeaebef","adfac","acedfdabccebc","efbfbef","bfccadbcbcfcdabfa","ffcaddbcf","dfae","ccadeeebeaabddebcadec","babaa","ebbdbaabddfdddad","fafaddbaebdaa","eeeeddadedfe","effbca","abcddfa","cbadcfeffeaaeecbbfe","ceaabcfaaaefeeadf","acecadddde","ece","dc","bfafdefbbdafacdcfb","fbdcad","dbaaffcdbcbdea","baaee","bebed","beaedceceaa","eacbcfdbcefbaddffcac","acddaedacfeedffad","efebff","efcbf","cdfffffaacfacafb","adacaceea","fceeffededbcfbfaaf","eafaeffcbfde","debadcddbdbabefdbe","ef","eeeeabfbaabddaecb","eeadcdcdaacaabe","ebcbffdefafdcbcebec","eb","adedefbaabfcefbea","ddceabfddaaefcea","ddffb","fdadfac","de","cbcdcbddcdabeb","ccffeeafbbbf","ccba","dab","bbdbeefdbef","cbec","ffcbefdbfdacdbdbf","adfad","ceacdcbfbdbbaebbd","ecfeaefff","ddbbdaefddeebd","eeeee","abdadc","eafecdbdef","aeedaeeaebaaeecdd","dddeebcbdea","bcaadedacb","ebdeadddcafa","ecbdbcbfccbdffaef","fddcffbfffa","accbdcfcedeabeab","cfbbefbddcaeecfbfacc","efffdaacbafeecdad","aaadfa","efeccbabdefaf","defebaddaafdcd","ecebcaacdaccaddcfeee","fdbbfecaffeafaa","bafccdbea","caa","deedefdeccead","bbfbfeaeddacfacea","daaefbbcbcdbfbfdda","aceed","cfeadadadbcff","eaefcdca","cefebdbaafeabdbdeaafd","abec","aeececad","cfeabcbaeaebdbcaada","aac","ebabffeb","fa","cf","dcebedefc","dbaedceecf","ffebaedafccceb","faefbeacaddefbe","eeadbfabfbbbfaeffaeea","affdecaca","ccfdcbdefcdfaddbbeaed","bc","feafaaabaceade","bebdfbbad","eeacaefaddacac","fff","aeddcd","ccffbabbdfc","ecddbcdeecdfbbb","debdbcdcafdcd","cfaebaeddbbd","efdada","becdccaeffeedcadbdd","feedbacc","cbbeebcdad","bfdcbdfdbcceadded","cfdbfdddafadadddcba","bcedeaeeaac","ffdcfccffdfffaebf","afffceaaadbbedfdd","faaeebdfbfefddebed","eedafbddeeaaadcdeaccc","eeceadafa","ebcfaccabea","eebdbbedcaedcbdcfaba","ecfcadaebacbdfdccebe","cbbabdadaee","cfeea","dec","cfedcbaabbaef","aacdabcbf","dfdbacadbebeedcd","bccccfdcdfe","cfcacdbcdccddcadce","dafafeccfaccaadeabbf","eaffaaffefccde","bbfbddccfda","fbdbbbbfbe","eafbcafbdbead","edbcdcefdc","fe","aafdcabce","ddafedceddcdcbfbcafe","dabcafbcfafeeadbbbef","beeaacd","cadeabebdbcfbbdfe","ecfefbfbbfa","fedacafcc","bcdcefecbcebaeeccdbd","fefde","cafba","bdabeaabbdbbbccecebda","dfeaadbeaaeefdfbed","dbaecde","cfbdfffbdeeeeb","fc","decadcacfaabca","cebbdff","badabbddcfed","fcce","fedfadefcf","acfccfbfcda","debfc","bebafeaeffe","ceaefbbcefacbbacb","cebbaeb","cadedfdafecdfb","bfefdfbaceddfcbade","cefeefaeddafbbdcade","faceadcefbffadb","cfbacafae","dfbfadfdccedbcbeaae","dbbccdddaf","ebbcbcebdddcedcfdcfaa","ccedffbcdbaedfaeb","ccfeaceaaaaeee","faade","afaaacaecbffdbadcbcd","cebfbbefbbdabbbffea","cdaadba","bbefdcacaaadbbbdedec","adabfbebdb","fcfefadcbadaacbdcfdbb","adddadebfc","fb","ecfebaacbdabece","dabacfdecfe","eeeecc","eabbe","fcdffababd","aafdbbcfdecbccca","efebaaadfecccecaa","cffefdbf","bcbdd","eaaccdcfdbbbcf"]
</code></pre>
<p><strong><em>Updating OP to help the people assisting me with this question!</em></strong>
I am now generating words using this:</p>
<pre><code>words = [''.join([choice('abcdefghijklmnopqrstuvwxyz') for _ in range(randrange(2, 22))]) for _ in range(250)]
</code></pre>
<p>I am performing the first algorithm as so:</p>
<pre><code>t1=time.time()
curr_max=0
for i in range(0,len(words)):
    for j in range(0,i):
        curr_word=set(words[i])
        other_word=words[j]
        for char in curr_word:
            if char in other_word:
                break
        else:
            curr_max=max(curr_max,len(words[i])*len(other_word))
print(curr_max)
t0=time.time()
print(t0-t1)
</code></pre>
<p>The results I am seeing are in the 0.1 seconds range.</p>
<p>The second algorithm I am using this:</p>
<pre><code>t1=time.time()
curr_max = 0
while words:
    curr_word = set(words[0])
    curr_len = len(words[0])
    words = words[1:]
    for word in words:
        for char in curr_word:
            if char in word:
                break
        else:
            curr_max = max(curr_max, curr_len*len(word))
print(curr_max)
t0=time.time()
print(t0-t1)
</code></pre>
<p>I am seeing results in the 0.04-0.05 seconds range. Can anyone replicate this?</p>
</p></div>
<div class="post-text" itemprop="text">
<p>Both algorithms look as if they the same amount of work. Both re-create the <a href="https://docs.python.org/3/library/itertools.html#itertools.combinations" rel="nofollow noreferrer"><code>itertools.combinations()</code> function</a>. 
However, the first method re-creates the sets more often, and executes an additional n**2 of <code>i &lt; j</code> tests (for n words)!</p>
<p>You are creating len(n) over 2 combinations, so n! / (n - 2)! (see <a href="https://en.wikipedia.org/wiki/Combination" rel="nofollow noreferrer">Wikipedia</a>), which is quite a lot less than n**2:</p>
<pre><code>&gt;&gt;&gt; import math
&gt;&gt;&gt; n = 250
&gt;&gt;&gt; math.factorial(n) / (math.factorial(2) * math.factorial(n - 2))
31125.0
&gt;&gt;&gt; n ** 2
62500
&gt;&gt;&gt; n ** 2 - (math.factorial(n) / (math.factorial(2) * math.factorial(n - 2)))
31375.0
</code></pre>
<p>So algorithm #1 executes more than twice as many loops as algorithm #2 for your specific case. As n increases, the product divided by the number of  combinations approaches 2, so it'll always do at least double the work.</p>
<p>Next, you create a set of <code>words[0]</code> in algorithm #2 only once, but you do it <em>for every inner loop</em> for algorithm #1:</p>
<pre><code># algorithm #1
for ...  # current word loop
    for ...   # other word loop
        set(words[i])

# algorithm #2
while    # current word loop
    set(words[0])
    for ...   # other word loop
</code></pre>
<p>It's those differences that causes it to be slower; creating (N over 2) sets vs. just N sets is probably costing you most of the performance here.</p>
<p>To make a proper comparison, you should use the <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow noreferrer"><code>timeit</code> module</a>, which repeats tests many times, makes sure to use the most accurate clock for measuring time spent and disables the Python garbage collector (so it won't interfere). I've included a randomised word list, and for the destructive algorithm (your #2), I had to clone the list each time, for which I compensate by subtracting the time for the same number of bare list copies.</p>
<p>The script I ran:</p>
<pre><code>from random import choice, randrange
from timeit import timeit

def naive_loop(words):
    curr_max=0
    for i in range(0,len(words)):
        for j in range(0,len(words)):
            if i&lt;j:
                curr_word=set(words[i])
                other_word=words[j]
                for char in curr_word:
                    if char in other_word:
                        break
                else:
                    curr_max=max(curr_max,len(words[i])*len(other_word))
    return curr_max

def destructive_loop(words):
    curr_max = 0
    while words:
        curr_word = set(words[0])
        curr_len = len(words[0])
        words = words[1:]
        for word in words:
            for char in curr_word:
                if char in word:
                    break
            else:
                curr_max = max(curr_max, curr_len*len(word))
    return curr_max

def reduced_set_calls_loop(words):
    curr_max=0
    for i in range(0,len(words)):
        curr_word=set(words[i])
        for j in range(0,len(words)):
            if i&lt;j:
                other_word=words[j]
                for char in curr_word:
                    if char in other_word:
                        break
                else:
                    curr_max=max(curr_max,len(words[i])*len(other_word))
    return curr_max


words = [''.join([choice('abcdef') for _ in range(randrange(2, 22))]) for _ in range(250)]
number = 100
print('Naive:', timeit('naive_loop(words)', 'from __main__ import naive_loop, words', number=number))
print('Destructive:',  # don't include time to copy a list
    timeit('destructive_loop(words[:])', 'from __main__ import destructive_loop, words', number=number) -
    timeit('words[:]', 'from __main__ import naive_loop, words', number=number))
print('Reduced set calls:', timeit('reduced_set_calls_loop(words)', 'from __main__ import reduced_set_calls_loop, words', number=number))
</code></pre>
<p>and the results:</p>
<pre><code>Naive: 1.8516130640055053
Destructive: 0.3646556100138696
Reduced set calls: 0.5927464940032223
</code></pre>
<p>So moving the <code>set()</code> call in <code>reduced_set_calls_loop()</code> dramatically improves the first version already. Reducing the number of loops by replacing <code>if i &lt; j</code> with a <code>for j in range(i):</code> loop further reduces the gap:</p>
<pre><code>&gt;&gt;&gt; def reduced_iteration_loop(words):
...     curr_max=0
...     for i in range(0,len(words)):
...         curr_word=set(words[i])
...         for j in range(i):
...             other_word=words[j]
...             for char in curr_word:
...                 if char in other_word:
...                     break
...             else:
...                 curr_max=max(curr_max,len(words[i])*len(other_word))
...     return curr_max
...
&gt;&gt;&gt; print('Reduced iteration:', timeit('reduced_iteration_loop(words)', 'from __main__ import reduced_iteration_loop, words', number=number))
Reduced iteration: 0.44450017900089733
</code></pre>
<p>What I find surprising is that your destructive loop is faster than using <code>itertools.combinations()</code> however:</p>
<pre><code>&gt;&gt;&gt; from itertools import combinations
&gt;&gt;&gt; def destructive_loop_empty(words):
...     while words:
...         curr_word, words = words[0], words[1:]
...         for word in words:
...             pass
...
&gt;&gt;&gt; def empty_combinations(words):
...     for a, b in combinations(words, 2):
...         pass
...
&gt;&gt;&gt; timeit('destructive_loop_empty(words[:])', 'from __main__ import destructive_loop_empty, words', number=1000)
0.324253979997593
&gt;&gt;&gt; timeit('empty_combinations(words[:])', 'from __main__ import empty_combinations, words', number=1000)
0.5626872480061138
</code></pre>
<p>We could possible make your algorithm #2 faster by using <a href="https://docs.python.org/3/library/stdtypes.html#set.isdisjoint" rel="nofollow noreferrer">set disjunctions</a>, rather than test each character individually. Because we'll be testing words repeatedly, it makes sense to create the sets up-front, in a dictionary, acting as a cache we can draw from when testing.</p>
<p>Finally, we can make a non-destructive version by storing lengths in the dictionary too and just looping over the values (we destroy the dictionary instead):</p>
<pre><code>def nondestructive_loop(words):
    curr_max = 0
    words = {w: (set(w), len(w)) for w in words}
    while words:
        curr_word, curr_word_length = words.popitem()[1]
        for other, other_length in words.values():
            if curr_word.isdisjoint(other):
                curr_max = max(curr_max, curr_word_length * other_length)
    return curr_max
</code></pre>
<p>This is the fastest I've been able to make this:</p>
<pre><code>&gt;&gt;&gt; print('Nondestructive:', timeit('nondestructive_loop(words)', 'from __main__ import nondestructive_loop, words', number=number))
Nondestructive: 0.2944725830020616
</code></pre>
<p>shaving off another 20%.</p>
<p>So, in conclusion, it is faster to iterate directly over a list, vs. generating indices from a <code>range()</code>, then indexing into the list. The difference is large enough that it is worth your while destroying the list (or dictionary)!</p>
<p>This is also what makes <code>itertools.combinations()</code> slower; it <em>has</em> to use indices, as it has to support combinations greater than 2 (which means you can't just delete from the input sequence).</p>
</div>
<span class="comment-copy">Would it make sense to do <code>wordsets = dict(zip(words, map(set, words)))</code> and change the if-test to <code>if wordsets[a].isdisjoint(wordsets[b])</code>?</span>
<span class="comment-copy">@StevenRumbalski: bingo, not even mapping <code>b</code> first already shaves of 66% of the time. Mapping <code>b</code> as well gives another couple of percentage points.</span>
<span class="comment-copy">Yep.  I get Naive: 2.598675707337638, Destructive: 2.6030930600383297,  Iterative: 2.04531153750888,  Iterative with set: 0.6089805807696393.  I am surprised my Iterative does not get as much speedup on my system as yours.  I'm running Python 3.6.0 [MSC v.1900 32 bit (Intel)] on win32 on 64-bit Windows 7.  (I deploy to 32-bit Windows 7 users which is why I have 32 bit Python.)</span>
<span class="comment-copy">I just realized that the above script is not using the correct algorithm for algorithm 2. I updated it and am again seeing the difference in timing I was finding earlier.</span>
<span class="comment-copy">What happens when u change the loop of algorithm 1 like I did at the end of my edited OP, where the inner loop only goes from 0 to j instead of Len(words)? Shouldn't that solve doing way too many iterations?</span>
