<div class="post-text" itemprop="text">
<p>Given a list of sets:</p>
<pre><code>allsets = [set([1, 2, 4]), set([4, 5, 6]), set([4, 5, 7])]
</code></pre>
<p>What is a pythonic way to compute the corresponding list of sets of elements having no overlap with other sets?</p>
<pre><code>only = [set([1, 2]), set([6]), set([7])]
</code></pre>
<p>Is there a way to do this with a list comprehension?</p>
</div>
<div class="post-text" itemprop="text">
<p>To avoid quadratic runtime, you'd want to make an initial pass to figure out which elements appear in more than one set:</p>
<pre><code>import itertools
import collections
element_counts = collections.Counter(itertools.chain.from_iterable(allsets))
</code></pre>
<p>Then you can simply make a list of sets retaining all elements that only appear once:</p>
<pre><code>nondupes = [{elem for elem in original if element_counts[elem] == 1}
            for original in allsets]
</code></pre>
<hr/>
<p>Alternatively, instead of constructing <code>nondupes</code> from <code>element_counts</code> directly, we can make an additional pass to construct a set of all elements that appear in exactly one input. This requires an additional statement, but it allows us to take advantage of the <code>&amp;</code> operator for set intersection to make the list comprehension shorter and more efficient:</p>
<pre><code>element_counts = collections.Counter(itertools.chain.from_iterable(allsets))
all_uniques = {elem for elem, count in element_counts.items() if count == 1}
#                                                     ^ viewitems() in Python 2.7
nondupes = [original &amp; all_uniques for original in allsets]
</code></pre>
<p>Timing seems to indicate that using an <code>all_uniques</code> set produces a substantial speedup for the overall duplicate-elimination process. It's up to about a <a href="http://ideone.com/NUApHy" rel="nofollow noreferrer">3.5x speedup</a> on Python 3 for heavily-duplicate input sets, though only about a <a href="http://ideone.com/8b70l4" rel="nofollow noreferrer">30% speedup</a> for the overall duplicate-elimination process on Python 2 due to more of the runtime being dominated by constructing the Counter. This speedup is fairly substantial, though not nearly as important as avoiding quadratic runtime by using <code>element_counts</code> in the first place. If you're on Python 2 and this code is speed-critical, you'd want to use an ordinary <code>dict</code> or a <code>collections.defaultdict</code> instead of a <code>Counter</code>.</p>
<p>Another way would be to construct a <code>dupes</code> set from <code>element_counts</code> and use <code>original - dupes</code> instead of <code>original &amp; all_uniques</code> in the list comprehension, as <a href="https://stackoverflow.com/a/35093930/2357112">suggested</a> by munk. Whether this performs better or worse than using an <code>all_uniques</code> set and <code>&amp;</code> would depend on the degree of duplication in your input and what Python version you're on, but it <a href="http://ideone.com/ugvQdw" rel="nofollow noreferrer">doesn't</a> <a href="http://ideone.com/7ddxZA" rel="nofollow noreferrer">seem</a> to make much of a difference either way.</p>
</div>
<div class="post-text" itemprop="text">
<p>Yes it can be done but is hardly pythonic </p>
<pre><code>&gt;&gt;&gt; [(i-set.union(*[j for j in allsets if j!= i])) for i in allsets]   
[set([1, 2]), set([6]), set([7])]
</code></pre>
<p>Some reference on sets can be found <a href="https://docs.python.org/3/library/stdtypes.html#set" rel="noreferrer">in the documentation</a>. The <code>*</code> operator is called <a href="https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists" rel="noreferrer">unpacking operator</a>. </p>
</div>
<div class="post-text" itemprop="text">
<p>A slightly different solution using Counter and comprehensions, to take advantage of the <code>-</code> operator for set difference.</p>
<pre><code>from itertools import chain
from collections import Counter

allsets = [{1, 2, 4}, {4, 5, 6}, {4, 5, 7}]
element_counts = Counter(chain.from_iterable(allsets))

dupes = {key for key in element_counts 
         if element_counts[key] &gt; 1}

only = [s - dupes for s in allsets]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Another solution with <code>itertools.chain</code>:</p>
<pre><code>&gt;&gt;&gt; from itertools import chain
&gt;&gt;&gt; [x - set(chain(*(y for y in allsets if y!=x))) for x in allsets]
[set([1, 2]), set([6]), set([7])]
</code></pre>
<p>Also doable without the unpacking and using <code>chain.from_iterable</code> instead.</p>
</div>
<span class="comment-copy">Related: <a href="http://stackoverflow.com/q/13714755/4279">Replace list of list with "condensed" list of list while maintaining order</a></span>
<span class="comment-copy">Certainly a better way. Some links for the OP 1. <a href="https://docs.python.org/3/library/itertools.html#itertools.chain.from_iterable" rel="nofollow noreferrer"><code>chain.from_iterable</code></a> 2. <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="nofollow noreferrer"><code>collections.Counter</code></a></span>
<span class="comment-copy">yep, this wins. +1</span>
<span class="comment-copy">Literal syntax could be a little nicer with [{elem for elem in original...}]</span>
<span class="comment-copy">@munk: Oh, right. I keep forgetting to use set literals and set comprehensions.</span>
<span class="comment-copy">Intersecting with unique elements is about 6x faster than subtracting duplicates on my real world data set. In my dataset, the unique elements are rare and the duplicates are plentiful.</span>
<span class="comment-copy">eww agreed. Avoid this like the plague. Prefer some verbose for loops (but great work Bhargav!)</span>
<span class="comment-copy">You don't need the inner list</span>
<span class="comment-copy">@PadraicCunningham You'd prefer a genexp there?</span>
<span class="comment-copy">I actually thought about that after I posted my original solution, though I used <code>&amp;</code> and made a <code>unique_elements</code> set instead of a <code>dupes</code> set. <a href="http://ideone.com/8b70l4" rel="nofollow noreferrer">Timing</a> showed <code>&amp;</code> to be about 30% faster than running a Python-level set comprehension every time. Whether <code>&amp;</code> or <code>-</code> performs better probably depends on the degree of element duplication and what Python version you're on.</span>
<span class="comment-copy">Selecting this solution as the best answer because 1) it is very readable, 2) 15%-30% faster than the user2357112 solution on my real world data</span>
<span class="comment-copy">Very nice and readable solution.  I originally selected this as the best answer based on readability and speed.  Later changed to user2357112's answer, which upon further testing is significantly faster.</span>
