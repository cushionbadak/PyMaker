<div class="post-text" itemprop="text">
<p>I'm writing a script that has a for loop to extract a list of variables from each 'data_i.csv' file in a folder, then appends that list as a new row in a single 'output.csv' file. </p>
<p>My objective is to define the headers of the file once and then append data to the 'output.csv' container-file so it will function as a backlog for a standard measurement. 
The first time I run the script it will add all the files in the folder. Next time I run it, I want it to only append files that have been added since. I thought one way of doing this would be to check for duplicates, but the codes I found for that so far only searched for consecutive duplicates.</p>
<p>Do you have suggestions?</p>
<p>Here's how I made it so far:</p>
<pre><code>import csv, os

# Find csv files
for csvFilename in os.listdir('.'):
    if not csvFilename.endswith('.csv'):
            continue    

# Read in csv file and choose certain cells
    csvRows = [] 
    csvFileObj = open(csvFilename) 
    csvData = csv.reader(csvFileObj,delimiter=' ',skipinitialspace='True') 
    csvLines = list(csvData) 

    cellID = csvLines[4][3] 

# Read in several variables...

    csvRows = [cellID]

    csvFileObj.close() 

    resultFile = open("Output.csv", 'a') #open in 'append' modus
    wr = csv.writer(resultFile) 
    wr.writerows([csvRows])     
    csvFileObj.close()
    resultFile.close()
</code></pre>
<p>This is the final script after mgc's answer:</p>
<pre><code>import csv, os

f = open('Output.csv', 'r+')
merged_files = csv.reader(f)
merged_files = list()
for csvFilename in os.listdir('.'):
    if not csvFilename.endswith('_spm.txt'):
        continue
    if csvFilename in merged_files:
        continue            

    csvRows = [] 
    csvFileObj = open(csvFilename) 
    csvData = csv.reader(csvFileObj,delimiter=' ',skipinitialspace='True')
    csvLines = list(csvData)
    waferID = csvLines[4][3] 
    temperature = csvLines[21][2]

    csvRows = [waferID,thickness]
    merged_files.append(csvRows)
    csvFileObj.close() 

wr = csv.writer(f)
wr.writerows(merged_files)
f.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can keep track of the name of each file already handled. If this log file don't need to be human readable, you can use <a href="https://docs.python.org/3/library/pickle.html" rel="nofollow">pickle</a>. At the start of your script, you can do :</p>
<pre><code>import pickle

try:
    with open('merged_log', 'rb') as f:
        merged_files = pickle.load(f)
except FileNotFoundError:
    merged_files = set()
</code></pre>
<p>Then you can add a condition to avoid files previously treated :</p>
<pre><code>if filename in merged_files: continue
</code></pre>
<p>Then when you are processing a file you can do :</p>
<pre><code>merged_files.add(filename)
</code></pre>
<p>And keep trace of your variable at the end of your script (so it will be used on a next use) :</p>
<pre><code>with open('merged_log', 'wb') as f:
    pickle.dump(merged_files, f)
</code></pre>
<p>(However there is other options to your problem, for example you can slightly change the name of your file once it has been processed, like changing the extension from <code>.csv</code> to <code>.csv_</code> or moving processed files in a subfolder, etc.)</p>
<p>Also, in the example in your question, i don't think that you need to open (and close) your output file on each iteration of your <code>for</code> loop. Open it once before your loop, write what you have to write, then close it when you have leaved the loop.</p>
</div>
<span class="comment-copy">When you will re-run the script (so, not the first time, but any other time), did data have been appended only in new csv files ? Or also in existing files ?</span>
<span class="comment-copy">When I run the script two times in a row, it will append the same data to the output file again. .</span>
<span class="comment-copy">You could somehow "mark and skip" files that you have already processed. One option is to rename the files with a particular extension, or prefix their names, so you won't search for them later. Better yet, save the list of files you have already processed either in an "ini" file or a simple sqlite database, and perform the lookup before you process any file, so you can skip what you've done already.</span>
<span class="comment-copy">Thanks for your comment! I agree that opening and closing the output file can be kept out of the for loop. I will look into the pickle module. What do you mean by "not human readable"? I would like to be able to read out and do manual data analysis on the output.csv in excel.</span>
<span class="comment-copy">You're welcome. Did the rest of the answer helps you to reach your objective ?</span>
<span class="comment-copy">I tried out the script and it works great: no duplicates are added during a second run of the same script. I'm still stuck with two questions:  1. I would like to add a list to "merged_files" but it seems to only take variables?  'code'csvRows = [cellID,temperature] merged_file.add(csvRows) 'code' returns: TypeError: unhashable type 'list' 2. I would like to read the script in excel as a .csv so I can do data analysis. Can I 'unpickle' the file at the end of the script?</span>
<span class="comment-copy">1) In my example <code>merged_files</code> variable is a <code>set</code> so it can only accept hashable (i.e. non-mutable) objects. You can also define <code>merged_files</code> as a list (and so append mutable objects, like <code>list</code>s, to it) but the look-up will be really more costly as the list will grow up. 2) If you're log file need to be human-readable you can save it in non-binary mode (without pickle) as a .csv or .txt file (or as you suggest, even if stored with pickle, you can load it and saved it to a csv to make analysis on it). I will modify my answer.</span>
<span class="comment-copy">Thank you mgc. I will add to my question the code that works for me in the end, following your comments.</span>
