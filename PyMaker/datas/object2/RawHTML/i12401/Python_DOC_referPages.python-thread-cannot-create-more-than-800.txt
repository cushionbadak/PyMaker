<div class="post-text" itemprop="text">
<p>below is my code and im really new to python. from my below code, i will actually create multiple threads (above 1000). but at some point, nearly 800 threads, i get an error message saying "error:cannot start new thread". i did read some about threadpool. i couldnt really understand. in my code, how can i implement threadpool? or at least please explain to me in a simple way</p>
<pre><code>    #!/usr/bin/python


    import threading
    import urllib

    lock = threading.Lock()

    def get_wip_info(query_str):
          try:
              temp = urllib.urlopen(query_str).read()
          except:
              temp = 'ERROR'
          return temp

    def makeURLcall(arg1, arg2, arg3, file_output, dowhat,        result) :

         url1 = "some URL call with args"
         url2 = "some URL call with args"

        if dowhat == "IN" :
             result = get_wip_info(url1)

        elif dowhat == "OUT" :
             result = get_wip_info(url2)

        lock.acquire()

        report = open(file_output, "a")
        report.writelines("%s - %s\n"%(serial, result))
        report.close()

        lock.release()

        return


    testername = "arg1"
    stationcode = "arg2"
    dowhat = "OUT"
    result = "PASS"
    file_source = "sourcefile.txt"
    file_output = "resultfile.txt"

    readfile = open(file_source, "r")
    Data = readfile.readlines()

    threads = []

    for SNs in Data :
        SNs = SNs.strip()
        print SNs
        thread = threading.Thread(target = makeURLcalls, args = (SNs, args1, testername, file_output, dowhat, result))
        thread.start()

        threads.append(thread)

    for thread in threads :
        thread.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Don't implement your own thread pool, use the one that ships with Python.</p>
<p>On Python 3, you can use <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow"><code>concurrent.futures.ThreadPoolExecutor</code></a> to use threads explicitly, on Python 2.6 and higher, you can <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow">import <code>Pool</code></a> from <a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy" rel="nofollow"><code>multiprocessing.dummy</code></a> which is similar to the <code>multiprocessing</code> API, but backed by threads instead of processes.</p>
<p>Of course, if you need to do CPU bound work in CPython (the reference interpreter), you'd want to use <code>multiprocessing</code> proper, not <code>multiprocessing.dummy</code>; Python threads are fine for I/O bound work, but <a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow">the GIL</a> makes them pretty bad for CPU bound work.</p>
<p>Here's code to replace your explicit use of <code>Thread</code>s with <code>multiprocessing.dummy</code>'s <code>Pool</code>, using a fixed number of workers that each complete tasks as fast as possible one after another, rather than having an infinite number of one job threads. First off, since the local I/O is likely to be fairly cheap, and you want to synchronize the output, we'll make the worker task return the resulting data rather than write it out itself, and have the main thread do the write to local disk (removing the need for locking, as well as the need for opening the file over and over). This changes <code>makeURLcall</code> to:</p>
<pre><code># Accept args as a single sequence to ease use of imap_unordered,
# and unpack on first line
def makeURLcall(args):
    arg1, arg2, arg3, dowhat, result = args

    url1 = "some URL call with args"
    url2 = "some URL call with args"

    if dowhat == "IN" :
         result = get_wip_info(url1)
    elif dowhat == "OUT" :
         result = get_wip_info(url2)

    return "%s - %s\n" % (serial, result)
</code></pre>
<p>And now for the code that replaces your explicit thread use:</p>
<pre><code>import multiprocessing.dummy as mp
from contextlib import closing

# Open input and output files and create pool
# Odds are that 32 is enough workers to saturate the connection,
# but you can play around; somewhere between 16 and 128 is likely to be the
# sweet spot for network I/O
with open(file_source) as inf,\
     open(file_output, 'w') as outf,\
     closing(mp.Pool(32)) as pool:
    # Define generator that creates tuples of arguments to pass to makeURLcall
    # We also read the file in lazily instead of using readlines, to
    # start producing results faster
    tasks = ((SNs.strip(), args1, testername, dowhat, result) for SNs in inf)
    # Pulls and writes results from the workers as they become available
    outf.writelines(pool.imap_unordered(makeURLcall, tasks))

# Once we leave the with block, input and output files are closed, and
# pool workers are cleaned up
</code></pre>
</div>
<span class="comment-copy">As an aside, why do you want that many threads? Threads have memory and CPU context-switching overhead that will reduce performance. For example, Windows allocates 1MB of stack per-thread. You might want to consider doing asynchronous IO if you're requesting a bunch of URLs. Have a look at <a href="http://unirest.io/python.html" rel="nofollow noreferrer">unirest.io/python.html</a></span>
<span class="comment-copy">hi, thank you very much for your reply. is it as simple as adding multiprocessing.pool(1000) ? does this means i can now create 1000 threads?</span>
<span class="comment-copy">@BarathanR: No, clearly your system doesn't seem to support more than 800 or so, whether due to <code>ulimit</code>s or lack of space for that many thread stacks is irrelevant, that's just your limit. The point of using the <code>Pool</code> is to have a fixed number of worker threads below the cap; in practice, even for I/O bound tasks, you're unlikely to gain anything from 800 threads running simultaneously (you'd need to test, but I suspect parallelism gains would max out somewhere between 16 and 128 threads; at a certain point, you're saturating the network anyway).</span>
<span class="comment-copy">The <code>Pool</code> benefit is that each thread can do more than one job; when they complete a task, the worker thread just grabs another job and starts doing it. I'll add some example code in a sec.</span>
<span class="comment-copy">@BarathanR: Code example added. Can't swear it's going to work straight out of the box (not tested, since your code isn't runnable on its own), but it should be pretty close.</span>
