<div class="post-text" itemprop="text">
<p>So, I am playing around with <code>multiprocessing.Pool</code> and <code>Numpy</code>, but it seems I missed some important point. Why is the <code>pool</code> version much slower? I looked at <code>htop</code> and I can see several processes be created, but they all share one of the CPUs adding up to ~100%. </p>
<pre><code>$ cat test_multi.py 
import numpy as np
from timeit import timeit
from multiprocessing import Pool


def mmul(matrix):
    for i in range(100):
        matrix = matrix * matrix
    return matrix

if __name__ == '__main__':
    matrices = []
    for i in range(4):
        matrices.append(np.random.random_integers(100, size=(1000, 1000)))

    pool = Pool(8)
    print timeit(lambda: map(mmul, matrices), number=20)
    print timeit(lambda: pool.map(mmul, matrices), number=20)

$ python test_multi.py 
16.0265390873
19.097837925
</code></pre>
<p><strong>[update]</strong> </p>
<ul>
<li>changed to <code>timeit</code> for benchmarking processes</li>
<li>init Pool with a number of my cores</li>
<li>changed computation so that there is more computation and less memory transfer (I hope)</li>
</ul>
<p>Still no change. <code>pool</code> version is still slower and I can see in <code>htop</code> that only one core is used also several processes are spawned.</p>
<p><strong>[update2]</strong></p>
<p>At the moment I am reading about @Jan-Philip Gehrcke's suggestion to use <code>multiprocessing.Process()</code> and <code>Queue</code>. But in the meantime I would like to know:</p>
<ol>
<li>Why does my example work for tiago? What could be the reason it is not working on my machine<a href="https://stackoverflow.com/questions/15414027/multiprocessing-pool-makes-numpy-matrix-multiplication-slower#comment22155148_15414027">1</a>? </li>
<li>Is in my example code any copying between the processes? I intended my code to give each thread one matrix of the matrices list.</li>
<li>Is my code a bad example, because I use <code>Numpy</code>?</li>
</ol>
<p>I learned that often one gets better answer, when the others know my end goal so: I have a lot of files, which are atm loaded and processed in a serial fashion. The processing is CPU intense, so I assume much could be gained by parallelization. My aim is it to call the python function that analyses a file in parallel. Furthermore this function is just an interface to C code, I assume, that makes a difference. </p>
<p><a href="https://stackoverflow.com/questions/15414027/multiprocessing-pool-makes-numpy-matrix-multiplication-slower#comment22155148_15414027">1</a> Ubuntu 12.04, Python 2.7.3, i7 860 @ 2.80 -  Please leave a comment if you need more info.</p>
<p><strong>[update3]</strong></p>
<p>Here are the results from Stefano's example code. For some reason there is no speed up. :/</p>
<pre><code>testing with 16 matrices
base  4.27
   1  5.07
   2  4.76
   4  4.71
   8  4.78
  16  4.79
testing with 32 matrices
base  8.82
   1 10.39
   2 10.58
   4 10.73
   8  9.46
  16  9.54
testing with 64 matrices
base 17.38
   1 19.34
   2 19.62
   4 19.59
   8 19.39
  16 19.34
</code></pre>
<p><strong>[update 4] answer to <a href="https://stackoverflow.com/questions/15414027/multiprocessing-pool-makes-numpy-matrix-multiplication-slower#comment22155148_15414027">Jan-Philip Gehrcke's comment</a></strong></p>
<p>Sorry that I haven't made myself clearer. As I wrote in Update 2 my main goal is it to parallelize many serial calls of a 3rd party Python library function. This function is an interface to some C code. I was recommended to use <code>Pool</code>, but this didn't work, so I tried something simpler, the shown above example with <code>numpy</code>. But also there I could not achieve a performance improvement, even though it looks for me 'emberassing parallelizable`. So I assume I must have missed something important. This information is what I am looking for with this question and bounty.</p>
<p><strong>[update 5]</strong></p>
<p>Thanks for all your tremendous input. But reading through your answers only creates more questions for me. For that reason I will read about the <a href="http://www.scipy.org/ParallelProgramming" rel="nofollow noreferrer">basics</a> and create new SO questions when I have a clearer understanding of what I don't know.</p>
</div>
<div class="post-text" itemprop="text">
<p>Regarding the fact that all of your processes are running on the same CPU, <a href="https://stackoverflow.com/questions/15639779/python-what-determines-whether-different-processes-are-assigned-to-the-same-or/15641148#15641148">see my answer here</a>.</p>
<p>During import, <code>numpy</code> changes the CPU affinity of the parent process, such that when you later use <code>Pool</code> all of the worker processes that it spawns will end up vying for for the same core, rather than using all of the cores available on your machine.</p>
<p>You can call <code>taskset</code> after you import <code>numpy</code> to reset the CPU affinity so that all cores are used:</p>
<pre><code>import numpy as np
import os
from timeit import timeit
from multiprocessing import Pool


def mmul(matrix):
    for i in range(100):
        matrix = matrix * matrix
    return matrix

if __name__ == '__main__':

    matrices = []
    for i in range(4):
        matrices.append(np.random.random_integers(100, size=(1000, 1000)))

    print timeit(lambda: map(mmul, matrices), number=20)

    # after importing numpy, reset the CPU affinity of the parent process so
    # that it will use all cores
    os.system("taskset -p 0xff %d" % os.getpid())

    pool = Pool(8)
    print timeit(lambda: pool.map(mmul, matrices), number=20)
</code></pre>
<p>Output:</p>
<pre><code>    $ python tmp.py                                     
    12.4765810966
    pid 29150's current affinity mask: 1
    pid 29150's new affinity mask: ff
    13.4136221409
</code></pre>
<p>If you watch CPU useage using <code>top</code> while you run this script, you should see it using all of your cores when it executes the 'parallel' part. As others have pointed out, in your original example the overhead involved in pickling data, process creation etc. probably outweigh any possible benefit from parallelisation.</p>
<p><strong>Edit:</strong> I suspect that part of the reason why the single process seems to be consistently faster is that <code>numpy</code> may have some tricks for speeding up that element-wise matrix multiplication that it cannot use when the jobs are spread across multiple cores.</p>
<p>For example, if I just use ordinary Python lists to compute the Fibonacci sequence, I can get a huge speedup from parallelisation. Likewise, if I do element-wise multiplication in a way that takes no advantage of vectorization, I get a similar speedup for the parallel version:</p>
<pre><code>import numpy as np
import os
from timeit import timeit
from multiprocessing import Pool

def fib(dummy):
    n = [1,1]
    for ii in xrange(100000):
        n.append(n[-1]+n[-2])

def silly_mult(matrix):
    for row in matrix:
        for val in row:
            val * val

if __name__ == '__main__':

    dt = timeit(lambda: map(fib, xrange(10)), number=10)
    print "Fibonacci, non-parallel: %.3f" %dt

    matrices = [np.random.randn(1000,1000) for ii in xrange(10)]
    dt = timeit(lambda: map(silly_mult, matrices), number=10)
    print "Silly matrix multiplication, non-parallel: %.3f" %dt

    # after importing numpy, reset the CPU affinity of the parent process so
    # that it will use all CPUS
    os.system("taskset -p 0xff %d" % os.getpid())

    pool = Pool(8)

    dt = timeit(lambda: pool.map(fib,xrange(10)), number=10)
    print "Fibonacci, parallel: %.3f" %dt

    dt = timeit(lambda: pool.map(silly_mult, matrices), number=10)
    print "Silly matrix multiplication, parallel: %.3f" %dt
</code></pre>
<p>Output:</p>
<pre><code>$ python tmp.py
Fibonacci, non-parallel: 32.449
Silly matrix multiplication, non-parallel: 40.084
pid 29528's current affinity mask: 1
pid 29528's new affinity mask: ff
Fibonacci, parallel: 9.462
Silly matrix multiplication, parallel: 12.163
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The unpredictable competition between communication overhead and computation speedup is definitely the issue here. What you are observing is perfectly fine. Whether you get a net speed-up depends on many factors and is something that has to be quantified properly (as you did). </p>
<p><strong>So why is <code>multiprocessing</code> so "unexpectedly slow" in your case?</strong> <code>multiprocessing</code>'s <code>map</code> and <code>map_async</code> functions actually pickle Python objects back and forth through pipes that connect the parent with the child processes. This may take a considerable amount of time. During that time, the child processes have almost nothing to do, which is what to see in <code>htop</code>. Between different systems, there might be a considerable pipe transport performance difference, which is also why for some people your pool code is faster than your single CPU code, although for you it is not (other factors might come into play here, this is just an example in order to explain the effect).</p>
<p><strong>What can you do to make it faster?</strong></p>
<ol>
<li><p><strong>Don't pickle the input on POSIX-compliant systems.</strong></p>
<p>If you are on Unix, you can get around the parent-&gt;child communication overhead via taking advantage of POSIX' process fork behavior (copy memory on write):</p>
<p>Create your job input (e.g. a list of large matrices) to work on in the parent process in <em>a globally accessible variable</em>. Then create worker processes by calling <code>multiprocessing.Process()</code> yourself. In the children, grab the job input from the global variable. Simply expressed, this makes the child access the memory of the parent without any communication overhead (*, explanation below). Send the result back to the parent, through e.g. a <code>multiprocessing.Queue</code>. This will save a lot of communication overhead, especially if the output is small compared to the input. This method won't work on e.g. Windows, because <code>multiprocessing.Process()</code> there creates an entirely new Python process that does not inherit the state of the parent.</p></li>
<li><p><strong>Make use of numpy multithreading.</strong>
Depending on your actual calculation task, it might happen that involving <code>multiprocessing</code> won't help at all. If you compile numpy yourself and enable OpenMP directives, then operations on larges matrices might become very efficiently multithreaded (and distributed over many CPU cores; the GIL is no limiting factor here) by themselves. Basically, this is the most efficient usage of multiple CPU cores you can get in the context of numpy/scipy.</p></li>
</ol>
<p>*The child cannot directly access the parent's memory in general. However, after <code>fork()</code>, parent and child are in an equivalent state. It would be stupid to copy the entire memory of the parent to another place in the RAM. That's why the copy-on-write principle jumps in. As long as the child does not <em>change</em> its memory state, it actually accesses the parent's memory. Only upon modification, the corresponding bits and pieces are copied into the memory space of the child.</p>
<p><strong>Major edit:</strong></p>
<p>Let me add a piece of code that crunches a large amount of input data with multiple worker processes and follows the advice "1. Don't pickle the input on POSIX-compliant systems.". Furthermore, the amount of information transferred back to the worker manager (the parent process) is quite low. The heavy computation part of this example is a single value decomposition. It can make heavy use of OpenMP. I have executed the example multiple times:</p>
<ul>
<li>Once with 1, 2, or 4 worker processes and <code>OMP_NUM_THREADS=1</code>, so each worker process creates a maximum load of 100 %. There, the mentioned number-of-workers-compute-time scaling behavior is almost linear and the net speedup factor up corresponds to the number of workers involved.</li>
<li>Once with 1, 2, or 4 worker processes and <code>OMP_NUM_THREADS=4</code>, so that each process creates a maximum load of 400 % (via spawning 4 OpenMP threads). My machine has 16 real cores, so 4 processes with max 400 % load each will <em>almost</em> get the maximum performance out of the machine. The scaling is not perfectly linear anymore and the speedup factor is not the number of workers involved, but the absolute calculation time becomes significantly reduced compared to <code>OMP_NUM_THREADS=1</code> and time still decreases significantly with the number of worker processes.</li>
<li>Once with larger input data, 4 cores, and <code>OMP_NUM_THREADS=4</code>. It results in an average system load of 1253 %.</li>
<li>Once with same setup as last, but <code>OMP_NUM_THREADS=5</code>. It results in an average system load of 1598 %, which suggests that we got everything from that 16 core machine. However, the actual computation wall time does not improve compared to the latter case.</li>
</ul>
<p>The code:</p>
<pre><code>import os
import time
import math
import numpy as np
from numpy.linalg import svd as svd
import multiprocessing


# If numpy is compiled for OpenMP, then make sure to control
# the number of OpenMP threads via the OMP_NUM_THREADS environment
# variable before running this benchmark.


MATRIX_SIZE = 1000
MATRIX_COUNT = 16


def rnd_matrix():
    offset = np.random.randint(1,10)
    stretch = 2*np.random.rand()+0.1
    return offset + stretch * np.random.rand(MATRIX_SIZE, MATRIX_SIZE)


print "Creating input matrices in parent process."
# Create input in memory. Children access this input.
INPUT = [rnd_matrix() for _ in xrange(MATRIX_COUNT)]


def worker_function(result_queue, worker_index, chunk_boundary):
    """Work on a certain chunk of the globally defined `INPUT` list.
    """
    result_chunk = []
    for m in INPUT[chunk_boundary[0]:chunk_boundary[1]]:
        # Perform single value decomposition (CPU intense).
        u, s, v = svd(m)
        # Build single numeric value as output.
        output =  int(np.sum(s))
        result_chunk.append(output)
    result_queue.put((worker_index, result_chunk))


def work(n_workers=1):
    def calc_chunksize(l, n):
        """Rudimentary function to calculate the size of chunks for equal 
        distribution of a list `l` among `n` workers.
        """
        return int(math.ceil(len(l)/float(n)))

    # Build boundaries (indices for slicing) for chunks of `INPUT` list.
    chunk_size = calc_chunksize(INPUT, n_workers)
    chunk_boundaries = [
        (i, i+chunk_size) for i in xrange(0, len(INPUT), chunk_size)]

    # When n_workers and input list size are of same order of magnitude,
    # the above method might have created less chunks than workers available. 
    if n_workers != len(chunk_boundaries):
        return None

    result_queue = multiprocessing.Queue()
    # Prepare child processes.
    children = []
    for worker_index in xrange(n_workers):
        children.append(
            multiprocessing.Process(
                target=worker_function,
                args=(
                    result_queue,
                    worker_index,
                    chunk_boundaries[worker_index],
                    )
                )
            )

    # Run child processes.
    for c in children:
        c.start()

    # Create result list of length of `INPUT`. Assign results upon arrival.
    results = [None] * len(INPUT)

    # Wait for all results to arrive.
    for _ in xrange(n_workers):
        worker_index, result_chunk = result_queue.get(block=True)
        chunk_boundary = chunk_boundaries[worker_index]
        # Store the chunk of results just received to the overall result list.
        results[chunk_boundary[0]:chunk_boundary[1]] = result_chunk

    # Join child processes (clean up zombies).
    for c in children:
        c.join()
    return results


def main():
    durations = []
    n_children = [1, 2, 4]
    for n in n_children:
        print "Crunching input with %s child(ren)." % n
        t0 = time.time()
        result = work(n)
        if result is None:
            continue
        duration = time.time() - t0
        print "Result computed by %s child process(es): %s" % (n, result)
        print "Duration: %.2f s" % duration
        durations.append(duration)
    normalized_durations = [durations[0]/d for d in durations]
    for n, normdur in zip(n_children, normalized_durations):
        print "%s-children speedup: %.2f" % (n, normdur)


if __name__ == '__main__':
    main()
</code></pre>
<p>The output:</p>
<pre><code>$ export OMP_NUM_THREADS=1
$ /usr/bin/time python test2.py 
Creating input matrices in parent process.
Crunching input with 1 child(ren).
Result computed by 1 child process(es): [5587, 8576, 11566, 12315, 7453, 23245, 6136, 12387, 20634, 10661, 15091, 14090, 11997, 20597, 21991, 7972]
Duration: 16.66 s
Crunching input with 2 child(ren).
Result computed by 2 child process(es): [5587, 8576, 11566, 12315, 7453, 23245, 6136, 12387, 20634, 10661, 15091, 14090, 11997, 20597, 21991, 7972]
Duration: 8.27 s
Crunching input with 4 child(ren).
Result computed by 4 child process(es): [5587, 8576, 11566, 12315, 7453, 23245, 6136, 12387, 20634, 10661, 15091, 14090, 11997, 20597, 21991, 7972]
Duration: 4.37 s
1-children speedup: 1.00
2-children speedup: 2.02
4-children speedup: 3.81
48.75user 1.75system 0:30.00elapsed 168%CPU (0avgtext+0avgdata 1007936maxresident)k
0inputs+8outputs (1major+809308minor)pagefaults 0swaps

$ export OMP_NUM_THREADS=4
$ /usr/bin/time python test2.py 
Creating input matrices in parent process.
Crunching input with 1 child(ren).
Result computed by 1 child process(es): [22735, 5932, 15692, 14129, 6953, 12383, 17178, 14896, 16270, 5591, 4174, 5843, 11740, 17430, 15861, 12137]
Duration: 8.62 s
Crunching input with 2 child(ren).
Result computed by 2 child process(es): [22735, 5932, 15692, 14129, 6953, 12383, 17178, 14896, 16270, 5591, 4174, 5843, 11740, 17430, 15861, 12137]
Duration: 4.92 s
Crunching input with 4 child(ren).
Result computed by 4 child process(es): [22735, 5932, 15692, 14129, 6953, 12383, 17178, 14896, 16270, 5591, 4174, 5843, 11740, 17430, 15861, 12137]
Duration: 2.95 s
1-children speedup: 1.00
2-children speedup: 1.75
4-children speedup: 2.92
106.72user 3.07system 0:17.19elapsed 638%CPU (0avgtext+0avgdata 1022240maxresident)k
0inputs+8outputs (1major+841915minor)pagefaults 0swaps

$ /usr/bin/time python test2.py 
Creating input matrices in parent process.
Crunching input with 4 child(ren).
Result computed by 4 child process(es): [21762, 26806, 10148, 22947, 20900, 8161, 20168, 17439, 23497, 26360, 6789, 11216, 12769, 23022, 26221, 20480, 19140, 13757, 23692, 19541, 24644, 21251, 21000, 21687, 32187, 5639, 23314, 14678, 18289, 12493, 29766, 14987, 12580, 17988, 20853, 4572, 16538, 13284, 18612, 28617, 19017, 23145, 11183, 21018, 10922, 11709, 27895, 8981]
Duration: 12.69 s
4-children speedup: 1.00
174.03user 4.40system 0:14.23elapsed 1253%CPU (0avgtext+0avgdata 2887456maxresident)k
0inputs+8outputs (1major+1211632minor)pagefaults 0swaps

$ export OMP_NUM_THREADS=5
$ /usr/bin/time python test2.py 
Creating input matrices in parent process.
Crunching input with 4 child(ren).
Result computed by 4 child process(es): [19528, 17575, 21792, 24303, 6352, 22422, 25338, 18183, 15895, 19644, 20161, 22556, 24657, 30571, 13940, 18891, 10866, 21363, 20585, 15289, 6732, 10851, 11492, 29146, 12611, 15022, 18967, 25171, 10759, 27283, 30413, 14519, 25456, 18934, 28445, 12768, 28152, 24055, 9285, 26834, 27731, 33398, 10172, 22364, 12117, 14967, 18498, 8111]
Duration: 13.08 s
4-children speedup: 1.00
230.16user 5.98system 0:14.77elapsed 1598%CPU (0avgtext+0avgdata 2898640maxresident)k
0inputs+8outputs (1major+1219611minor)pagefaults 0swaps
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Your code is correct. I just ran it my system (with 2 cores, hyperthreading) and obtained the following results:</p>
<pre><code>$ python test_multi.py 
30.8623809814
19.3914041519
</code></pre>
<p>I looked at the processes and, as expected, the parallel part showing several processes working at near 100%. This must be something in your system or python installation. </p>
</div>
<div class="post-text" itemprop="text">
<p>Measuring arithmetic throughput is a very difficult task: basically your test case is too simple, and I see many problems.</p>
<p>First you are testing integer arithmetic: is there a special reason? With floating point you get results that are comparable across many different architectures.</p>
<p><strike>Second <code>matrix = matrix*matrix</code> overwrites the input parameter (matrices are passed by ref and not by value), and each sample has to work on different data... </strike></p>
<p>Last tests should be conducted over a wider range of problem size and number of workers, in order to grasp general trends.</p>
<p>So here is my modified test script</p>
<pre><code>import numpy as np
from timeit import timeit
from multiprocessing import Pool

def mmul(matrix):
    mymatrix = matrix.copy()
    for i in range(100):
        mymatrix *= mymatrix
    return mymatrix

if __name__ == '__main__':

    for n in (16, 32, 64):
        matrices = []
        for i in range(n):
            matrices.append(np.random.random_sample(size=(1000, 1000)))

        stmt = 'from __main__ import mmul, matrices'
        print 'testing with', n, 'matrices'
        print 'base',
        print '%5.2f' % timeit('r = map(mmul, matrices)', setup=stmt, number=1)

        stmt = 'from __main__ import mmul, matrices, pool'
        for i in (1, 2, 4, 8, 16):
            pool = Pool(i)
            print "%4d" % i, 
            print '%5.2f' % timeit('r = pool.map(mmul, matrices)', setup=stmt, number=1)
            pool.close()
            pool.join()
</code></pre>
<p>and my results:</p>
<pre><code>$ python test_multi.py 
testing with 16 matrices
base  5.77
   1  6.72
   2  3.64
   4  3.41
   8  2.58
  16  2.47
testing with 32 matrices
base 11.69
   1 11.87
   2  9.15
   4  5.48
   8  4.68
  16  3.81
testing with 64 matrices
base 22.36
   1 25.65
   2 15.60
   4 12.20
   8  9.28
  16  9.04
</code></pre>
<p>[UPDATE] I run this example at home on a different computer, obtaining a consistent slow-down:</p>
<pre><code>testing with 16 matrices
base  2.42
   1  2.99
   2  2.64
   4  2.80
   8  2.90
  16  2.93
testing with 32 matrices
base  4.77
   1  6.01
   2  5.38
   4  5.76
   8  6.02
  16  6.03
testing with 64 matrices
base  9.92
   1 12.41
   2 10.64
   4 11.03
   8 11.55
  16 11.59
</code></pre>
<p>I have to confess that I do not know who is to blame (numpy, python, compiler, kernel)...</p>
</div>
<div class="post-text" itemprop="text">
<p>By default, <code>Pool</code> only uses n processes, where n is the number of CPUs on your machine. You need to specify how many processes you want it to use, like <code>Pool(5)</code>.</p>
<p><a href="http://docs.python.org/3/library/multiprocessing.html" rel="nofollow">See here for more info</a></p>
</div>
<div class="post-text" itemprop="text">
<p>Since you mention that you have a lot of files, I would suggest the following solution;</p>
<ul>
<li>Make a list of filenames. </li>
<li>Write a function that loads and processes a single file named as the input parameter.</li>
<li>Use <code>Pool.map()</code> to apply the function to the list of files.</li>
</ul>
<p>Since every instance now loads its own file, the only data passed around are filenames, not (potentially large) numpy arrays.</p>
</div>
<div class="post-text" itemprop="text">
<p>I also noticed that when I ran numpy matrix multiplication inside of a Pool.map() function, it ran much slower on certain machines.  My goal was to parallelize my work using Pool.map(), and run a process on each core of my machine.  When things were running fast, the numpy matrix multiplication was only a small part of the overall work performed in parallel.  When I looked at the CPU usage of the processes, I could see that each process could use e.g. 400+% CPU on the machines where it ran slow, but always &lt;=100% on the machines where it ran fast.  For me, the solution was to <a href="https://stackoverflow.com/questions/17053671/python-how-do-you-stop-numpy-from-multithreading">stop numpy from multithreading</a>.  It turns out that numpy was set up to multithread on exactly the machines where my Pool.map() was running slow.  Evidently, if you are already parallelizing using Pool.map(), then having numpy also parallelize just creates interference.  I just called <code>export MKL_NUM_THREADS=1</code> before running my Python code and it worked fast everywhere.</p>
</div>
<span class="comment-copy">I guess the overhead of creating processes kills you here. Try using the <code>timeit</code> module or at least move the <code>pool = Pool()</code> function out of the timing routine.</span>
<span class="comment-copy">I could be wrong, but I suspect most of the time is spent sending the matrices back and forth between your processes.</span>
<span class="comment-copy">But shouldn't all the processes/threads work on their own matrix? Like each process taking one matrix from the list and working with that?</span>
<span class="comment-copy">But you have to pass them between different processes (i.e. copy the memory). Matrix multiplication is rather quick (it takes about 6ms according to your timings) such that this overhead is significant.</span>
<span class="comment-copy">I changed the example, so that there is more computation and less memory transfer.</span>
<span class="comment-copy">I think the first sentence of this answer is pretty much the whole answer. Everything is getting run on the same core, therefore it's slightly slower (because there's extra overhead) rather than faster (because there's no parallelization).</span>
<span class="comment-copy">Actually I still believe this is more likely to be related to the quirks of <code>numpy</code> rather than just to do with CPU utilization. Even when I parallelize Framester's original code so that it actually makes use of all of my CPUs I <i>still</i> find that it's marginally slower than running it serially.  It's only when I deliberately <i>avoid</i> doing things that <code>numpy</code> is particularly good at that I see any performance gain from parallelization.</span>
<span class="comment-copy">You're right; sorry, I didn't read far enough, I just started testing on my own trivial/stupid sample code. Never mind. :)</span>
<span class="comment-copy">For comparison, you have to show what happens when you leave <code>os.system("taskset -p 0xff %d" % os.getpid())</code> out.</span>
<span class="comment-copy">Why? If I leave that line out then (at least on my machine) only one core will be utilised, so of course I don't see any speedup from the parallel version.</span>
<span class="comment-copy">Additional question regarding point2: <a href="http://stackoverflow.com/questions/15531556/how-to-recompile-numpy-with-enabled-openmp-directives" title="how to recompile numpy with enabled openmp directives">stackoverflow.com/questions/15531556/…</a></span>
<span class="comment-copy">+1: most plausible explanation. Let me just add that apart from allowing OpenMP in numpy, one should also use vendor blas libraries, if available.</span>
<span class="comment-copy">I wonder whether the reason you <i>don't</i> see much of a noticeable performance gain when parallelizing matrix manipulations in <code>numpy</code> might actually be <i>because</i> <code>numpy</code> uses external BLAS and LAPACK libraries, which are often compiled to use multiple cores simultaneously.  If you try and run something like <code>svd</code> in parallel (which uses LAPACK) perhaps each worker still behaves as though it is executing on multiple cores, and does 'sub-optimal' things like writing to each other's caches etc.</span>
<span class="comment-copy">@ali_m: In the first example, we see ideal scaling (1-children speedup: 1.00, 2-children speedup: 2.02, 4-children speedup: 3.81). I guess what you are talking about is: Compute duration in case 4 children / <code>OMP_NUM_THREADS=1</code>: 4.37 s vs. 2.95 s with <code>OMP_NUM_THREADS=4</code>. Yes, this by far is not a change of factor 4 (as would have been ideal). However, this is expected. As SVD on huge matrices involves shifting around tons of data between RAM, cache, and registers, the corresponding pipelines (esp. between CPU and RAM, i.e. Hypertransport/Quickpath/FSB) are the bottleneck. Very simple.</span>
<span class="comment-copy">Thanks for the example code. Unfortunately sometimes the code stops after 'Crunching input with 1 child(ren)' and stays there forever. But I haven't checked the OMP support of my numpy version.</span>
<span class="comment-copy">Thanks for trying my code +1 and your assessment. Any idea what could be wrong, or what I could google for?</span>
<span class="comment-copy">Not sure what could be wrong. What system are you using? I'd try other <code>multiprocessing</code> methods aside from <code>Pool</code> to start, or even <code>Pool</code> with different processes working on parts of a shared array.</span>
<span class="comment-copy">Thanks, but I get following error messages: 'Exception RuntimeError: RuntimeError('cannot join current thread',) in &lt;Finalize object, dead&gt; ignored'</span>
<span class="comment-copy">@Framester please add <code>pool.join()</code> after <code>pool.close()</code>; if run time is short you can increase the number of iterations in <code>timeit</code>.</span>
<span class="comment-copy">Thanks, I tried the code and I also get no speedup?</span>
<span class="comment-copy">No one to blame except for the code! :) I tried it on a modern 16 core E5-2650 system. I observe a speed up for an mp pool of size 2 and 4. Above that, the execution time becomes worse again. The parallelization method of this code is by far not efficient. Stefano: the speed up you have observed on one computer is not at all linear to the number of cores involved. A reasonable theory explaining the differences between your two computers: in the first example, the ratio between single core speed and pipe transport performance is smaller than in the second example.</span>
<span class="comment-copy">thanks for your input, please see my update</span>
