<div class="post-text" itemprop="text">
<p>I am currently working on my first Python project and I need to parse through a 2GB file.
I've found out that if I went line by line it would be very very slow...
However the buffering method, using:</p>
<pre><code>f = open(filename)                  
lines = 0
buf_size = 1024 * 1024
read_f = f.read 
buf = read_f(buf_size)
while buf:
    for line in buf:
      #code for string search
      print line
    buf = read_f(buf_size)
</code></pre>
<p>Here the print line doesn't print a "line", it only prints a character at a time per line. So I am having problem doing substring find on it...
Please Help!</p>
</div>
<div class="post-text" itemprop="text">
<p><code>print line</code> prints a character because <code>buf</code> is a string, and iterating over a string yields the characters of the string as 1-character strings.</p>
<p>When you say that reading line-by-line was slow, how did you implement the read? If you were using readlines(), that would explain the slowness (see  <a href="http://stupidpythonideas.blogspot.com/2013/06/readlines-considered-silly.html" rel="nofollow">http://stupidpythonideas.blogspot.com/2013/06/readlines-considered-silly.html</a>).</p>
<p>Files are iterable over their lines, and Python will pick a buffer size when iterating, so this might suit your needs:</p>
<pre><code>for line in f:
    # do search stuff
</code></pre>
<p>If you want to specify the buffer size manually, you could also do this:</p>
<pre><code>buf = f.readlines(buffersize)
while buf:
    for line in buf:
        # do search stuff
    buf = f.readlines(buffersize)
</code></pre>
<p>Though, the first of the two is usually better.</p>
</div>
<div class="post-text" itemprop="text">
<p>The problem is that buf is a string...</p>
<p>Say buf = "abcd"</p>
<p>That means, buf[0] = a, buf[1]=b and so on.</p>
<pre><code>for line in buf:
    print line
</code></pre>
<p>would result in 
a
b
c
d</p>
<p>That means in your for-loop, you do not loop over "lines", but over all elements of the buf-string. You may use readlines or split your buffer to single lines by looking for "\n".</p>
</div>
<span class="comment-copy">The "for line" thing works with files because the file iterator is built to break input into lines. The string iterator you have here is built to break strings into characters. You'll get better performance with a larger file buffer but I can't make any promises about how much! Go back to iterating the file line by line and try a 128K buffer <code>open(filename, "r", 128*1024)</code>.</span>
<span class="comment-copy">Note: you can use <a href="http://docs.python.org/3/library/functions.html#iter" rel="nofollow noreferrer"><code>iter(callable, sentinel)</code></a> to avoid the <code>while</code> loop: <code>for chunk in iter(lambda: f.read(1024 * 1024), ''): #search the substring</code>. In this case <code>iter</code> will create an iterable that calls its <code>callable</code> argument (i.e. does <code>callable()</code>) until the <code>sentinel</code> value is found. Anyway reading a 2GB file <i>will</i> take some time. Assuming your hard-disk can be read at 200 MB/s, it will take 10 secons <i>at the least</i>, and I believe HDDs are usually between 50 and 150 MB/s!</span>
<span class="comment-copy">Thanks, looks like i misunderstood about what was buf. If I do do 'for line in f:' iterating a 2G file takes me around 2 minutes. Can this be reduce even more?</span>
<span class="comment-copy">If you don't mind throwing memory to the wind, you can mmap the file. (see <a href="http://stackoverflow.com/questions/8151684/how-to-read-lines-from-mmap-file-in-python" title="how to read lines from mmap file in python">stackoverflow.com/questions/8151684/â€¦</a>). Other than that, you can manually try varying the buffer size.</span>
<span class="comment-copy">do you mean something like <code>for line in buf: l=line.readline()</code> ?</span>
<span class="comment-copy">@MojingLiu No, he means <code>for line in buf.split('\n')</code>.</span>
