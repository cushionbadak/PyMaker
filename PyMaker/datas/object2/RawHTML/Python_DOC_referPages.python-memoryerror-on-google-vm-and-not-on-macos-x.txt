<div class="post-text" itemprop="text">
<pre><code>import numpy as np

array = np.zeros((210000, 210000)) # default numpy.float64
array.nbytes
</code></pre>
<p>When I run the above code on my 8GB memory MacBook with macOS, no error occurs. But running the same code on a 16GB memory PC with Windows 10, or a 12GB memory Ubuntu laptop, or even on a 128GB memory Linux supercomputer, the Python interpreter will raise a MemoryError. All the test environments have 64-bit Python 3.6 or 3.7 installed.</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://stackoverflow.com/a/54961805/">@Martijn Pieters' answer</a> is on the right track, but not quite right: this has nothing to do with memory compression, but instead it has to do with <a href="https://en.wikipedia.org/wiki/Virtual_memory" rel="noreferrer">virtual memory</a>.</p>
<p>For example, try running the following code on your machine:</p>
<pre><code>arrays = [np.zeros((21000, 21000)) for _ in range(0, 10000)]
</code></pre>
<p>This code allocates 32TiB of memory, but you won't get an error (at least I didn't, on Linux). If I check htop, I see the following:</p>
<pre><code>  PID USER      PRI  NI  VIRT   RES   SHR S CPU% MEM%   TIME+  Command
31362 user       20   0 32.1T 69216 12712 S  0.0  0.4  0:00.22 python
</code></pre>
<p>This because the OS is perfectly willing to <a href="https://www.mjmwired.net/kernel/Documentation/vm/overcommit-accounting" rel="noreferrer">overcommit on virtual memory</a>. It won't actually assign pages to physical memory until it needs to. The way it works is:</p>
<ul>
<li><code>calloc</code> asks the OS for some memory to use</li>
<li>the OS looks in the process's page tables, and finds a chunk of memory that it's willing to assign. This is fast operation, the OS just stores the memory address range in an internal data structure.</li>
<li>the program writes to one of the addresses.</li>
<li>the OS receives a <a href="https://en.wikipedia.org/wiki/Page_fault" rel="noreferrer">page fault</a>, at which point it looks and actually assigns the page to physical memory. <a href="https://unix.stackexchange.com/q/128213/70735">A page is usually a few KiB in size</a>.</li>
<li>the OS passes control back to the program, which proceeds without noticing the interruption.</li>
</ul>
<p>Creating a single huge array doesn't work on Linux because, by default, a <a href="http://engineering.pivotal.io/post/virtual_memory_settings_in_linux_-_the_problem_with_overcommit/" rel="noreferrer">"heuristic algorithm is applied to figure out if enough memory is available".</a> (<a href="https://stackoverflow.com/questions/54961554/why-can-a-352gb-numpy-ndarray-be-used-on-an-8gb-memory-macos-computer/54964553#comment96694843_54964553">thanks @Martijn Pieters!</a>) Some experiments on my system show that for me, the kernel is unwilling to provide more than <code>0x3BAFFFFFF</code> bytes. However, if I run <code>echo 1 | sudo tee /proc/sys/vm/overcommit_memory</code>, and then try the program in the OP again, it works fine.</p>
<p>For fun, try running <code>arrays = [np.ones((21000, 21000)) for _ in range(0, 10000)]</code>. You'll definitely get an out of memory error, even on MacOs or Linux with swap compression. Yes, certain OSes can compress RAM, but they can't compress it to the level that you wouldn't run out of memory.</p>
</div>
<span class="comment-copy">I tried your first example which indeed the Linux allocated 32t virtual memory on a 128GB memory server. However, MemoryError raised with my example <code>array = np.zeros((210000, 210000))</code>. My example will only need 352GB virtual memory which seems more reasonable than the 32t virtual memory.</span>
<span class="comment-copy">@BlaiseWang Right, I addressed that in my answer "I have no idea why creating a single huge array doesn't work on Linux or Windows, but I'd expect it to have more to do with the platform's implementation of libc and the limits imposed there than the operating system." If you'd really like to know why, I'd suggest you review the code in <a href="https://code.woboq.org/userspace/glibc/malloc/malloc.c.html" rel="nofollow noreferrer">code.woboq.org/userspace/glibc/malloc/malloc.c.html</a> (I can't be bothered to do so)</span>
<span class="comment-copy">@BlaiseWang It's because NumPy is filling the array with zeroes. malloc() typically doesn't care about what's in the allocated memory when it gets it, so Unix might do the same with the pages it returns - as soon as you write zeroes, though, it gets allocated. Some versions of Unix, like the one on user60651's machine, might also guarantee that new pages are zeroed out, and not allocate memory unless the written value is not a zero (and just return zero if an unwritten page is read).</span>
<span class="comment-copy">@TheHansinator It's a reasonable guess but still this cannot explain why 32TiB virtual memory can be allocated but 350GB cannot be.</span>
<span class="comment-copy">Indeed, my interpretation of the compression system was wrong, it applies to inactive pages <i>in physical memory</i>, not to VM unaddressed pages of the current process. The VM overcommitting behaviour of the MacOS kernel is very hard to track down (Apple stopped updating their Virtual Memory and kernel internals documentation in 2013), but since the behaviour is configurable in Linux perhaps the OP should experiment with adjusting the <a href="http://engineering.pivotal.io/post/virtual_memory_settings_in_linux_-_the_problem_with_overcommit/" rel="nofollow noreferrer">overcommit policy</a>.</span>
