<div class="post-text" itemprop="text">
<p>I am trying to find the best way to compare large sets of numerical sequences to other large sets, in order to rank them against each other. Maybe the following toy example clarifies the issue, where lists a, b, and c represent shingles of size 3 in a time series.</p>
<pre><code>a = [(1,2,3),(2,3,4),(3,4,5)]
b = [(1,2,3),(2,3,4),(3,4,7),(4,7,8)]
c = [(1,2,3),(2,3,5)]

set_a, set_b, set_c  = set(a), set(b), set(c)

jaccard_ab = float(len(set_a.intersection(set_b)))/float(len(set_a.union(set_b)))
jaccard_bc = float(len(set_b.intersection(set_c)))/float(len(set_b.union(set_c)))
jaccard_ac = float(len(set_a.intersection(se t_c)))/float(len(set_a.union(set_c)))
</code></pre>
<p>The similarity among these sets is:</p>
<pre><code>jaccard_ab, jaccard_bc, jaccard_ac
(0.4, 0.2, 0.25)
</code></pre>
<p>So in this example, we can see that set a and b are the most similar with a score of 0.4.</p>
<p>I am having a design problem:
1) Since each set will be composed of ~1000 shingles, do I gain speed by transforming every shingle into a unique hash and then comparing hashes?
2) Initially, I have over 10,000 sets to compare so I think I am much better off storing the shingles (or hashes, depending on answer to 1) in a database or pickling. Is this a good approach?
3) As a new set is added to my workflow, I need to rank it against all existing sets and display, let's say, the top 10 most similar. Is there a better approach than the one in the toy example?</p>
</div>
<div class="post-text" itemprop="text">
<p>1) Members of a set have to be hashable, so python is already computing hashes. Storing sets of hashes of items would be duplicated effort, so there's no need to do that. </p>
<p>2) The <a href="https://wiki.python.org/moin/TimeComplexity" rel="nofollow">complexity</a> of the set intersection and union is approximately linear. The Jaccard isn't comptationally expensive, and 10,000 sets isn't <em>that</em> many (about 50 million<sup>1</sup> computations). It will probably take an hour to compute your initial results, but it won't take days. </p>
<p>3) Once you have all of your combinations, ranking another set against your existing results means doing only 10,000 more comparisons. I can't think of a simpler way than that. </p>
<p>I'd say just do it. </p>
<p>If you want to go <em>faster</em>, then you should be able to use a multiprocessing approach fairly easily with this dataset. (Each computation is independent of the other ones, so they can all run in parallel).</p>
<p>Here's an example adapted from the <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow"><code>concurrent.futures</code> examples</a> (Python3). </p>
<pre><code>import concurrent.futures

data = [
    {(1, 2, 3), (2, 3, 4), (3, 4, 5), ...},
    {(12, 13, 14), (15, 16, 17), ...},
    ...
]

def jaccard(A, B):
    return len(A &amp; B) / len(A | B) 

with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
    futures = {executor.submit(jaccard, *sets): sets
               for sets in combinations(data, 2)}

    for future in concurrent.futures.as_completed(futures):
        jaccard_index = future.result()
        print(jaccard_index) # write output to a file or database here
</code></pre>
<hr/>
<p>[1]: </p>
<pre><code>&gt;&gt;&gt; from itertools import combinations
&gt;&gt;&gt; print(sum(1 for i in combinations(range(10000), 2)))
49995000
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>1) This is done internally anyways when constructing the <code>set()</code>.</p>
<p>2) I'm not sure you'll be able to stay with python for your size of the data set, so I'd suggest using some simple (text) format so it can be easily loaded eg in C/C++. Do you need to store the shingles at all? What about generating them on the fly?</p>
<p>3) If you need all to all comparison for your initial data set, something like <a href="https://code.google.com/p/google-all-pairs-similarity-search/" rel="nofollow">google-all-pairs</a> or <a href="https://github.com/teh/ppjoin" rel="nofollow">ppjoin</a> will surely help. It works by reducing the candidate set for each comparison using predefined similarity threshold. You can modify the code to keep the index for further searches.</p>
</div>
<div class="post-text" itemprop="text">
<p>You should definitely consider utilizing multi-cores as this problem is very suitable for this task. You might consider PyPy, as I see 2-3X speedup comparing to Python 3 for large set comparison. Then you might checkout <a href="http://matpalm.com/resemblance/jaccard_coeff/" rel="nofollow">part 1: resemblance with the jaccard coefficient</a> for a magic C++ implementation to get further speed-ups. This C++ / OpenMP solution is the fastest I have tested yet.</p>
</div>
<span class="comment-copy">Is there any advantage of using concurrent module over multiprocessing for this problem?</span>
<span class="comment-copy">@Seth, thank you for your answer!</span>
<span class="comment-copy">How do you create data for google-all-pairs?</span>
<span class="comment-copy">@GökhanSever the format is not very nice, you have create a counterpart to the <a href="https://code.google.com/p/google-all-pairs-similarity-search/source/browse/trunk/data-source-iterator.cc#85" rel="nofollow noreferrer">deserializer</a>, some description of the format can be found <a href="https://code.google.com/p/google-all-pairs-similarity-search/source/browse/trunk/README#36" rel="nofollow noreferrer">here</a>. You need to do at least two sorts - one on the feature frequencies, one on the record lengths.</span>
<span class="comment-copy">that seems not very practical to use then given those extra steps.</span>
<span class="comment-copy">@GökhanSever depends on the size of your dataset. Around 10 million items any O(n^2) solution  is much less practical than two sorts and binary write..</span>
<span class="comment-copy">With ~100K items, other solutions do the job for me. I will look further if my dataset extends to the size you mentioned.</span>
