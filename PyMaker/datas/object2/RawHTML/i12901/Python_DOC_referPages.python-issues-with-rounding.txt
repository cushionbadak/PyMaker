<div class="post-text" itemprop="text">
<p>I have a rounding issue in Python 2.7 leading to unexpected output. I'm trying to get combinations of p1 and p2 that sum up to 0.6 or less in total.</p>
<pre><code>from itertools import product
P = []
p1 = [0.0,0.2,0.4,0.6]
p2 = [0.0,0.2,0.4,0.6] 
for p11,p22 in product(p1,p2):
    if p11+p22 &lt;= max(p1):    
        P.append((p11,p22))
</code></pre>
<p>However, when I run this, it does not include all values for which p11+p22 = 0.6:</p>
<pre><code>[(0.0, 0.0),
 (0.0, 0.2),
 (0.0, 0.4),
 (0.0, 0.6),
 (0.2, 0.0),
 (0.2, 0.2),
 (0.4, 0.0),
 (0.6, 0.0)]
</code></pre>
<p>It does work correctly when I set <code>p11+p22 &lt;= max(p1)+0.01</code>. For different <code>p1</code> and <code>p2</code> the problem may or may not occur. I find this behavior extremely strange, leading to very unreliable results.</p>
<p>It is probably related to floating precision issues. In my opinion this behaviour shouldn't exist in Python, since R and Matlab do not have this behaviour either. Are there any simple ways around this? </p>
</div>
<div class="post-text" itemprop="text">
<h2>What is happening?</h2>
<p>Computers have an internal representation of numbers. In most cases, those representations have a fixed number of bits. This leads to only a fixed amount of numbers being representable. For example, you might know that languages Like C have a maximum value for integers.</p>
<p>Similar, you can't store the exact representation of some floating point numbers. As the computer uses base two, there are some numbers in base 10 which have a short, finite representation but the binary one is long. For more details, see <a href="https://en.wikipedia.org/wiki/IEEE_floating_point" rel="nofollow noreferrer">IEEE 754</a>.</p>
<h2>How can it be "fixed"?</h2>
<p>There is nothing to be fixed here as everything is working like it was specified. But you have to know about these types of problems. When you are aware of the fact that there is a problem, then there are two strategies to get around it.</p>
<p>Either use <strong>epsilons</strong> (-&gt; don't compare with exact numbers, but check if the number is within a very small interval around the number. The length of this interval is often called "epsilon") or use arbitrary precision representations (see <a href="https://docs.python.org/3/library/fractions.html" rel="nofollow noreferrer"><code>fractions</code></a>. The second only works when you can influence how the number is put into the program, e.g. </p>
<pre><code>from itertools import product
from fractions import Fraction
P = []
p1 = [Fraction(0.0), Fraction(2, 10), Fraction(4, 10), Fraction(6, 10)]
p2 = [Fraction(0.0), Fraction(2, 10), Fraction(4, 10), Fraction(6, 10)]
for p11, p22 in product(p1, p2):
    if p11+p22 &lt;= max(p1):
        P.append((p11, p22))
</code></pre>
<h2>See also</h2>
<ul>
<li><a href="https://stackoverflow.com/q/588004/562769">Is floating point math broken?</a></li>
<li><a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html" rel="nofollow noreferrer">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a></li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>Because the <a href="http://docs.python.org/3/tutorial/floatingpoint.html" rel="nofollow">limitations</a> on fixed bit width floating point, you need to use an arbitrary precision floating point package or explicitly compare with +/- an epsilon amount. </p>
<p>Python includes <a href="https://docs.python.org/2/library/decimal.html" rel="nofollow">decimal</a> (for 'arithmetic that works in the same way as the arithmetic that people learn at school'):</p>
<pre><code>from itertools import product
import decimal

P = []
p1 = map(decimal.Decimal, ['0.0','0.2','0.4','0.6'])
p2 = map(decimal.Decimal, ['0.0','0.2','0.4','0.6'])
for p11,p22 in product(p1,p2):
    if p11+p22 &lt;= max(p1):    
        P.append((p11,p22))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you want to compare with matlab or R, or have performance issues , here is a numpy approach, with <code>np.isclose()</code> as workaround.</p>
<pre><code>p1 = [0.0,0.2,0.4,0.6]
p2 = [0.0,0.2,0.4,0.6] 
sums=np.add.outer(p1,p2)
P1,P2=np.meshgrid(p1,p2)
compare = (sums&lt;0.6) | np.isclose(sums,0.6)
print(np.dstack((P1,P2))[compare])
</code></pre>
<p>which gives :</p>
<pre><code>[[ 0.   0. ]
 [ 0.2  0. ]
 [ 0.4  0. ]
 [ 0.6  0. ]
 [ 0.   0.2]
 [ 0.2  0.2]
 [ 0.4  0.2]
 [ 0.   0.4]
 [ 0.2  0.4]
 [ 0.   0.6]]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>One way around it would be to define a different inequality comparison function, </p>
<pre><code>function less_or_nearly_equal(a,b)
    return (a &lt;= b + .000001)
</code></pre>
<p>and use that instead of &lt;= in your comparison</p>
<pre><code>for p11,p22 in product(p1,p2):
    if less_or_nearly_equal(p11+p22,max(p1)):    
        P.append((p11,p22))
</code></pre>
<p>(I hope my syntax is correct in the function definition.)</p>
</div>
<span class="comment-copy">This is a general problem: <a href="http://docs.python.org/3/tutorial/floatingpoint.html" rel="nofollow noreferrer">Floating Point Arithmetic: Issues and Limitations</a>. R and Matlab may implicitly convert comparisons for equality into comparisons for "equality +/- epsilon", but in Python, you need to do this explicitly.</span>
<span class="comment-copy">"<i>I find this behavior extremely strange</i>" Why so? It is the expected behavior of almost every general purpose language with IEEE floating point support. The general lesson is: Be extra careful when comparing floats.</span>
<span class="comment-copy">FWIW <code>(0.4 + 0.2) &lt;= 0.6</code> gives <code>FALSE</code> in R and <code>0</code>  in Octave for me, just like Python, as you'd expect</span>
<span class="comment-copy">@Forzaa Only if you're naively trying to apply your mathematical knowledge of adding small decimals. If you convert to binary and look up how floating-point numbers are stored in computers, the math on this (and the problems with it) becomes perfectly clear.</span>
<span class="comment-copy">@Forzaa That I can understand. Unfortunately, abstract numbers with infinite precision are somewhat hard to represent in the real world. Welcome to the wonderful science of <i>numerics</i>.</span>
<span class="comment-copy">"Computers have an internal representation of numbers" ... it's not per definition a problem for <i>computers</i>. The result of <code>1/3</code> cannot be represented exactly in decimal, similar to the result of <code>1/10</code> in floating point binary (which is what computers happen to use internall).</span>
<span class="comment-copy">@Jongware Correct (+1). I thought about adding exactly this example, but I wanted to keep the answer short. If somebody is interested in details, I can recommend the last link ("What Every Computer Scientist Should Know About Floating-Point Arithmetic")</span>
<span class="comment-copy">Absolutely agree -  that particular link should pop up automatically for <i>every</i> question that states "my computer is broken" :) I wanted to point out that it is not limited to a 'computer' problem.</span>
