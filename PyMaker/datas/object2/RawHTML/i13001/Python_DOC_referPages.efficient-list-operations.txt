<div class="post-text" itemprop="text">
<p>I have a large matrix (1,017,209 rows) from which I need to read out elements, make operations on them, and collecting the results into lists. When I do it on 10,000 rows or even 100,000 it finishes in a reasonable time, however 1,000,000 does not. Here is my code:</p>
<pre><code>import pandas as pd

data = pd.read_csv('scaled_train.csv', index_col=False, header=0)
new = data.as_matrix()

def vectorized_id(j):
    """Return a 1115-dimensional unit vector with a 1.0 in the j-1'th position
    and zeroes elsewhere.  This is used to convert the store ids (1...1115)
    into a corresponding desired input for the neural network.
    """
    j = j - 1    
    e = [0] * 1115
    e[j] = 1.0
    return e

def vectorized_day(j):
    """Return a 7-dimensional unit vector with a 1.0 in the j-1'th position
    and zeroes elsewhere.  This is used to convert the days (1...7)
    into a corresponding desired input for the neural network.
    """
    j = j - 1
    e = [0] * 7
    e[j] = 1.0
    return e

list_b = []
list_a = []

for x in xrange(0,1017209):
    a1 = vectorized_id(new[x][0])
    a2 = vectorized_day(new[x][1])
    a3 = [new[x][5]]
    a = a1 + a2 + a3
    b = new[x][3]
    list_a.append(a)
    list_b.append(b)
</code></pre>
<p>What makes it slow at that scale (what is the bottleneck)? Are there ways to optimize it?</p>
</div>
<div class="post-text" itemprop="text">
<p>A couple of things:</p>
<ol>
<li>Don't read in the entire file at once, you don't appear to be doing anything that requires multiple lines.</li>
<li>Look at using <a href="https://docs.python.org/3/library/csv.html#csv.reader" rel="nofollow"><code>csv.reader</code></a> for loading your data.</li>
<li>Really stop indexing in the giant <code>new</code> list.</li>
</ol>
</div>
<span class="comment-copy">You look up <code>new[x]</code> four times.</span>
<span class="comment-copy">How much memory do you have? You're using a ton of it.</span>
<span class="comment-copy">You only need one row at a time, but appear to read the whole file into memory.</span>
<span class="comment-copy">As @user2357112 said, you are using a ton of memory. Each element of <code>list_a</code> has length 1115 + 7 + 1 = 1123, and <code>list_a</code> has 1017209 elements. So, roughly 1k x 1m = 1g numbers you're trying to store. So, the memory consumption is easily around 4 GB, 8 GB. And much of it is just zeros, so you should take advantage of that sparsity somehow.</span>
<span class="comment-copy">@Rishi indeed, I run into a memory problem. I started using csv.reader but it still does not work, at around line 320,000 my memory usage is at 95% and it basically stops.   I figured the vectorized_id() causes the problem as it gives a 1115 length vector for each line. So it would produce a list of several GBs as Rishi mentioned right? Then I guess I should save intermediate results on hard drive, and empty memory. Any tips on how to do that?</span>
<span class="comment-copy">As @Rishi mentioned, you're using a lot of memory. If you really need to have all the records you're generating, you'll probably need to write them to files and reload the files later when you need them.</span>
