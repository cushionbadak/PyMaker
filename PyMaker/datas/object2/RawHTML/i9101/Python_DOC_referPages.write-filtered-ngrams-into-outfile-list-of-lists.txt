<div class="post-text" itemprop="text">
<p>I extracted threegrams from a bunch of HTML files following a certain pattern. When I print them, I get a list of lists (where each line is a threegram). I would like to print it to an outfile for further text analysis, but when I try it, it only prints the first threegram. How can I print all the threegrams to the outfile? (The list of list of threegrams). I would ideally like to merge all the threegrams into one list instead of having multiple lists with one threegram. Your help would be highly appreciated.</p>
<p>My code looks like this so far:</p>
<pre><code>from nltk import sent_tokenize, word_tokenize
from nltk import ngrams
from bs4 import BeautifulSoup
from string import punctuation
import glob
import sys
punctuation_set = set(punctuation) 

# Open and read file
text = glob.glob('C:/Users/dell/Desktop/python-for-text-analysis-master/Notebooks/TEXTS/*')   
for filename in text:
with open(filename, encoding='ISO-8859-1', errors="ignore") as f:
    mytext = f.read()  

# Extract text from HTML using BeautifulSoup
soup = BeautifulSoup(mytext, "lxml")
extracted_text = soup.getText()
extracted_text = extracted_text.replace('\n', '')

# Split the text in sentences (using the NLTK sentence splitter) 
sentences = sent_tokenize(extracted_text)

# Create list of tokens with their POS tags (after pre-processing: punctuation removal, tokenization, POS tagging)
all_tokens = []

for sent in sentences:
    sent = "".join([char for char in sent if not char in punctuation_set]) # remove punctuation from sentence (optional; comment out if necessary)
    tokenized_sent = word_tokenize(sent) # split sentence into tokens (using NLTK word tokenization)
    all_tokens.extend(tokenized_sent) # add tagged tokens to list

n=3
threegrams = ngrams(all_tokens, n)


# Find ngrams with specific pattern
for (first, second, third) in threegrams: 
    if first == "a":
        if second.endswith("bb") and second.startswith("leg"):
            print(first, second, third)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Firstly, the punctuation removal could have been simpler, see <a href="https://stackoverflow.com/questions/10017147/removing-a-list-of-characters-in-string">Removing a list of characters in string</a> </p>
<pre><code>&gt;&gt;&gt; from string import punctuation
&gt;&gt;&gt; text = "The lazy bird's flew, over the rainbow. We'll not have known."
&gt;&gt;&gt; text.translate(None, punctuation)
'The lazy birds flew over the rainbow Well not have known'
</code></pre>
<p>But it's not really correct to remove punctuations before you do tokenization, you see that <code>We'll</code> -&gt; <code>Well</code>, which I think it's not desired. </p>
<p>Possibly this is a better approach:</p>
<pre><code>&gt;&gt;&gt; from nltk import sent_tokenize, word_tokenize
&gt;&gt;&gt; [[word for word in word_tokenize(sent) if word not in punctuation] for sent in sent_tokenize(text)]
[['The', 'lazy', 'bird', "'s", 'flew', 'over', 'the', 'rainbow'], ['We', "'ll", 'not', 'have', 'known']]
</code></pre>
<p>But do note that the idiom above don't handle multi-character punctuation. </p>
<p>E.g. , we see that that the <code>word_tokenize()</code> changes <code>"</code> -&gt; `` , and using the idiom above it didn't remove it:</p>
<pre><code>&gt;&gt;&gt; sent = 'He said, "There is no room for room"'
&gt;&gt;&gt; word_tokenize(sent)
['He', 'said', ',', '``', 'There', 'is', 'no', 'room', 'for', 'room', "''"]
&gt;&gt;&gt; [word for word in word_tokenize(sent) if word not in punctuation]
['He', 'said', '``', 'There', 'is', 'no', 'room', 'for', 'room', "''"]
</code></pre>
<p>To handle that, explicitly make <code>punctuation</code> into a list and append the multi-character punctuations to it:</p>
<pre><code>&gt;&gt;&gt; sent = 'He said, "There is no room for room"'
&gt;&gt;&gt; punctuation
'!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~'
&gt;&gt;&gt; list(punctuation)
['!', '"', '#', '$', '%', '&amp;', "'", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '&lt;', '=', '&gt;', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~']
&gt;&gt;&gt; list(punctuation) + ['...', '``', "''"]
['!', '"', '#', '$', '%', '&amp;', "'", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '&lt;', '=', '&gt;', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~', '...', '``', "''"]
&gt;&gt;&gt; p = list(punctuation) + ['...', '``', "''"]
&gt;&gt;&gt; [word for word in word_tokenize(sent) if word not in p]
['He', 'said', 'There', 'is', 'no', 'room', 'for', 'room']
</code></pre>
<p>As for getting the document stream (as you called it <code>all_tokens</code>), here's a neat way to get it:</p>
<pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; from nltk import sent_tokenize, word_tokenize
&gt;&gt;&gt; from string import punctuation
&gt;&gt;&gt; p = list(punctuation) + ['...', '``', "''"]
&gt;&gt;&gt; text = "The lazy bird's flew, over the rainbow. We'll not have known."
&gt;&gt;&gt; [[word for word in word_tokenize(sent) if word not in p] for sent in sent_tokenize(text)]
[['The', 'lazy', 'bird', "'s", 'flew', 'over', 'the', 'rainbow'], ['We', "'ll", 'not', 'have', 'known']]
</code></pre>
<hr/>
<p>And now to the part of your actual question.</p>
<p>What you really need isn't checking the string in the ngrams, rather, you should consider a regex pattern matching.</p>
<p>You want to find the pattern <code>\ba\b\s\bleg[\w]+bb\b\s\b[\w]+\b</code>, see <a href="https://regex101.com/r/zBVgp4/4" rel="nofollow noreferrer">https://regex101.com/r/zBVgp4/4</a></p>
<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; re.findall(r"\ba\b\s\bleg[\w]+bb\b\s\b[\w]+\b", "This is a legobatmanbb cave hahaha")
['a legobatmanbb cave']
&gt;&gt;&gt; re.findall(r"\ba\b\s\bleg[\w]+bb\b\s\b[\w]+\b", "This isa legobatmanbb cave hahaha")
[]
</code></pre>
<p>Now to write a string to a file, you can use this idiom, see <a href="https://docs.python.org/3/whatsnew/3.0.html#print-is-a-function" rel="nofollow noreferrer">https://docs.python.org/3/whatsnew/3.0.html#print-is-a-function</a>:</p>
<pre><code>with open('filename.txt', 'w') as fout:
    print('Hello World', end='\n', file=fout)
</code></pre>
<hr/>
<p>In fact, if you are only interested in the ngrams without the tokens, there's no need to filter or tokenize the text ;P</p>
<p>You can simply your code to this:</p>
<pre><code>soup = BeautifulSoup(mytext, "lxml")
extracted_text = soup.getText()
pattern = r"\ba\b\s\bleg[\w]+bb\b\s\b[\w]+\b"

with open('filename.txt', 'w') as fout:
    for interesting_ngram in re.findall(pattern, extracted_text):
        print(interesting_ngram, end='\n', file=fout)
</code></pre>
</div>
<span class="comment-copy">Thanks a lot. I got the regex pattern matching but I still cannot print to the text file. In the same way it only prints the first line unfortunately. As you could suspect I'm totally new so I might do something wrong...</span>
<span class="comment-copy">Are you using code snippet in the answer? Or still using yours? What is the input file, could you share it? Also what is the expected output?</span>
<span class="comment-copy">Check your indents. Delete all the code and then start checking from the file iteration, e.g. print out the content line by line.The NLP part of the code should be fine.</span>
<span class="comment-copy">Thanks for getting back. I'm extracting info from a list of html files- No EN                                                                 base_url = "<a href="http://mek.oszk.hu//01000/0" rel="nofollow noreferrer">mek.oszk.hu//01000/0</a>" for i in range(1000, 1100):#1100):     target_url = base_url + str(i) + '/' + '0' + str(i) + ('.htm')     r = requests.get(target_url)     with open(str(i) + ".htm", 'w', encoding="iso-8859-1") as f:         f.write(r.text) .I'm using regex now. I wanted to get certain patterns from the texts, then store them in an outfile and merge them to one list/string instead of list of lists so I can count them, etc.</span>
<span class="comment-copy">I had to change "w" mode to "a" mode and it gives the output as expected. Thanks for your help!!</span>
