<div class="post-text" itemprop="text">
<p>I'm dealing with a big array <code>D</code> with which I'm running into memory problems. However, the entries of that big array are in fact just copies of elements of a much smaller array <code>B</code>. Now my idea would be to use something like a "dynamic view" into <code>B</code> instead of constructing the full <code>D</code>. For example, is it possible to use a function <code>D_fun</code> like an array which the reads the correct element of <code>B</code>? I.e. something like</p>
<pre><code>def D_fun(B, I, J):
    i = convert_i_idx(I, J)
    j = convert_j_idx(I, J)

    return B[i,j]
</code></pre>
<p>And then I could use <code>D_fun</code> to do some matrix and vector multiplications.</p>
<p>Of course, anything else that would keep me form copying the elements of <code>B</code> repeatedly into a huge matrix would be appreciated.</p>
<p>Edit: I realized that if I invest some time in my other code I can get the matrix <code>D</code> to be a block matrix with the <code>B</code>s on the diagonal and zeros otherwise.</p>
</div>
<div class="post-text" itemprop="text">
<p>This is usually done by <a href="https://docs.scipy.org/doc/numpy/user/basics.subclassing.html" rel="nofollow noreferrer">subclassing numpy.ndarray</a> and overloading <a href="https://docs.python.org/3/reference/datamodel.html#object.__getitem__" rel="nofollow noreferrer"><code>__getitem__, __setitem__, __delitem__</code></a> 
 (array-like access via <code>[]</code>) to remap the indices like <code>D_fun(..)</code> does. Still, I am not sure if this will work in combination with the numpy parts implemented in C. </p>
<p><strong>Some concerns:</strong>
When you're doing calculations on your big matrix <code>D</code> via the small matrix <code>B</code>, numpy might create a copy of D with its real dimensions, thus using more space than wanted.</p>
<p>If several <code>(I1,J1), (I2,J2)..</code> are mapped to the same <code>(i,j)</code>, <code>D[I1,J1] = newValue</code> will also set <code>D(I2,J2)</code> to <code>newValue</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p><code>np.dot</code> uses compiled libraries to perform fast matrix products.  That constrains the data type (integer, floats), and requires that the data be contiguous.  I'd suggest studying this recent question about large dot products, <a href="https://stackoverflow.com/questions/41942115/numpy-efficient-large-dot-products">numpy: efficient, large dot products</a></p>
<p>Defining a class with a custom <code>__getitem__</code> is a way of accessing a object with indexing syntax.  Look in <code>numpy/lib/index_tricks.py</code> for some interesting examples of this, <code>np.mgrid</code>,<code>np.r_</code>, <code>np.s_</code> etc.  But this is largely a syntax enhancement.  It doesn't avoid the issues of defining a robust and efficient mapping between your <code>D</code> and <code>B</code>.</p>
<p>And before trying to do much with subclassing <code>ndarray</code> take a look at the implementation for <code>np.matrix</code> or <code>np.ma</code>.  <code>scipy.sparse</code> also creates classes that behave like <code>ndarray</code> in many ways, but does not subclass <code>ndarray</code>.</p>
<p>In your <code>D_fun</code> are <code>I</code> and <code>J</code> scalars?  If so this conversion would be horribly in efficient.  It would be better if they could be arrays, lists or slices (anything that <code>B[atuple]</code> implements), but that can be a lot of work.</p>
<pre><code>def D_fun(B, I, J):
    i = convert_i_idx(I, J)
    j = convert_j_idx(I, J)    
    return B[i,j]

def __getitem__(self, atuple):
    # sketch of a getitem version of your function
    I, J = atuple
    &lt;select B based on I,J?&gt;
    i = convert_i_idx(I, J)
    j = convert_j_idx(I, J) 
    return B.__getitem__((i,j))
</code></pre>
<p>What is the mapping from <code>D</code> to <code>B</code> like?  The simplest, and most efficient mapping would be that <code>D</code> is just a higher dimensional collection of <code>B</code>, i.e.</p>
<pre><code> D = np.array([B0,B1,B2,...,Bn])
 D[0,...] == B0  
</code></pre>
<p>Slightly more complicated is the case where <code>D[n1:n2,....] == B0</code>, a slice</p>
<p>But if the <code>B0</code> values are scattered around <code>D</code> you chances of efficient, reliable mapping a very small.</p>
</div>
<span class="comment-copy">Oh and FYI, D is nor quite dense but it's also not sparse enough that using a scipy.sparse solves the memory issue.</span>
<span class="comment-copy">What exactly is the implementation of <code>convert_i_idx</code>? If its particularly simple, you might find that <code>as_strided</code> can help</span>
<span class="comment-copy">Thanks for the tip, but I'm afraid it's not simple enough for that.</span>
<span class="comment-copy">Element multiplication or <code>np.dot</code> like?</span>
<span class="comment-copy">Matrix and vector multiplications in the mathematical sense, so <code>np.dot</code> like</span>
<span class="comment-copy">I'll give that a try! Your second concern is not an issue. I'll probably won't change the array once created and even if I had to, that would be the desired behavior.</span>
<span class="comment-copy">I'm not sure if I got the last part and how that could help me. I realized that <code>D</code> can be written in diagonal block form (see edit above). Does that work in that direction?</span>
<span class="comment-copy">Take a look at <code>scipy.sparse</code> <code>block_diag</code> construction.</span>
<span class="comment-copy">Mhh, but how could that exploit the fact that the matrices on the diagonal are identical in order to save memory?</span>
