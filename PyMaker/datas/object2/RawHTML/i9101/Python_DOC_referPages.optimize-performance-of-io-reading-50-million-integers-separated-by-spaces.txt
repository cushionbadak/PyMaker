<div class="post-text" itemprop="text">
<p>If using interpreted Python 2.7.6, and trying to read about 50 million integers (signed, 32 bits) from a file linked to stdin, what's the fastest (performance) way to do this if they come in a single line (no \n at the end), <em>space</em> separated?, or perhaps comma separated? Preferably using generators and/or reading in chunks so that the whole file is not read into memory at once, or a list of all 50M integers stored at once. The list should be reduced to sum of all adjacent element xors <code>(A[0]^A[1] + A[1]^A[2] + ... )</code>, the numbers are very close to each other so the reduction does not break 32 bits signed integer.</p>
<p>An initial line can be added to have either the number of integers (n), and/or the length of the line (L).</p>
<p>I am not proficient on python, and I get unacceptable results (&gt;30 seconds). For a tenth of the limits I do about 6 seconds, so I feel I need to improve this much much more.</p>
<p>It seems to me if they had come separated with line breaks this might have been possible. Is there a way to tell python to use a different delimiter for readline()?</p>
<p>Tried:</p>
<ul>
<li><code>for ch in stdin.read()</code>, it takes 3 seconds to loop all ch, but building the integers with multiplications and then doing the reduction manually takes too long.</li>
<li><code>read(n)</code>, reading in chunks, then storing the incomplete tail for later, using split and map int, for xrange and reduce on the chunk sequentially to build the reduction list, but again seems to take too long.</li>
</ul>
<p>I have done this on faster languages already thanks, looking for interpreted python answers.</p>
<p>This is my best code, runs in 18 seconds in SOME cases, in others it is too slow. But it is faster than the version where I built the integers with multiplications on an accumulator. It is also faster than reading byte per byte: <code>read(1)</code>.</p>
<pre><code>def main():
    n,l=map(int,raw_input().split())
    #print n
    #print l

    r = 0 #result
    p = 0 #previous
    q = 0 #current

    b = [] #buffer
    for c in sys.stdin.read(): #character
        if c == ' ':
            q = int(''.join(b))
            b = []
            r += q ^ p #yes there is a bug with A[0] but lets optimize the loop for now
            p = q
        else:
            b.append(c)
    r += int(''.join(b)) ^ p

    print r
main()
</code></pre>
<p>I can see it could (maybe) be improved if it was possible to initialize b only once and then not using append but actually accessing the index, but when I tried <code>b = [None]*12</code> I got an RTE during join <code>cant join None</code>, need a join over a range, so I dropped the idea for the moment. Also faster functions to do what I already do.</p>
<p>Update:</p>
<pre><code>import re
import sys

from collections import deque

def main():
    n,l=map(int,raw_input().split())
    #print n
    #print l

    r = 0
    p = 0
    q = 0

    b = sys.stdin.read(l)

    b = deque(b.rsplit(' ',4000000))
    n = len(b)
    while n == 4000001:
        c = b.popleft()
        b = map(int,b)
        for i in xrange(n-2,0,-1):
            r += b[i] ^ b[i-1]

        m = b[0]
        b = deque(c.rsplit(' ',3999999))
        b.append(m)
        n = len(b)


    b = map(int,b)
    for i in xrange(n-1,0,-1):
        r += b[i] ^ b[i-1]

    print r
main()
</code></pre>
<p>This is 3 times faster (10 million can be done in 6 seconds, but 50 take over 30), for 50 million, it is still too slow, IO seems not to be the main bottleneck, but the data processing.</p>
<p>Instead of the deque a regular list can be used, calling pop(0) instead of popleft. It is also possible not to call len(b) on every loop, as you have n at the beginning and can subtract instead, but besides that this seems the fastest so far.</p>
</div>
<div class="post-text" itemprop="text">
<p>Read a stream of bytes until EOF. Once you hit a space, convert a list of "digit" bytes to an integer, do your XOR, and reset the list. Or just keep appending digits to a list until you do hit a space. Something like the following untested code:</p>
<pre><code>f = open("digits.txt", "rb")
try:
    bytes = []
    previous_num = None
    byte = f.read(1)
    while byte != "":
        if byte != " ":
            bytes.append(byte)
        else:
            # convert bytes to a number and reset list
            current_num = int(''.join(map(str, bytes)))
            if not previous_num:
                previous_num = current_num
            else:
                # do your operation on previous and current number
            bytes = []
        byte = f.read(1)
finally:
    f.close()
</code></pre>
<p>You could probably optimize this by reading in chunks of bytes, instead of one byte at a time. Another way to optimize this, perhaps, is to keep a kind of "nul" terminator for the list, an index that keeps the "length" of the list. Instead of wiping it clear on every loop, you do your <code>map</code> operation on a start-/end-indexed subset of <code>bytes</code>. But hopefully this demonstrates the principle.</p>
<p>Short of this, you could perhaps use a Unix utility like <code>sed</code> to replace spaces with newlines and pipe the output of <code>sed</code> to a Python script, and have Python read from the <code>stdin</code> stream, using its (perhaps optimized) ability to read a line at a time.</p>
<p>(But, really, Python is probably the wrong answer for anything that needs speedy I/O.)</p>
</div>
<div class="post-text" itemprop="text">
<p>I ran this code:</p>
<pre><code>#!python2.7
from __future__ import print_function
import os, time

numbers = "100 69 38 24 17 11 3 22 "
print("Numbers:", numbers)


if not os.path.isfile('numbers.txt'):
    with open('numbers.txt', 'w') as outfile:
        n = 7*1000*1000
        print("Repeating %d times..." % n)
        print(numbers * n, file=outfile)

print("Starting read. Time:", time.strftime("%c"))
total = 0
with open('numbers.txt') as f:
    prv = None
    for nxt in f.read().split():
        nxt = int(nxt)
        if prv is not None:
            total += prv ^ nxt
        prv = nxt

print("Finished. Time:", time.strftime("%c"))
print("Total:", total)
</code></pre>
<p>And got these results:</p>
<pre><code>$ python2.7 test.py
Numbers: 100 69 38 24 17 11 3 22
Starting read. Time: Fri Feb  3 19:20:32 2017
Finished. Time: Fri Feb  3 19:21:36 2017
Total: 2603999886
</code></pre>
<p>That's 56 million (small) numbers, on a 5 year old MacBook Pro, in 64 seconds or so - about 1m numbers per second. Can you give us your timings, and what you expect to get?</p>
</div>
<div class="post-text" itemprop="text">
<p>I would be surprised if you could find a much faster implementation than <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html#numpy-fromfile" rel="nofollow noreferrer"><code>numpy.fromfile</code></a></p>
<p>However, parsing ints from a text files is much slower than just reading binary data. Here's some quick and dirty benchmarks using two files with the same ~50M integers. The first is text format, the other is binary (written using <code>numpy.ndarray.tofile</code>)</p>
<pre><code>%timeit numpy.fromfile('numbers.txt', dtype=int, sep=' ')
1 loop, best of 3: 23.6 s per loop

%timeit numpy.fromfile('numbers.bin')
1 loop, best of 3: 2.55 s per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>how about this</p>
<pre><code>from itertools import tee, izip as zip
import re

def pairwise(iterable):
    a,b = tee(iterable)
    next(b,None)
    return zip(a,b)

def process_data(data):
    return sum( a^b for a,b in pairwise(data) )

def process_str_file_re(fname):
    exp = re.compile(r"\d+")
    with open(fname,"rb") as archi:
        return process_data( int( data.group() ) for data in exp.finditer(archi.read()) )
</code></pre>
<p>instead of going 1 character at the time, use a module that specialize it character manipulation like <code>re</code></p>
</div>
<span class="comment-copy">This does not seem to be a [csv] related question, since you are saying the numbers are space separated. Can you show us the code you've tried so far - perhaps the fastest version?</span>
<span class="comment-copy">question says that it could be comma separated (if there was anything that processes commas and not spaces, which I doubt)</span>
<span class="comment-copy">If the file was in binary format, you could use <code>array.fromfile</code>, which should be quite fast. <a href="https://docs.python.org/3/library/array.html#array.array.fromfile" rel="nofollow noreferrer">docs.python.org/3/library/array.html#array.array.fromfile</a>  Do you have any control over how the file was written?</span>
<span class="comment-copy">th's means each 32 bits id have to store one of the integers right? no, the file is in csv/space separated text</span>
<span class="comment-copy">"An initial line can be added to have either the number of integers (n), and/or the length of the line (L)."â€”to have them be <i>what</i>?</span>
<span class="comment-copy">I tried with read(1) but I got slower results than doing <code>for c in read()</code>, I'm not sure I understand the mapping chunks idea you have.</span>
<span class="comment-copy">Take a look at this post for ideas: <a href="http://rabexc.org/posts/io-performance-in-python" rel="nofollow noreferrer">rabexc.org/posts/io-performance-in-python</a></span>
<span class="comment-copy">Hope to get at least &lt;20, (I just managed to get &lt;20 on a few, but others are too slow still), but the faster the better. You made me realize my code has a bug :P, but I guess it doesnt matter while the loop is too slow still</span>
<span class="comment-copy">Oh, no. First you make it right, <i>then</i> you make it fast. Always.</span>
<span class="comment-copy">it was just a small bug... made it right and fast, but still not fast enough</span>
<span class="comment-copy">I changed the open to open('rb') and went down to about 37 seconds. It seems likely that the suggestions in the linked article are going to be pretty valid. Notably, when he shifted to python 3 he got REALLY good performance using one of his patterns.</span>
