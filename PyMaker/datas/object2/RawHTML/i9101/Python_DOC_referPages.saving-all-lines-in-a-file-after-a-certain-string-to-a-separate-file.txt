<div class="post-text" itemprop="text">
<p>I have a file containing a block of introductory text for an unknown number of lines, then the rest of the file contains data. Before the data block begins, there are column titles and I want to skip those also. So the file looks something like this:</p>
<pre class="lang-none prettyprint-override"><code>this is an introduction..
blah blah blah...
...
UniqueString
Time Position Count
0 35 12
1 48 6
2 96 8
...
1000 82 37
</code></pre>
<p>I want to record the Time Position and Count data to a separate file. Time Position and Count Data appears only after <code>UniqueString</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>Is it what you're looking for?</p>
<pre><code>reduce(lambda x, line: (x and (outfile.write(line) or x)) or line=='UniqueString\n', infile)
</code></pre>
<p>How it works:</p>
<ul>
<li>files can be iterated, so we can read <code>infile</code> line by line by simply doing <code>[... for line in infile]</code></li>
<li>in the operation part, we use the fact that <code>writeline()</code> will not be triggered if the first operand for and is <code>False</code>.</li>
<li>in the <code>or</code> part, we set up the trigger if the desired line is found, so <code>writeline</code> will be fired for the next and consequent lines</li>
<li>default initial value for reduce is None, which evaluates to <code>False</code></li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>You could extract and write the data to another file like this:</p>
<pre><code>with open("data.txt", "r") as infile:
    x = infile.readlines()

x = [i.strip() for i in x[x.index('UniqueString\n') + 1:] if i != '\n' ]

with open("output.txt", "w") as outfile:
    for i in x[1:]:
        outfile.write(i+"\n")
</code></pre>
<p>It is pretty straight forward I think: The file is opened and all lines are read, a list comprehension slices the list beginning with the header string and the desired remaining lines are wrote to file again.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could create a <a href="https://docs.python.org/2/howto/functional.html#generators" rel="nofollow noreferrer"><strong><em>generator function</em></strong></a> (and more info <a href="https://docs.python.org/3/whatsnew/2.3.html#pep-255-simple-generators" rel="nofollow noreferrer"><strong><em>here</em></strong></a>) that filtered the file for you. <br/>
It operates incrementally so doesn't require reading the entire file into memory at one time.</p>
<pre><code>def extract_lines_following(file, marker=None):
    """Generator yielding all lines in file following the line following the marker.
    """
    marker_seen = False
    while True:
        line = file.next()
        if marker_seen:
            yield line
        elif line.strip() == marker:
            marker_seen = True
            file.next()  # skip following line, too

# sample usage
with open('test_data.txt', 'r') as infile, open('cleaned_data.txt', 'w') as outfile:
    outfile.writelines(extract_lines_following(infile, 'UniqueString'))
</code></pre>
<p>This could be optimized a little if you're using Python 3, but the basic idea would be the same.</p>
</div>
<span class="comment-copy">I would recommend using a CSV reader (since your file after the header line is formatted as a table) with a <code>while</code> loop that skips the lines until it finds the headers.</span>
<span class="comment-copy">Also, please consider that the data could easily have 1e8 to 1e9 rows.</span>
<span class="comment-copy">I have tried this (copied it directly and just changed variable names), but it only records the second line in the block of introductory text.</span>
<span class="comment-copy">missed one condition (<code>(outfile.write(line) or x)</code>). Please try again</span>
<span class="comment-copy"><code>readlines()</code> and subsequent <code>"\n".join</code> are not memory efficient. I.e. it will cause problems if infile is sufficiently big</span>
<span class="comment-copy">How big are we talking?</span>
<span class="comment-copy">Data can easily contain 100 million to 1 billion rows. Also, the actual data files I'm working with have 12 columns</span>
<span class="comment-copy">Updated it. Does this do what you wanted to achieve?</span>
