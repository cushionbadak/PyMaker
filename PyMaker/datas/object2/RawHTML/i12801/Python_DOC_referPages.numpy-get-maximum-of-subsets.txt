<div class="post-text" itemprop="text">
<p>I have an array of values, said <code>v</code>, (e.g. <code>v=[1,2,3,4,5,6,7,8,9,10]</code>) and an array of indexes, say <code>g</code> (e.g.            <code>g=[0,0,0,0,1,1,1,1,2,2]</code>).</p>
<p>I know, for instance, how to take the first element of each group, in a very numpythonic way, doing:</p>
<pre><code>import numpy as np
v=np.array([1,2,3,4,74,73,72,71,9,10])
g=np.array([0,0,0,0,1,1,1,1,2,2])
mask=np.concatenate(([True],np.diff(g)!=0))
v[mask]
</code></pre>
<p>returns:</p>
<pre><code>array([1, 74, 9])
</code></pre>
<p>Is there any <code>numpy</code>thonic way (avoiding explicit loops) to get the maximum of each subset?</p>
<hr/>
<h2>Tests:</h2>
<p>Since I received two good answers, one with the python <code>map</code> and one with a <code>numpy</code> routine, and I was searching the most performing, here some timing tests:</p>
<pre><code>import numpy as np
import time
N=10000000
v=np.arange(N)
Nelemes_per_group=10
Ngroups=N/Nelemes_per_group
s=np.arange(Ngroups)
g=np.repeat(s,Nelemes_per_group)

start1=time.time()
r=np.maximum.reduceat(v, np.unique(g, return_index=True)[1])
end1=time.time()
print('END first method, T=',(end1-start1),'s')

start3=time.time()
np.array(list(map(np.max,np.split(v,np.where(np.diff(g)!=0)[0]+1))))
end3=time.time()
print('END second method,  (map returns an iterable) T=',(end3-start3),'s')
</code></pre>
<p>As a result I get:</p>
<pre><code>END first method, T= 1.6057236194610596 s
END second method,  (map returns an iterable) T= 8.346540689468384 s
</code></pre>
<p>Interestingly, most of the slowdown of the <code>map</code> method is due to the <code>list()</code> call. If I do not try to reconvert my <code>map</code> result to a <code>list</code> ( but I have to, because <code>python3.x</code> returns an iterator:  <a href="https://docs.python.org/3/library/functions.html#map" rel="nofollow">https://docs.python.org/3/library/functions.html#map</a> )</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use <code>np.maximum.reduceat</code>:</p>
<pre><code>&gt;&gt;&gt; _, idx = np.unique(g, return_index=True)
&gt;&gt;&gt; np.maximum.reduceat(v, idx)
array([ 4, 74, 10])
</code></pre>
<p>More about the workings of the ufunc <code>reduceat</code> method can be found <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.ufunc.reduceat.html" rel="nofollow">here</a>.</p>
<hr/>
<p><em>Remark about performance</em></p>
<p><code>np.maximum.reduceat</code> is very fast. Generating the indices <code>idx</code> is what takes most of the time here.</p>
<p>While <code>_, idx = np.unique(g, return_index=True)</code> is an elegant way to get the indices, it is not particularly quick.</p>
<p>The reason is that <code>np.unique</code> needs to sort the array first, which is O(n log n) in complexity. For large arrays, this is much more expensive than using several O(n) operations to generate <code>idx</code>. </p>
<p>Therefore, for large arrays it is much faster to use the following instead:</p>
<pre><code>idx = np.concatenate([[0], 1+np.diff(g).nonzero()[0]])
np.maximum.reduceat(v, idx)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's one convoluted vectorized approach using <a href="http://docs.scipy.org/doc/numpy-1.10.0/reference/arrays.indexing.html#boolean-array-indexing" rel="nofollow"><code>masking</code></a> and <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" rel="nofollow"><code>broadcasting</code></a>  that puts each group into rows of a regular 2D array and then finds maximum along each row -</p>
<pre><code># Mask of valid numbers from each group to be put in a regular 2D array
counts = np.bincount(g)
mask = np.arange(counts.max()) &lt; counts[:,None]

# Group each group into rows of a 2D array and find max along ech row
grouped_2Darray = np.empty(mask.shape)
grouped_2Darray.fill(np.nan)
grouped_2Darray[mask] = v
out = np.nanmax(grouped_2Darray,1)
</code></pre>
<p>Sample run -</p>
<pre><code>In [52]: g
Out[52]: array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2])

In [53]: v
Out[53]: array([ 1,  2,  3,  4, 74, 73, 72, 71,  9, 10])

In [54]: grouped_2Darray # Notice how elements from v are stacked
Out[54]: 
array([[  1.,   2.,   3.,   4.],
       [ 74.,  73.,  72.,  71.],
       [  9.,  10.,  nan,  nan]])

In [55]: np.nanmax(grouped_2Darray,1)
Out[55]: array([  4.,  74.,  10.])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can create your mask like following and use <code>map</code> function :</p>
<pre><code>&gt;&gt;&gt; mask=np.diff(g)!=0
&gt;&gt;&gt; map(np.max,np.split(v,np.where(mask)[0]+1))
[4, 74, 10]
</code></pre>
<p>If you don't want to get a generator with <code>map</code> you can use a list comprehension to achieve same result in list, and note that the iteration of list comprehension has performed at C language speed inside the interpreter, like built-in functions.</p>
<pre><code>[np.max(arr) for arr in np.split(v,np.where(mask)[0]+1)]
</code></pre>
<p>But I think the numpythonic solution still is better to use.</p>
</div>
<span class="comment-copy">Probably - <code>_,i = np.unique(g,return_index=True)</code>.</span>
<span class="comment-copy">amazing! everytime I discover some new strange <code>numpy</code> function ;D</span>
<span class="comment-copy">Thanks for the suggestion @Divakar - that's much nicer.</span>
<span class="comment-copy"><code>reduceat</code> combined with a ranking (here not necessary because <code>g</code> is already in a nice form) is I think the "approved" way to get a groupby in pure numpy.</span>
<span class="comment-copy">nice one, I will probably use it as long as I do not have any <code>numpy</code>tonic solution. In fact, this will have a (non C based) loop over the subset, which in my real case, is quite large.</span>
<span class="comment-copy">@AntonioRagagnin <code>map()</code> is a built in function in python and its iterations performed at C language speed inside the interpreter.</span>
<span class="comment-copy">interesting, please see my update on the anwer where I compared the codes.</span>
<span class="comment-copy">Thanks for the answer, I really didn't know those iterations where made at a C level, and in fact they are ways faster than I thougt, and difference with numpy comes only with objects of sizes &gt;10000000 elements</span>
<span class="comment-copy">@AntonioRagagnin Your welcome. Yes, numpy showe it's power against huge data sets ;-) Read this for more info <a href="http://stackoverflow.com/questions/31598677/why-list-comprehension-is-much-faster-than-numpy-for-multiplying-arrays" title="why list comprehension is much faster than numpy for multiplying arrays">stackoverflow.com/questions/31598677/â€¦</a></span>
