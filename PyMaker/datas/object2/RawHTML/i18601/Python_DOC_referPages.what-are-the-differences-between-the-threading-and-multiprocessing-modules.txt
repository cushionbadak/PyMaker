<div class="post-text" itemprop="text">
<p>I am learning how to use the <code>threading</code> and the <code>multiprocessing</code> modules in Python to run certain operations in parallel and speed up my code.</p>
<p>I am finding this hard (maybe because I don't have any theoretical background about it) to understand what the difference is between a <code>threading.Thread()</code> object and a <code>multiprocessing.Process()</code> one.</p>
<p>Also, it is not entirely clear to me how to instantiate a queue of jobs and having only 4 (for example) of them running in parallel, while the other wait for resources to free before being executed.</p>
<p>I find the examples in the documentation clear, but not very exhaustive; as soon as I try to complicate things a bit, I receive a lot of weird errors (like a method that can't be pickled, and so on).</p>
<p>So, when should I use the <code>threading</code> and <code>multiprocessing</code> modules?</p>
<p>Can you link me to some resources that explain the concepts behind these two modules and how to use them properly for complex tasks?</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://stackoverflow.com/a/18114475">What Giulio Franco says</a> is true for multithreading vs. multiprocessing <em>in general</em>.</p>
<p>However, Python<sup>*</sup> has an added issue: There's a Global Interpreter Lock that prevents two threads in the same process from running Python code at the same time. This means that if you have 8 cores, and change your code to use 8 threads, it won't be able to use 800% CPU and run 8x faster; it'll use the same 100% CPU and run at the same speed. (In reality, it'll run a little slower, because there's extra overhead from threading, even if you don't have any shared data, but ignore that for now.)</p>
<p>There are exceptions to this. If your code's heavy computation doesn't actually happen in Python, but in some library with custom C code that does proper GIL handling, like a numpy app, you will get the expected performance benefit from threading. The same is true if the heavy computation is done by some subprocess that you run and wait on.</p>
<p>More importantly, there are cases where this doesn't matter. For example, a network server spends most of its time reading packets off the network, and a GUI app spends most of its time waiting for user events. One reason to use threads in a network server or GUI app is to allow you to do long-running "background tasks" without stopping the main thread from continuing to service network packets or GUI events. And that works just fine with Python threads. (In technical terms, this means Python threads give you concurrency, even though they don't give you core-parallelism.)</p>
<p>But if you're writing a CPU-bound program in pure Python, using more threads is generally not helpful.</p>
<p>Using separate processes has no such problems with the GIL, because each process has its own separate GIL. Of course you still have all the same tradeoffs between threads and processes as in any other languages—it's more difficult and more expensive to share data between processes than between threads, it can be costly to run a huge number of processes or to create and destroy them frequently, etc. But the GIL weighs heavily on the balance toward processes, in a way that isn't true for, say, C or Java. So, you will find yourself using multiprocessing a lot more often in Python than you would in C or Java.</p>
<hr/>
<p>Meanwhile, Python's "batteries included" philosophy brings some good news: It's very easy to write code that can be switched back and forth between threads and processes with a one-liner change.</p>
<p>If you design your code in terms of self-contained "jobs" that don't share anything with other jobs (or the main program) except input and output, you can use the <a href="http://docs.python.org/3/library/concurrent.futures.html" rel="noreferrer"><code>concurrent.futures</code></a> library to write your code around a thread pool like this:</p>
<pre><code>with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
    executor.submit(job, argument)
    executor.map(some_function, collection_of_independent_things)
    # ...
</code></pre>
<p>You can even get the results of those jobs and pass them on to further jobs, wait for things in order of execution or in order of completion, etc.; read the section on <code>Future</code> objects for details.</p>
<p>Now, if it turns out that your program is constantly using 100% CPU, and adding more threads just makes it slower, then you're running into the GIL problem, so you need to switch to processes. All you have to do is change that first line:</p>
<pre><code>with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
</code></pre>
<p>The only real caveat is that your jobs' arguments and return values have to be pickleable (and not take too much time or memory to pickle) to be usable cross-process. Usually this isn't a problem, but sometimes it is.</p>
<hr/>
<p>But what if your jobs can't be self-contained? If you can design your code in terms of jobs that <em>pass messages</em> from one to another, it's still pretty easy. You may have to use <code>threading.Thread</code> or <code>multiprocessing.Process</code> instead of relying on pools. And you will have to create <code>queue.Queue</code> or <code>multiprocessing.Queue</code> objects explicitly. (There are plenty of other options—pipes, sockets, files with flocks, … but the point is, you have to do <em>something</em> manually if the automatic magic of an Executor is insufficient.)</p>
<p>But what if you can't even rely on message passing? What if you need two jobs to both mutate the same structure, and see each others' changes? In that case, you will need to do manual synchronization (locks, semaphores, conditions, etc.) and, if you want to use processes, explicit shared-memory objects to boot. This is when multithreading (or multiprocessing) gets difficult. If you can avoid it, great; if you can't, you will need to read more than someone can put into an SO answer.</p>
<hr/>
<p>From a comment, you wanted to know what's different between threads and processes in Python. Really, if you read Giulio Franco's answer and mine and all of our links, that should cover everything… but a summary would definitely be useful, so here goes:</p>
<ol>
<li>Threads share data by default; processes do not.</li>
<li>As a consequence of (1), sending data between processes generally requires pickling and unpickling it.<sup>**</sup></li>
<li>As another consequence of (1), directly sharing data between processes generally requires putting it into low-level formats like Value, Array, and <code>ctypes</code> types.</li>
<li>Processes are not subject to the GIL.</li>
<li>On some platforms (mainly Windows), processes are much more expensive to create and destroy.</li>
<li>There are some extra restrictions on processes, some of which are different on different platforms. See <a href="http://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming" rel="noreferrer">Programming guidelines</a> for details.</li>
<li>The <code>threading</code> module doesn't have some of the features of the <code>multiprocessing</code> module. (You can use <code>multiprocessing.dummy</code> to get most of the missing API on top of threads, or you can use higher-level modules like <code>concurrent.futures</code> and not worry about it.)</li>
</ol>
<hr/>
<p><sub>* It's not actually Python, the language, that has this issue, but CPython, the "standard" implementation of that language. Some other implementations don't have a GIL, like Jython.</sub></p>
<p><sub>** If you're using the <a href="https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods" rel="noreferrer">fork</a> start method for multiprocessing—which you can on most non-Windows platforms—each child process gets any resources the parent had when the child was started, which can be another way to pass data to children.</sub></p>
</div>
<div class="post-text" itemprop="text">
<p>Multiple threads can exist in a single process.
The threads that belong to the same process share the same memory area (can read from and write to the very same variables, and can interfere with one another).
On the contrary, different processes live in different memory areas, and each of them has its own variables. In order to communicate, processes have to use other channels (files, pipes or sockets).</p>
<p>If you want to parallelize a computation, you're probably going to need multithreading, because you probably want the threads to cooperate on the same memory.</p>
<p>Speaking about performance, threads are faster to create and manage than processes (because the OS doesn't need to allocate a whole new virtual memory area), and inter-thread communication is usually faster than inter-process communication. But threads are harder to program. Threads can interfere with one another, and can write to each other's memory, but the way this happens is not always obvious (due to several factors, mainly instruction reordering and memory caching), and so you are going to need synchronization primitives to control access to your variables.</p>
</div>
<div class="post-text" itemprop="text">
<p>I believe <a href="http://www.quantstart.com/articles/Parallelising-Python-with-Threading-and-Multiprocessing" rel="nofollow noreferrer">this link</a> answers your question in an elegant way.</p>
<p>To be short, if one of your sub-problems has to wait while another finishes, multithreading is good (in I/O heavy operations, for example); by contrast, if your sub-problems could really happen at the same time, multiprocessing is suggested. However, you won't create more processes than your number of cores.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here's some performance data for python 2.6.x that calls to question the  notion that threading is more performant that multiprocessing in IO-bound scenarios. These results are from a 40-processor IBM System x3650 M4 BD.</p>
<p>IO-Bound Processing : Process Pool performed better than Thread Pool</p>
<pre><code>&gt;&gt;&gt; do_work(50, 300, 'thread','fileio')
do_work function took 455.752 ms

&gt;&gt;&gt; do_work(50, 300, 'process','fileio')
do_work function took 319.279 ms
</code></pre>
<p>CPU-Bound Processing : Process Pool performed better than Thread Pool</p>
<pre><code>&gt;&gt;&gt; do_work(50, 2000, 'thread','square')
do_work function took 338.309 ms

&gt;&gt;&gt; do_work(50, 2000, 'process','square')
do_work function took 287.488 ms
</code></pre>
<p>These aren't rigorous tests, but they tell me that multiprocessing isn't entirely unperformant in comparison to threading.</p>
<p>Code used in the interactive python console for the above tests</p>
<pre><code>from multiprocessing import Pool
from multiprocessing.pool import ThreadPool
import time
import sys
import os
from glob import glob

text_for_test = str(range(1,100000))

def fileio(i):
 try :
  os.remove(glob('./test/test-*'))
 except : 
  pass
 f=open('./test/test-'+str(i),'a')
 f.write(text_for_test)
 f.close()
 f=open('./test/test-'+str(i),'r')
 text = f.read()
 f.close()


def square(i):
 return i*i

def timing(f):
 def wrap(*args):
  time1 = time.time()
  ret = f(*args)
  time2 = time.time()
  print '%s function took %0.3f ms' % (f.func_name, (time2-time1)*1000.0)
  return ret
 return wrap

result = None

@timing
def do_work(process_count, items, process_type, method) :
 pool = None
 if process_type == 'process' :
  pool = Pool(processes=process_count)
 else :
  pool = ThreadPool(processes=process_count)
 if method == 'square' : 
  multiple_results = [pool.apply_async(square,(a,)) for a in range(1,items)]
  result = [res.get()  for res in multiple_results]
 else :
  multiple_results = [pool.apply_async(fileio,(a,)) for a in range(1,items)]
  result = [res.get()  for res in multiple_results]


do_work(50, 300, 'thread','fileio')
do_work(50, 300, 'process','fileio')

do_work(50, 2000, 'thread','square')
do_work(50, 2000, 'process','square')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Well, most of the question is answered by Giulio Franco. I will further elaborate on the consumer-producer problem, which I suppose will put you on the right track for your solution to using a multithreaded app.</p>
<pre><code>fill_count = Semaphore(0) # items produced
empty_count = Semaphore(BUFFER_SIZE) # remaining space
buffer = Buffer()

def producer(fill_count, empty_count, buffer):
    while True:
        item = produceItem()
        empty_count.down();
        buffer.push(item)
        fill_count.up()

def consumer(fill_count, empty_count, buffer):
    while True:
        fill_count.down()
        item = buffer.pop()
        empty_count.up()
        consume_item(item)
</code></pre>
<p>You could read more on the synchronization primitives from:</p>
<pre><code> http://linux.die.net/man/7/sem_overview
 http://docs.python.org/2/library/threading.html
</code></pre>
<p>The pseudocode is above. I suppose you should search the producer-consumer-problem to get more references.</p>
</div>
<span class="comment-copy">There's more, there's also the <code>Thread</code> module (called <code>_thread</code> in python 3.x). To be honest, I've never understood the differences myself...</span>
<span class="comment-copy">@Dunno: As the <code>Thread</code>/<code>_thread</code> documentation explicitly says, it's "low-level primitives". You might use it to build custom synchronization objects, to control the join order of a tree of threads, etc. If you can't imagine why you'd need to use it, don't use it, and stick with <code>threading</code>.</span>
<span class="comment-copy">thanks, but I am not sure I understood everything. Anyway I am trying to do it a bit for learning purposes, and a bit because with a naive use of thread I halved the speed of my code (starting more than 1000 threads at the same time, each calling an external app.. this saturates the cpu, yet there is a x2 increase in speed). I think managing the thread smartly might really improve the speed of my code..</span>
<span class="comment-copy">@LucaCerone: Ah, if your code spends most of its time waiting on external programs, then yes, it will benefit from threading. Good point. Let me edit the answer to explain that.</span>
<span class="comment-copy">@LucaCerone: Meanwhile, what parts do you not understand? Without knowing the level of knowledge you're starting with, it's hard to write a good answer… but with some feedback, maybe we can come up with something that's helpful to you and to future readers as well.</span>
<span class="comment-copy">@LucaCerone You should read the PEP for multiprocessing <a href="http://www.python.org/dev/peps/pep-0371/" rel="nofollow noreferrer">here</a>.  It gives timings and examples of threads vs multiprocessing.</span>
<span class="comment-copy">@LucaCerone: If the object that the method is bound to doesn't have any complex state, the simplest workaround for the pickling issue is to write a stupid wrapper function that generates the object and calls its method. If it <i>does</i> have complex state, then you probably need to make it picklable (which is pretty easy; the <code>pickle</code> docs explain it), and then at worst your stupid wrapper is <code>def wrapper(obj, *args): return obj.wrapper(*args)</code>.</span>
<span class="comment-copy">This is missing some very important information about the GIL, which makes it misleading.</span>
<span class="comment-copy">Threads are <b>not</b> faster for non-io bound purposes.</span>
<span class="comment-copy">@mr2ert: Yes, that's the very important information in a nutshell. :) But it's a bit more complicated than that, which is why I wrote a separate answer.</span>
<span class="comment-copy">I thought I commented saying that @abarnert is right, and I forgot about the GIL in answering here. So this answer is wrong, you should not upvote it.</span>
<span class="comment-copy">I downvoted this answer because it still does not answer at all what is the difference between Python <code>threading</code> and <code>multiprocessing</code>.</span>
<span class="comment-copy">I have used your code (removed the <b>glob</b> part) and have found this interesting results with Python 2.6.6: <code>&gt;&gt;&gt; do_work(50, 300, 'thread', 'fileio')  --&gt; 237.557 ms </code> <code>&gt;&gt;&gt; do_work(50, 300, 'process', 'fileio')  --&gt;  323.963 ms </code> <code>&gt;&gt;&gt; do_work(50, 2000, 'thread', 'square') --&gt; 232.082 ms </code> <code>&gt;&gt;&gt; do_work(50, 2000, 'process', 'square') --&gt; 282.785 ms </code></span>
<span class="comment-copy">sorry innosam, but this seems C++ to me? thanks for the links :)</span>
<span class="comment-copy">Actually, ideas behind multiprocessing and multithreading are language independent. The solution would be similar to the above code.</span>
<span class="comment-copy">Yes, but I don't know how to do it in C++ nor in Python...</span>
<span class="comment-copy">This isn't C++; it's pseudocode (or it's code for a mostly-dynamically-typed language with a C-like syntax. That being said, I think it's more useful to write Python-like pseudocode for teaching Python users. (Especially since the Python-like psuedocode often turns out to be runnable code, or at least close to it, which is rarely true for C-like pseudocode…)</span>
<span class="comment-copy">I've rewritten it as Python-like pseudocode (also using OO and passing parameters instead of using global objects); feel free to revert if you think that makes things less clear.</span>
