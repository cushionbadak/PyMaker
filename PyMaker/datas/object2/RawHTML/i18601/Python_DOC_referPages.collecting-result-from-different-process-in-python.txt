<div class="post-text" itemprop="text">
<p>I am doing several process togethere. Each of the proccess returns some results. How would I collect those results from those process. </p>
<pre><code> task_1 = Process(target=do_this_task,args=(para_1,para_2))
 task_2 = Process(target=do_this_task,args=(para_1,para_2))
</code></pre>
<p><code>do_this_task</code> returns some results. I would like to collect those results and save them in some variable.  </p>
</div>
<div class="post-text" itemprop="text">
<p>So right now I would suggest you should use the python <a href="http://docs.python.org/2/library/multiprocessing.html" rel="nofollow noreferrer"><code>multiprocessing</code></a> module's Pool as it handles quite  a bit for you. Could you elaborate what you're doing and why you want to use what I assume to be <code>multiprocessing.Process</code> directly?</p>
<p>If you still want to use <code>multiprocessing.Process</code> directly you should use a Queue to get the return values.</p>
<p>example given in the docs:</p>
<p>"</p>
<pre><code>from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print q.get()    # prints "[42, None, 'hello']"
    p.join()
</code></pre>
<p>"-<a href="http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Process" rel="nofollow noreferrer">Multiprocessing Docs</a></p>
<p>So processes are things that usually run in the background to do something in general, if you do multiprocessing with them you need to 'throw around' the data since processes don't have shared memory like threads - so that's why you use the Queue - it does it for you. Another thing you can do is pipes, and conveniently they give an example for that as well :).
"</p>
<pre><code>from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print parent_conn.recv()   # prints "[42, None, 'hello']"
    p.join()
</code></pre>
<p>"
-<a href="http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Process" rel="nofollow noreferrer">Multiprocessing Docs</a></p>
<p>what this does is manually use pipes to throw around the finished results to the 'parent process' in this case.</p>
<p>Also sometimes I find cases which <code>multiprocessing</code> cannot pickle well so I use this great answer (or my modified specialized variants of) by <code>mrule</code> that he posts <a href="https://stackoverflow.com/questions/3288595/multiprocessing-using-pool-map-on-a-function-defined-in-a-class">here</a>: </p>
<p>"</p>
<pre><code>from multiprocessing import Process, Pipe
from itertools import izip

def spawn(f):
    def fun(pipe,x):
        pipe.send(f(x))
        pipe.close()
    return fun

def parmap(f,X):
    pipe=[Pipe() for x in X]
    proc=[Process(target=spawn(f),args=(c,x)) for x,(p,c) in izip(X,pipe)]
    [p.start() for p in proc]
    [p.join() for p in proc]
    return [p.recv() for (p,c) in pipe]

if __name__ == '__main__':
    print parmap(lambda x:x**x,range(1,5))
</code></pre>
<p>"</p>
<p>you should be warned however that this takes over control manually of the processes so certain things can leave 'dead' processes lying around  - which is not a good thing, an example being unexpected signals - this is an example of using pipes for multi-processing though :).</p>
<p>If those commands are not in python, e.g. you want to run <code>ls</code> then you might be better served by using <code>subprocess</code>, as <code>os.system</code> isn't a good thing to use anymore necessarily as it is now considered that <code>subprocess</code> is an easier-to-use and more flexible tool, a <em>small</em> discussion is presented <a href="https://stackoverflow.com/questions/4813238/difference-between-subprocess-popen-and-os-system">here</a>. </p>
</div>
<div class="post-text" itemprop="text">
<p>You can do something like this with <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">multiprocessing</a></p>
<pre><code>from multiprocessing import Pool

mydict = {}
with Pool(processes=5) as pool:
        task_1 = pool.apply_async(do_this_task,args=(para_1,para_2))
        task_2 = pool.apply_async(do_this_task,args=(para_1,para_2))
        mydict.update({"task_1": task_1.get(), "task_2":task_2.get()})
print(mydict)
</code></pre>
<p>or if you would like to try multithreading with <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">concurrent.futures</a> then take a look at this <a href="https://stackoverflow.com/questions/32404825/how-to-run-multiple-functions-at-same-time#answer-32405154">answer</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>If the processes are external scripts then try using the <a href="http://docs.python.org/3/library/subprocess.html" rel="nofollow noreferrer">subprocess</a> module. However, your code suggests you want to run functions in parallel. For this, try the <a href="http://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">multiprocessing</a> module. Some code from <a href="https://stackoverflow.com/a/14299004/2387370">this</a> answer for specific details of using multiprocessing:</p>
<pre><code>def foo(bar, baz):
    print 'hello {0}'.format(bar)
    return 'foo' + baz

from multiprocessing.pool import ThreadPool
pool = ThreadPool(processes=1)

async_result = pool.apply_async(foo, ('world', 'foo')) # tuple of args for foo

# do some other stuff in the other processes

return_val = async_result.get()  # get the return value from your function.
</code></pre>
</div>
<span class="comment-copy">Use <code>apply_async</code> from the <code>multiprocessing</code> module, as described <a href="http://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers" rel="nofollow noreferrer">here</a></span>
<span class="comment-copy">Any reason for the downvote?</span>
