<div class="post-text" itemprop="text">
<p>The data that I process is ~ 6 Million and it takes a lot of time to write to a file. How do I improve it ?</p>
<p>The following are the two approaches that I tried:</p>
<pre><code>import numpy as np
import time
test_data = np.random.rand(6000000,12)
T1 = time.time()
np.savetxt('test',test_data, fmt='%.4f', delimiter=' ' )
T2 = time.time() 
print "Time:",T2-T1,"Sec"
file3=open('test2','w')
for i in range(6000000):
    for j in range(12):
        file3.write('%6.4f\t' % (test_data[i][j]))
    file3.write('\n')
T3 = time.time() 
print "Time:",T3-T2,"Sec" 
</code></pre>
<p>Time: 56.6293179989 Sec</p>
<p>Time: 115.468323946 Sec</p>
<p>I am dealing with atleast 100 files like this and the total time is a lot, please help. Also, I am not writing in .npy or compressed format as I need to read them in matlab and do further processing.</p>
</div>
<div class="post-text" itemprop="text">
<p>Have you considered <a href="http://docs.h5py.org/en/latest/quick.html" rel="nofollow noreferrer">h5py</a>?</p>
<p>Here's a cursory single-run time comparison:</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import time
&gt;&gt;&gt; import h5py
&gt;&gt;&gt; test_data = np.random.rand(6000000,12)
&gt;&gt;&gt; file = h5py.File('arrays.h5', 'w')

&gt;&gt;&gt; %time file.create_dataset('test_data', data=test_data, dtype=data.dtype)
CPU times: user 1.28 ms, sys: 224 ms, total: 225 ms
Wall time: 280 ms
&lt;HDF5 dataset "test_data": shape (6000000, 12), type "&lt;f8"&gt;

&gt;&gt;&gt; %time np.savetxt('test',test_data, fmt='%.4f', delimiter=' ' )
CPU times: user 24.4 s, sys: 617 ms, total: 25 s
Wall time: 26.3 s

&gt;&gt;&gt; file.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html" rel="nofollow noreferrer"><code>save</code></a> will almost always be hugely faster than <code>savetxt</code>. It just dumps the raw bytes, without having to format them as text. It also writes smaller files, which means less I/O. And you'll get equal benefits at load time: less I/O, and no text parsing.</p>
<p>Everything else below is basically a variant on top of the benefits of <code>save</code>. And if you look at the times at the end, all of them are within an order of magnitude of each other, but all around two orders of magnitude faster than <code>savetxt</code>. So, you may just be happy with the 200:1 speedup and not care about trying to tweak things any farther. But, if you do need to optimize further, read on.</p>
<hr/>
<p><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez_compressed.html#numpy.savez_compressed" rel="nofollow noreferrer"><code>savez_compressed</code></a> saves the array with <code>DEFLATE</code> compression. This means you waste a bunch of CPU, but save some I/O. If it's a slow disk that's slowing you down, that's a win. Note that with smallish arrays, the constant overhead will probably hurt more than the compression speedup will help, and if you have a random array there's little to no compression possible.</p>
<p><code>savez_compressed</code> is also a multi-array save. That may seem unnecessary here, but if you chunk a huge array into, say, 20 smaller ones, this can sometimes go significantly faster. (Even though I'm not sure why.) The cost is that if you just <code>load</code> ip the <code>.npz</code> and <code>stack</code> the arrays back together, you don't get contiguous storage, so if that matters, you have to write more complicated code.</p>
<p>Notice that my test below uses a random array, so the compression is just wasted overhead. But testing against <code>zeros</code> or <code>arange</code> would be just as misleading in the opposite direction, so… this is something to test on your real data.</p>
<p>Also, I'm on a computer with a pretty fast SSD, so the tradeoff between CPU and I/O may not be as imbalanced as on whatever machine you're running on.</p>
<hr/>
<p><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html" rel="nofollow noreferrer"><code>numpy.memmap</code></a>, or an array allocated into a stdlib <a href="https://docs.python.org/3/library/mmap.html" rel="nofollow noreferrer"><code>mmap.mmap</code></a>, is backed to disk with a write-through cache. This shouldn't reduce the total I/O time, but it means that the I/O doesn't happen all at once at the end, but is instead spread around throughout your computation—which often means it can happen in parallel with your heavy CPU work. So, instead of spending 50 minutes calculating and then 10 minutes saving, you spend 55 minutes calculating-and-saving.</p>
<p>This one is hard to test in any sensible way with a program that isn't actually doing any computation, so I didn't bother.</p>
<hr/>
<p><a href="https://docs.python.org/3/library/pickle.html" rel="nofollow noreferrer"><code>pickle</code></a> or one of its alternatives like <a href="https://pypi.org/project/dill/" rel="nofollow noreferrer"><code>dill</code></a> or <a href="https://pypi.org/project/cloudpickle/" rel="nofollow noreferrer"><code>cloudpickle</code></a>. There's really no good reason a pickle should be faster than a raw array dump, but occasionally it seems to be. </p>
<p>For a simple contiguous array like the one in my tests, the pickle is just a small wrapper around the exact same bytes as the binary dump, so it's just pure overhead.</p>
<hr/>
<p>For comparison, here's how I'm testing each one:</p>
<pre><code>In [70]: test_data = np.random.rand(1000000,12)
In [71]: %timeit np.savetxt('testfile', test_data)
9.95 s ± 222 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
In [72]: os.stat('testfile').st_size
Out[74]: 300000000
</code></pre>
<p>Notice the use of <code>%timeit</code> there. If you're not using IPython, use the <code>timeit</code> module in the stdlib to do the same thing a little verbosely. Testing with <code>time</code> has all kinds of problems (as described in the <code>timeit</code> docs, but the biggest is that you're only doing a single rep. And for I/O-based benchmarks, that's especially bad.</p>
<hr/>
<p>Here's the results for each—but, given the caveats above, you should really only consider the first two meaningful.</p>
<ul>
<li><code>savetxt</code>: 9.95s, 300MB</li>
<li><code>save</code>: 45.8 ms, 96MB</li>
<li><code>savez_compressed</code>: 360ms, 90MB</li>
<li><code>pickle</code>: 287ms, 96MB</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>How about using pickle? I found that it is more fast.</p>
<pre><code>import numpy as np
import time
import pickle
test_data = np.random.rand(1000000,12)

T1 = time.time()
np.savetxt('testfile',test_data, fmt='%.4f', delimiter=' ' )
T2 = time.time()
print ("Time:",T2-T1,"Sec")

file3=open('testfile','w')
for i in range(test_data.shape[0]):
    for j in range(test_data.shape[1]):
        file3.write('%6.4f\t' % (test_data[i][j]))
    file3.write('\n')
file3.close()
T3 = time.time()
print ("Time:",T3-T2,"Sec")

file3 = open('testfile','wb')
pickle.dump(test_data, file3)
file3.close()
T4 = time.time()
print ("Time:",T4-T3,"Sec")

# load data
file4 = open('testfile', 'rb')
obj = pickle.load(file4)
file4.close()
print(obj)
</code></pre>
<p>the output is </p>
<pre><code>Time: 9.1367928981781 Sec
Time: 16.366491079330444 Sec
Time: 0.41736602783203125 Sec
</code></pre>
</div>
<span class="comment-copy">You really shouldn't be using <code>time</code> for benchmarking, and a single-run test for something that's mostly disk I/O is especially bad, but since you've already done it, why don't you tell us what the results were?</span>
<span class="comment-copy">Anyway, sometimes saving in compressed format (e.g., with <code>savez_compressed</code>) is faster, sometimes it's slower. (Basically, there's a lot more CPU work, but significantly less I/O, so it depends on how fast your drive is and how fast and how loaded-down your CPU is.) So, you should test that as well.</span>
<span class="comment-copy">I have updated the times</span>
<span class="comment-copy">Disk I/O is slow. The biggest performance gain you can get is from upgrading hardware, especially from an HDD to an SSD. Otherwise, software optimization will only give relatively smaller gains.</span>
<span class="comment-copy">For MATLAB use you could try <code>scipy/io/savemat</code>, which handles the traditional MATLAB <code>.mat</code> format.  MATLAB can also handle <code>hdf5</code> file format, though its <code>save/load</code> format is somewhat involved, and won't be easy to replicate with the Python <code>h5py</code> tool.</span>
<span class="comment-copy">I use a commercial software called Abaqus within which I run my python scripts to get data and there is no h5py module in abaqus :(</span>
