<div class="post-text" itemprop="text">
<p>The python <code>time</code> module offers five clocks: <code>time</code>, <code>monotonic</code>, <code>perf_counter</code>, <code>process_time</code>, and <code>thread_time</code> (and the deprecated <code>clock</code>). What these clocks do is well explained in the <a href="https://docs.python.org/3/library/time.html" rel="nofollow noreferrer">documentation</a>. </p>
<p>You can get system-depended information about these clocks with the <code>time.get_clock_info(&lt;clock_name&gt;)</code> function. This in particular returns the <code>resolution</code> of this clock, which is in my understanding the minimal difference between to consecutive calls to that clock, that is not zero. To test this, I created the following code:</p>
<pre><code>import time

names = ['time', 'monotonic', 'perf_counter', 'process_time', 'thread_time']

for n in names:
    info = time.get_clock_info(n)
    f = getattr(time, n)
    print(n, info)
    l = []
    for _ in range(1000000):
        l.append(f())
    deltas = [l[i] - l[i - 1] for i in range(1, len(l)) if l[i] - l[i - 1] != 0]
    print(min(deltas, default=0))
</code></pre>
<p>This code prints the following for me:</p>
<pre><code>time 0.015625
0.00650477409362793
monotonic 0.015625
0.015000000013969839
perf_counter 3.77580764525532e-07
3.7699999988483057e-07
process_time 1e-07
0.015625
thread_time 1e-07
0.015625
</code></pre>
<p>This is quiet surprising, because the actual minimal delta of <code>time</code> is 2.4x smaller than promised and the actual minimal delta of <code>process_time</code> and <code>thread_time</code> are both 150.000 times bigger than promised.</p>
<p>Why is that? Is that implementation depending and python doesn't know the actual resolution? If it is that way, why does it pretend to know? Or am I misunderstanding something?</p>
<p>System Information:</p>
<ul>
<li>Windows 10 64bit</li>
<li>Python3.7 64bit</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>The values "150.000 times bigger than promised" is completely fine. The execution of your code is going to take some time. What you're promised is that this is the minimal difference you'll that can be registered, not that 2 consecutive calls will get that value. Also, that's the resolution of the values in the timer, not the timer itself. You can have a timer which counts in 100 ns increments, but actually adds 10 of them every 1us. </p>
<p>For the other way: You can follow "time" implementation for windows to <a href="https://github.com/python/cpython/blob/a5293b4ff2c1b5446947b4986f98ecf5d52432d4/Python/pytime.c#L674-L680" rel="nofollow noreferrer">pygettimeofday</a> (which uses <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/ms724394(v=vs.85).aspx" rel="nofollow noreferrer">GetSystemTimeAdjustment</a>) You'll have to dig into those descriptions to see what is the result. An interesting bit is that the <code>lpTimeIncrement</code> apparently only matters if <code>lpTimeAdjustmentDisabled</code> is set - and that's something cpython doesn't check.</p>
<p>The <code>perf_counter</code> is just weird and unlikely to be correct. It takes the value from <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/ms644904(v=vs.85).aspx" rel="nofollow noreferrer">QueryPerformanceCounter</a>, but executing the time measurement in 377ns is very unlikely.</p>
<p>For a better idea how the timers behave, you could also do a histogram plot of the results rather than using the minimum.</p>
</div>
<span class="comment-copy">What platform are you on? Because I get 1e-6 or 1e-9 on every platform, but the actual differences are in the millisecond range. Also, which Python 3.x version (especially if you're on Windows, where either 3.5 or 3.6 rewrote a bunch of the internals)?</span>
<span class="comment-copy">@abarnert Edited</span>
<span class="comment-copy">Damn, I was hoping you were going to say Windows before 3.5 or 3.6 (whichever one that change was in), because then I'd have a good guess. I think that change removed the <code>GetTickCount </code> workaround that was necessary to make XP/2003 work but unnecessarily forced a bunch of things to share the same clock rate (10, 16, 60, 64, or 100/sec… and you happen to have 64/sec…) in later versions of Windows.</span>
<span class="comment-copy">If you say the 'your code will take sometime to execute', how come that <code>perf_counter</code> is so fast? Do you mean the implementation of <code>GetProcessTimes</code> is so slow? I don't believe that.</span>
<span class="comment-copy">I don't know what's happening with <code>perf_counter</code> - I haven't looked into that implementation, but you can find it in github as well. You can check yourself how fast <code>GetProcessTimes</code> is - run it a few billion times, measure the time externally in seconds and divide by call count.</span>
<span class="comment-copy">@MegaIng updated for <code>perf_counter</code> and some idea you can try</span>
<span class="comment-copy">I have no idea how <code>QueryPerformanceCounter</code> is implemented on modern Windows, but in the stone ages… In 2000, it just read the cycle counter off the CPU, which took ~1ns. That's just wrong if you have multiple CPUs on separate dies, or a CPU with SpeedStep, or a if the computer goes to sleep, etc., but it's blazingly fast, even when it's wrong and useless. (Of course you have to divide it by the <code>QueryPerformanceFrequency</code> value if you want the time in seconds as opposed to cycles, which is much slower…)</span>
<span class="comment-copy">But anyway, that means the resolutions in the 100ns-500ns range that modern Windows boxes seem to report (while actually being correct) are definitely believable. Cache coherency issues could easily add a couple hundreds cycles, but the actual work when there's only one thread going is probably still single-digit cycles.</span>
