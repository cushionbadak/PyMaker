<div class="post-text" itemprop="text">
<p>Is it possible to get the results of a test (i.e. whether all assertions have passed) in a tearDown() method? I'm running Selenium scripts, and I'd like to do some reporting from inside tearDown(), however I don't know if this is possible.</p>
</div>
<div class="post-text" itemprop="text">
<p>CAVEAT: I have no way of double checking the following theory at the moment, being away from a dev box. So this may be a shot in the dark.</p>
<p>Perhaps you could check the return value of <code>sys.exc_info()</code> inside your tearDown() method, if it returns <code>(None, None, None)</code>, you know the test case succeeded. Otherwise, you could use returned tuple to interrogate the exception object.</p>
<p>See <a href="http://docs.python.org/library/sys.html#sys.exc_info">sys.exc_info</a> documentation.</p>
<p>Another more explicit approach is to write a method decorator that you could slap onto all your test case methods that require this special handling. This decorator can intercept assertion exceptions and based on that modify some state in <code>self</code> allowing your tearDown method to learn what's up.</p>
<pre><code>@assertion_tracker
def test_foo(self):
    # some test logic
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you take a look at the implementation of <code>unittest.TestCase.run</code>, you can see that all test results are collected in the result object (typically a <code>unittest.TestResult</code> instance) passed as argument. No result status is left in the <code>unittest.TestCase</code> object.</p>
<p>So there isn't much you can do in the <code>unittest.TestCase.tearDown</code> method unless you mercilessly break the elegant decoupling of test cases and test results with something like this:</p>
<pre><code>import unittest

class MyTest(unittest.TestCase):

    currentResult = None # holds last result object passed to run method

    def setUp(self):
        pass

    def tearDown(self):
        ok = self.currentResult.wasSuccessful()
        errors = self.currentResult.errors
        failures = self.currentResult.failures
        print ' All tests passed so far!' if ok else \
                ' %d errors and %d failures so far' % \
                (len(errors), len(failures))

    def run(self, result=None):
        self.currentResult = result # remember result for use in tearDown
        unittest.TestCase.run(self, result) # call superclass run method

    def test_onePlusOneEqualsTwo(self):
        self.assertTrue(1 + 1 == 2) # succeeds

    def test_onePlusOneEqualsThree(self):
        self.assertTrue(1 + 1 == 3) # fails

    def test_onePlusNoneIsNone(self):
        self.assertTrue(1 + None is None) # raises TypeError

if __name__ == '__main__':
    unittest.main()
</code></pre>
<p>EDIT: This works for Python 2.6 - 3.3,
(modified for new Python <a href="https://stackoverflow.com/a/39606065/448474">bellow</a>).</p>
</div>
<div class="post-text" itemprop="text">
<p>This solution is for <strong>Python</strong> versions <strong>2.7 to 3.7</strong> (the highest current version), without any decorators or other modification in any code before <code>tearDown</code>. Everything works according to the builtin classification of results. Also skipped tests or <code>expectedFailure</code> are recognized correctly. It evaluates the result of the current test, not a summary of all tests passed so far. Compatible also with <a href="https://docs.pytest.org/" rel="nofollow noreferrer">pytest</a>.</p>
<pre class="lang-py prettyprint-override"><code>import unittest

class MyTest(unittest.TestCase):
    def tearDown(self):
        if hasattr(self, '_outcome'):  # Python 3.4+
            result = self.defaultTestResult()  # these 2 methods have no side effects
            self._feedErrorsToResult(result, self._outcome.errors)
        else:  # Python 3.2 - 3.3 or 3.0 - 3.1 and 2.7
            result = getattr(self, '_outcomeForDoCleanups', self._resultForDoCleanups)
        error = self.list2reason(result.errors)
        failure = self.list2reason(result.failures)
        ok = not error and not failure

        # demo:   report short info immediately (not important)
        if not ok:
            typ, text = ('ERROR', error) if error else ('FAIL', failure)
            msg = [x for x in text.split('\n')[1:] if not x.startswith(' ')][0]
            print("\n%s: %s\n     %s" % (typ, self.id(), msg))

    def list2reason(self, exc_list):
        if exc_list and exc_list[-1][0] is self:
            return exc_list[-1][1]

    # DEMO tests
    def test_success(self):
        self.assertEqual(1, 1)

    def test_fail(self):
        self.assertEqual(2, 1)

    def test_error(self):
        self.assertEqual(1 / 0, 1)
</code></pre>
<p><strong>Comments:</strong> Only one or zero exceptions (error or failure) need be reported because not more can be expected before <code>tearDown</code>. The package <code>unittest</code> expects that a second exception can be raised by tearDown. Therefore the lists <code>errors</code> and <code>failures</code> can contain only one or zero elements together before tearDown. Lines after "demo" comment are reporting a short result.</p>
<p><strong>Demo output:</strong> (not important)<br/>
</p>
<pre><code>$ python3.5 -m unittest test

EF.
ERROR: test.MyTest.test_error
     ZeroDivisionError: division by zero
FAIL: test.MyTest.test_fail
     AssertionError: 2 != 1

==========================================================
... skipped usual output from unittest with tracebacks ...
...
Ran 3 tests in 0.002s

FAILED (failures=1, errors=1)
</code></pre>
<hr/>
<p><strong>Comparision to other solutions</strong> - (with respect to commit history of Python source repository):  </p>
<ul>
<li><p><strong>This solution</strong> uses a <em>private attribute</em> of TestCase instance like many
other solutions,
but I checked carefully all relevant commits in the Python source repository
that three alternative names cover the code history since Python
2.7 to 3.6.2 without any gap. It can be a problem after some new major
Python release, but it could be clearly recognized, skipped and easily fixed
later for a new Python. An advantage is that nothing is modified before
running tearDown, it should never break the test and all functionality of
unittest is supported, works with pytest and it could work many extending packages, but not with nosetest (not a suprise becase nosetest is not compatible e.g. with unittest.expectedFailure).</p></li>
<li><p>The solutions with <em>decorators</em> on the user test methods or with a customized
<em>failureException</em> (<a href="https://stackoverflow.com/a/45950855/448474">mgilson</a>, Pavel Repin 2nd way, kenorb) are robust against
future Python
versions, but if everything should work completely, they would grow like a
snow ball with more supported exceptions and more replicated internals
of unittest. The decorated functions have less readable tracebacks
(even more levels added by one decorator), they are more complicated for
debugging and it is unpleassant if another more important decorator
has a problem. (Thanks to mgilson the basic functionality is ready and known
issues can be fixed.)</p></li>
<li><p>The solution with modifired <code>run</code> method and catched <code>result</code>
parameter</p>
<ul>
<li>(<a href="https://stackoverflow.com/a/4415062/448474">scoffey</a>) should work
also for Python 2.6. The interpretation of results can be improved to
requirements of the question, but nothing can work in Python 3.4+,
because <code>result</code> is updated after tearDown call, never before.</li>
<li>Mark G.: (tested with Python 2.7, 3.2, 3.3, 3.4 and with nosetest)</li>
</ul></li>
<li><p>solution by <code>exc_info()</code> (Pavel Repin 2st way) works only with Python 2.</p></li>
<li><p>Other solutions are principially similar, but less complete or with more
disadvantages.</p></li>
</ul>
<hr/>
<p><strong>Explained by Python source repository</strong><br/>
= <strong>Lib/unittest/case.py</strong> =<br/>
Python v 2.7 - 3.3
</p>
<pre><code>class TestCase(object):
    ...
    def run(self, result=None):
        ...
        self._outcomeForDoCleanups = result   # Python 3.2, 3.3
        # self._resultForDoCleanups = result  # Python 2.7
        #                                     # Python 2.6 - no result saved
        ...
        try:
            testMethod()
        except...   # many times for different exception classes
            result.add...(self, sys.exc_info())  # _addSkip, addError, addFailure
        ...
        try:
            self.tearDown()
        ...
</code></pre>
<p>Python v. 3.4 - 3.6</p>
<pre><code>    def run(self, result=None):
        ...
        # outocome is a context manager to catch and collect different exceptions
        self._outcome = outcome  
        ...
        with outcome...(self):
            testMethod()
        ...
        with outcome...(self): 
            self.tearDown() 
        ... 
        self._feedErrorsToResult(result, outcome.errors)
</code></pre>
<p><strong>Note</strong> (by reading Python commit messages): A <strong>reason</strong> why test results are so much decoupled from tests is <em>memory leaks</em> prevention. Every exception info can access to frames of the failed process state including all local variables. If a frame is assigned to a local variable in a code block that could also fail, then a cross memory refence could be easily created. It is not terrible, thanks to garbage collector, but the free memory can became fragmented more quickly than if the memory would be released correctly. This is a reason why exception information and traceback are converted very soon to strings and why temporary objects like <code>self._outcome</code> are encapsulated and are set to None in a <code>finally</code> block in order to memory leaks are prevented.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you are using Python2 you can use the method <code>_resultForDoCleanups</code>. This method return a <a href="http://docs.python.org/2/library/unittest.html#unittest.TextTestResult" rel="noreferrer"><code>TextTestResult</code></a> object:</p>
<p><code>&lt;unittest.runner.TextTestResult run=1 errors=0 failures=0&gt;</code></p>
<p>You can use this object to check the result of your tests:</p>
<pre><code>def tearDown(self):
    if self._resultForDoCleanups.failures:
        ...
    elif self._resultForDoCleanups.errors:
        ...
    else:
        #Success
</code></pre>
<p>If you are using Python3 you can use <code>_outcomeForDoCleanups</code>:</p>
<pre><code>def tearDown(self):
    if not self._outcomeForDoCleanups.success:
        ...
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Following on from amatellanes' answer, if you're on Python3.4, you can't use <code>_outcomeForDoCleanups</code>.  Here's what I managed to hack together:</p>
<pre><code>def _test_has_failed(self):
    for method, error in self._outcome.errors:
        if error:
            return True
    return False
</code></pre>
<p>yucky, but it seems to work.</p>
</div>
<div class="post-text" itemprop="text">
<p>It depends what kind of reporting you'd like to produce. </p>
<p>In case you'd like to do some actions on failure (such as <a href="https://stackoverflow.com/q/12290336/55075">generating a screenshots</a>), instead of using <code>tearDown()</code>, you may achieve that by overriding <code>failureException</code>.</p>
<p>For example:</p>
<pre><code>@property
def failureException(self):
    class MyFailureException(AssertionError):
        def __init__(self_, *args, **kwargs):
            screenshot_dir = 'reports/screenshots'
            if not os.path.exists(screenshot_dir):
                os.makedirs(screenshot_dir)
            self.driver.save_screenshot('{0}/{1}.png'.format(screenshot_dir, self.id()))
            return super(MyFailureException, self_).__init__(*args, **kwargs)
    MyFailureException.__name__ = AssertionError.__name__
    return MyFailureException
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's a solution for those of us who are uncomfortable using solutions that rely on <code>unittest</code> internals:</p>
<p>First, we create a decorator that will set a flag on the <code>TestCase</code> instance to determine whether or not the test case failed or passed:</p>
<pre><code>import unittest
import functools

def _tag_error(func):
    """Decorates a unittest test function to add failure information to the TestCase."""

    @functools.wraps(func)
    def decorator(self, *args, **kwargs):
        """Add failure information to `self` when `func` raises an exception."""
        self.test_failed = False
        try:
            func(self, *args, **kwargs)
        except unittest.SkipTest:
            raise
        except Exception:  # pylint: disable=broad-except
            self.test_failed = True
            raise  # re-raise the error with the original traceback.

    return decorator
</code></pre>
<p>This decorator is actually pretty simple.  It relies on the fact that <code>unittest</code> detects failed tests via <em>Exceptions</em>.  As far as I'm aware, the only <em>special</em> exception that needs to be handled is <code>unittest.SkipTest</code> (which does not indicate a test failure).  All other exceptions indicate test failures so we mark them as such when they bubble up to us.</p>
<p>We can now use this decorator directly:</p>
<pre><code>class MyTest(unittest.TestCase):
    test_failed = False

    def tearDown(self):
        super(MyTest, self).tearDown()
        print(self.test_failed)

    @_tag_error
    def test_something(self):
        self.fail('Bummer')
</code></pre>
<hr/>
<p>It's going to get really annoying writing this decorator all the time.  Is there a way we can simplify?  Yes there is!<sup>*</sup>  We can write a metaclass to handle applying the decorator for us:</p>
<pre><code>class _TestFailedMeta(type):
    """Metaclass to decorate test methods to append error information to the TestCase instance."""
    def __new__(cls, name, bases, dct):
        for name, prop in dct.items():
            # assume that TestLoader.testMethodPrefix hasn't been messed with -- otherwise, we're hosed.
            if name.startswith('test') and callable(prop):
                dct[name] = _tag_error(prop)

        return super(_TestFailedMeta, cls).__new__(cls, name, bases, dct)
</code></pre>
<p>Now we apply this to our base <code>TestCase</code> subclass and we're all set:</p>
<pre><code>import six  # For python2.x/3.x compatibility

class BaseTestCase(six.with_metaclass(_TestFailedMeta, unittest.TestCase)):
    """Base class for all our other tests.

    We don't really need this, but it demonstrates that the
    metaclass gets applied to all subclasses too.
    """


class MyTest(BaseTestCase):

    def tearDown(self):
        super(MyTest, self).tearDown()
        print(self.test_failed)

    def test_something(self):
        self.fail('Bummer')
</code></pre>
<p>There are likely a number of cases that this doesn't handle properly.  For example, <strong>it does not correctly detect failed <a href="https://docs.python.org/3/library/unittest.html#distinguishing-test-iterations-using-subtests" rel="nofollow noreferrer">subtests</a></strong> or expected failures.  I'd be interested in other failure modes of this, so if you find a case that I'm not handling properly, let me know in the comments and I'll look into it.</p>
<hr/>
<p><sup>*</sup>If there wasn't an easier way, I wouldn't have made <code>_tag_error</code> a private function ;-)</p>
</div>
<div class="post-text" itemprop="text">
<p>Python 2.7.</p>
<p>You can also get result after unittest.main():</p>
<pre><code>t = unittest.main(exit=False)
print t.result
</code></pre>
<p>or use suite:</p>
<pre><code>suite.addTests(tests)
result = unittest.result.TestResult()
suite.run(result)
print result
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Name of current test can be retrieved with <a href="https://docs.python.org/2.7/library/unittest.html#unittest.TestCase.id" rel="nofollow">unittest.TestCase.id()</a> method. So in tearDown you can check self.id().  </p>
<p>Example shows how to: </p>
<ul>
<li>find if current test has error or failure in errors or failures list</li>
<li>print test id with PASS or FAIL or EXCEPTION</li>
</ul>
<p>Tested example here works with @scoffey 's nice example. </p>
<pre><code>def tearDown(self):
    result = "PASS"
    #### find and show result for current test
    # I did not find any nicer/neater way of comparing self.id() with test id stored in errors or failures lists :-7
    id = str(self.id()).split('.')[-1]
    # id() e.g. tup[0]:&lt;__main__.MyTest testMethod=test_onePlusNoneIsNone&gt;
    #           str(tup[0]):"test_onePlusOneEqualsThree (__main__.MyTest)"
    #           str(self.id()) = __main__.MyTest.test_onePlusNoneIsNone
    for tup in self.currentResult.failures:
        if str(tup[0]).startswith(id):
            print ' test %s failure:%s' % (self.id(), tup[1])
            ## DO TEST FAIL ACTION HERE
            result = "FAIL"
    for tup in self.currentResult.errors:
        if str(tup[0]).startswith(id):
            print ' test %s error:%s' % (self.id(), tup[1])
            ## DO TEST EXCEPTION ACTION HERE
            result = "EXCEPTION"

    print "Test:%s Result:%s" % (self.id(), result)
</code></pre>
<p>example of result:</p>
<pre><code>python run_scripts/tut2.py 2&gt;&amp;1 
E test __main__.MyTest.test_onePlusNoneIsNone error:Traceback (most recent call last):
  File "run_scripts/tut2.py", line 80, in test_onePlusNoneIsNone
    self.assertTrue(1 + None is None) # raises TypeError
TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'

Test:__main__.MyTest.test_onePlusNoneIsNone Result:EXCEPTION
F test __main__.MyTest.test_onePlusOneEqualsThree failure:Traceback (most recent call last):
  File "run_scripts/tut2.py", line 77, in test_onePlusOneEqualsThree
    self.assertTrue(1 + 1 == 3) # fails
AssertionError: False is not true

Test:__main__.MyTest.test_onePlusOneEqualsThree Result:FAIL
Test:__main__.MyTest.test_onePlusOneEqualsTwo Result:PASS
.
======================================================================
ERROR: test_onePlusNoneIsNone (__main__.MyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "run_scripts/tut2.py", line 80, in test_onePlusNoneIsNone
    self.assertTrue(1 + None is None) # raises TypeError
TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'

======================================================================
FAIL: test_onePlusOneEqualsThree (__main__.MyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "run_scripts/tut2.py", line 77, in test_onePlusOneEqualsThree
     self.assertTrue(1 + 1 == 3) # fails
AssertionError: False is not true

----------------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=1, errors=1)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Inspired by <a href="https://stackoverflow.com/questions/4414234/getting-pythons-unittest-results-in-a-teardown-method/4415062#4415062">scoffey’s answer</a>, I decided to take <em>mercilessnes</em> to the next level, and have come up with the following.</p>
<p>It works in both vanilla unittest, and also when run via nosetests, and also works in Python versions 2.7, 3.2, 3.3, and 3.4 (I did not specifically test 3.0, 3.1, or 3.5, as I don’t have these installed at the moment, but if I read the <a href="https://github.com/python/cpython/blob/3.5/Lib/unittest/case.py#L593-L608" rel="nofollow noreferrer">source code</a> correctly, it should work in 3.5 as well):</p>
<pre><code>#! /usr/bin/env python

from __future__ import unicode_literals
import logging
import os
import sys
import unittest


# Log file to see squawks during testing
formatter = logging.Formatter(fmt='%(levelname)-8s %(name)s: %(message)s')
log_file = os.path.splitext(os.path.abspath(__file__))[0] + '.log'
handler = logging.FileHandler(log_file)
handler.setFormatter(formatter)
logging.root.addHandler(handler)
logging.root.setLevel(logging.DEBUG)
log = logging.getLogger(__name__)


PY = tuple(sys.version_info)[:3]


class SmartTestCase(unittest.TestCase):

    """Knows its state (pass/fail/error) by the time its tearDown is called."""

    def run(self, result):
        # Store the result on the class so tearDown can behave appropriately
        self.result = result.result if hasattr(result, 'result') else result
        if PY &gt;= (3, 4, 0):
            self._feedErrorsToResultEarly = self._feedErrorsToResult
            self._feedErrorsToResult = lambda *args, **kwargs: None  # no-op
        super(SmartTestCase, self).run(result)

    @property
    def errored(self):
        if (3, 0, 0) &lt;= PY &lt; (3, 4, 0):
            return bool(self._outcomeForDoCleanups.errors)
        return self.id() in [case.id() for case, _ in self.result.errors]

    @property
    def failed(self):
        if (3, 0, 0) &lt;= PY &lt; (3, 4, 0):
            return bool(self._outcomeForDoCleanups.failures)
        return self.id() in [case.id() for case, _ in self.result.failures]

    @property
    def passed(self):
        return not (self.errored or self.failed)

    def tearDown(self):
        if PY &gt;= (3, 4, 0):
            self._feedErrorsToResultEarly(self.result, self._outcome.errors)


class TestClass(SmartTestCase):

    def test_1(self):
        self.assertTrue(True)

    def test_2(self):
        self.assertFalse(True)

    def test_3(self):
        self.assertFalse(False)

    def test_4(self):
        self.assertTrue(False)

    def test_5(self):
        self.assertHerp('Derp')

    def tearDown(self):
        super(TestClass, self).tearDown()
        log.critical('---- RUNNING {} ... -----'.format(self.id()))
        if self.errored:
            log.critical('----- ERRORED -----')
        elif self.failed:
            log.critical('----- FAILED -----')
        else:
            log.critical('----- PASSED -----')


if __name__ == '__main__':
    unittest.main()
</code></pre>
<p>When run with <code>unittest</code>:</p>
<pre><code>$ ./test.py -v
test_1 (__main__.TestClass) ... ok
test_2 (__main__.TestClass) ... FAIL
test_3 (__main__.TestClass) ... ok
test_4 (__main__.TestClass) ... FAIL
test_5 (__main__.TestClass) ... ERROR
[…]

$ cat ./test.log
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_1 ... -----
CRITICAL __main__: ----- PASSED -----
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_2 ... -----
CRITICAL __main__: ----- FAILED -----
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_3 ... -----
CRITICAL __main__: ----- PASSED -----
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_4 ... -----
CRITICAL __main__: ----- FAILED -----
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_5 ... -----
CRITICAL __main__: ----- ERRORED -----
</code></pre>
<p>When run with <code>nosetests</code>:</p>
<pre><code>$ nosetests ./test.py -v
test_1 (test.TestClass) ... ok
test_2 (test.TestClass) ... FAIL
test_3 (test.TestClass) ... ok
test_4 (test.TestClass) ... FAIL
test_5 (test.TestClass) ... ERROR

$ cat ./test.log
CRITICAL test: ---- RUNNING test.TestClass.test_1 ... -----
CRITICAL test: ----- PASSED -----
CRITICAL test: ---- RUNNING test.TestClass.test_2 ... -----
CRITICAL test: ----- FAILED -----
CRITICAL test: ---- RUNNING test.TestClass.test_3 ... -----
CRITICAL test: ----- PASSED -----
CRITICAL test: ---- RUNNING test.TestClass.test_4 ... -----
CRITICAL test: ----- FAILED -----
CRITICAL test: ---- RUNNING test.TestClass.test_5 ... -----
CRITICAL test: ----- ERRORED -----
</code></pre>
<h2>Background</h2>
<p>I <em>started</em> with this:</p>
<pre><code>class SmartTestCase(unittest.TestCase):

    """Knows its state (pass/fail/error) by the time its tearDown is called."""

    def run(self, result):
        # Store the result on the class so tearDown can behave appropriately
        self.result = result.result if hasattr(result, 'result') else result
        super(SmartTestCase, self).run(result)

    @property
    def errored(self):
        return self.id() in [case.id() for case, _ in self.result.errors]

    @property
    def failed(self):
        return self.id() in [case.id() for case, _ in self.result.failures]

    @property
    def passed(self):
        return not (self.errored or self.failed)
</code></pre>
<p>However, this only works in Python 2. In Python 3, up to and including 3.3, the control flow appears to have changed a bit: Python 3’s unittest package <a href="https://github.com/python/cpython/blob/3.3/Lib/unittest/case.py#L435-L451" rel="nofollow noreferrer">processes results</a> <em>after</em> calling each test’s <code>tearDown()</code> method… this behavior can be confirmed if we simply add an extra line (or six) to our test class:</p>
<pre><code>@@ -63,6 +63,12 @@
             log.critical('----- FAILED -----')
         else:
             log.critical('----- PASSED -----')
+        log.warning(
+            'ERRORS THUS FAR:\n'
+            + '\n'.join(tc.id() for tc, _ in self.result.errors))
+        log.warning(
+            'FAILURES THUS FAR:\n'
+            + '\n'.join(tc.id() for tc, _ in self.result.failures))


 if __name__ == '__main__':
</code></pre>
<p>Then just re-run the tests:</p>
<pre><code>$ python3.3 ./test.py -v
test_1 (__main__.TestClass) ... ok
test_2 (__main__.TestClass) ... FAIL
test_3 (__main__.TestClass) ... ok
test_4 (__main__.TestClass) ... FAIL
test_5 (__main__.TestClass) ... ERROR
[…]
</code></pre>
<p>…and you will see that you get this as a result:</p>
<pre><code>CRITICAL __main__: ---- RUNNING __main__.TestClass.test_1 ... -----
CRITICAL __main__: ----- PASSED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:

CRITICAL __main__: ---- RUNNING __main__.TestClass.test_2 ... -----
CRITICAL __main__: ----- PASSED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:

CRITICAL __main__: ---- RUNNING __main__.TestClass.test_3 ... -----
CRITICAL __main__: ----- PASSED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:
__main__.TestClass.test_2
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_4 ... -----
CRITICAL __main__: ----- PASSED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:
__main__.TestClass.test_2
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_5 ... -----
CRITICAL __main__: ----- PASSED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:
__main__.TestClass.test_2
__main__.TestClass.test_4
</code></pre>
<p>Now, compare the above to Python 2’s output:</p>
<pre><code>CRITICAL __main__: ---- RUNNING __main__.TestClass.test_1 ... -----
CRITICAL __main__: ----- PASSED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:

CRITICAL __main__: ---- RUNNING __main__.TestClass.test_2 ... -----
CRITICAL __main__: ----- FAILED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:
__main__.TestClass.test_2
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_3 ... -----
CRITICAL __main__: ----- PASSED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:
__main__.TestClass.test_2
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_4 ... -----
CRITICAL __main__: ----- FAILED -----
WARNING  __main__: ERRORS THUS FAR:

WARNING  __main__: FAILURES THUS FAR:
__main__.TestClass.test_2
__main__.TestClass.test_4
CRITICAL __main__: ---- RUNNING __main__.TestClass.test_5 ... -----
CRITICAL __main__: ----- ERRORED -----
WARNING  __main__: ERRORS THUS FAR:
__main__.TestClass.test_5
WARNING  __main__: FAILURES THUS FAR:
__main__.TestClass.test_2
__main__.TestClass.test_4
</code></pre>
<p>Since Python 3 processes errors/failures <em>after</em> the test is torn down, we can’t readily infer the result of a test using <code>result.errors</code> or <code>result.failures</code> in every case. (I think it probably makes more sense architecturally to process a test’s results <em>after</em> tearing it down, however, it <em>does</em> make the perfectly valid use-case of following a different end-of-test procedure depending on a test’s pass/fail status a bit harder to meet…)</p>
<p>Therefore, instead of relying on the overall <code>result</code> object, instead we can reference <code>_outcomeForDoCleanups</code> as <a href="https://stackoverflow.com/a/23176373/2420847">others</a> have <a href="https://stackoverflow.com/a/22597494/2420847">already</a> mentioned, which contains the result object for the currently running test, and has the necessary <code>errors</code> and <code>failrues</code> attributes, which we can use to infer a test’s status by the time <code>tearDown()</code> has been called:</p>
<pre><code>@@ -3,6 +3,7 @@
 from __future__ import unicode_literals
 import logging
 import os
+import sys
 import unittest


@@ -16,6 +17,9 @@
 log = logging.getLogger(__name__)


+PY = tuple(sys.version_info)[:3]
+
+
 class SmartTestCase(unittest.TestCase):

     """Knows its state (pass/fail/error) by the time its tearDown is called."""
@@ -27,10 +31,14 @@

     @property
     def errored(self):
+        if PY &gt;= (3, 0, 0):
+            return bool(self._outcomeForDoCleanups.errors)
         return self.id() in [case.id() for case, _ in self.result.errors]

     @property
     def failed(self):
+        if PY &gt;= (3, 0, 0):
+            return bool(self._outcomeForDoCleanups.failures)
         return self.id() in [case.id() for case, _ in self.result.failures]

     @property
</code></pre>
<p>This adds support for the early versions of Python 3.</p>
<p>As of Python 3.4, however, this private member variable <a href="https://stackoverflow.com/questions/4414234/getting-pythons-unittest-results-in-a-teardown-method/4415062#comment35444968_22597494">no longer exists</a>, and instead, a new (albeit <em>also</em> private) method was added: <a href="https://github.com/python/cpython/blob/3.4/Lib/unittest/case.py#L573-L588" rel="nofollow noreferrer"><code>_feedErrorsToResult</code></a>.</p>
<p>This means that for versions 3.4 (<a href="https://github.com/python/cpython/blob/3.5/Lib/unittest/case.py#L593-L608" rel="nofollow noreferrer">and later</a>), if the need is great enough, one can — <strong><em>very hackishly</em></strong> — <em>force</em> one’s way in to make it all work again like it did in version 2…</p>
<pre><code>@@ -27,17 +27,20 @@
     def run(self, result):
         # Store the result on the class so tearDown can behave appropriately
         self.result = result.result if hasattr(result, 'result') else result
+        if PY &gt;= (3, 4, 0):
+            self._feedErrorsToResultEarly = self._feedErrorsToResult
+            self._feedErrorsToResult = lambda *args, **kwargs: None  # no-op
         super(SmartTestCase, self).run(result)

     @property
     def errored(self):
-        if PY &gt;= (3, 0, 0):
+        if (3, 0, 0) &lt;= PY &lt; (3, 4, 0):
             return bool(self._outcomeForDoCleanups.errors)
         return self.id() in [case.id() for case, _ in self.result.errors]

     @property
     def failed(self):
-        if PY &gt;= (3, 0, 0):
+        if (3, 0, 0) &lt;= PY &lt; (3, 4, 0):
             return bool(self._outcomeForDoCleanups.failures)
         return self.id() in [case.id() for case, _ in self.result.failures]

@@ -45,6 +48,10 @@
     def passed(self):
         return not (self.errored or self.failed)

+    def tearDown(self):
+        if PY &gt;= (3, 4, 0):
+            self._feedErrorsToResultEarly(self.result, self._outcome.errors)
+

 class TestClass(SmartTestCase):

@@ -64,6 +71,7 @@
         self.assertHerp('Derp')

     def tearDown(self):
+        super(TestClass, self).tearDown()
         log.critical('---- RUNNING {} ... -----'.format(self.id()))
         if self.errored:
             log.critical('----- ERRORED -----')
</code></pre>
<p><em>…provided,</em> of course, all consumers of this class remember to <code>super(…, self).tearDown()</code> in their respective <code>tearDown</code> methods…</p>
<p><strong>Disclaimer:</strong> Purely educational, don’t try this at home, etc. etc. etc. I’m not particularly proud of this solution, but it seems to work well enough for the time being, and is the best I could hack up after fiddling for an hour or two on a Saturday afternoon…</p>
</div>
<div class="post-text" itemprop="text">
<p>Tested for python 3.7 - sample code for getting info of failing assertion but can give idea how to deal with errors:</p>
<pre><code>def tearDown(self):
    if self._outcome.errors[1][1] and hasattr(self._outcome.errors[1][1][1], 'actual'):
        print(self._testMethodName)
        print(self._outcome.errors[1][1][1].actual)
        print(self._outcome.errors[1][1][1].expected)
</code></pre>
</div>
<span class="comment-copy">What kind of reporting? What exactly are you trying to do?</span>
<span class="comment-copy">For instance, your test produces intermediate files (that are normally clean ed in tearDown) and you want to collect them if the test fails.</span>
<span class="comment-copy">Name of current test can be retrieved with unittest.id(). So in tearDown you can check self.id().</span>
<span class="comment-copy">Unfortunately this doesn't make the distinction between "errors" and "failures" -- <a href="http://docs.python.org/library/unittest.html#organizing-test-code" rel="nofollow noreferrer">docs.python.org/library/unittest.html#organizing-test-code</a></span>
<span class="comment-copy">didn't work for me. <code>sys.exc_info</code> was always 3 Nones, even in tests with failures. May be difference with Python3 unittest?</span>
<span class="comment-copy">Using this method, if you are using nose.plugins.skip.SkipTest to mark tests as skipped, skipped tests will be reported as errors, since you <code>raise SkipTest</code>. This is probably not what you want in this case.</span>
<span class="comment-copy">For the curious, this does NOT work for recent python versions.  I believe that it breaks at python3.4 (ish).</span>
<span class="comment-copy">@mgilson This solution by <code>exc_info()</code> has been broken since Python 3.0 (the latest pre-release of 3.0 or rather 3.1.0 stable in Jun 2009) because the original <code>sys.exc_info()</code> is accessible only in the innermost <code>try: ... except: ...</code> block in Python 3. It is automatically cleared outside. Module unittest in Python 3.2 to 3.7-dev saves <code>exc_info()</code> before leaving the "except" block or converts the important part of exc_info to string in Python 3.0, 3.1. (I verified it now on all aforesaid Python versions.)</span>
<span class="comment-copy">This works when running directly, but can cause this with <code>nosetests</code>: <a href="https://stackoverflow.com/questions/11980375/getting-pythons-nosetests-results-in-a-teardown-method" title="getting pythons nosetests results in a teardown method">stackoverflow.com/questions/11980375/…</a></span>
<span class="comment-copy">Try <code>print(self.currentResult)</code> at the end of <code>tearDown</code> and at the end of <code>run</code> for this code snippet. For tests with <code>F</code>, the <code>failures</code> count increments for <code>print</code> in <code>run</code> but not for <code>tearDown</code> it seems. Was this intended? I would want to know in <code>tearDown</code> if the unit test that is being "tear down" failed or succeeded.</span>
<span class="comment-copy">It is the best solution I found so far after 2 days of continuous surfing.</span>
<span class="comment-copy">@ShanthaDodmane: Thanks. I found this solution also after 2 days :-) of reading Python git repository, to verify that it is correct, but too late to get any attention here.</span>
<span class="comment-copy">This really ought to be the accepted answer, it's far more complete and accurate.</span>
<span class="comment-copy">Note that with <code>pytest --pdb</code>, <code>self._outcome</code> can be <code>None</code>. If you just want to know whether the last test failed, use something like <code>last_test_failed = self._outcome and any(exc_info for test_case, exc_info in self._outcome.errors)</code></span>
<span class="comment-copy"><code>._outcomeForDoCleanups</code> has gone in 3.4.  there is a thing called <code>._outcome</code>, but it doesn't seem to expose the test pass/fail state...</span>
<span class="comment-copy">Accessing "private" members is generally frowned upon, and this API can change at any moment. Also: sometimes the <code>failures</code> attribute doesn't appear to be set, causing the tearDown to throw an <code>AttributeError</code>.</span>
<span class="comment-copy">Bingo... this code is specific to <code>unittest</code>. It is NOT compatible with Py.test.</span>
<span class="comment-copy">This is clever, but I feel like it is a little sketchy.  First, <code>failureException</code> should accept an argument (see <a href="https://docs.python.org/2.7/library/unittest.html#unittest.TestCase.addTypeEqualityFunc" rel="nofollow noreferrer">docs.python.org/2.7/library/…</a>).  Second, it is documented to be an <code>Exception</code> whereas you've replaced it with a function.  In principle, that should be OK as long as no other code actually <i>relies</i> on the fact that <code>failureException</code> is an exception class (i.e. <code>raise self.failureException</code> will now start failing where it would have succeeded before).</span>
<span class="comment-copy">Did you tried KeyboardInterrupt exception, expectedFailure decorator and <a href="https://docs.python.org/3/library/unittest.html#distinguishing-test-iterations-using-subtests" rel="nofollow noreferrer">Distinguishing test iterations using subtests</a>? If you don't want to break that, you probably must use more internal names and monkey patch some unittest code. I vote up because basic features of unittest will probably work in every future Python version. Debugging of exceptions is more complicated if they are reraised by decorator. My solution should support every current unittest feature without explicitly enumerate anything.</span>
<span class="comment-copy">@hynekcer -- You're right about <code>KeyboardInterrupt -- I should be </code>except Exception` rather than a bare except.  And I agree, this <i>might</i> not work properly with subtests.  It also might not work properly with expected failures and a few other cases.  However, it does work robustly for a wide range of normal cases.</span>
<span class="comment-copy">The only verified problem is <code>expectedFailure</code> decorator that seems very hard to be fixed in your case. Subtests are problematic only with expectedFailure decorator, but relative easy by intercepting <code>expectedFailure</code>. Keyboard interrupt works and the result of test_failed will be anyway never seen after it. I tried your way last year before writing my solution, but I work sometimes on other test decorators and it was terrible to debug them in a combination. On the other hand, it is trivial to verify that a data structure in an undocumented attribute is the same in a new Python. ...</span>
<span class="comment-copy">It works with Python master branch two weeks before Python 3.7 alpha 1. So it has at least two and half year until 3.8 stable. Changes in unittest since 3.5 are minimal.</span>
<span class="comment-copy">@hynekcer -- Yeah, fixing <code>expectedFailure</code> is a bugger without relying on the implementation.  If you rely on the implementation, it's as simple as checking <code>func</code> and test-case for a truthy <code>__unittest_expecting_failure__</code> attribute and <i>not</i> setting the failed flag in that case.  But of course, the entire point of the answer was to <i>avoid</i> relying on these implementation details :-)</span>
<span class="comment-copy">+1 but: You should not add to the object self.result directly, otherwise you get the failures from test methods reported twice or you lose eventually errors from tearDown if something goes wrong. A new temporary result object only for the errors from the test method is a solution. (I did not see this end of screen, until I began to write an answer.)</span>
