<div class="post-text" itemprop="text">
<p>Since my scaper is running so slow (one page at a time) so I'm trying to use thread to make it work faster. I have a function scrape(website) that take in a website to scrape, so easily I can create each thread and call start() on each of them.</p>
<p>Now I want to implement a num_threads variable that is the number of threads that I want to run at the same time. What is the best way to handle those multiple threads?</p>
<p>For ex: supposed num_threads = 5 , my goal is to start 5 threads then grab the first 5 website in the list and scrape them, then if thread #3 finishes, it will grab the 6th website from the list to scrape immidiately, not wait until other threads end.</p>
<p>Any recommendation for how to handle it? Thank you</p>
</div>
<div class="post-text" itemprop="text">
<p>It depends.</p>
<p>If your code is spending most of its time waiting for network operations (likely, in a web scraping application), threading is appropriate.  The best way to implement a thread pool is to use <code>concurrent.futures</code> in 3.4.  Failing that, you can create a <code>threading.Queue</code> object and write each thread as an infinite loop that consumes work objects from the queue and processes them.</p>
<p>If your code is spending most of its time processing data after you've downloaded it, threading is useless due to the GIL.  <code>concurrent.futures</code> provides support for process concurrency, but again only works in 3.4+.  For older Pythons, use <code>multiprocessing</code>.  It provides a <code>Pool</code> type which simplifies the process of creating a process pool.</p>
<p>You should profile your code (using <code>cProfile</code>) to determine which of those two scenarios you are experiencing.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you're using Python 3, have a look at <code>concurrent.futures.ThreadPoolExecutor</code></p>
<p>Example pulled from the docs <a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example" rel="nofollow">ThreadPoolExecutor Example</a>:</p>
<pre><code>import concurrent.futures
import urllib.request

URLS = ['http://www.foxnews.com/',
        'http://www.cnn.com/',
        'http://europe.wsj.com/',
        'http://www.bbc.co.uk/',
        'http://some-made-up-domain.com/']

# Retrieve a single page and report the url and contents
def load_url(url, timeout):
    conn = urllib.request.urlopen(url, timeout=timeout)
    return conn.readall()

# We can use a with statement to ensure threads are cleaned up promptly
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # Start the load operations and mark each future with its URL
    future_to_url = {executor.submit(load_url, url, 60): url for url in URLS}
    for future in concurrent.futures.as_completed(future_to_url):
        url = future_to_url[future]
        try:
            data = future.result()
        except Exception as exc:
            print('%r generated an exception: %s' % (url, exc))
        else:
            print('%r page is %d bytes' % (url, len(data)))
</code></pre>
<hr/>
<p>If you're using Python 2, there is a backport available:</p>
<p><a href="http://pythonhosted.org//futures/#threadpoolexecutor-example" rel="nofollow">ThreadPoolExecutor Example</a>:</p>
<pre><code>from concurrent import futures
import urllib.request

URLS = ['http://www.foxnews.com/',
        'http://www.cnn.com/',
        'http://europe.wsj.com/',
        'http://www.bbc.co.uk/',
        'http://some-made-up-domain.com/']

def load_url(url, timeout):
    return urllib.request.urlopen(url, timeout=timeout).read()

with futures.ThreadPoolExecutor(max_workers=5) as executor:
    future_to_url = dict((executor.submit(load_url, url, 60), url)
                         for url in URLS)

    for future in futures.as_completed(future_to_url):
        url = future_to_url[future]
        if future.exception() is not None:
            print('%r generated an exception: %s' % (url,
                                                     future.exception()))
        else:
            print('%r page is %d bytes' % (url, len(future.result())))
</code></pre>
</div>
<span class="comment-copy">Perhaps this can help? <a href="http://stackoverflow.com/questions/2846653/python-multithreading-for-dummies" title="python multithreading for dummies">stackoverflow.com/questions/2846653/â€¦</a> But I'm not sure you really need threading in your case? Just some kind of queue?</span>
<span class="comment-copy">Have you considered using <a href="http://scrapy.org/" rel="nofollow noreferrer">Scrapy</a>.</span>
<span class="comment-copy">@trainoasis yes it's like queue, 5 threads will pull data from that queue to execute them</span>
<span class="comment-copy">thanks, i spend most of the time on my program to scrape, the data will be save to a text file and will be used by another script later</span>
<span class="comment-copy">thanks, i'm using 2.7, is that lib applicable for 2.7 also?</span>
<span class="comment-copy">There is a backport available, I edited my answer to include it</span>
