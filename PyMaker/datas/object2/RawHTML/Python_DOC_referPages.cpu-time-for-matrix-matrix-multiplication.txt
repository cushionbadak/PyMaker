<div class="post-text" itemprop="text">
<p>I am trying to decide wether several similar but independent problems should be dealt with simultaneously or sequentially (possibly in parallel on different computers). In order to decide, I need to compare the cpu times of the following operations : </p>
<ul>
<li><p>time_1 is the time for computing X(with shape (n,p)) @ b (with shape (p,1)).</p></li>
<li><p>time_k is the time for computing X(with shape (n,p)) @ B (with shape (p,k)).</p></li>
</ul>
<p>where X, b and B are random matrices. The difference between the two operations is the width of the second matrix. </p>
<p>Naively, we expect that time_k = k x time_1. With faster matrix multiplication algorithms (Strassen algorithm, Coppersmithâ€“Winograd algorithm), time_k could be smaller than k x time_1 but the complexity of these algorithms remains much larger than what I observed in practice. Therefore my question is :
How to explain the large difference in terms of cpu times for these two computations ? </p>
<p><a href="https://i.stack.imgur.com/gFqtp.png" rel="nofollow noreferrer"><img alt="Comparison of the CPU times in terms of the width k" src="https://i.stack.imgur.com/gFqtp.png"/></a></p>
<p>The code I used is the following : </p>
<pre><code>import time
import numpy as np
import matplotlib.pyplot as plt

p     = 100
width = np.concatenate([np.arange(1, 20), np.arange(20, 100, 10), np.arange(100, 4000, 100)]).astype(int)

mean_time = []
for nk, kk in enumerate(width):
    timings = []
    nb_tests = 10000 if kk &lt;= 300 else 100
    for ni, ii in enumerate(range(nb_tests)):
        print('\r[', nk, '/', len(width), ', ',  ni, '/', nb_tests, ']', end = '')
        x     = np.random.randn(p).reshape((1, -1))
        coef  = np.random.randn(p, kk)
        d     = np.zeros((1, kk))
        start = time.time()
        d[:]  = x @ coef
        end   = time.time()
        timings.append(end - start)

    mean_time.append(np.mean(timings))

mean_time = np.array(mean_time)


fig, ax = plt.subplots(figsize =(14,8))
plt.plot(width, mean_time, label =  'mean(time\_k)')
plt.plot(width, width*mean_time[0], label = 'k*mean(time\_1)')
plt.legend()
plt.xlabel('k')
plt.ylabel('time (sec)')
plt.show()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This detail of the reason is very complex. You know that when PC run the <code>X @ b</code>, it will execute many other required instructions, maybe <code>load data from RAM to cache</code> and so on. In other words, the cost time contains two parts - the 'real calculate instructions' in CPU represented by <code>Cost_A</code> and 'other required instructions' represented by <code>Cost_B</code>. I have a idea, just my guess, that it's the <code>Cost_B</code> lead to <code>time_k &lt;&lt; k x time_1</code>.</p>
<p>For the shape of b is small (eg 1000 x 1), the 'other required instructions' cost relatively the most time. For the shape of b is huge (eg 1000 x 10000), it's relatively small. The following group of experiments could give a less rigorous proof. We can see that when the shape of b increases from (1000 x 1) to (1000 x ) the cost time increases very slowly.</p>
<pre><code>import numpy as np
import time

X = np.random.random((1000, 1000))

b = np.random.random((1000, 1))
b3 = np.random.random((1000, 3))
b5 = np.random.random((1000, 5))
b7 = np.random.random((1000, 7))
b9 = np.random.random((1000, 9))
b10 = np.random.random((1000, 10))
b30 = np.random.random((1000, 30))
b60 = np.random.random((1000, 60))
b100 = np.random.random((1000, 100))
b1000 = np.random.random((1000, 1000))

def test_cost(X, b):
    begin = time.time()
    for i in range(100):
        _ = X @ b
    end = time.time()
    print((end-begin)/100.)

test_cost(X, b)
test_cost(X, b3)
test_cost(X, b5)
test_cost(X, b7)
test_cost(X, b9)
test_cost(X, b10)
test_cost(X, b30)
test_cost(X, b60) 
test_cost(X, b100)
test_cost(X, b1000)

output:
0.0003210139274597168
0.00040063619613647463
0.0002452659606933594
0.00026523590087890625
0.0002449488639831543
0.00024344682693481446
0.00040068864822387693
0.000691361427307129
0.0011700797080993653
0.009680757522583008
</code></pre>
<p>For more, I do a set of experiments with <code>pref</code> in linux. For the <code>pref</code>, the <code>Cost_B</code> maybe more big. I have 8 python files, the first one is as follows.</p>
<pre><code>import numpy as np
import time
def broken2():
    mtx = np.random.random((1, 1000))
    c = None
    c = mtx ** 2

broken2()
</code></pre>
<p>I had process the output to table A, as follows.
<a href="https://i.stack.imgur.com/aA6dS.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/aA6dS.png"/></a></p>
<p>I do a simple analysis that I divide the error of the number of operation  (likes, cache-misses) in neighbor experiments by the error of <code>time elapsed(seconds)</code> . Then, I get the following table B. From the table, we can find that as the shape of b increasing the linear relation between of shape and cost time is more obvious. And maybe the main reason that lead to <code>time_k &lt;&lt; k x time_1</code> is <code>cache misses</code>(load data from RAM to cache), <strong>for it stabilized firstly</strong>. </p>
<p><a href="https://i.stack.imgur.com/xcekK.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/xcekK.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>You aren't only timing multiplication operation. <code>time.time()</code> takes time to complete.</p>
<pre><code>&gt;&gt;&gt; print(time.time() - time.time())
-9.53674316406e-07
</code></pre>
<p>When multiplied by the number of tries (10000) then the number of instances it becomes significant overhead, for n=100 you are in fact comparing what is 1.000.000 calls to <code>time.time()</code> to 100 regular numpy array multiplications.</p>
<p>For quick benchmarking, Python provides a dedicated module that doesn't have this problem : see <a href="https://docs.python.org/2/library/timeit.html" rel="nofollow noreferrer">timeit</a></p>
</div>
<span class="comment-copy">What is your mean? Show we what you confused.</span>
<span class="comment-copy">I agree that timeit is a better tool for timing but I do not believe that this was the cause for the differences, in particular because I computed the averaged times over 'nb_tests' trials so the offsets are not summed.  I tried to use timeit and replaced the inner loop with the following :  <code>mean_time.append(timeit.timeit(stmt   = 'd[:]  = x @ coef', setup  = 'import numpy as np; p    = 100; x    = np.random.randn(p).reshape((1, -1)); coef = np.random.randn(p, {width_k});  d    = np.zeros((1, {width_k}));'.format(width_k = kk),))</code>.  The observations remain the same.</span>
<span class="comment-copy">By removing the inner loop and adding a number argument to timeit instead, I obtained a 60:1 ratio between the orange curve and the blue one at the 4000 end which sound reasonable. After all, it <i>is</i> slower to multiply bit by bit.</span>
