<div class="post-text" itemprop="text">
<p>This is how i applied dictionary for stemming. My dictionary (d) is imported and it's in this format now <code>d={'nada.*':'nadas', 'mila.*':'milas'}</code>
I wrote this code to stemm tokens, but it runs TOO SLOW, so i stopped it before it finished. I guess it's problem because dict is large, and there is large number of tokens. 
So, how can i implement my stem dictionary, so that code can run normaly?
I tried to find a method in nltk package to apply custom dict, but i didn't find it.</p>
<pre><code>#import stem dict
d = {}
with open("Stem rečnik.txt") as f:
    for line in f:
       key, val = line.split(":")
       d[key.replace("\n","")] = val.replace("\n","")

#define tokenizer
def custom_tokenizer(text):
    #split- space
    tokens = nltk.tokenize.word_tokenize(text)
    #stemmer
    for i, token in enumerate(tokens):
        for key, val in d.items():
            if re.match(key, token):
                tokens[i] = val
                break
    return tokens 
</code></pre>
<p>Dictionary sample:</p>
<pre><code>bank.{1}$:banka
intes.{1}$:intesa
intes.{1}$:intesa
intez.{1}$:intesa
intezin.*:intesa
banke:banka
banaka:banka
bankama:banka
</code></pre>
<p>post_text sample:</p>
<pre><code>post_text = [
    'Banca intesa #nocnamora',
    'Banca intesa',
    'banka haosa i neorganizovanosti!',
    'Cucanje u banci umesto setnje posle rucka.',
    "Lovin' it #intesa'"
]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Notice that while the keys in your stem dict are regexes, they all start with a short string of some specific characters.  Let's say the minimum length of specific characters is 3.  Then, construct a dict like this:</p>
<pre><code>'ban' : [('bank.$', 'banka'),
         ('banke', 'banka'),
         ('banaka', 'banka'),
         ('bankama', 'banka'),
        ],
'int' : [('inte[sz].$', 'intesa'),
         ('intezin.*', 'intesa'),
        ],
</code></pre>
<p>Of course, you should <code>re.compile()</code> all those patterns at the beginning.</p>
<p>Then you can do a cheaper, three-character lookup in this dict:</p>
<pre><code>def custom_tokenizer(text):
    tokens = nltk.tokenize.word_tokenize(text)
    for i, token in enumerate(tokens):
        for key, val in d.get(token[:3], []):
            if re.match(key, token):
                tokens[i] = val
                break
    return tokens
</code></pre>
<p>Now instead of checking all 500 stems, you only need to check the few that start with the right prefix.</p>
</div>
<span class="comment-copy">Please provide <code>d</code> to make this a <a href="https://stackoverflow.com/help/mcve">Minimal, Complete, and Verifiable example</a>. Why use <code>re.match</code> instead of just string matching? Or even better: check whether <code>token  in d</code>—searches in O(1) time instead of O(n).</span>
<span class="comment-copy">You provided code that defines <code>d</code> for you.  But it reads a file that we can't see.  You need to either forgo the file reference and provide a direct definition of a dictionary, or provide the contents of that file.  Also, you should provide a sample of <code>text</code> that you are passing to  the function <code>custom_tokenizer</code>.  Think of a <a href="http://stackoverflow.com/help/mcve"><b>MCVE</b></a> this way, I need to be able to copy and paste the code you've provided into my system and it needs to run.  If you have any reference to information that I can't see, it won't run.</span>
<span class="comment-copy">Are all the regex patterns like that, ie. the stem followed by zero or more characters? (<code>stem.*</code>)</span>
<span class="comment-copy">@hope94 <code>d</code> is a dict, not a list of pairs.</span>
<span class="comment-copy">The only obvious optimization I can see is to pre-<a href="https://docs.python.org/3/library/re.html#re.compile" rel="nofollow noreferrer">compile</a> all your regexes: <code>d[re.compile(key)] = val.replace("\n","")</code></span>
