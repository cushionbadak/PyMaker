<div class="post-text" itemprop="text">
<p>I am reading from a huge file (232MB) line by line.
First, i recognize each line according to a Regular Expression.
Then for each line, I am writing to different city.txt files under the 'report' directory according to a cityname in each line. However, this process takes a while. I am wondering if there is anyway of speeding up the process?</p>
<p>Example of input file: (each column split by a \t)</p>
<p>2015-02-03  19:20   Sane Diebgo Music   692.08  Cash</p>
<p>Actually i have tested the code with writing to different files and not writing to different file(simply process the large file and come up with 2 dicts) the time difference is huge. 80% of the time is spent writing to different files</p>
<pre><code>def processFile(file):

    pattern = re.compile(r"(\d{4}-\d{2}-\d{2})\t(\d{2}:\d{2})\t(.+)\t(.+)\t(\d+\.\d+|\d+)\t(\w+)\n")

    f = open(file)

    total_sale = 0

    city_dict = dict()

    categories_dict = dict()

    os.makedirs("report", exist_ok = True)

    for line in f:
        valid_entry = pattern.search(line)

        if valid_entry == None:
            print("Invalid entry: '{}'".format(line.strip()))
            continue

        else:               
            entry_sale = float(valid_entry.group(5))

            total_sale += entry_sale

            city_dict.update({valid_entry.group(3) : city_dict.get(valid_entry.group(3), 0) + entry_sale})

            categories_dict.update({valid_entry.group(4) : categories_dict.get(valid_entry.group(4), 0) + entry_sale})


            filename = "report/" + valid_entry.group(3) + ".txt"
            if os.path.exists(filename):
                city_file = open(filename, "a")
                city_file.write(valid_entry.group(0))
                city_file.close()
            else:
                city_file = open(filename, "w")
                city_file.write(valid_entry.group(0))
                city_file.close()

    f.close()
    return (city_dict, categories_dict, total_sale)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The dictionary lookups and updates could be improved by using <code>defaultdict</code>:</p>
<pre><code>from collections import defaultdict

city_dict = defaultdict(float)
categories_dict = defaultdict(float)

...

city = valid_entry.group(3)
category = valid_entry.group(4)

...

city_dict[city] += entry_sale
category_dict[category] += entry_sale
</code></pre>
</div>
<span class="comment-copy">I'd venture a guess that you're bottleneck is the regex. But you really should done benchmarks with something like cprofile to see for yourself: <a href="https://docs.python.org/3/library/profile.html" rel="nofollow noreferrer">docs.python.org/3/library/profile.html</a></span>
<span class="comment-copy">I agree with @BaileyParker. Profile it to confirm, but regex often tanks performance. If that proves to be the case, you may be able to write your own dead-basic string parser (since you are needing it for a specific use case.)</span>
<span class="comment-copy">Why do you have to open and close the file in the loop? Use a <code>dict</code> to save opened files and close them after the loop.</span>
<span class="comment-copy">Can you post a few sample lines of your file? I believe the performance may be significantly improved using another regex implementation, which does not use backtracking, but I would need a few tests in order to do so.</span>
<span class="comment-copy">@AlexanderHuszagh 2012-01-01	09:00	San Jose	Men's Clothing	214.05	Amex 2012-01-01	09:00	San Diego	Music	66.08	Cash 2012-01-01	09:00	Pittsburgh	Pet Supplies	493.51	Discover</span>
<span class="comment-copy">I doubt this will have a major speed-up, since most regex implementations pre-extract groups and then merely return copies (if immutable strings) or references (mutable strings) to the capture group. In this case, CPython has marks that store the appropriate indexes in the stored string, and return a copy of the string from those bounds. The time should be only a little bit longer than copying a string.  This is how CPython does it: <a href="https://github.com/python/cpython/blob/master/Modules/_sre.c#L1991" rel="nofollow noreferrer">github.com/python/cpython/blob/master/Modules/_sre.c#L1991</a> <a href="https://github.com/python/cpython/blob/master/Modules/_sre.c#L1965" rel="nofollow noreferrer">github.com/python/cpython/blob/master/Modules/_sre.c#L1965</a></span>
<span class="comment-copy">Agreed that it will have a likely negligible effect beyond the method call overhead. The replacement of a <code>dict.get</code> and <code>dict.update</code> with the <code>defaultdict</code> implementation was the more salient portion of my answer.</span>
<span class="comment-copy">@AlexanderHuszagh I found (using <code>timeit</code>) that a loop using <code>defaultdict</code> is approximately 3-4x more performant than a loop using <code>.update</code> and <code>.get</code> to accomplish the same outcome over a large dataset. I removed my note about regex groups.</span>
<span class="comment-copy">I never downvoted you, but now that you've corrected the answer, that's worth at least a +1 for the major speedup.</span>
<span class="comment-copy">I have tested with defaultdict. the speed up is kind of negligible. The major processing time is spent on writing to different files</span>
