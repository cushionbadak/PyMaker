<div class="post-text" itemprop="text">
<p>I have to run python in a resource constrained environment with only a few GB of virtual memory. Worse yet, I have to fork children from my main process as part of application design, all of which receive a copy-on-write allocation of this same amount of virtual memory on fork. The result is that after forking only 1 - 2 children, the process group hits the ceiling and shuts everything down. Finally, I am not able to remove numpy as a dependency; it is a strict requirement.</p>
<p>Any advice on how I can bring this initial memory allocation down?  </p>
<p>e.g.  </p>
<ol>
<li>Change the default amount allocated to numpy on import?</li>
<li>Disable the feature and force python / numpy to allocate more dynamically?  </li>
</ol>
<p><br/>
Details:</p>
<p>Red Hat Enterprise Linux Server release 6.9 (Santiago)<br/>
Python 3.6.2<br/>
numpy&gt;=1.13.3</p>
<p>Bare Interpreter:</p>
<pre><code>import os
os.system('cat "/proc/{}/status"'.format(os.getpid()))

# ... VmRSS: 7300 kB
# ... VmData: 4348 kB
# ... VmSize: 129160 kB

import numpy
os.system('cat "/proc/{}/status"'.format(os.getpid()))

# ... VmRSS: 21020 kB
# ... VmData: 1003220 kB
# ... VmSize: 1247088 kB  
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Thank you, skullgoblet1089, for raising questions on SO and at <a href="https://github.com/numpy/numpy/issues/10455" rel="nofollow noreferrer">https://github.com/numpy/numpy/issues/10455</a> , and for answering.
Citing your 2018-01-24 post:</p>
<p>Reducing threads with <code>export OMP_NUM_THREADS=4</code> will bring down VM allocation.</p>
</div>
<span class="comment-copy">I don't notice such drastic jumps with my system, but it does shoot up by 200MB when I import the module</span>
<span class="comment-copy">May I ask how you do the forking? Are you calling <code>os.fork</code> or are you using <code>multiprocessing</code>?</span>
<span class="comment-copy">multiprocessing via concurrent.futures.ProcessPoolExecutor</span>
<span class="comment-copy">I suggest you switch to using python's multiprocessing module, unless you are bound to using the concurrent module. Switching allows you to <a href="https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods" rel="nofollow noreferrer"><code>spawn processes rather than forking them</code></a>. Also, dropping a <a href="https://github.com/numpy/numpy/issues" rel="nofollow noreferrer">ticket</a> in the numpy repo to let them know, and maybe they could provide some guidance. It could be that numpy does not behave with multiprocessing</span>
<span class="comment-copy">The behavior on fork seems reasonable to me. My primary concern is why on import numpy has so much memory allocated to the process by default. I think this feature of numpy would make it so that spawning would have the same effect, no? Your advice to raise ticket is good idea I will do that.</span>
