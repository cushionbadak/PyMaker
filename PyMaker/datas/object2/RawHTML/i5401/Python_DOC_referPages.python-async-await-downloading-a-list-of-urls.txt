<div class="post-text" itemprop="text">
<p>I'm trying to download over 30,000 files from a FTP server, and after some googling using asynchronous IO seemed a good idea. However, the code below failed to download any files and returns a Timeout Error. I'd really appreciate any help! Thanks!</p>
<pre><code>class pdb:
    def __init__(self):
        self.ids = []
        self.dl_id = []
        self.err_id = []


    async def download_file(self, session, url):
        try:
            with async_timeout.timeout(10):
                async with session.get(url) as remotefile:
                    if remotefile.status == 200:
                        data = await remotefile.read()
                        return {"error": "", "data": data}
                    else:
                        return {"error": remotefile.status, "data": ""}
        except Exception as e:
            return {"error": e, "data": ""}

    async def unzip(self, session, work_queue):
        while not work_queue.empty():
            queue_url = await work_queue.get()
            print(queue_url)
            data = await self.download_file(session, queue_url)
            id = queue_url[-11:-7]
            ID = id.upper()
            if not data["error"]:
                saved_pdb = os.path.join("./pdb", ID, f'{ID}.pdb')
                if ID not in self.dl_id:
                    self.dl_id.append(ID)
                with open(f"{id}.ent.gz", 'wb') as f:
                    f.write(data["data"].read())
                with gzip.open(f"{id}.ent.gz", "rb") as inFile, open(saved_pdb, "wb") as outFile:
                    shutil.copyfileobj(inFile, outFile)
                os.remove(f"{id}.ent.gz")
            else:
                self.err_id.append(ID)

    def download_queue(self, urls):
        loop = asyncio.get_event_loop()
        q = asyncio.Queue(loop=loop)
        [q.put_nowait(url) for url in urls]
        con = aiohttp.TCPConnector(limit=10)
        with aiohttp.ClientSession(loop=loop, connector=con) as session:
            tasks = [asyncio.ensure_future(self.unzip(session, q)) for _ in range(len(urls))]
            loop.run_until_complete(asyncio.gather(*tasks))
        loop.close()
</code></pre>
<p>Error message if I remove the <code>try</code> part:  </p>
<blockquote>
<p>Traceback (most recent call last):<br/>
  File "test.py", line 111, in <br/>
      x.download_queue(urls)<br/>
  File "test.py", line 99, in download_queue<br/>
      loop.run_until_complete(asyncio.gather(*tasks))<br/>
  File "/home/yz/miniconda3/lib/python3.6/asyncio/base_events.py", line 467, in run_until_complete<br/>
      return future.result()<br/>
  File "test.py", line 73, in unzip<br/>
      data = await self.download_file(session, queue_url)<br/>
  File "test.py", line 65, in download_file<br/>
      return {"error": remotefile.status, "data": ""}<br/>
  File "/home/yz/miniconda3/lib/python3.6/site- packages/async_timeout/<strong>init</strong>.py", line 46, in <strong>exit</strong><br/>
      raise asyncio.TimeoutError from None<br/>
  concurrent.futures._base.TimeoutError  </p>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<pre><code>tasks = [asyncio.ensure_future(self.unzip(session, q)) for _ in range(len(urls))]
loop.run_until_complete(asyncio.gather(*tasks))
</code></pre>
<p>Here you start process of downloading concurrently for all of your urls. It means that you start to count timeout for all of them also. Once it's a big number such as 30,000 it can't be physically done within 10 seconds due to networks/ram/cpu capacity.</p>
<p>To avoid this situation you should guarantee limit of coroutines started simultaneously. Usually synchronization primitives like <a href="https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore" rel="noreferrer">asyncio.Semaphore</a> can be used to achieve this.</p>
<p>It'll look like this:</p>
<pre><code>sem = asyncio.Semaphore(10)

# ...

async def download_file(self, session, url):
    try:
        async with sem:  # Don't start next download until 10 other currently running
            with async_timeout.timeout(10):
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>As an alternative to @MikhailGerasimov's semaphore approach, you might consider using the <a href="http://aiostream.readthedocs.io/en/latest/operators.html#aiostream.stream.map" rel="nofollow noreferrer">aiostream.stream.map</a> operator:</p>
<pre><code>from aiostream import stream, pipe

async def main(urls):
    async with aiohttp.ClientSession() as session:
        ws = stream.repeat(session)
        xs = stream.zip(ws, stream.iterate(urls))
        ys = stream.starmap(xs, fetch, ordered=False, task_limit=10)
        zs = stream.map(ys, process)
        await zs
</code></pre>
<p>Here's an equivalent implementation using pipes:</p>
<pre><code>async def main3(urls):
    async with aiohttp.ClientSession() as session:
        await (stream.repeat(session)
               | pipe.zip(stream.iterate(urls))
               | pipe.starmap(fetch, ordered=False, task_limit=10)
               | pipe.map(process))
</code></pre>
<p>You can test it with the following coroutines:</p>
<pre><code>async def fetch(session, url):
    await asyncio.sleep(random.random())
    return url

async def process(data):
    print(data)
</code></pre>
<p>See more aiostream examples in this <a href="https://github.com/vxgmichel/aiostream#demonstration" rel="nofollow noreferrer">demonstration</a> and the <a href="http://aiostream.readthedocs.io/en/latest/examples.html" rel="nofollow noreferrer">documentation</a>.</p>
<p><sub>Disclaimer: I am the project maintainer.</sub></p>
</div>
<span class="comment-copy">No error message? Anything diagnose you did already?</span>
<span class="comment-copy">maybe because you have timeout set to 10 seconds: async_timeout.timeout(10)</span>
<span class="comment-copy">@KlausD. Sorry for not including that part. Just edited.</span>
<span class="comment-copy">@Matej Yeah but if I remove that line, the code just freezes, so I guess there's still something wrong elsewhere.</span>
<span class="comment-copy">You may just increate the value - 10 seconds may not be enough for a file to download</span>
<span class="comment-copy">I figured out what the problem was... I guess aiohttp can't handle ftp urls. Thanks for the advice though!</span>
