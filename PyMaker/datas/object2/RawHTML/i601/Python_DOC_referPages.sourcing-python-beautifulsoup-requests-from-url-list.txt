<div class="post-text" itemprop="text">
<p>I am planning to use a url list to scrape several pages consecutively, using the code below. </p>
<p>Is there a smart way to replace the manually inserted terms for "desired_google_queries" through a reference to an extensive url list (which could be a CSV or Excel file)?</p>
<pre><code>from bs4 import BeautifulSoup
import urllib.request
import csv

desired_google_queries = ['Word' , 'lifdsst', 'yvou', 'should', 'load']

for query in desired_google_queries:

    url = 'http://google.com/search?q=' + query

    req = urllib.request.Request(url, headers={'User-Agent' : "Magic Browser"})
    response = urllib.request.urlopen( req )
    html = response.read()

    soup = BeautifulSoup(html, 'html.parser')

    resultStats = soup.find(id="resultStats").string
    print(resultStats)

with open('queries.csv', 'w', newline='') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter=' ',
              quotechar='|', quoting=csv.QUOTE_MINIMAL)
      spamwriter.writerow(['query', 'resultStats'])
      for query in desired_google_queries:
      ...
      spamwriter.writerow([query, resultStats])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can put your scraping logic into a function, and then call it on each of the <code>query</code>'s you read from your <code>.csv</code> file.</p>
<pre><code>from bs4 import BeautifulSoup
import urllib.request
import csv


def scrape_site(query):
    url = 'http://google.com/search?q=' + query

    req = urllib.request.Request(url, headers={'User-Agent' : "Magic Browser"})
    response = urllib.request.urlopen( req )
    html = response.read()

    soup = BeautifulSoup(html, 'html.parser')

    resultStats = soup.find(id="resultStats").string
    return resultStats

##################################################### 
# Read in queries from .csv to desired_google_queries

with open('queries.csv', 'w', newline='') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter=' ',
              quotechar='|', quoting=csv.QUOTE_MINIMAL)
    spamwriter.writerow(['query', 'resultStats'])

    for query in desired_google_queries:
       resultStats = scrape_site(query)
       spamwriter.writerow([query, resultStats])
</code></pre>
</div>
<span class="comment-copy">There seems to be an issue with the line "for query in desired_google_queries:"</span>
<span class="comment-copy">Which reference would you take instead of "desired_google_queries" for the loop in that line, given that the old reference scheme has been amended?</span>
<span class="comment-copy">See my edit for where you could add in logic to read a file with your desired google queries. Depending on what type of file format you would have to write different logic to do so. If you have a <code>.txt</code> file with one query on each line you can use <code>open</code> as well -&gt; <a href="https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files" rel="nofollow noreferrer">docs.python.org/3/tutorial/â€¦</a></span>
<span class="comment-copy">Ok so you would once again read a txt file called 'desired_google_queries' that contains the respective queries? The queries have been defined through 'scrape_site' before though, so the query list would exist twice (if I understood you correctly)...</span>
