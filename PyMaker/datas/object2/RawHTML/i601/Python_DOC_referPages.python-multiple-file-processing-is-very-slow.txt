<div class="post-text" itemprop="text">
<p>I have to read through 2 different types of files at the same time in order to synchronise their data. The files are generated in parallel with different frequencies.</p>
<p>File 1, which will be very big in size (&gt;10 GB) has the structure as follows : <strong>DATA</strong> is a field containing 100 characters and the number that follows it is a synchronisation signal that is <em>common for both files</em> (i.e. they change at the same time in both files).</p>
<pre><code>DATA 1
DATA 1
... another 4000 lines
DATA 1
DATA 0
... another 4000 lines and so on
</code></pre>
<p>File 2, small in size (at most 10 MB but more in number) has the same structure the difference being in the number of rows between the synchronisation signal change:</p>
<pre><code>DATA 1
... another 300-400 lines
DATA 1
DATA 0
... and so on
</code></pre>
<p>Here is the code that I use to read the files:</p>
<pre><code>def getSynchedChunk(fileHandler, lastSynch, end_of_file):

    line_vector = [];                         # initialize output array
    for line in fileHandler:                  # iterate over the file
        synch = int(line.split(';')[9]);      # get synch signal
        line_vector.append(line);         
        if synch != lastSynch:                # if a transition is detected
            lastSynch = synch;                # update the lastSynch variable for later use
            return (lastSynch, line_vector, True); # and exit - True = sycnh changed

     return (lastSynch, line_vector, False); # exit if end of file is reached
</code></pre>
<p>I have to synchronise the data chunks (the lines that have the same synch signal value) and write the new lines to another file.
I am using <strong>Spyder</strong>.</p>
<p>For testing, I used smaller sized files, 350 MB for FILE 1 and 35 MB for FILE 2.
I also used the built-in Profiler to see where is the most time spent and it seems that 28s out of 46s is spent in actually reading the data from the files. The rest is used in synchronising the data and writing to the new file.</p>
<p>If I scale the time up to files sized in gigs, it will take hours to finish the processing. I will try to change the way I do the processing to make it faster, but is there a faster way to read through big files?</p>
<hr/>
<p>One line of data looks like this :</p>
<pre><code>01/31/19 08:20:55.886;0.049107050;-0.158385641;9.457415342;-0.025256720;-0.017626805;-0.000096349;0.107;-0.112;0
</code></pre>
<p>The values are sensor measurements. The last number is the synch value.</p>
</div>
<div class="post-text" itemprop="text">
<p>I recommend reading in the <strong>whole files first</strong> and then do the processing. This has the huge advantage, that all the appending/concatenating etc. while reading is done internally with optimized modules. The synching can be done afterwards.</p>
<p>For this purpose I <strong>strongly recommend</strong> using <a href="https://pandas.pydata.org/" rel="nofollow noreferrer"><code>pandas</code></a>, which is imho by far the best tool to work with timeseries data like measurements.</p>
<p>Importing your files, guessing <code>csv</code> in a text file is the correct format, can be done with:</p>
<pre><code>df = pd.read_csv(
    'DATA.txt', sep=';', header=None, index_col=0, 
    parse_dates=True, infer_datetime_format=True, dayfirst=True)
</code></pre>
<p>To reduce memory consumption, you can either specify a <code>chunksize</code> to split the file reading, or <code>low_memory=True</code> to internally split the file reading process (assuming that the final dataframe fits in your memory):</p>
<pre><code>df = pd.read_csv(
    'DATA.txt', sep=';', header=None, index_col=0, 
    parse_dates=True, infer_datetime_format=True, dayfirst=True,
    low_memory=True)
</code></pre>
<p>Now your data will be stored in a <code>DataFrame</code>, which is perfect for time series. The index is already converted to a DateTimeIndex, which will allow for nice plotting, resampling etc. etc...</p>
<p>The <code>sync</code> state can now be easily accessed like in a numpy array (just adding the <code>iloc</code> accessing method) with:</p>
<pre><code>df.iloc[:, 8]  # for all sync states
df.iloc[0, 8]  # for the first synch state
df.iloc[1, 8]  # for the second synch state
</code></pre>
<p>This is ideal for using <strong>fast vectorized</strong> synching of two or more files.</p>
<hr/>
<p><strong>To read the file depending on the available memory</strong>:</p>
<pre><code>try:
    df = pd.read_csv(
        'DATA.txt', sep=';', header=None, index_col=0, 
        parse_dates=True, infer_datetime_format=True, dayfirst=True)
except MemoryError:
    df = pd.read_csv(
        'DATA.txt', sep=';', header=None, index_col=0, 
        parse_dates=True, infer_datetime_format=True, dayfirst=True,
        low_memory=True)
</code></pre>
<p>This <code>try/except</code> solution might not be an elegant solution since it will take some time before the MemoryError is raised, but it is failsafe. And since <code>low_memory=True</code> will most probably reduce the file reading performance in most cases, the <code>try</code> block should be faster in most cases.</p>
</div>
<div class="post-text" itemprop="text">
<p>I'm not used to Spyder but you can try to use multithreading for chunking the big files, Python has an option for this without any external library so it will probably work with Spyder as well. (<a href="https://docs.python.org/3/library/threading.html" rel="nofollow noreferrer">https://docs.python.org/3/library/threading.html</a>)</p>
<p>The process of chunking:</p>
<ol>
<li>Get the length of the file in lines</li>
<li>Start cutting the list to halfs until its "not too big"</li>
<li>Use a thread for each small chunk.</li>
<li>Profit </li>
</ol>
</div>
<span class="comment-copy">Spyder is an IDE and should in general not influence the outcome of you script. Far more interesting would be information like the file extension of <code>DATA</code> and an excerpt of the first lines of the file.</span>
<span class="comment-copy">According to your code, you are joining the first line where synch changed with the previous lines. Is it the expected behavior ?</span>
<span class="comment-copy">Do the lines of the two files need to be interleaved, or can you just concatenate the chunks?</span>
<span class="comment-copy">@EduardPalkoMate Ok, I extended my solution. Does it work?</span>
<span class="comment-copy">You are welcome! If it is working without <code>low_memory=True</code>, you are better off not using it. It will split the reading in chunks which will reduce the memory consumption, <b>but will most probably make it slower</b>. Thus only use <code>low_memory=True</code> if you run into a <code>MemoryError</code>. I'll add a short example in my answer in the next minutes.</span>
<span class="comment-copy">What about memory consumption in this approach? This should work with files bigger than 10GB.</span>
<span class="comment-copy">I was just about to add some information concerning this matter. :)</span>
<span class="comment-copy">I will check it out. Thank you.</span>
<span class="comment-copy">This is way too simplistic. A divide and conquer approach only benefits if processing the chunks can be non-linear. It also doesn't account for the necessity of thread workloads containing data of different logical chunks.</span>
<span class="comment-copy">That's right. It needs to be tested. There is a point when the threading actually will slow down the things. That's why I wrote not too big. So do not try to divide it down to really small pieces.</span>
