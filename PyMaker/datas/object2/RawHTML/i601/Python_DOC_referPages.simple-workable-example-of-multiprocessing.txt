<div class="post-text" itemprop="text">
<p>I am looking for a simple example of python <code>multiprocessing</code>.</p>
<p>I am trying to figure out workable example of python <code>multiprocessing</code>. I have found an example on breaking large numbers into primes. That worked because there was little input (one large number per core) and lot of computing (breaking the numbers into primes). </p>
<p>However, my interest is different - I have lot of input data on which I perform simple calculations. I wonder if there is a simple way to modify the below code so that multicores really beats single core. I am running python 3.6 on Win10 machine with 4 physical cores and 16 GB RAM. </p>
<p>Here comes my sample code.</p>
<pre><code>import numpy as np
import multiprocessing as mp
import timeit

# comment the following line to get version without queue
queue = mp.Queue()
cores_no = 4


def npv_zcb(bnd_info, cores_no):

     bnds_no = len(bnd_info)
     npvs = []

     for bnd_idx in range(bnds_no):

         nom = bnd_info[bnd_idx][0]
         mat = bnd_info[bnd_idx][1]
         yld = bnd_info[bnd_idx][2]

         npvs.append(nom / ((1 + yld) ** mat))

     if cores_no == 1:
         return npvs
     # comment the following two lines to get version without queue
     else:
         queue.put(npvs)

# generate random attributes of zero coupon bonds

print('Generating random zero coupon bonds...')


bnds_no = 100

bnd_info = np.zeros([bnds_no, 3])
bnd_info[:, 0] = np.random.randint(1, 31, size=bnds_no)
bnd_info[:, 1] = np.random.randint(70, 151, size=bnds_no)
bnd_info[:, 2] = np.random.randint(0, 100, size=bnds_no) / 100
bnd_info = bnd_info.tolist()

# single core
print('Running single core...')
start = timeit.default_timer()
npvs = npv_zcb(bnd_info, 1)
print('   elapsed time: ', timeit.default_timer() - start, ' seconds')

# multiprocessing
print('Running multiprocessing...')
print('   ', cores_no, ' core(s)...')
start = timeit.default_timer()

processes = []

idx = list(range(0, bnds_no, int(bnds_no / cores_no)))
idx.append(bnds_no + 1)

for core_idx in range(cores_no):
     input_data = bnd_info[idx[core_idx]: idx[core_idx + 1]]

     process = mp.Process(target=npv_zcb,
                          args=(input_data, cores_no))
     processes.append(process)
     process.start()

for process_aux in processes:
     process_aux.join()

# comment the following three lines to get version without queue
mylist = []
while not queue.empty():
     mylist.append(queue.get())

print('   elapsed time: ', timeit.default_timer() - start, ' seconds')
</code></pre>
<p>I would be very grateful if anyone could advice me how to modify the code so that multiple core run beats single core run. I have also noticed that increasing variable <code>bnds_no</code> to 1,000 leads to <code>BrokenPipeError</code>. One would expect that increasing amount of input would lead to longer computational time rather than an error... What is wrong here?</p>
</div>
<div class="post-text" itemprop="text">
<p>The <code>BrokenPipeError</code> is not due to larger input but it is due to race condition which occurres due to the use of <code>queue.empty()</code> and <code>queue.get()</code> in separate steps.</p>
<p>You don't see it with smaller inputs for most the times is because the queue items get processed pretty fast and race condition does not occur but with larger datasets the chances of race condition increases.</p>
<p>Even with smaller inputs try running your script multiple times, maybe 10 15 times and you will see <code>BrokenPipeError</code> occurring. </p>
<p>One solution to this is to pass a sentinel value to the queue which you can use to test if all the data in the queue has been processed.</p>
<p>Try modifying your code to something like this</p>
<pre><code>q = mp.Queue()
 &lt;put the data in the queue&gt;
 q.put(None)


while True:
    data = q.get()
    if data is not None:
        &lt;process the data here &gt;
    else:
        q.put(None)
        return
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This doesn't directly answer your question but if you were using RxPy for reactive Python programming you could check out their small example on multiprocessing: <a href="https://github.com/ReactiveX/RxPY/tree/release/v1.6.x#concurrency" rel="nofollow noreferrer">https://github.com/ReactiveX/RxPY/tree/release/v1.6.x#concurrency</a></p>
<p>Seems a bit easier to manage concurrency with ReactiveX/RxPy than trying to do it manually.</p>
</div>
<div class="post-text" itemprop="text">
<p>OK, so I removed queue related parts from the code to see if get rid of the <code>BrokenPipeError</code> (above I updated the original code indicating what should be commented out). Unfortunately it did not help.</p>
<p>I tested the code on my personal PC with Linux (Ubuntu 18.10, python 3.6.7). Quite surprisingly the code behaves differently on the two systems. On Linux the version without queue runs without problems; the version with queue runs forever. On Windows there is no difference - I always end up with <code>BrokenPipeError</code>.</p>
<p>PS: In some other post (<a href="https://stackoverflow.com/questions/48078722/no-multiprocessing-print-outputs-spyder">No multiprocessing print outputs (Spyder)</a>) I found that there might be some problem with multiprocessing when using Spyder editor. I experienced exactly the same problem on Windows machine. So, not all examples in official documentation work as expected...</p>
</div>
<div class="post-text" itemprop="text">
<p>This doesn't answer your question—I'm only posting it to illustrate what I was said in comments about when multiprocessing might be able to speed processing up.</p>
<p>In the code below which is based on yours, I've added a <code>REPEAT</code> constant that makes the <code>npv_zcb()</code> do its computations over again that many times to simulate it using the CPU more. Changing this constant's value generally slows-down or speeds-up the single core processing much more than it does the multiprocessing part — in-fact it hardly affects the latter at all.  </p>
<pre><code>import numpy as np
import multiprocessing as mp
import timeit


np.random.seed(42)  # Generate same set of random numbers for testing.

REPEAT = 10  # Number of times to repeat computations performed in npv_zcb.


def npv_zcb(bnd_info, queue):

    npvs = []

    for _ in range(REPEAT):  # To simulate more computations.

        for bnd_idx in range(len(bnd_info)):

            nom = bnd_info[bnd_idx][0]
            mat = bnd_info[bnd_idx][1]
            yld = bnd_info[bnd_idx][2]
            v = nom / ((1 + yld) ** mat)

    npvs.append(v)

    if queue:
        queue.put(npvs)
    else:
        return npvs


if __name__ == '__main__':

    print('Generating random zero coupon bonds...')
    print()

    bnds_no = 100
    cores_no = 4

    # generate random attributes of zero coupon bonds

    bnd_info = np.zeros([bnds_no, 3])
    bnd_info[:, 0] = np.random.randint(1, 31, size=bnds_no)
    bnd_info[:, 1] = np.random.randint(70, 151, size=bnds_no)
    bnd_info[:, 2] = np.random.randint(0, 100, size=bnds_no) / 100
    bnd_info = bnd_info.tolist()

    # single core
    print('Running single core...')
    start = timeit.default_timer()
    npvs = npv_zcb(bnd_info, None)
    print('   elapsed time: {:.6f} seconds'.format(timeit.default_timer() - start))

    # multiprocessing
    print()
    print('Running multiprocessing...')
    print('  ', cores_no, ' core(s)...')
    start = timeit.default_timer()

    queue = mp.Queue()
    processes = []

    idx = list(range(0, bnds_no, int(bnds_no / cores_no)))
    idx.append(bnds_no + 1)

    for core_idx in range(cores_no):
        input_data = bnd_info[idx[core_idx]: idx[core_idx + 1]]

        process = mp.Process(target=npv_zcb, args=(input_data, queue))
        processes.append(process)
        process.start()

    for process in processes:
        process.join()

    mylist = []
    while not queue.empty():
        mylist.extend(queue.get())

    print('   elapsed time: {:.6f} seconds'.format(timeit.default_timer() - start))
</code></pre>
</div>
<span class="comment-copy">On my Windows 7 system with Python 3.7.2 the code currently in your question raises a <code>RuntimeError</code> because it doesn't have an <code>if __name__ == '__main__':</code> to protect its entry point. See the section titled "Safe importing of main module" of the multiprocessing <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming" rel="nofollow noreferrer">Programming guidelines</a>. Fixing this problem isn't an answer to your question, but posting code that actually works might help you get a good one...</span>
<span class="comment-copy">I wouldn't expect <code>multiprocessing</code> to speed things up in this case—borne out by the results I obtained after fixing your code—because, paraphrasing an article I recently read: An improvement due to parallel processing only makes sense if tasks are "CPU-bound" where the majority of the task is spent in the CPU in contrast to I/O bound tasks (i.e. tasks processing data from disk) — which is not true of your <code>npv_zcb()</code> function.</span>
<span class="comment-copy">@martineau Do you mean your comment in general or related to python and its implementation of multiprocessing? Few years ago I managed to implement something similar in C++ and there the speed was scaling with number of cores quite nicely...</span>
<span class="comment-copy">I meant it generally. Multiprocessing <i>can</i> speed things up in any programming language when applied to certain kinds of problems depending on the kind of processing is being done and where most of the time is being spent. In this particular case, the overhead of passing the data between the processes takes a lot more processing than the simple things the <code>npv_zcb()</code> does.</span>
<span class="comment-copy">Thx for suggestion. Unfortunately I am working in a corporate with rigid ICT department. It is a complete sci-fi that our ICT install a new package just for me :(.</span>
<span class="comment-copy">@Macky are they using apt-get to install python packages or just have a whitelist of packages? only suggested it because concurrent coding became way easier for me at least with RxPy (RX.js is pretty popular for JavaScript)</span>
<span class="comment-copy">At work  we are forced to use Win10. I can only install packages that are approved and signed by our ICT. They have very rigid procedures for everything - just for illustration - HDD upgrade was a 6 month "project" for them... I will try to have look anyhow during the weekend. However, even if it worked I have very little chance I will persuade them to approve the package...</span>
