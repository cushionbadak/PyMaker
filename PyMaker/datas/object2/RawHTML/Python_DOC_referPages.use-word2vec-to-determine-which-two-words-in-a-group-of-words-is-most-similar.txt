<div class="post-text" itemprop="text">
<p>I am trying to use the python wrapper around Word2vec. I have a word embedding or group of words which can be seen below and from them I am trying to determine which two words are most similar to each other. </p>
<p>How can I do this?</p>
<p>['architect', 'nurse', 'surgeon', 'grandmother', 'dad']</p>
</div>
<div class="post-text" itemprop="text">
<p>@rylan-feldspar's answer is generally the correct approach and will work, but you could do this a bit more compactly using standard Python libraries/idioms, especially <code>itertools</code>, a list-comprehension, and sorting functions. </p>
<p>For example, first use <code>combinations()</code> from <code>itertools</code> to generate all pairs of your candidate words:</p>
<pre class="lang-py prettyprint-override"><code>from itertools import combinations
candidate_words = ['architect', 'nurse', 'surgeon', 'grandmother', 'dad']
all_pairs = combinations(candidate_words, 2)
</code></pre>
<p>Then, decorate the pairs with their pairwise similarity:</p>
<pre class="lang-py prettyprint-override"><code>scored_pairs = [(w2v_model.wv.similarity(p[0], p[1]), p)
                for p in all_pairs]
</code></pre>
<p>Finally, sort to put the most-similar pair first, and report that score &amp; pair:</p>
<pre class="lang-py prettyprint-override"><code>sorted_pairs = sorted(scored_pairs, reverse=True)
print(sorted_pairs[0])  # first item is most-similar pair
</code></pre>
<p>If you wanted to be compact but a bit less readable, it could be a (long) "1-liner":</p>
<pre class="lang-py prettyprint-override"><code>print(sorted([(w2v_model.wv.similarity(p[0], p[1]), p) 
              for p in combinations(candidate_words, 2)
             ], reverse=True)[0])
</code></pre>
<p><strong>Update:</strong></p>
<p>Integrating @ryan-feldspar's suggestion about <code>max()</code>, and going for minimality, this should also work to report the best pair (but not its score):</p>
<pre class="lang-py prettyprint-override"><code>print(max(combinations(candidate_words, 2),
          key=lambda p:w2v_model.wv.similarity(p[0], p[1])))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Given you're using gensim's word2vec, according to your comment:</p>
<p>Load up or train the model for your embeddings and then, on your model, you can call: </p>
<pre><code>min_distance = float('inf')
min_pair = None
word2vec_model_wv = model.wv  # Unsure if this can be done in the loop, but just to be safe efficiency-wise
for candidate_word1 in words:
    for candidate_word2 in words:
        if candidate_word1 == candidate_word2:
            continue  # ignore when the two words are the same

        distance = word2vec_model_wv.distance(candidate_word1, candidate_word2)
        if distance &lt; min_distance:
            min_pair = (candidate_word1, candidate_word2)
            min_distance = distance

</code></pre>
<p><a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.distance" rel="nofollow noreferrer">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.distance</a></p>
<p>Could also be similarity (I'm not entirely sure if there's a difference). <a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity" rel="nofollow noreferrer">https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity</a></p>
<p>If similarity gets bigger with closer words, as I'd expect, then you'll want to maximize not minimize and just replace the distance function calls with similarity calls. Basically this is just the simple min/max function over the pairs.</p>
</div>
<span class="comment-copy">Can you post what code you have so far using the Word2Vec python package? Edit: While I'm at it, can you specifically link to the wrapper you're using? There is more than one.</span>
<span class="comment-copy">If you're using gensim's word2vec, it looks like <a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_to_given" rel="nofollow noreferrer">radimrehurek.com/gensim/models/…</a> is probably close to what you want.</span>
<span class="comment-copy">And if you're using Danielfrg's python google word2vec interface then here's some examples: <a href="https://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb#Predictions" rel="nofollow noreferrer">nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/…</a> I'd definitely recommend looking at the last example in the predictions section.</span>
<span class="comment-copy">@RylanFeldspar I am using Gensim</span>
<span class="comment-copy">@RylanFeldspar for Gensim that does not do what I am asking. Given ['architect', 'nurse', 'surgeon', 'grandmother', 'dad'] I am looking for the two most similar words to be returned for example ['nurse', 'surgeon']</span>
<span class="comment-copy">I have to admit, I've never heard of <code>combinations</code>! You learn something new every day! If we're just looking for the minimum value as the question-asker suggests, it would likely be more efficient for long lists of scored pairs to use <code>max(scored_pairs, key=lambda p: p[1])</code> with similarity or the <code>min</code> for difference.</span>
<span class="comment-copy">Often people want the top several, so my habit is to sort, but you're right - a <code>max()</code> would be slightly more efficient than a full sort, grabbing the best item in a single pass. With the score in the first (<code>[0]</code>) index of a tuple, I don't think you'd need to specify a <code>key</code>: a comparison happens element-wise, starting at <code>p[0]</code>, only progressing to a comparison of <code>p[1]</code> (etc) if there's a tie in the earlier elements. (But you could also skip the comprehension/sort completely, and use a <code>max()</code> and <code>key</code> function that calcs the pairs' similarity!)</span>
<span class="comment-copy">You may also be able to use <a href="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.distances" rel="nofollow noreferrer">radimrehurek.com/gensim/models/…</a> for fewer calls to the model, but I'll leave that exercise up to the reader.</span>
