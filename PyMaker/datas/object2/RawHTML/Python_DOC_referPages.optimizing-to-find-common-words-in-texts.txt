<div class="post-text" itemprop="text">
<pre><code>for s_index, s in enumerate(sentences):
        s_tokens = s.split()
        if (local_q_set.intersection(set(s_tokens)) == local_q_set):
            q_results.append(s_index)
</code></pre>
<p>The code snippet above is the core algorithm I used to find relevant sentences in massive text data that include all tokens in the query. For example, for a query "happy apple", it finds all sentences that include exactly one or more of all of the given tokens, (i.e. "happy" and "apple"). My method is very simple: find common intersecting sets and see if they match. However, I am not getting enough performance. If anyone has seen an optimization for such problem, I would highly appreciate any direction or link for the idea- Thank you for time in advance</p>
</div>
<div class="post-text" itemprop="text">
<p>There are a few things you can do to increase performance of the sequential search but the real boost would come from indexing tokens.</p>
<p><strong>set.difference</strong></p>
<p>Using <code>not local_q_set.difference(s_tokens)</code> instead of comparing the intersection to the original set may be somewhat faster.</p>
<p><strong>Regular Expression filter</strong></p>
<p>If your sentences are long, using a regular expression may provide some speed improvements by isolating the potential tokens out of the sentence before checking them against the token set:</p>
<pre><code>import re
tokens     = re.compile("|".join(local_q_set))
tokenCount = len(local_q_set)
for s_index, s in enumerate(sentences):
    s_tokens = tokens.findall(s)
    if len(s_tokens) &lt; tokenCount or local_q_set.difference(s.split()):
       continue
    q_results.append(s_index) 
</code></pre>
<p><strong>Filter using the in operator</strong></p>
<p>You can also use a simple <code>in</code> operator to check for the presence of tokens instead of a regular expression (this should be faster when you have few tokens in the query):</p>
<pre><code>result = []
tokenSet = set(queryTokens)
for index, sentence in enumerate(sentences):
     if any( token not in sentence for token in queryTokens) \
     or tokenSet.difference(sentence.split()):
         continue
     result.append(index)
</code></pre>
<p><strong>Caching sentence word sets</strong></p>
<p>To improve on the sequential search when multiple queries are made to the same list of sentences, you can build a cache of word sets corresponding to the sentences.  This will eliminate the work of parsing the sentences while going through them to find a match.</p>
<pre><code>cachedWords = []

queryTokens = ["happy","apple"]

queryTokenSet = set(queryTokens)
if not cachedWords:
    cachedWords = [ set(sentence.split()) for sentence in sentences ]
result = [ index for index,words in enumerate(cachedWords) if not queryTokenSet.difference(words) ]
</code></pre>
<p><strong>Token Indexing</strong></p>
<p>If you are going to perform many queries against the same list of sentences, it will be more efficient to create a mapping between tokens and sentence indexes. You can do that using a dictionary and then obtain query results directly by intersecting the sentence indexes of the queried tokens:</p>
<pre><code>tokenIndexes = dict()
for index,sentence in enumerate(sentences):
    for token in sentence.lower().split():
        tokenIndexes.setdefault(token,[]).append(index)

def tokenSet(token): return set(tokenIndexes.get(token,[]))

queryTokens = ["happy","apple"]

from functools import reduce
result = reduce(set.intersection , (tokenSet(token) for token in queryTokens) )
</code></pre>
<p>This  will allow you to economically implement complex queries using set operators.  For example:</p>
<pre><code>import re

querySring = " happy &amp; ( apple | orange | banana ) "
result = eval(re.sub("(\w+)",r"tokenSet('\1')", querySring)) 

# re.sub(...) transforms the query string into " tokenSet('happy') &amp; ( tokenSet('apple') | tokenSet('orange') | tokenSet('banana') ) "
</code></pre>
<p><strong>Performance Tests:</strong></p>
<p>I made a few performance tests (finding two tokens in one sentence out of 80,000):</p>
<pre><code>original algorithm: 105 ms           1x
set.difference:      88 ms         1.2x
regular expression:  60 ms         1.8x
"in" operator:       43 ms         2.4x
caching word sets:   23 ms         4.6x (excluding 187ms to build cache)
token indexing:       0.0075 ms  14000x (excluding 238ms to build tokenIndexes)
</code></pre>
<p><em>So, if you're going to be performing several queries on the same sentences, with  token indexing, you'll get 14 thousand times faster responses once the tokenIndexes dictionary is built.</em></p>
</div>
<span class="comment-copy">Your runtime is pretty much O(n). I don't really know how much faster you can get besides adding parallelization to the problem. Also caching might help especially if you have a lot of duplicate queries being performed</span>
<span class="comment-copy">I agree, Thank you for your comment Greg. I forgot to add that there are multiple queries for the same set of sentences. Would that change any logic for optimization? I was looking into possible loop unrolling</span>
<span class="comment-copy">Thank you so much for the great explanation. This saved my life. One day l hope to be experienced enough to contribute to the community like you do.</span>
