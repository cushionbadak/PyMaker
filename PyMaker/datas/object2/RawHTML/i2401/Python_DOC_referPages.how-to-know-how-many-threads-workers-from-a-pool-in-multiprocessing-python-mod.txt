<div class="post-text" itemprop="text">
<p>I am using imapala shell to compute some stats  over a text file containing the table names <br/></p>
<p>I am using python multiprocessing module to pool the processes.
<br/>
<strong><em>The thing is thing task is very time consuming . So I need to keep track of how many files have been completed to see the job progress.<br/></em></strong>
<strong>So let me give you some ideas about the functions that I am using.</strong></p>
<p><code>job_executor</code> is the function that takes a list of tables and perform the tasks.</p>
<p><code>main()</code> is the functions , that takes file location , no of executors(pool_workers), converts the file containing table to list of tables and does the multiprocessing thing <br/></p>
<p>I want to see the progress like how much file has been processed by job_executor , But I can't find a solution . Usin a counter also doesn't work . Help Me </p>
<pre><code>def job_executor(text):

    impala_cmd = "impala-shell -i %s -q  'compute stats %s.%s'" % (impala_node, db_name, text)
    impala_cmd_res = os.system(impala_cmd)  #runs impala Command    

    #checks for execution type(success or fail)
    if impala_cmd_res == 0:
        print ("invalidated the metadata.")
    else:
        print("error while performing the operation.")


def main(args):
    text_file_path = args.text_file_path
    NUM_OF_EXECUTORS = int(args.pool_executors)

    with open(text_file_path, 'r') as text_file_reader:
        text_file_rows = text_file_reader.read().splitlines()  # this will return list of all the tables in the file.
        process_pool = Pool(NUM_OF_EXECUTORS)
        try:
            process_pool.map(job_executor, text_file_rows)
            process_pool.close()
            process_pool.join()
        except Exception:
            process_pool.terminate()
            process_pool.join()


def parse_args():
    """
    function to take scrap arguments from  test_hr.sh file
    """
    parser = argparse.ArgumentParser(description='Main Process file that will start the process and session too.')
    parser.add_argument("text_file_path",
                        help='provide text file path/location to be read. ')  # text file fath
    parser.add_argument("pool_executors",
                        help='please provide pool executors as an initial argument') # pool_executor path

    return parser.parse_args() # returns list/tuple of all arguments.


if __name__ == "__main__":
    mail_message_start()

    main(parse_args())

    mail_message_end()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you insist on needlessly doing it via <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer"><code>multiprocessing.pool.Pool()</code></a>, the easiest way to keep a track of what's going on is to use a non-blocking mapping (i.e. <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map_async" rel="nofollow noreferrer"><code>multiprocessing.pool.Pool.map_async()</code></a>):</p>
<pre><code>def main(args):
    text_file_path = args.text_file_path
    NUM_OF_EXECUTORS = int(args.pool_executors)

    with open(text_file_path, 'r') as text_file_reader:
        text_file_rows = text_file_reader.read().splitlines()
        total_processes = len(text_file_rows)  # keep the number of lines for reference
        process_pool = Pool(NUM_OF_EXECUTORS)
        try:
            print('Processing {} lines.'.format(total_processes))
            processing = process_pool.map_async(job_executor, text_file_rows)
            processes_left = total_processes  # number of processing lines left
            while not processing.ready():  # start a loop to wait for all to finish
                if processes_left != processing._number_left:
                    processes_left = processing._number_left
                    print('Processed {} out of {} lines...'.format(
                        total_processes - processes_left, total_processes))
                time.sleep(0.1)  # let it breathe a little, don't forget to `import time`
            print('All done!')
            process_pool.close()
            process_pool.join()
        except Exception:
            process_pool.terminate()
            process_pool.join()
</code></pre>
<p>This will check every 100ms if some of the processes finished processing and if something changed since the last check it will print out the number of lines processed so far. If you need more insight into what's going on with your subprocesses, you can use some of the shared structures like <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue" rel="nofollow noreferrer"><code>multiprocessing.Queue()</code></a> or <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.sharedctypes.multiprocessing.Manager" rel="nofollow noreferrer"><code>multiprocessing.Manager()</code></a> structures to directly report from within your processes. </p>
</div>
<span class="comment-copy">Lets start with the obvious question - why are you using multiprocessing just to run an external process? External processes will be run as separate processes anyway so the whole multiprocessing dance you're doing is one big waste. Threads would be perfectly fine for this and you can then share pointers to each of your subprocesses to see how they are progressing.</span>
<span class="comment-copy">@zwer .Actually these impala is being spawned on hadoop clusters , So it must be having multiple cpus . So to take advantage of the thing we are using this . The thing is zwer that this code is already present and I am not authorized to make any changes in its functional methods . So if you can tell me or help me to tell how to keep a track of the progress for the above code only it would be really really helpful .Thanks</span>
<span class="comment-copy">If you cannot change the code how do you expect to add process tracking? Then you can only do it externally by monitoring <code>impala-shell</code> processes and monitoring their resource usage. Also, multiprocessing in this instance has nothing to do with using multiple CPUs - you can run all your <code>job_executor()</code> calls in a single thread, in a single loop and they would all still execute on separate CPUs, if available (just don't use the semi-deprecated <code>os.system()</code>, use <a href="https://docs.python.org/3/library/subprocess.html#subprocess.Popen" rel="nofollow noreferrer"><code>subprocess.Popen()</code></a> instead to ensure a non-blocking call).</span>
<span class="comment-copy">@zwer . I have been provided with authority to add some lines of code that takes care of tracking progress only . I am not supposed to add or remove any other line of code in this file . Hope you can understand.</span>
<span class="comment-copy">Thank you so much . After doing some internet search I had also used this pool.map_async() instead of map () and am using the exact same type of code as mentioned by you . Thank you so much . And I can even understand your frustation  due to me .</span>
