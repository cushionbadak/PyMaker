<div class="post-text" itemprop="text">
<pre><code>from time import time

mylist1 = []
mylist2 = []

start1 = time()
for i in range(100000000):
    mylist1.append(i)
end1 = time()

start2 = time()
mylist2 = [0] * 100000000
end2 = time()

print(end1-start1, end2-start2)
</code></pre>
<p>When I timed both actions to fill the list, I get 14 seconds for using for loop and 0.5 seconds for <code>mylist2 = [0] * 100000000</code></p>
<p>So it seems it is obvious to use the second method if I need to insert massive amount of items at once.</p>
<p>But if I do the second thing, I have to insert the same number for all, or manually type  numbers that will repeat.</p>
<p>Is there a way to perform</p>
<pre><code>for i in range(100000000):
    mylist1.append(i)
</code></pre>
<p>this action resulting in [0,1,2,3,...,n] with good speed?</p>
<p>The code doesn't necessarily have to be short if the speed is fast.</p>
</div>
<div class="post-text" itemprop="text">
<p>For full portability, <code>list(range(N))</code> will get the best performance <a href="https://stackoverflow.com/a/52434858/364696">as Prune notes</a>. That said, if you're purely targeting Python 3.5 or higher, you can use <a href="https://www.python.org/dev/peps/pep-0448/" rel="nofollow noreferrer">PEP 448's additional unpacking generalizations</a> to speed it up a small amount, with:</p>
<pre><code>[*range(N)]
</code></pre>
<p>Note that this is a fixed savings, not per-item; all it does is bypass the lookup of <code>list</code> in the built-in namespace, and the generalized function call dispatch and <code>__init__</code> argument processing of the normal <code>list</code> constructor. So when you're talking about 100 million items, the savings are going to be lost in the noise; all this does is reduce the fixed overhead by (on my 3.6 install) 170Â±10 ns (e.g. <code>list(range(0))</code> takes 417 ns per call, vs. 247 ns per call for <code>[*range(0)]</code>).</p>
<p>In specific cases though, there is an even faster option:</p>
<pre><code>mynotlist = range(100000000)
</code></pre>
<p>In modern Python, <a href="https://docs.python.org/3/library/stdtypes.html#typesseq-range" rel="nofollow noreferrer"><code>range</code> objects</a> are <a href="https://docs.python.org/3/glossary.html#term-sequence" rel="nofollow noreferrer">full fledged sequences</a>, they're just not mutable. So you can construct them, index them, slice them, compute their length, iterate them forwards and backwards, check membership (in <code>O(1)</code> for <code>int</code>s, unlike <code>list</code> where membership testing is <code>O(n)</code>), etc. The only non-mutability related features they lack are concatenation and repetition (with <code>+</code> and <code>*</code>), though you can simulate that with <a href="https://docs.python.org/3/library/itertools.html" rel="nofollow noreferrer"><code>itertools</code> functions</a> like <a href="https://docs.python.org/3/library/itertools.html#itertools.chain" rel="nofollow noreferrer"><code>chain</code></a> (for concatenation), and <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow noreferrer"><code>islice</code>ing</a> a <a href="https://docs.python.org/3/library/itertools.html#itertools.cycle" rel="nofollow noreferrer"><code>cycle</code></a> (for repetition).</p>
<p><strong>If you don't need to mutate the sequence, just read from it, using the <code>range</code> "raw" is <em>by far</em> the best option</strong>; <code>range</code>s are lazy, consuming no memory, while still producing their values extremely efficiently. That laziness can be important; <code>list(range(100000000))</code> will require (on 64 bit Python) 3.45 <em>giga</em>bytes of memory for the <code>list</code> itself plus all the <code>int</code>s it contains; <code>range(100000000)</code> requires 48 <em>bytes</em>. The trivial cost of generating the values on the fly is more than worth it, given the memory savings.</p>
<p>If you need mutability, you can still save a bit on memory. If <code>numpy</code> is an option, <a href="https://stackoverflow.com/a/52434904/364696">sacul's answer</a> has you covered; if not, <a href="https://docs.python.org/3/library/array.html" rel="nofollow noreferrer">Python's array module</a> will save you a little bit of time, and a <em>lot</em> of memory. Compared to:</p>
<pre><code> list(range(100000000))
</code></pre>
<p>the <code>array</code> alternative:</p>
<pre><code> array.array('I', range(100000000))
</code></pre>
<p>takes about 10% less time (microbenchmarks had <code>list</code> at 3.39 sec, vs. <code>array.array</code> at 3.07 sec), and consumes <em>far</em> less memory (under ~391 MB, vs. the ~3529 MB of the <code>list</code> of <code>int</code>s). The main cost of <code>array</code> is limited range of values (e.g. for <code>'I'</code>, four byte <code>unsigned int</code>s can only store values in <code>range(2**32)</code>; the maximum range for <code>q</code>/<code>Q</code> format codes, using twice the memory, would be <code>range(-2**63, 2**63)</code>/<code>range(2**64)</code>).</p>
</div>
<div class="post-text" itemprop="text">
<p>Since you say you need speed, I think that <code>np.arange</code> is the best way to go, it's even faster than creating the list of all <code>0</code>s. Here are the timings on my machine:</p>
<pre><code>import timeit
import numpy as np

def m1(n=100000000):
    mylist = []
    for i in range(n):
        mylist.append(i)
    return mylist

def m2(n=100000000):
    return [0] * n

def m3(n=100000000):
    return list(range(n))

def m4(n=100000000):
    return np.arange(n)

&gt;&gt;&gt; timeit.timeit(m1,number=1)
17.615584995000972
&gt;&gt;&gt; timeit.timeit(m2,number=1)
0.7669911839911947
&gt;&gt;&gt; timeit.timeit(m3,number=1)
9.909814337006537
&gt;&gt;&gt; timeit.timeit(m4,number=1)
0.5374436590063851
</code></pre>
<p>Note that <code>np.arange()</code> returns a <code>np.array</code>. If you need to convert it back to a list, you lose the speed. Better to just use the array...</p>
<pre><code>def m4(n=100000000):
    return np.arange(n).tolist()

&gt;&gt;&gt; timeit.timeit(m4,number=1)
11.485261309993803
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Try making a list of the <code>range</code> output:</p>
<pre><code>mylist3 = list(range(100000000))
</code></pre>
<p>I added this to your tests and got these times:</p>
<pre><code>append: 18.42
all-0:   0.23
list:    2.63       &lt;== The new one
</code></pre>
</div>
<span class="comment-copy">Why not use just <code>list(range(100000000))</code>, or better yet <code>np.arange(100000000)</code>?</span>
<span class="comment-copy">What are you even doing with a list of a hundred million sequential integers, anyway? A list may not be the right data structure for the job - for example, <code>range(10**8)</code> may already be enough.</span>
<span class="comment-copy">If you make a list from an object with a <code>len</code>, Python can use that value to guess at the size of the list it will end up making, so it doesn't have to constantly resize the list, which is expensive.</span>
<span class="comment-copy">@PatrickHaugh: <code>range</code> has a <code>len</code>, so <code>list(range(N))</code> (and <code>[*range(N)]</code> on Python 3.5+) will take advantage of that optimization already. That said, resizing the <code>list</code> isn't <i>that</i> expensive; since Python <code>list</code>s are just arrays of pointers, the worst case cost of each resize is just the worst case of <code>realloc</code>: Allocating new space, <code>memcpy</code>, free old space. The bigger cost here is repeatedly binding and calling <code>append</code>; there is non-trivial overhead to creating a bound method object (though 3.7 should reduce the need for it) and general function dispatch.</span>
<span class="comment-copy">For example, using <code>ipython</code>'s <code>%%timeit</code> for microbenchmarking, the OP's code that repeatedly binds <code>append</code> takes about 9.3 seconds on my Linux 64 bit install of Python 3.6. If I tweak it to add <code>appendmylist1 = mylist1.append</code> outside the loop, and make the call in the loop <code>appendmylist1(i)</code>, that, by itself, reduces the cost of the whole loop to 6.19 seconds. A solid third of the total time (and about half the overhead above to <code>list(range(100000000))</code>) was just repeatedly performing the <code>LOAD_ATTR</code> involved in <code>mylist1.append</code>. Much of the rest is likely overhead of <i>calling</i> <code>append</code>.</span>
<span class="comment-copy">For completeness, do <code>list(np.arange(n))</code></span>
<span class="comment-copy">@MadPhysicist: <code>np.arange(n).tolist()</code> would be more appropriate - it's faster, and produces ordinary Python ints rather than NumPy ints.</span>
<span class="comment-copy">@MadPhysicist, then you lose the speed advantage... see edits</span>
<span class="comment-copy">@sacul: If the values are known to fall within <code>range(-2**31, 2**31)</code> or <code>range(2**32)</code>, you can speed up <code>np.arange</code> (and halve the memory usage) by explicitly providing a <code>dtype</code> of <code>np.int32</code> or <code>np.uint32</code> respectively. <code>np.arange</code> kinda infers the type from the arguments, but the heuristic is "if arguments are integers, and fit in a C <code>int64_t</code>, the <code>dtype</code> is <code>int64</code>, otherwise it's <code>float64</code> for huge positive values, and <code>object</code> for huge negative values". When you know better, <code>dtype=np.int32</code> halves memory and almost halves runtime to boot (for me, ~150 ms dropped to ~82 ms).</span>
<span class="comment-copy">That's an important nit.  Thanks.  Answer edited.</span>
