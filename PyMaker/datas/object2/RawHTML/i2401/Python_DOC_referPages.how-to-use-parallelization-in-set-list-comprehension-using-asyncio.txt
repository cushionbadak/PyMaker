<div class="post-text" itemprop="text">
<p>I want to create a multiprocess comprehension in Python 3.7.</p>
<p>Here's the code I have:</p>
<pre><code>async def _url_exists(url):
  """Check whether a url is reachable"""
  request = requests.get(url)
  return request.status_code == 200:

async def _remove_unexisting_urls(rows):
  return {row for row in rows if await _url_exists(row[0])}

rows = [
  'http://example.com/',
  'http://example.org/',
  'http://foo.org/',
]
rows = asyncio.run(_remove_unexisting_urls(rows))
</code></pre>
<p>In this code example, I want to remove non-existing URLs from a list. (Note that I'm using a set instead of a list because I also want to remove duplicates).</p>
<p>My issue is that I still see that the execution is sequential. HTTP Requests make the execution wait.
When compared to a serial execution, the execution time is the same.</p>
<ul>
<li>Am I doing something wrong?</li>
<li>How should these await/async keywords be used with python comprehension?</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p><code>asyncio</code> itself doesn't run different <code>async</code> functions concurrently. However, with the <code>multiprocessing</code> module's <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map" rel="nofollow noreferrer"><code>Pool.map</code></a>, you can schedule functions to run in another process:</p>
<pre><code>from multiprocessing.pool import Pool

pool = Pool()

def fetch(url):
    request = requests.get(url)
    return request.status_code == 200

rows = [
  'http://example.com/',
  'http://example.org/',
  'http://foo.org/',
]
rows = [r for r in pool.map(fetch, rows) if r]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><code>requests</code> does not support <code>asyncio</code>. If you want to go for true asynchronous execution, you will have to look at libs like <a href="https://aiohttp.readthedocs.io/en/stable/" rel="nofollow noreferrer">aiohttp</a> or <a href="https://asks.readthedocs.io/en/latest/" rel="nofollow noreferrer">asks</a></p>
<p>Your set should be built before offloading to the tasks, so you don't even execute for duplicates, instead of streamlining the result.</p>
<p>With <code>requests</code> itself, you can fall back to <code>run_in_executor</code> which will execute your requests inside a <a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor" rel="nofollow noreferrer">ThreadPoolExecutor</a>, so not really asynchronous I/O:</p>
<pre><code>import asyncio
import time
from requests import exceptions, get

def _url_exists(url):
    try:
        r = get(url, timeout=10)
    except (exceptions.ConnectionError, exceptions.ConnectTimeout):
        return False
    else:
        return r.status_code is 200

async def _remove_unexisting_urls(l, r):
    # making a set from the list before passing it to the futures
    # so we just have three tasks instead of nine
    futures = [l.run_in_executor(None, _url_exists, url) for url in set(r)]
    return [await f for f in futures]

rows = [ # added some dupes
    'http://example.com/',
    'http://example.com/',
    'http://example.com/',
    'http://example.org/',
    'http://example.org/',
    'http://example.org/',
    'http://foo.org/',
    'http://foo.org/',
    'http://foo.org/',
]

loop = asyncio.get_event_loop()
print(time.time())
result = loop.run_until_complete(_remove_unexisting_urls(loop, rows))
print(time.time())
print(result)
</code></pre>
<p>Output</p>
<pre><code>1537266974.403686
1537266986.6789136
[False, False, False]
</code></pre>
<p>As you can see, there is a penalty from initializing the thread pool, ~2.3 seconds in this case. However, given that fact that each of the three tasks runs for ten seconds until timeout on my box (my IDE is not allowed through the proxy), an overall of twelve seconds execution time looks quite concurrent.</p>
</div>
<span class="comment-copy"><code>map</code> is an instance method of <code>Pool</code>; you can't call it on the class (without passing an instance of <code>Pool</code> to it), so that would need to be <code>Pool.map(Pool(), fetch, set(rows))</code>. Although <code>with Pool() as pool: rows = [r for r in pool.map(fetch, set(rows)) is r]</code> is probably the more common choice. (Using <code>set(rows)</code> to remove the duplicates as per the OP's concern)</span>
<span class="comment-copy">Oops! Your're right, I've updated the answer. Thanks!</span>
