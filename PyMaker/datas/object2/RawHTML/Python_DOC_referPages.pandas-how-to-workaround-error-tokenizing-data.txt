<div class="post-text" itemprop="text">
<p>A lot of questions have been already asked <a href="https://stackoverflow.com/q/18039057/7295599">about this topic on SO</a>. 
(and many others).
Among the numerous answers, none of them was really helpful to me so far. If I missed  <strong>the</strong> useful one, please let me know.</p>
<p>I simply would like to read a CSV file with pandas into a dataframe. Sounds like a simple task.</p>
<p>My file <code>Test.csv</code></p>
<pre><code>1,2,3,4,5
1,2,3,4,5,6
,,3,4,5
1,2,3,4,5,6,7
,2,,4
</code></pre>
<p>My code:</p>
<pre><code>import pandas as pd
df = pd.read_csv('Test.csv',header=None)
</code></pre>
<p>My error:</p>
<pre><code>pandas.errors.ParserError: Error tokenizing data. C error: Expected 5 fields in line 2, saw 6
</code></pre>
<p>My guess about the issue is that Pandas looks to the first line and expects the same number of tokens in the following rows. If this is not the case it will stop with an error.</p>
<p>In the numerous answers, the suggestions for using options are, e.g.:
<code>error_bad_lines=False</code> or <code>header=None</code> or <code>skiprows=3</code> and more non-helpful suggestions. </p>
<p>However, I don't want to ignore any lines or skip. And I don't know in advance how many columns and rows the datafile has.</p>
<p>So it basically boils down to how to find the maximum number of columns in the datafile. Is this the way to go? I hoped that there was an easy way to simply read a CSV file which does not have the maximum column number in the first line. Thank you for any hints. I'm using Python 3.6.3, Pandas 0.24.1 on Win7.</p>
</div>
<div class="post-text" itemprop="text">
<p>Thank you @ALollz for the "very fresh" link (lucky coincidence) and @Rich Andrews for pointing out that my example actually is not "strictly correct" CSV data.</p>
<p>So, the way it works for me for the time being is adapted from @ALollz' compact solution (<a href="https://stackoverflow.com/a/55129746/7295599">https://stackoverflow.com/a/55129746/7295599</a>)</p>
<pre><code>### reading an "incorrect" CSV to dataframe having a variable number of columns/tokens 
import pandas as pd

df = pd.read_csv('Test.csv', header=None, sep='\n')
df = df[0].str.split(',', expand=True)
# ... do some modifications with df
### end of code
</code></pre>
<p><code>df</code> contains empty string <code>''</code> for the missing entries at the beginning and the middle, and <code>None</code> for the missing tokens at the end.</p>
<pre><code>   0  1  2  3     4     5     6
0  1  2  3  4     5  None  None
1  1  2  3  4     5     6  None
2        3  4     5  None  None
3  1  2  3  4     5     6     7
4     2     4  None  None  None
</code></pre>
<p>If you write this again to a file via: </p>
<p><code>df.to_csv("Test.tab",sep="\t",header=False,index=False)</code></p>
<pre><code>1   2   3   4   5       
1   2   3   4   5   6   
        3   4   5       
1   2   3   4   5   6   7
    2       4           
</code></pre>
<p><code>None</code> will be converted to empty string <code>''</code> and everything is fine.</p>
<p>The next level would be to account for data strings in quotes which contain the separator, but that's another topic.</p>
<pre><code>1,2,3,4,5
,,3,"Hello, World!",5,6
1,2,3,4,5,6,7
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Read the csv using the tolerant python csv module, and fix the loaded file prior to handing it off to pandas, which will fails on the otherwise malformed csv data regardless of the csv engine pandas uses.</p>
<pre><code>import pandas as pd
import csv

not_csv = """1,2,3,4,5
1,2,3,4,5,6
,,3,4,5
1,2,3,4,5,6,7
,2,,4
"""

with open('not_a.csv', 'w') as csvfile:
    csvfile.write(not_csv)

d = []
with open('not_a.csv') as csvfile:
    areader = csv.reader(csvfile)
    max_elems = 0
    for row in areader:
        if max_elems &lt; len(row): max_elems = len(row)
    csvfile.seek(0)
    for i, row in enumerate(areader):
        # fix my csv by padding the rows
        d.append(row + ["" for x in range(max_elems-len(row))])

df = pd.DataFrame(d)
print df

# the default engine
# provides "pandas.errors.ParserError: Error tokenizing data. C error: Expected 5 fields in line 2, saw 6 "
#df = pd.read_csv('Test.csv',header=None, engine='c')

# the python csv engine
# provides "pandas.errors.ParserError: Expected 6 fields in line 4, saw 7 "
#df = pd.read_csv('Test.csv',header=None, engine='python')

</code></pre>
<p>Preprocess file outside of python if concerned about extra code inside python creating too much python code.</p>
<pre><code>Richs-MBP:tmp randrews$ cat test.csv
1,2,3
1,
2
1,2,
,,,
Richs-MBP:tmp randrews$ awk 'BEGIN {FS=","}; {print $1","$2","$3","$4","$5}' &lt; test.csv
1,2,3,,
1,,,,
2,,,,
1,2,,,
,,,,
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I have a different take on the solution. Let pandas take care of creating the table and deleting None values and let us take care of writing a proper tokenizer.</p>
<h2>Tokenizer</h2>
<pre><code>def tokenize(str):
    idx = [x for x, v in enumerate(str) if v == '\"']
    if len(idx) % 2 != 0:
        idx = idx[:-1]
    memory = {}
    for i in range(0, len(idx), 2):
        val = str[idx[i]:idx[i+1]+1]
        key = "_"*(len(val)-1)+"{0}".format(i)
        memory[key] = val
        str = str.replace(memory[key], key, 1)        
    return [memory.get(token, token) for token in str.split(",")]  
</code></pre>
<h1>Test cases for Tokenizer</h1>
<pre><code>print (tokenize("1,2,3,4,5"))
print (tokenize(",,3,\"Hello, World!\",5,6"))
print (tokenize(",,3,\"Hello,,,, World!\",5,6"))
print (tokenize(",,3,\"Hello, World!\",5,6,,3,\"Hello, World!\",5,6"))
print (tokenize(",,3,\"Hello, World!\",5,6,,3,\"Hello,,5,6"))
</code></pre>
<p>Output </p>
<p><code>['1', '2', '3', '4', '5']
['', '', '3', '"Hello, World!"', '5', '6']
['', '', '3', '"Hello,,,, World!"', '5', '6']
['', '', '3', '"Hello, World!"', '5', '6', '', '3', '"Hello, World!"', '5', '6']
['', '', '3', '"Hello, World!"', '5', '6', '', '3', '"Hello', '', '5', '6']</code></p>
<h1>Putting the tokenizer into action</h1>
<pre><code>with open("test1.csv", "r") as fp:
    lines = fp.readlines()

lines = list(map(lambda x: tokenize(x.strip()), lines))
df = pd.DataFrame(lines).replace(np.nan, '')
</code></pre>
<h1>Advantage:</h1>
<p>Now we can teak the tokenizer function as per our needs </p>
</div>
<span class="comment-copy">Seems to be the same issue someone had yesterday: <a href="https://stackoverflow.com/questions/55129640/read-csv-into-a-dataframe-with-varying-row-lengths-using-pandas" title="read csv into a dataframe with varying row lengths using pandas">stackoverflow.com/questions/55129640/â€¦</a>. Either read in full lines and split after, alter the original file adding a header row with an excessive amount of columns just to be safe, or use the <code>csv</code> module</span>
<span class="comment-copy">thanks for the link. I will check. I don't want to change the datafile, I simply want to read it.</span>
<span class="comment-copy">Yes, the upvoted answers in that SO link should help solve this problem. I like the solution with <code>pd.read_fwf('path_to_csv', header=None)</code> as it just needs the csv path.</span>
<span class="comment-copy">the <code>fwf</code> solution is very case specific and can break very easily since the underlying data isn't actually fixed-width delimited.</span>
<span class="comment-copy">The reason why the pandas csv support seems lacking is that the input file is not a csv.  Looks like one, but csv records are expected to have the same sequence of fields per record.  The one above does not.  Therefore, don't be shy about pre-processing the input data to obtain a csv.  <a href="https://en.wikipedia.org/wiki/Comma-separated_values#Specification" rel="nofollow noreferrer">en.wikipedia.org/wiki/Comma-separated_values#Specification</a></span>
<span class="comment-copy">Thank you for your suggestion and pointing out that my example actually is not "strictly correct" CSV data. I prefer to keep it short and besides using Pandas not using another module which probably blows up my final executable further.</span>
<span class="comment-copy">Also consider processing the file outside of python if  concerned about code bulk, separation of concerns, make someone else do the preprocessing, etc.  - 'awk' is an excellent tool for doing what you need.  I'll append it to my answer.</span>
<span class="comment-copy">thank your for the addition. Unless absolute necessary, I prefer to use the least possible number of tools, modules, external programs, pre-processing steps etc. I haven't use awk, but heard several times of it.</span>
<span class="comment-copy">thank you for this suggestion. I will (re-)consider it when there will be absolute need to cover commas in quotation marks.</span>
