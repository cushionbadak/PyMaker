<div class="post-text" itemprop="text">
<p>the heading is very generic but issue might not be.</p>
<p>I have a script that is compiling some code with the parameters passed from a file(xls file). Based on number of configurations on xls i have to compile certain files.
I want to store result of each compilation(stdout and stderr) in text files whose names comes from configuration.</p>
<p>I have been able to do all this but to speed up things i want to run all the compilation in parallel. Is there a way to do this?</p>
<p>Sample file..</p>
<pre><code>for n in num_rows: # num_rows store all the rows read using xlrd object
    parameters_list = [...] # has all the parameters read from xls
    .
    .
    .
    logfile = ...txt #name is based on name read from xls

    p = subprocess.Popen(parameters_list, stderr=logfile)
    p.wait()
    logfile.close()
</code></pre>
<p>I have to wait for each process to be over before closing the file.</p>
<p>My problem might be too long but any help or leads are welcomed.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can do this using a multiprocessing.Pool:</p>
<pre><code>def parse_row(n):
    parameters_list = [...] # has all the parameters read from xls
    .
    .
    .
    logfile = ...txt #name is based on name read from xls
    p = subprocess.Popen(parameters_list, stderr=logfile)
    p.wait()
    logfile.close()
pool = multiprocessing.Pool()
pool.map_async(parse_row, num_rows)
pool.close()
pool.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Assuming your processes will all be writing to different logfiles, the answer is quite simple: the <code>subprocess</code> module will already run things in parallel. Just create a different <code>Popen</code> object for each one, and store them in a list:</p>
<pre><code>processes = []
logfiles = []
for n in num_rows: # num_rows store all the rows read using xlrd object
    parameters_list = [...] # has all the parameters read from xls
    .
    .
    .
    logfile = ...txt #name is based on name read from xls
    logfiles.append(logfile)

    p = subprocess.Popen(parameters_list, stderr=logfile)
    logfiles.append(logfile)
    processes.append(p)

# Now, outside the for loop, the processes are all running in parallel.
# Now we can just wait for each of them to finish, and close its corresponding logfile

for p, logfile in zip(processes, logfiles):
    p.wait() # This will return instantly if that process was already finished
    logfile.close()
</code></pre>
</div>
<span class="comment-copy"><a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">docs.python.org/3/library/multiprocessing.html</a></span>
<span class="comment-copy">While subprocess is quite useful, for parallel processing, the <code>multiprocessing</code> package works nicer...</span>
<span class="comment-copy">@xtofl - Absolutely. Upvoted your answer, since it's better than mine.</span>
<span class="comment-copy">that's very sympathetic :)  I'm an objective outstander, though: the credits go to ppperry.</span>
