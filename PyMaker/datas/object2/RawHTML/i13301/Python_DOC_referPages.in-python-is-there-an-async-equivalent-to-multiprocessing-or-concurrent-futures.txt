<div class="post-text" itemprop="text">
<p>Basically, I'm looking for something that offers a parallel map using python3 coroutines as the backend instead of threads or processes. I believe there should be less overhead when performing highly parallel IO work.</p>
<p>Surely something similar already exists, be it in the standard library or some widely used package?</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>DISCLAIMER</strong> <a href="https://www.python.org/dev/peps/pep-0492/" rel="noreferrer">PEP 0492</a> defines only syntax and usage for coroutines. They require an event loop to run, which is most likely <a href="https://docs.python.org/3/library/asyncio-eventloop.html" rel="noreferrer"><code>asyncio</code>'s event loop</a>.</p>
<h3>Asynchronous map</h3>
<p>I don't know any implementation of <code>map</code> based on coroutines. However it's trivial to implement basic <code>map</code> functionality using <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.gather" rel="noreferrer"><code>asyncio.gather()</code></a>:</p>
<pre><code>def async_map(coroutine_func, iterable):
    loop = asyncio.get_event_loop()
    future = asyncio.gather(*(coroutine_func(param) for param in iterable))
    return loop.run_until_complete(future)
</code></pre>
<p>This implementation is really simple. It creates a coroutine for each item in the <code>iterable</code>, joins them into single coroutine and executes joined coroutine on event loop.</p>
<p>Provided implementation covers part of the cases. However it has a problem. With long iterable you would probably want to limit amount of coroutines running in parallel. I can't come up with simple implementation, which is efficient and preserves order at the same time, so I will leave it as an exercise for a reader.</p>
<h3>Performance</h3>
<p>You claimed:</p>
<blockquote>
<p>I believe there should be less overhead when performing highly parallel IO work.</p>
</blockquote>
<p>It requires proof, so here is a comparison of <code>multiprocessing</code> implementation, <code>gevent</code> implementation by <a href="https://stackoverflow.com/a/33007988/1377864">a p</a> and my implementation based on coroutines. All tests were performed on Python 3.5.</p>
<p>Implementation using <code>multiprocessing</code>:</p>
<pre><code>from multiprocessing import Pool
import time


def async_map(f, iterable):
    with Pool(len(iterable)) as p:  # run one process per item to measure overhead only
        return p.map(f, iterable)

def func(val):
    time.sleep(1)
    return val * val
</code></pre>
<p>Implementation using <code>gevent</code>:</p>
<pre><code>import gevent
from gevent.pool import Group


def async_map(f, iterable):
    group = Group()
    return group.map(f, iterable)

def func(val):
    gevent.sleep(1)
    return val * val
</code></pre>
<p>Implementation using <code>asyncio</code>:</p>
<pre><code>import asyncio


def async_map(f, iterable):
    loop = asyncio.get_event_loop()
    future = asyncio.gather(*(f(param) for param in iterable))
    return loop.run_until_complete(future)

async def func(val):
    await asyncio.sleep(1)
    return val * val
</code></pre>
<p>Testing program is usual <code>timeit</code>:</p>
<pre><code>$ python3 -m timeit -s 'from perf.map_mp import async_map, func' -n 1 'async_map(func, list(range(10)))'
</code></pre>
<p>Results:</p>
<ol>
<li><p>Iterable of <code>10</code> items:</p>
<ul>
<li><code>multiprocessing</code> - 1.05 sec</li>
<li><code>gevent</code> - 1 sec</li>
<li><code>asyncio</code> - 1 sec</li>
</ul></li>
<li><p>Iterable of <code>100</code> items:</p>
<ul>
<li><code>multiprocessing</code> - 1.16 sec</li>
<li><code>gevent</code> - 1.01 sec</li>
<li><code>asyncio</code> - 1.01 sec</li>
</ul></li>
<li><p>Iterable of <code>500</code> items:</p>
<ul>
<li><code>multiprocessing</code> - 2.31 sec</li>
<li><code>gevent</code> - 1.02 sec</li>
<li><code>asyncio</code> - 1.03 sec</li>
</ul></li>
<li><p>Iterable of <code>5000</code> items:</p>
<ul>
<li><code>multiprocessing</code> - <strong>failed</strong> (spawning 5k processes is not so good idea!)</li>
<li><code>gevent</code> - 1.12 sec</li>
<li><code>asyncio</code> - 1.22 sec</li>
</ul></li>
<li><p>Iterable of <code>50000</code> items:</p>
<ul>
<li><code>gevent</code> - 2.2 sec</li>
<li><code>asyncio</code> - 3.25 sec</li>
</ul></li>
</ol>
<h3>Conclusions</h3>
<p>Concurrency based on event loop works faster, when program do mostly I/O, not computations. Keep in mind, that difference will be smaller, when there are less I/O and more computations are involved.</p>
<p>Overhead introduced by spawning processes is significantly bigger, than overhead introduced by event loop based concurrency. It means that your assumption is correct. </p>
<p>Comparing <code>asyncio</code> and <code>gevent</code> we can say, that <code>asyncio</code> has 33-45% bigger overhead. It means that creation of greenlets is cheaper, than creation of coroutines. </p>
<p>As a final conclusion: <code>gevent</code> has better performance, but <code>asyncio</code> is part of the standard library. Difference in performance (absolute numbers) isn't very significant. <code>gevent</code> is quite mature library, while <code>asyncio</code> is relatively new, but it advances quickly.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could use <a href="https://greenlet.readthedocs.org/en/latest/" rel="nofollow">greenlets</a> (lightweight threads, basically coroutines) for this, or the somewhat higher-level <a href="http://www.gevent.org/" rel="nofollow">gevent</a> lib built on top of them: </p>
<p>(from the <a href="http://sdiehl.github.io/gevent-tutorial/#groups-and-pools" rel="nofollow">docs</a>)</p>
<pre><code>import gevent
from gevent import getcurrent
from gevent.pool import Group

group = Group()

def hello_from(n):
    print('Size of group %s' % len(group))
    print('Hello from Greenlet %s' % id(getcurrent()))

group.map(hello_from, xrange(3))

def intensive(n):
    gevent.sleep(3 - n)
    return 'task', n

print('Ordered')

ogroup = Group()
for i in ogroup.imap(intensive, xrange(3)):
    print(i)

print('Unordered')

igroup = Group()
for i in igroup.imap_unordered(intensive, xrange(3)):
    print(i)
</code></pre>
<p>Yields output: </p>
<pre><code>Size of group 3
Hello from Greenlet 31904464
Size of group 3
Hello from Greenlet 31904944
Size of group 3
Hello from Greenlet 31905904
Ordered
('task', 0)
('task', 1)
('task', 2)
Unordered
('task', 2)
('task', 1)
('task', 0)
</code></pre>
<p>The standard constraints of lightweight-vs-proper-multicore-usage apply to greenlets vs threads. That is, they're concurrent but not necessarily parallel. </p>
<p>Quick edit for people who see this in the future, since Yaroslav has done a great job of outlining some differences between Python's asyncio and gevent:</p>
<p>Why gevent over async/await? (these are all super subjective but have applied to me in the past)<br/>
- Not portable/easily accesible (not just 2.X, but 3.5 brought new keywords)<br/>
- async and await have a tendency to spread and infect codebases - when someone else has encapsulated this for you, it's super duper nice in terms of development and readability/maintainability<br/>
- In addition to above, I (personally) feel like the high-level interface of gevent is very "pythonic".<br/>
- Less rope to hang yourself with. In simple examples the two seem similar, but the more you want to do with async calls, the more chance you have to fuck up something basic and create race conditions, locks, unexpected behaviors. No need to reinvent the noose imho.<br/>
- Gevent's performance scales past trivial examples and is used and tested in lots of production environments. If you don't know much about asynchronous programming, it's a good place to start. </p>
<p>Why asyncio and not Gevent?<br/>
- If you can guarantee a version of Python and don't have access to 3rd party packages/pip, it gives you out of the box support.<br/>
- Similar to above, if you don't want to be tied in to a project that's been slow to adopt Py3k, rolling your own small toolset is a good option.<br/>
- If you want to fine tune things, you're in charge!    </p>
</div>
<span class="comment-copy">This <a href="https://mail.python.org/pipermail/python-list/2014-August/thread.html#676571" rel="nofollow noreferrer">thread</a> discusses basically the same idea. Not so much useful information though.</span>
<span class="comment-copy">So I guess nothing prevents adding a CoroutineExecutor to concurrent.futures? Do you agree it would be a good idea?</span>
<span class="comment-copy">@static_rtti It would be nice to have it. One small thing, which worries me is that this <code>CoroutineExecutor</code> expects coroutine function, while executors based on processes and threads expect normal function. Anyway it's much better to start such discussion in the <a href="https://www.python.org/community/irc/" rel="nofollow noreferrer">IRC</a> channel or <a href="https://www.python.org/community/lists/" rel="nofollow noreferrer">mailing list</a> to get feedback for your idea. I'm not involved in the Python development, so can't advice you much on where to start.</span>
<span class="comment-copy">You don't need to do the iteration on   <code>future = asyncio.gather(*(f(param) for param in iterable))</code> you can simply use <code>map</code>.    <code>future = asyncio.gather(*map(f, iterable))</code></span>
<span class="comment-copy">You can use an asyncio.Semaphore to limit the number of concurrent requests: <a href="https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html" rel="nofollow noreferrer">pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/â€¦</a></span>
<span class="comment-copy">I think you shouldn't use that code to test multiprocessing, it's unfair. You should use <code>Pool(CPU_CORE_NUMBER+1)</code> and <code>imap_unordered</code> .</span>
<span class="comment-copy">Thanks! Isn't gevent somewhat deprecated now that coroutines are integrated in the main python distribution? Or do they work well together?</span>
<span class="comment-copy">It isn't deprecated (as far as I know!) but it's not Guido's Favorite Way either, so what you heard may have been more to do with that. Still, it's performant, well-tested, easy as pie, and supports 3.3+ and 2.7+.</span>
<span class="comment-copy">Support for Python 3 is in beta at the moment.</span>
