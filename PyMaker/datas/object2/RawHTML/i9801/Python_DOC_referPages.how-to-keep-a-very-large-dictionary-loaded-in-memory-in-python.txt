<div class="post-text" itemprop="text">
<p>I have a very large dictionary of size ~ 200 GB which I need to query very often for my algorithm. To get quick results, I want to put it in memory which is possible, because fortunately I have a 500GB RAM. </p>
<p>However, my main issue is that I want to load it only once in memory and then let other processes query the same dictionary, rather than having to load it again everytime I create a new process or iterate over my code.</p>
<p>So, I would like something like this:</p>
<p>Script 1:</p>
<pre><code> # Load dictionary in memory
 def load(data_dir):
     dictionary = load_from_dir(data_dir) ... 
</code></pre>
<p>Script 2:  </p>
<pre><code> # Connect to loaded dictionary (already put in memory by script 1)
 def use_dictionary(my_query):
     query_loaded_dictionary(my_query) 
</code></pre>
<p>What's the best way to achieve this ? I have considered a rest API, but I wonder if going over a REST request will erode all the speed I gained by putting the dictionary in memory in the first place.</p>
<p>Any suggestions ?</p>
</div>
<div class="post-text" itemprop="text">
<p>Either run a separate service that you access with a REST API like you mentioned, or use an in-memory database. </p>
<p>I had a very good experience with <a href="http://redis.io/" rel="nofollow noreferrer">Redis</a> personally, but there are many others (<a href="https://memcached.org/" rel="nofollow noreferrer">Memcached</a> is also popular). Redis was easy to use with Python and Django.</p>
<p>In both solutions there can be data serialization though, so some performance will be dropped. There is a way to fill Redis with simple structures such as lists, but I haven't tried. I packed my numeric arrays and serialized them (with numpy), it was fast enough in the end. If you use simple string key-value pairs anyway, then the performance will be optimal, and maybe better with memcached.</p>
</div>
<span class="comment-copy">maybe use something like redis would be help.</span>
<span class="comment-copy">yes, redis is also a good idea. I was wondering if there is a way to do this in python. I earlier tried mongodb, but the latest wiredtiger version of mongodb, doesn't have an option to pre-load all the data in memory.  The problem is that I don't have root password, so I didn't want to install too many software. But let me try redis.</span>
<span class="comment-copy">Are you trying to re-invent the NOSQL databases? Why not rely on existing ones?</span>
<span class="comment-copy">Another possibility is to use a <code>shelve</code>: <a href="https://docs.python.org/3/library/shelve.html" rel="nofollow noreferrer">docs.python.org/3/library/shelve.html</a></span>
<span class="comment-copy">It's mainly because I already wrote a thread-safe code to quickly load and serve data from memory which was significantly faster than mongodb. I just wanted to find out if there was a strictly "python" way to re-use what I did without having to rewrite code for porting to a new database.</span>
