<div class="post-text" itemprop="text">
<p>I'm trying to remove values from files containing 3 dimensional arrays corresponding with the dimensions corresponding to <code>[time][long][lat]</code>.</p>
<p>I have separate files for each years worth of data.  I have a list of observed data points <code>T_obs</code> for a duration of time <code>start_time_index</code> to <code>end_time_index</code> that I want to compare to the mean value over that time period in the file data.</p>
<p>The data sets contained in the files are sufficiently large that my code is running very slowly and I want to optimize my execution time. The code I have currently is below. Are there any ways I could significantly save time?</p>
<pre><code>T_obs = [1.5, 3.6, 4.5]
start_time_index = [20, 300, 10]
end_time_index = [40, 328, 200]
long_obs = [45, 54, 180]
lat_obs = [34, 65, 32]
LE = np.zeros(len(T_obs))
t = 1984

for filename in os.listdir("C:\\Directory"):
    if filename.endswith(".nc"):
        print(filename)
        fh = Dataset("C:\\Directory %s"
                     % filename, 'r').variables['matrix']
        for i in range(0, len(long_obs)):
            if year[i] == t and start_time_index[i] &gt; 0:
                LE_t = []
                for x in range(int(start_time_index[i]), int(end_time_index[i])):
                    LE_t = np.append(LE_t,float(fh[x][long_obs[i]+180][lat_obs[i]*-1+90])/10)
                LE[i] = np.mean(LE_t)
        t += 1
        continue
    else:
        continue 
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Are you going to do this kind of work many times (with different values), or just once?</p>
<p>If the former, you can try to put your files data in a database (such as <a href="https://www.mysql.com/" rel="nofollow noreferrer">MySQL</a>, which is simple to setup) and create indexes on the start and end time. This would make your reads mighty faster, as you would not need to do a full table scan (which is pretty much what you are doing by reading the whole file).</p>
<p>If you are doing it just once, then I just suggest you wait it out. There is no trivial way to make I/O (which is your bottleneck) less expensive and it seems like to make your checks / compare the data, you actually need to go through the whole dataset.</p>
</div>
<span class="comment-copy">Some questions and remarks: 1) Consider using <a href="https://docs.python.org/3/library/os.path.html#os.path.join" rel="nofollow noreferrer">os.path.join</a> to build the path to you file, it's more robus than your solution. 2) Where is the variable <code>year</code> coming from? 3) What is the <code>Dataset</code> Class doing? 4) Have you tried using <a href="http://pandas.pydata.org/" rel="nofollow noreferrer">pandas</a> for this? - It's usually the right tool for this kind of job and faster in most cases. 5) You can remove the last three lines, they have no effect.</span>
<span class="comment-copy">consider turning this into a multi thread process? You are processing each file individually, so you could run 50+ of those simultaneously. look at the threading module</span>
<span class="comment-copy">If your code is otherwise working without any errors, a better place to ask could be <a href="https://codereview.stackexchange.com">Code Review</a>. Stack Overflow specializes in not-working code.</span>
<span class="comment-copy">You could define your <code>fh</code> variable before your loops. You may want to look into python generators and possibly some multi-threading. If you profile your code, you can find non-performant areas and write them in c++ instead. <a href="https://docs.python.org/3.4/extending/extending.html" rel="nofollow noreferrer">docs.python.org/3.4/extending/extending.html</a></span>
<span class="comment-copy">You can replace your loops with a nested list comprehension</span>
