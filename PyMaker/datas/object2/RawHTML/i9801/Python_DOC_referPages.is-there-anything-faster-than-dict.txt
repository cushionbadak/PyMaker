<div class="post-text" itemprop="text">
<p>I need a faster way to store and access around 3GB of <code>k:v</code> pairs. Where <code>k</code> is a <code>string</code> or an <code>integer</code> and <code>v</code> is an <code>np.array()</code> that can be of different shapes.
Is there any object, that is faster than the standard python dict in storing and accessing such a table? For example, <code>a pandas.DataFrame</code>?</p>
<p>As far I have understood python dict is a quite fast implementation of a hashtable, is there anything better than that for my specific case?</p>
</div>
<div class="post-text" itemprop="text">
<p>No there is nothing faster than a dictionary for this task and that’s because the complexity of its indexing and even membership checking is approximately O(1).</p>
<p>Once you saved your items in a dictionary, you can access them in a constant time. That said, the problem is not the indexing process. But you might be able to make the process slightly faster by doing some changes in your objects and their types. This might cause some optimizations in under the hood's operations. For example, if your strings (keys) are not very large you can intern them, in order to be cashed in memory rather than being created as an object. If the keys in a dictionary are interned, and the lookup key is interned, the key comparisons (after hashing) can be done by a pointer compare instead of a string compare. That makes the access to object very faster. Python has provided an <a href="https://docs.python.org/3/library/sys.html#sys.intern" rel="noreferrer"><code>intern()</code></a> function within <code>sys</code> module that you can use it for this aim.</p>
<blockquote>
<p>Enter string in the table of “interned” strings and return the interned string – which is string itself or a copy. Interning strings is useful to gain a little performance on <strong>dictionary lookup</strong>...</p>
</blockquote>
<p>Here is an example:</p>
<pre><code>In [49]: d = {'mystr{}'.format(i): i for i in range(30)}

In [50]: %timeit d['mystr25']
10000000 loops, best of 3: 46.9 ns per loop

In [51]: d = {sys.intern('mystr{}'.format(i)): i for i in range(30)}

In [52]: %timeit d['mystr25']
10000000 loops, best of 3: 38.8 ns per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>No, I don't think there is anything faster than <code>dict</code>. The time complexity of its index checking is <strong><code>O(1)</code></strong>.</p>
<pre><code>-------------------------------------------------------
Operation    |  Average Case  | Amortized Worst Case  |
-------------------------------------------------------
Copy[2]      |    O(n)        |       O(n)            | 
Get Item     |    O(1)        |       O(n)            | 
Set Item[1]  |    O(1)        |       O(n)            | 
Delete Item  |    O(1)        |       O(n)            | 
Iteration[2] |    O(n)        |       O(n)            | 
-------------------------------------------------------
</code></pre>
<p>PS <a href="https://wiki.python.org/moin/TimeComplexity" rel="nofollow noreferrer">https://wiki.python.org/moin/TimeComplexity</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You can think of storing them in Data structure like Trie given your key is string. Even to store and retrieve from Trie you need O(N) where N is maximum length of key. Same happen to hash calculation which computes hash for key. Hash is used to find and store in Hash Table. We often don't consider the hashing time or computation. </p>
<p>You may give a shot to Trie, Which should be almost equal performance, may be little bit faster( if hash value is computed differently for say </p>
<pre><code>HASH[i] = (HASH[i-1] + key[i-1]*256^i % BUCKET_SIZE ) % BUCKET_SIZE 
</code></pre>
<p>or something similar due to collision we need to use 256^i.</p>
<p>You can try to store them in Trie and see how it performs. </p>
</div>
<span class="comment-copy">If your using Python 3.5 or lower, then <a href="http://stackoverflow.com/questions/39980323/dictionaries-are-implemented-as-ordered-in-cpython-3-6">the dictionary built in in Python 3.6 is said to be 20-25% faster than the old dictionary builtin in Python 3.5</a>. So you may get better performance using The latest stable version of Python.</span>
<span class="comment-copy">Unless your code doesn't do anything else, I'd be quite surprised if dictionary access was your bottleneck.  Do you have profiling information showing this is the problem?</span>
<span class="comment-copy">I think dict are pretty fast. Instead of finding the alternative, you consider in optimizing rest of your code :)</span>
<span class="comment-copy">If your use case involved swapping -- if your data structure were larger than available RAM -- then there would be better answers, but it's not clear whether that's the case.</span>
<span class="comment-copy">@alec_djinn: if your code only loops over the dict, it's easy to make it faster -- remove the loop!  But if your code does something <i>inside</i> the loop (say printing, or finding the maximum of the value, or anything other than <code>pass</code>), then if that takes longer than the dictionary access (and it almost certainly will), improving dict access won't improve your net performance at all.  At this point, you're going to have to show some code if you want any real advice.</span>
<span class="comment-copy">While I think the OP is heading in the wrong direction, performance is about more than big-O.</span>
<span class="comment-copy">That is the kind of trick I was looking for! It did speed up my code a few percent. Thanks!</span>
<span class="comment-copy">Nice answer, but I think the example would be clearer if the values were interned strings too, rather than integers (but it works I guess because integers are interned within this range as an implementation detail)</span>
<span class="comment-copy">@Chris_Rands Indeed. They're smaller than 256 ;-)</span>
<span class="comment-copy">The first sentence is kinda misleading. The Big-O notation isn't about speed in general, it's about how the computation time changes when the size of the input changes. So, O(1) could be terribly slow as well as O(n!).</span>
<span class="comment-copy">true, but it doesn't mean it's gonna be <i>fast</i> or <i>faster</i> than O(anything) or even <i>the fastest</i> among all the <code>O</code>s possible. It's about <i>how the amount of work changes when the input size changes</i>.</span>
<span class="comment-copy">...which is to say: In the real world, constant factors matter. An O(1) process that takes a constant-time 1000ms will often be a worse choice than an O(n) process that takes 1ns per item of input.</span>
<span class="comment-copy">O(1) means "constant time", which means "whatever input is, the computation time is the same". Sudoku solving <i>is</i> constant time if we just examine all possibilities, but it's way more slower than many algorithms.</span>
