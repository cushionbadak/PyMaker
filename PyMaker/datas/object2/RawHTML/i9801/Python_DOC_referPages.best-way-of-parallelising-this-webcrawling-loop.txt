<div class="post-text" itemprop="text">
<p>I am making a webcrawler, and I have some "sleep" functions that make the crawl quite long.
For now I am doing : </p>
<pre><code>for speciality in range(1,25):
    for year in range(1997, 2017):
        for quarter in [1,2]:
            deal_with (driver, year, quarter, speciality, ok)
</code></pre>
<p>The <code>deal_with</code> function is opening several webpages, waiting a few second for complete html download before moving on. The execution time is then very long : there is 25 * 10 * 2 = 500 loops, with no less than a minute by loop.</p>
<p>I would like to use my 4 physical Cores (8 threads) to enjoy parallelism.
I read about tornado, multiprocessing, joblib... and can't really make my mind on an easy solution to adapt to my code.</p>
<p>Any insight welcome :-)</p>
</div>
<div class="post-text" itemprop="text">
<p><em>tl;dr</em> <strong>Investing in any choice without fully understanding the bottlenecks you are facing will not help you.</strong></p>
<p>At the end of the day, there are only two fundamental approaches to scaling out a task like this:</p>
<h2>Multiprocessing</h2>
<p>You launch a number of Python processes, and distribute tasks to each of them. This is the approach you think will help you right now. </p>
<p>Some sample code for how this works, though you could use any appropriate wrapper:</p>
<pre><code>import multiprocessing

# general rule of thumb: launch twice as many processes as cores

process_pool = multiprocessing.Pool(8) # launches 8 processes

# generate a list of all inputs you wish to feed to this pool

inputs = []

for speciality in range(1,25):
    for year in range(1997, 2017):
        for quarter in [1,2]:
            inputs.append((driver, year, quarter, speciality, ok))

# feed your list of inputs to your process_pool and print it when done
print(process_pool.map(deal_with, inputs))
</code></pre>
<p>If this is all you wanted, you can stop reading now. </p>
<h2>Asynchronous Execution</h2>
<p>Here, you are content with a single thread or process, but you don't want it to be sitting idle waiting for stuff like network reads or disk seeks to come back - you want it to go on and do other, more important things while it's waiting.</p>
<p>True native asynchronous I/O support is provided in Python 3 and does not exist in Python 2.7 outside of the Twisted networking library. </p>
<pre><code>import concurrent.futures

# generate a list of all inputs you wish to feed to this pool

inputs = []

for speciality in range(1,25):
    for year in range(1997, 2017):
        for quarter in [1,2]:
            inputs.append((driver, year, quarter, speciality, ok))

# produce a pool of processes, and make sure they don't block each other
# - get back an object representing something yet to be resolved, that will
# only be updated when data comes in.

with concurrent.futures.ProcessPoolExecutor() as executor:
    outputs = [executor.submit(input_tuple) for input_tuple in inputs]

    # wait for all of them to finish - not ideal, since it defeats the purpose
    # in production, but sufficient for an example

    for future_object in concurrent.futures.as_completed(outputs):
         # do something with future_object.result()
</code></pre>
<h1>So What's the Difference?</h1>
<p>My main point here it to emphasise that choosing from a list of technologies isn't as hard as figuring out where the real bottleneck is.</p>
<p>In the examples above, <strong>there isn't any difference</strong>. Both follow a simple pattern:</p>
<ol>
<li>Have a lot of workers</li>
<li>Allow these workers to pick something from a queue of tasks right away</li>
<li>When one is free, set them to work on the next one right away.</li>
</ol>
<p>Thus, you gain no conceptual difference altogether if you follow these examples verbatim, even though they use entirely different technologies and claim to use entirely different techniques. </p>
<p>Any technology you pick will be for naught if you write it in this pattern - even though you'll get some speedup, you will be sorely disappointed if you expected a massive performance boost.</p>
<p><strong>Why is this pattern bad?</strong> Because it doesn't solve your problem.</p>
<p>Your problem is simple: you have <em>wait</em>. While your process is waiting for something to come back, it can't do anything else! It can't call more pages for you. It can't process an incoming task. All it can do is wait.  </p>
<p>Having more processes that ultimately wait is not the true solution. An army of troops that has to march to Waterloo will not be faster if you split it into regiments - each regiment eventually has to sleep, though they may sleep at different times and for different lengths, and what will happen is that all of them will arrive at almost roughly the same time. </p>
<p>What you <em>need</em> is an army that never sleeps.</p>
<h2>So What Should You Do?</h2>
<p>Abstract <em>all</em> I/O bound tasks into something <em>non-blocking</em>. This is your true bottleneck. If you're waiting for a network response, don't let the poor process just sit there - give it something to do. </p>
<p>Your task is made somewhat difficult in that by default reading from a socket is blocking. It's the way operating systems are. Thankfully, you <em>don't</em> need to get Python 3 to solve it (though that is always the preferred solution) - the <a href="https://docs.python.org/2/library/asyncore.html" rel="nofollow noreferrer">asyncore</a> library (<a href="https://stackoverflow.com/a/4385667/2271269">though Twisted is comparably superior in every way</a>) already exists in Python 2.7 to make network reads and writes truly in the background. </p>
<p>There is one and only one case where true multiprocessing needs to be used in Python, and that's if you are doing CPU-bound or CPU-intensive work. From your description, it doesn't sound like that's the case.</p>
<p>In short, you should edit your <code>deal_with</code> function to avoid the incipient wait. Make that wait in the background, if needed, using a suitable abstraction from Twisted or asyncore. But don't make it consume your process completely. </p>
</div>
<div class="post-text" itemprop="text">
<p>If you're using python3, I would check out the <a href="https://docs.python.org/3/library/asyncio-task.html" rel="nofollow noreferrer">asycio module</a>. I believe you can just decorate <code>deal_with</code> with <code>@asyncio.coroutine</code>. You will likely have to adjust what <code>deal_with</code> does to properly work with the event loop as well.</p>
</div>
<span class="comment-copy">You don't really need many CPUs to handle this kind of workload, since web crawling is mostly I/O bound. Try using Tornado</span>
<span class="comment-copy">Well, he could be doing text mining which can be CPU bound.</span>
<span class="comment-copy">For now my solution is to have several notebooks open, each taking care of a subset of the speciality range...</span>
<span class="comment-copy">The easiest would probably be using the <code>thread</code> module to download more than one webpage at the same time (assuming your web connection has the bandwidth).</span>
<span class="comment-copy">Where is your sleep? Inside the <code>deal_with</code> method? Would a solution to move to one loop help?</span>
<span class="comment-copy">Great answer ! thanks :)</span>
<span class="comment-copy">I have to say I am in python 2 :-/</span>
<span class="comment-copy">You can always <a href="https://docs.python.org/2/library/2to3.html" rel="nofollow noreferrer">port</a> your code to 3.</span>
<span class="comment-copy">yeah.... I have to kick my self to do so :-s</span>
