<div class="post-text" itemprop="text">
<p>I am using Celery with RabbitMQ to process data from API requests. The process goes as follows:</p>
<p>Request -&gt; API -&gt; RabbitMQ -&gt; Celery Worker -&gt; Return</p>
<p>Ideally I would spawn more celery workers but I am restricted by memory constraints. </p>
<p>Currently, the bottleneck in my process is fetching and downloading the data from the URLs passed into the worker. Roughy, the process looks like this:</p>
<pre><code>celery_gets_job(url):
    var data = fetches_url(url) # takes 0.1s to 1.0s (bottleneck)
    var result = processes_data(data) # takes 0.1ss
    return result
</code></pre>
<p>This is unacceptable as the worker is locked up for a while while fetching the URL. I am looking at improving this through threading, but I am unsure what the best practices are:</p>
<ul>
<li><p>Is there a way to make the celery worker download the incoming data asynchronously while processing the data at the same time in a different thread?</p></li>
<li><p>Should I have separate workers fetching and processing, with some form of message passing, possibly via RabbitMQ?</p></li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>Using the <a href="http://eventlet.net/doc/patching.html" rel="nofollow noreferrer"><code>eventlet</code></a> library, you can patch the standard libraries for making them asynchronous.</p>
<p>First import the async urllib2:</p>
<pre><code>from eventlet.green import urllib2
</code></pre>
<p>So you will get the url body with:</p>
<pre><code>def fetch(url):
    body = urllib2.urlopen(url).read()
    return body
</code></pre>
<p>See more <code>eventlet</code> examples <a href="http://eventlet.net/doc/examples.html" rel="nofollow noreferrer">here</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>I would create two tasks, one for downloading the data and the other for processing it once it is downloaded. This way you could scale the two tasks independently. See: <a href="http://docs.celeryproject.org/en/latest/getting-started/next-steps.html#routing" rel="nofollow noreferrer">Routing</a>, <a href="https://celery.readthedocs.io/en/latest/userguide/canvas.html#chains" rel="nofollow noreferrer">Chains</a>.</p>
</div>
<span class="comment-copy">You can consider using something like <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe" rel="nofollow noreferrer">multiprocessing pipes</a>  within celery task by creating two multiprocesses. Ofcourse your multiprocessing processes should be restriced by pool. Sharing fetched url's large data over rabbitmq/result backend would not be good idea if I am not wrong. Celery low level api's can also have some similar kind of functionality.</span>
<span class="comment-copy">I am not aware of RabbitMQ but what I think is multiprocessing will be more suitable for you than multithreading as <code>celery_gets_job</code> has multiple non-atomic operations and this will create problems while using multithreading. You can use Queue where data is populated by pool of processes running <code>fetches_url(url)</code> and another process(es) to carry out <code>processes_data(data)</code></span>
<span class="comment-copy">This may be what you are looking for: <a href="http://stackoverflow.com/questions/28315657/celery-eventlet-non-blocking-requests" title="celery eventlet non blocking requests">stackoverflow.com/questions/28315657/…</a></span>
<span class="comment-copy">This post <a href="https://news.ycombinator.com/item?id=11889549" rel="nofollow noreferrer">news.ycombinator.com/item?id=11889549</a> by the creator of Celery may be what you are looking for.</span>
<span class="comment-copy">Also, directly using the eventlet execution pools <a href="http://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html" rel="nofollow noreferrer">docs.celeryproject.org/en/latest/userguide/concurrency/…</a> should automatically monkey patch io calls.</span>
<span class="comment-copy">But then wouldn't <code>processes_data(data)</code> still block and make the combined result slower than before?</span>
