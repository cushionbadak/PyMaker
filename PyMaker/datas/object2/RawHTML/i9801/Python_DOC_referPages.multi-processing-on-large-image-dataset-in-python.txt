<div class="post-text" itemprop="text">
<p>I have a very large image dataset (&gt;50G, single images in a folder) for training, to make loading of images more efficient, I firstly load parts of the images onto RAM and then send small batches to GPU for training. </p>
<p>I want to further speed up the data preparation process <strong><em>before</em></strong> feeding the images to the GPU and was thinking about multi-processing. But I'm not sure how should I do it, any ideas?</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>For speed I would advise to used HDF5 or LMDB:</strong></p>
<p>I have successfully used  <a href="https://github.com/vicolab/ml-pyxis" rel="nofollow noreferrer">ml-pyxis</a> for creating deep learning datasets using LMDBs.</p>
<p>It allows to create binary blobs (LMDB) and they can be read quite fast.
The link above comes with some simple examples on how to create and read the data. Including python generators/ iteratos</p>
<p><strong>For multi-processing:</strong></p>
<p>I personally work with Keras, and by using a python generator it is possible train with mutiple-processing for data using the fit_generator method.</p>
<pre><code>fit_generator(self, generator, samples_per_epoch,
              nb_epoch, verbose=1, callbacks=[],
              validation_data=None, nb_val_samples=None,
              class_weight={}, max_q_size=10, nb_worker=1,
              pickle_safe=False)
</code></pre>
<p>Fits the model on data generated batch-by-batch by a Python generator. The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU. You can find the <a href="https://github.com/fchollet/keras/blob/master/keras/models.py#L817" rel="nofollow noreferrer">source code here</a> , and the <a href="https://keras.io/models/model/#methods" rel="nofollow noreferrer">documentation here</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>Don't know whether you prefer tensorflow/keras/torch/caffe whatever. </p>
<p>Multiprocessing is simply <a href="https://github.com/Theano/Theano/wiki/Using-Multiple-GPUs" rel="nofollow noreferrer"><strong>Using Multiple GPUs</strong></a></p>
<p>Basically you are trying to leverage more hardware by delegating or spawning one child process for every GPU and let them do their magic. The example above is for <strong>Logistic Regression</strong>.</p>
<p>Of course you would be more keen on looking into Convnets - 
This <a href="http://www.hpc.lsu.edu/training/weekly-materials/2016-Fall/machine_learning_qb2_fall_2016.pdf" rel="nofollow noreferrer"><strong>LSU Material</strong></a> (Pgs 48-52[Slides 11-14]) builds some intuition</p>
<p>Keras is yet to officially provide support but you can <a href="https://github.com/fchollet/keras/pull/3582" rel="nofollow noreferrer">"proceed at your own risk"</a></p>
<p>For multiprocessing, tensorflow is a better way to go about this (my opinion)
In fact they have some good <strong><a href="https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#training-a-model-using-multiple-gpu-cards" rel="nofollow noreferrer">documentation</a></strong> on it too</p>
</div>
<span class="comment-copy">How about multiple processes loading images into a <a href="https://docs.python.org/3/library/collections.html#collections.deque" rel="nofollow noreferrer">collections.deque</a> and a single process pulling from the deque and feeding the gpu?   <a href="https://docs.python.org/3/library/queue.html#module-queue" rel="nofollow noreferrer">docs.python.org/3/library/queue.html#module-queue</a>??</span>
<span class="comment-copy"><a href="https://www.youtube.com/watch?v=8moSZM8GnIk" rel="nofollow noreferrer">Consumer/Producer Queues in Python - youtube</a></span>
<span class="comment-copy">I just watched and practiced <a href="https://youtu.be/Bv25Dwe84g0" rel="nofollow noreferrer">Thinking about Concurrency, Raymond Hettinger</a> - it is worthwhile and should get you started.</span>
<span class="comment-copy">thanks for the reply. I'm also using fit_generator in Keras by feeding a data_generator. I find that changing the number of works doesn't make a difference at all...</span>
<span class="comment-copy">@GoC 1 . Are you reading from files or from a binary blob  or are you reading images independently? 2. Are you feeding it as generator or data iterator ? I found that it worked fine for me when I was working with LMDBs.and with data generators. 3. Can you check the the time that is taking to read each batch on your process ? A simple time print inside you generator would help a lot.</span>
<span class="comment-copy">@GoC. If yopu please reply the comment we can try to fix your problem.</span>
<span class="comment-copy">I'm reading single images in one folder and using a data generator. I'll try using LMDB tomorrow and let you know, thanks a lot for your help!</span>
<span class="comment-copy">Yes. Binarys blobs like LMDB and HDF5 are much faster for reading.  One because you loose the overhead of opening many files, two because the data location is stored in look up tables. My suggestion for speeding up. When creating a dataset, shuffle the data before enrolling it to the dataset. This way you can use a sequential reader ( python generator). This would avoid you doing seek comands with your hardrive. These is the kind of solution used in Caffe and Digits.</span>
<span class="comment-copy">Thanks for your reply, I think you may misunderstood my point, I just want to further speed up the data preparation process <b><i>before</i></b> feeding the images to the GPU.</span>
<span class="comment-copy">what exactly do you mean by data preparation? Can you list the steps, be a little specific?</span>
