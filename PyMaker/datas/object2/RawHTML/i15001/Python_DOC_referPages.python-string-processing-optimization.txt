<div class="post-text" itemprop="text">
<p>So lately I've been making a python script for extracting data from a large text files ( &gt; 1 GB ). The problem basically sums up to selecting lines of text from the file, and searching them for strings from some array ( this array can have as many as 1000 strings in it). The problem here is that i have to find a specific occurrence of the string, and the string may appear unlimited number of times in that file. Also, some decoding and encoding is required, which additionally slows the script down. 
Code looks something like this:</p>
<pre><code>strings = [a for a in open('file.txt')]

with open("er.txt", "r") as f:
    for chunk in f:
        for s in strings
            #do search, trimming, stripping ..
</code></pre>
<p>My question here is:
Is there a way to optimize this? I tried multiprocessing, but it helps little ( or at least the way i implemented it ) The problem here is that these chunk operations aren't independent and <code>strings</code> list may be altered during one of them. 
Any optimization would help (string search algorithms, file reading etc.) I did as much as i could regarding loop breaking, but it still runs pretty slow. </p>
</div>
<div class="post-text" itemprop="text">
<p>If you can know exactly how the string is encoded in binary (ASCII, UTF-8), you can <a href="https://docs.python.org/3/library/mmap.html" rel="nofollow noreferrer"><code>mmap</code></a> the <em>entire</em> file into memory at a time; it would behave exactly like a large <code>bytearray/bytes</code> (or <code>str</code> in Python 2) obtained by <code>file.read()</code>  would; then such a <code>mmap</code> object would be searchable by a <code>str</code> regular expression (Python 2), or <code>bytes</code> regular expression (Python 3).</p>
<p>The <code>mmap</code> is the fastest solution on many operating systems, because the read-only mapping means that the OS can freely map in the pages as they're ready; no swap space is required, because the data is backed by a file. The OS can also directly map the data from the buffer cache with zero copying - thus a win-win-win over bare reading.</p>
<p>Example:</p>
<pre><code>import mmap
import re

pattern = re.compile(b'the ultimate answer is ([0-9]+)')
with open("datafile.txt", "rb") as f:
    # memory-map the file, size 0 means whole file
    mm = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)

    # PROT_READ only on *nix as the file is not writable
    for match in pattern.finditer(mm):
        # process match
        print("The answer is {}".format(match.group(1).decode('ascii')))

    mm.close()
</code></pre>
<p>Now, if the <code>datafile.txt</code> contained the text:</p>
<pre><code>the ultimate answer is 42
</code></pre>
<p>somewhere along the 1 gigabyte of data, this program would be among the fastest python solutions to spit out:</p>
<pre><code>The answer is 42
</code></pre>
<p>Notice that <a href="https://docs.python.org/3/library/re.html#re.RegexObject.finditer" rel="nofollow noreferrer"><code>pattern.finditer</code></a> also accepts <code>start</code> and <code>end</code> parameters that can used to limit the range where the match is attempted.</p>
<hr/>
<p>As noted by <a href="https://stackoverflow.com/users/648265/ivan-pozdeev">ivan_pozdeev</a>, this requires 1 gigabyte of free virtual address space for mapping a gigabyte file (but not necessarily 1 gigabyte of RAM), which might be difficult in a 32-bit process but can almost certainly be assumed a "no-problem" on 64-bit operating system and CPUs. On 32-bit processes the approach still works, but you need to map big files in smaller chunks - thus now the bits of the operating system and processor truly matter.</p>
</div>
<div class="post-text" itemprop="text">
<p>Think about calling an external process (grep and the like) to speed up processing and reducing the data volume you have to handle within Python.</p>
<p>Another route to go would be to filter or pre-filter your data with a compiled regex, since then your inner loop uses the optimized code of the standard library.</p>
<p>You could also try Cython or similar for the hot inner loops, see e.g. <a href="https://books.google.de/books?id=QSsOBQAAQBAJ&amp;dq=high+perf+python&amp;hl=en" rel="nofollow">https://books.google.de/books?id=QSsOBQAAQBAJ&amp;dq=high+perf+python&amp;hl=en</a> for details on that.</p>
</div>
<span class="comment-copy">Wow. This did an amazing job :D You rock!!</span>
<span class="comment-copy">Note that you have to have a large enough chunk of free, contiguous address space. Getting a chunk of 1GB is a problem in x32, <a href="http://stackoverflow.com/questions/19016512/c-mapviewoffile-fails" title="c mapviewoffile fails">stackoverflow.com/questions/19016512/c-mapviewoffile-fails</a> is a live example.</span>
<span class="comment-copy">Yes, if you are doing big data analysis, better to get a 64-bit computer, 64-bit operating system and 64-bit Python :)</span>
