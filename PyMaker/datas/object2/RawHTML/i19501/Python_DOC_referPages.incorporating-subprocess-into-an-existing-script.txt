<div class="post-text" itemprop="text">
<p>I have an existing python script, which I'd like to now revise to have more threads (subprocesses) running. For the purpose of this example, let's say revised to run 3 threads at the same time. </p>
<p>Incidentally the script is nothing more than generating a client request to a web server and measuring the response time. </p>
<pre><code>#!/usr/bin/python26

from library.rpc.client import EllisClient

ec = EllisClient(ellis_user='fred', ellis_pass='flintstone')
params={'domain_name':'alestel.com','mig_name':'terramexico2'}


def test_response():
    L = []
    L = ec.get_full_domain(params)

if __name__ == '__main__':
    from timeit import Timer

    t = Timer("test_response()", "from __main__ import test_response")
    print t.timeit(number=10)
</code></pre>
<p>As a relative noob, the documentation isn't quite clear to me. Any suggestions would be appreciated.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you want explicit control over the processes you're running, you want <a href="http://docs.python.org/2/library/multiprocessing.html#the-process-class" rel="nofollow"><code>multiprocessing.Process</code></a>:</p>
<pre><code>def test_3_parallel_responses():
    procs = [multiprocess.Process(target=test_response) for _ in range(3)]
    for proc in procs:
        proc.start()
    for proc in procs:
        proc.join()
</code></pre>
<p>That's all there is to it.</p>
<p>There are various differences between threads and processes, but the big one is that you don't get to implicitly share values between processes; you have to pass them around (through the startup <code>args</code> and return value, or through a <code>Queue</code>, or some external means like a socket or pipe) or explicitly share them (through a <code>Value</code> or <code>Array</code>, or some external means like an file).</p>
<p>For a more realistic use case, you usually don't want to directly control what the processes are doing; you want to create a pool of processes, and just queue up jobs to get done by whichever process is free next. For that, you want either <a href="http://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.pool" rel="nofollow"><code>multiprocessing.Pool</code></a> or <a href="http://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor" rel="nofollow"><code>concurrent.futures.ProcessPoolExecutor</code></a>. The latter is a bit simpler, but requires Python 3.2 or a third-party library, so I'll show the former:</p>
<pre><code>def test_3_pooled_responses():
    pool = multiprocessing.Pool(3)
    for i in range(3):
        pool.apply(test_response)
    pool.close()
    pool.join()
</code></pre>
<p>More commonly, you want to actually pass parameters to the function. In the simplest case, this actually makes things even simplerâ€”if you can write the sequential version as a list comprehension or <code>map</code> call, you can write the parallel version as a <code>pool.map</code> call. Let's say you had a <code>test_response(host)</code> call that returns some value, and you wanted to run it on <code>host1</code>, <code>host2</code>, and <code>host3</code>:</p>
<pre><code>def test_3_pooled_responses():
    pool = multiprocessing.Pool(3)
    responses = pool.map(test_response, ['host1', 'host2', 'host3'])
    pool.close()
    pool.join()
</code></pre>
</div>
<span class="comment-copy">Note that a (sub)process is not the same as a thread...</span>
<span class="comment-copy">If you want to use subprocesses almost as if they were threads, use the <code>multiprocessing</code> module. (The <code>subprocess</code> module is for running other programs, not for parallelizing your own program.)</span>
<span class="comment-copy">thanks for this information and examples, as it was very helpful</span>
