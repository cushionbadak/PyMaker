<div class="post-text" itemprop="text">
<p>I want to generate an ordered list of the least common words within a large body of text, with the least common word appearing first along with a value indicating how many times it appears in the text.</p>
<p>I scraped the text from some online journal articles, then simply assigned and split;</p>
<pre><code>article_one = """ large body of text """.split() 
=&gt; ("large","body", "of", "text")
</code></pre>
<p>Seems like a regex would be appropriate for the next steps, but being new to programming I'm not well versed-
If the best answer includes a regex, could someone point me to a good regex tutorial other than pydoc? </p>
</div>
<div class="post-text" itemprop="text">
<p><a href="http://docs.python.org/2/library/collections.html#counter-objects" rel="nofollow noreferrer">ready made answer</a> from the mothership.</p>
<pre><code># From the official documentation -&gt;&gt;
&gt;&gt;&gt; # Tally occurrences of words in a list
&gt;&gt;&gt; cnt = Counter()
&gt;&gt;&gt; for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:
...     cnt[word] += 1
&gt;&gt;&gt; cnt
Counter({'blue': 3, 'red': 2, 'green': 1})
## ^^^^--- from the standard documentation.

&gt;&gt;&gt; # Find the ten most common words in Hamlet
&gt;&gt;&gt; import re
&gt;&gt;&gt; words = re.findall('\w+', open('hamlet.txt').read().lower())
&gt;&gt;&gt; Counter(words).most_common(10)
[('the', 1143), ('and', 966), ('to', 762), ('of', 669), ('i', 631),
 ('you', 554),  ('a', 546), ('my', 514), ('hamlet', 471), ('in', 451)]

&gt;&gt;&gt; def least_common(adict, n=None):
.....:       if n is None:
.....:               return sorted(adict.iteritems(), key=itemgetter(1), reverse=False)
.....:       return heapq.nsmallest(n, adict.iteritems(), key=itemgetter(1))
</code></pre>
<p>Obviously adapt to suite :D</p>
</div>
<div class="post-text" itemprop="text">
<p>How about a shorter/simpler version with a defaultdict, Counter is nice but needs Python 2.7, this works from 2.5 and up :)</p>
<pre><code>import collections

counter = collections.defaultdict(int)
article_one = """ large body of text """

for word in article_one.split():
    counter[word] += 1

print sorted(counter.iteritems(), key=lambda x: x[::-1])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Finding least common elements in a list. According to Counter class in <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="nofollow">Collections module</a></p>
<pre><code>c.most_common()[:-n-1:-1]       # n least common elements
</code></pre>
<p>So Code for least common element in list is</p>
<pre><code>from collections import Counter
Counter( mylist ).most_common()[:-2:-1]
</code></pre>
<p>Two least common elements is </p>
<pre><code>from collections import Counter
Counter( mylist ).most_common()[:-3:-1]
</code></pre>
<p><a class="post-tag" href="/questions/tagged/python-3.x" rel="tag" title="show questions tagged 'python-3.x'">python-3.x</a></p>
</div>
<div class="post-text" itemprop="text">
<p>This uses a slightly different approach but it appears to suit your needs. Uses code from <a href="https://stackoverflow.com/a/613218/392143">this answer</a>.</p>
<pre><code>#!/usr/bin/env python
import operator
import string

article_one = """A, a b, a b c, a b c d, a b c d efg.""".split()
wordbank = {}

for word in article_one:
    # Strip word of punctuation and capitalization
    word = word.lower().strip(string.punctuation)
    if word not in wordbank:
        # Create a new dict key if necessary
        wordbank[word] = 1
    else:
        # Otherwise, increment the existing key's value
        wordbank[word] += 1

# Sort dict by value
sortedwords = sorted(wordbank.iteritems(), key=operator.itemgetter(1))

for word in sortedwords:
    print word[1], word[0]
</code></pre>
<p>Outputs:</p>
<pre><code>1 efg
2 d
3 c
4 b
5 a
</code></pre>
<p>Works in Python &gt;= 2.4, and Python 3+ if you parenthesize the <code>print</code> statement at the bottom and change <code>iteritems</code> to <code>items</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you need a fixed number of least-common words, e.g., the 10 least common, you probably want a solution using a counter <code>dict</code> and a <code>heapq</code>, as suggested by sotapme's answer (with WoLpH's suggestion) or WoLpH's answer:</p>
<pre><code>wordcounter = collections.Counter(article_one)
leastcommon = word counter.nsmallest(10)
</code></pre>
<p>However, if you need an unbounded number of them, e.g., all words with fewer than 5 appearances, which could be 6 in one run and 69105 in the next, you might be better of just sorting the list:</p>
<pre><code>wordcounter = collections.Counter(article_one)
allwords = sorted(wordcounter.items(), key=operator.itemgetter(1))
leastcommon = itertools.takewhile(lambda x: x[1] &lt; 5, allwords)
</code></pre>
<p>Sorting takes longer than heapifying, but extracting the first M elements is a lot faster with a <code>list</code> than a <code>heap</code>. Algorithmically, the difference is just some <code>log N</code> factors, so the constants are going to be important here. So the best thing to do is test.</p>
<p>Taking my <a href="http://pastebin.com/0JntxRbM" rel="nofollow">code at pastebin</a>, and a file made by just doing <code>cat reut2* &gt;reut2.sgm</code> on the <a href="http://www.daviddlewis.com/resources/testcollections/reuters21578/" rel="nofollow">Reuters-21578</a> corpus (without processing it to extract the text, so this is obviously not very good for serious work, but should be fine for benchmarking, because none of the SGML tags are going to be in the least common…):</p>
<pre><code>$ python leastwords.py reut2.sgm # Apple 2.7.2 64-bit
heap: 32.5963380337
sort: 22.9287009239
$ python3 leastwords.py reut2.sgm # python.org 3.3.0 64-bit
heap: 32.47026552911848
sort: 25.855643508024514
$ pypy leastwords.py reut2.sgm # 1.9.0/2.7.2 64-bit
heap: 23.95291996
sort: 16.1843900681
</code></pre>
<p>I tried various ways to speed up each of them (including: <code>takewhile</code> around a genexp instead of a loop around <code>yield</code> in the heap version, popping optimistic batches with <code>nsmallest</code> and throwing away any excess, making a <code>list</code> and sorting in place, decorate-sort-undecorate instead of a key, <code>partial</code> instead of <code>lambda</code>, etc.), but none of them made more than 5% improvement (and some made things significantly slower).</p>
<p>At any rate, these are closer than I expected, so I'd probably go with whichever one is simpler and more readable. But I think sort beats heap there, as well, so…</p>
<p>Once again: If you just need the N least common, for reasonable N, I'm willing to bet without even testing that the heap implementation will win.</p>
</div>
<span class="comment-copy">Are you looking for something like "the 10 least common words", or for something like "all words with fewer than 5 appearances" (which could be 3 in one run, 69105 in another)? I ask because <code>heapq.nsmallest</code> is probably your best bet for the former, but a heap may not be as good for the latter.</span>
<span class="comment-copy">This requires python 2.7, which is fine see this post if you have to use &lt;2.7. <a href="http://stackoverflow.com/questions/2161752/how-to-count-the-frequency-of-the-elements-in-a-list" title="how to count the frequency of the elements in a list">stackoverflow.com/questions/2161752/…</a></span>
<span class="comment-copy">Am I missing something?  I thought OP wanted the <b>least common</b> ...</span>
<span class="comment-copy">Well I did say <b>adapt to suite</b>:( - Although they seem to have forgotten about <code>least_commom()</code> :((</span>
<span class="comment-copy">If you look at the <a href="http://hg.python.org/cpython/file/2.7/Lib/collections.py" rel="nofollow noreferrer">source</a> for <code>most_common</code>, it's pretty easy to see how to adapt it for a <code>least_common</code>.   (I believe they use <code>heapq</code>)</span>
<span class="comment-copy">@sotapme shorter version: <code>collections.Counter(article_one.split())</code></span>
<span class="comment-copy">If you truely need the 10 least common and don't care about the order of the others, you could probably work with <code>heapq</code> to get the minimum elements in O(N) rather than O(NlogN).</span>
<span class="comment-copy">@mgilson: wouldn't you still need to store all of them? Otherwise you don't know which are and which aren't in there.</span>
<span class="comment-copy">Yeah, you still need the Counter or defaultdict as you have here.  I'm just proposing an alternative to the last line (<code>sorted</code>)</span>
<span class="comment-copy">Ah... now I understand what you meant, wouldn't adding them to the heapq still be <code>n</code> times a <code>log n</code> operation? Depends on the variant of course: <a href="http://en.wikipedia.org/wiki/Heap_(data_structure)#Comparison_of_theoretic_bounds_for_variants" rel="nofollow noreferrer">en.wikipedia.org/wiki/…</a></span>
<span class="comment-copy">Shouldn't sort be <code>print sorted(counter.iteritems(),  key=itemgetter(1))</code> to sort by <code>count</code> and not <code>word</code>.</span>
<span class="comment-copy">I believe you might want to do <code>article_one.split()</code> instead, you're counting letters instead of words now :)</span>
<span class="comment-copy">WoLpH: <code>article_one</code> is already being <code>.split()</code> upon creation.</span>
<span class="comment-copy">I missed that part, simply looked at the output. I stand corrected :)</span>
