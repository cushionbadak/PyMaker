<div class="post-text" itemprop="text">
<p>I have code like this.</p>
<pre><code>for p in range(1,1000):
    result = False
    while result is False:
        ret = urllib2.Request('http://server/?'+str(p))
        try:
            result = process(urllib2.urlopen(ret).read())
        except (urllib2.HTTPError, urllib2.URLError):
            pass
    results.append(result)
</code></pre>
<p>I would like to make two or three request at the same time to accelerate this. Can I use urllib2 for this, and how? If not which other library should I use? Thanks.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use asynchronous IO to do this.</p>
<p><a href="https://github.com/kennethreitz/requests">requests</a> + <a href="http://www.gevent.org/">gevent</a> = <a href="https://github.com/kennethreitz/grequests">grequests</a></p>
<p>GRequests allows you to use Requests with Gevent to make asynchronous HTTP Requests easily.</p>
<pre><code>import grequests

urls = [
    'http://www.heroku.com',
    'http://tablib.org',
    'http://httpbin.org',
    'http://python-requests.org',
    'http://kennethreitz.com'
]

rs = (grequests.get(u) for u in urls)
grequests.map(rs)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Take a look at <a href="http://www.gevent.org/" rel="noreferrer">gevent</a> â€” a coroutine-based Python networking library that uses greenlet to provide a high-level synchronous API on top of libevent event loop.</p>
<p>Example:</p>
<pre><code>#!/usr/bin/python
# Copyright (c) 2009 Denis Bilenko. See LICENSE for details.

"""Spawn multiple workers and wait for them to complete"""

urls = ['http://www.google.com', 'http://www.yandex.ru', 'http://www.python.org']

import gevent
from gevent import monkey

# patches stdlib (including socket and ssl modules) to cooperate with other greenlets
monkey.patch_all()

import urllib2


def print_head(url):
    print 'Starting %s' % url
    data = urllib2.urlopen(url).read()
    print '%s: %s bytes: %r' % (url, len(data), data[:50])

jobs = [gevent.spawn(print_head, url) for url in urls]

gevent.joinall(jobs)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>So, it's 2016 ðŸ˜‰ and we have Python 3.4+ with built-in <a href="https://docs.python.org/3/library/asyncio.html" rel="noreferrer">asyncio</a> module for asynchronous I/O. We can use <a href="http://aiohttp.readthedocs.io/en/stable/" rel="noreferrer">aiohttp</a> as HTTP client to download multiple URLs in parallel.</p>
<pre><code>import asyncio
from aiohttp import ClientSession

async def fetch(url):
    async with ClientSession() as session:
        async with session.get(url) as response:
            return await response.read()

async def run(loop, r):
    url = "http://localhost:8080/{}"
    tasks = []
    for i in range(r):
        task = asyncio.ensure_future(fetch(url.format(i)))
        tasks.append(task)

    responses = await asyncio.gather(*tasks)
    # you now have all response bodies in this variable
    print(responses)

loop = asyncio.get_event_loop()
future = asyncio.ensure_future(run(loop, 4))
loop.run_until_complete(future)
</code></pre>
<p>Source: copy-pasted from <a href="http://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html" rel="noreferrer">http://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html</a></p>
</div>
<div class="post-text" itemprop="text">
<p>I know this question is a little old, but I thought it might be useful to promote another async solution built on the requests library.</p>
<pre class="lang-py prettyprint-override"><code>list_of_requests = ['http://moop.com', 'http://doop.com', ...]

from simple_requests import Requests
for response in Requests().swarm(list_of_requests):
    print response.content
</code></pre>
<p>The docs are here: <a href="http://pythonhosted.org/simple-requests/" rel="nofollow">http://pythonhosted.org/simple-requests/</a></p>
</div>
<div class="post-text" itemprop="text">
<p>Either you figure out <a href="http://docs.python.org/library/threading.html" rel="nofollow">threads</a>, or you <a href="http://twistedmatrix.com" rel="nofollow">use Twisted</a> (<a href="http://twistedmatrix.com/documents/current/web/examples/" rel="nofollow">which is asynchronous</a>).</p>
</div>
<div class="post-text" itemprop="text">
<p>maybe using <a href="http://docs.python.org/dev/library/multiprocessing.html" rel="nofollow">multiprocessing</a> and divide you work on 2 process or so .</p>
<p>Here is an <strong>example</strong> (it's not tested)</p>
<pre><code>import multiprocessing
import Queue
import urllib2


NUM_PROCESS = 2
NUM_URL = 1000


class DownloadProcess(multiprocessing.Process):
    """Download Process """

    def __init__(self, urls_queue, result_queue):

        multiprocessing.Process.__init__(self)

        self.urls = urls_queue
        self.result = result_queue

    def run(self):
        while True:

             try:
                 url = self.urls.get_nowait()
             except Queue.Empty:
                 break

             ret = urllib2.Request(url)
             res = urllib2.urlopen(ret)

             try:
                 result = res.read()
             except (urllib2.HTTPError, urllib2.URLError):
                     pass

             self.result.put(result)


def main():

    main_url = 'http://server/?%s'

    urls_queue = multiprocessing.Queue()
    for p in range(1, NUM_URL):
        urls_queue.put(main_url % p)

    result_queue = multiprocessing.Queue()

    for i in range(NUM_PROCESS):
        download = DownloadProcess(urls_queue, result_queue)
        download.start()

    results = []
    while result_queue:
        result = result_queue.get()
        results.append(result)

    return results

if __name__ == "__main__":
    results = main()

    for res in results:
        print(res)
</code></pre>
</div>
<span class="comment-copy">Could you detail how to pass a function to process the response ? Docs don't seem to mention it</span>
<span class="comment-copy">@Overdrivr You can use <a href="http://docs.python-requests.org/en/master/user/advanced/#event-hooks" rel="nofollow noreferrer">docs.python-requests.org/en/master/user/advanced/#event-hooks</a> example: <code>grequests.get(u, hooks=dict(response=print_url))</code> or you can use <code>grequests.get(u,  callback=print_url)</code></span>
<span class="comment-copy">coroutine-based libraries have the benefits of both and simpler than threads and Twisted: gevent, eventlet, concurrence</span>
<span class="comment-copy">Threading is the right answer, not complex layered things like Twisted.  I'd use threading rather than multiprocessing; the process-based multiprocessing module is only needed for CPU-bound tasks, not this I/O-bound one.</span>
