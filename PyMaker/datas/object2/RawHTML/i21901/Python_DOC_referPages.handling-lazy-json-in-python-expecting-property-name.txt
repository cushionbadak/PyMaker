<div class="post-text" itemprop="text">
<p>Using Pythons (2.7) 'json' module I'm looking to process various JSON feeds. Unfortunately some of these feeds do not conform with JSON standards - in specific some keys are not wrapped in double speech-marks ("). This is causing Python to bug out.</p>
<p>Before writing an ugly-as-hell piece of code to parse and repair the incoming data, I thought I'd ask - is there any way to allow Python to either parse this malformed JSON or 'repair' the data so that it would be valid JSON?</p>
<p>Working example</p>
<pre><code>import json
&gt;&gt;&gt; json.loads('{"key1":1,"key2":2,"key3":3}')
{'key3': 3, 'key2': 2, 'key1': 1}
</code></pre>
<p>Broken example</p>
<pre><code>import json
&gt;&gt;&gt; json.loads('{key1:1,key2:2,key3:3}')
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "C:\Python27\lib\json\__init__.py", line 310, in loads
    return _default_decoder.decode(s)
  File "C:\Python27\lib\json\decoder.py", line 346, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Python27\lib\json\decoder.py", line 362, in raw_decode
    obj, end = self.scan_once(s, idx)
ValueError: Expecting property name: line 1 column 1 (char 1)
</code></pre>
<p>I've written a small REGEX to fix the JSON coming from this particular provider, but I forsee this being an issue in the future. Below is what I came up with.</p>
<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; s = '{key1:1,key2:2,key3:3}'
&gt;&gt;&gt; s = re.sub('([{,])([^{:\s"]*):', lambda m: '%s"%s":'%(m.group(1),m.group(2)),s)
&gt;&gt;&gt; s
'{"key1":1,"key2":2,"key3":3}'
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You're trying to use a JSON parser to parse something that isn't JSON.  Your best bet is to get the creator of the feeds to fix them.</p>
<p>I understand that isn't always possible.  You might be able to fix the data using regexes, depending on how broken it is:</p>
<pre><code>j = re.sub(r"{\s*(\w)", r'{"\1', j)
j = re.sub(r",\s*(\w)", r',"\1', j)
j = re.sub(r"(\w):", r'\1":', j)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Another option is to use the <a href="http://deron.meranda.us/python/demjson/">demjson</a> module which can parse json in non-strict mode.</p>
</div>
<div class="post-text" itemprop="text">
<p>The regular expressions pointed out by Ned and cheeseinvert don't take into account when the match is inside a string.</p>
<p>See the following example (using cheeseinvert's solution):</p>
<pre class="lang-python prettyprint-override"><code>&gt;&gt;&gt; fixLazyJsonWithRegex ('{ key : "a { a : b }", }')
'{ "key" : "a { "a": b }" }'
</code></pre>
<p>The problem is that the expected output is:</p>
<pre class="lang-python prettyprint-override"><code>'{ "key" : "a { a : b }" }'
</code></pre>
<p>Since JSON tokens are a subset of python tokens, we can use python's <a href="http://docs.python.org/2/library/tokenize.html" rel="nofollow noreferrer">tokenize module</a>.</p>
<p>Please correct me if I'm wrong, but the following code will fix a lazy json string in all the cases:</p>
<pre class="lang-python prettyprint-override"><code>import tokenize
import token
from StringIO import StringIO

def fixLazyJson (in_text):
  tokengen = tokenize.generate_tokens(StringIO(in_text).readline)

  result = []
  for tokid, tokval, _, _, _ in tokengen:
    # fix unquoted strings
    if (tokid == token.NAME):
      if tokval not in ['true', 'false', 'null', '-Infinity', 'Infinity', 'NaN']:
        tokid = token.STRING
        tokval = u'"%s"' % tokval

    # fix single-quoted strings
    elif (tokid == token.STRING):
      if tokval.startswith ("'"):
        tokval = u'"%s"' % tokval[1:-1].replace ('"', '\\"')

    # remove invalid commas
    elif (tokid == token.OP) and ((tokval == '}') or (tokval == ']')):
      if (len(result) &gt; 0) and (result[-1][1] == ','):
        result.pop()

    # fix single-quoted strings
    elif (tokid == token.STRING):
      if tokval.startswith ("'"):
        tokval = u'"%s"' % tokval[1:-1].replace ('"', '\\"')

    result.append((tokid, tokval))

  return tokenize.untokenize(result)
</code></pre>
<p>So in order to parse a json string, you might want to encapsulate a call to fixLazyJson once json.loads fails (to avoid performance penalties for well-formed json):</p>
<pre class="lang-python prettyprint-override"><code>import json

def json_decode (json_string, *args, **kwargs):
  try:
    json.loads (json_string, *args, **kwargs)
  except:
    json_string = fixLazyJson (json_string)
    json.loads (json_string, *args, **kwargs)
</code></pre>
<p>The only problem I see when fixing lazy json, is that if the json is malformed, the error raised by the second json.loads won't be referencing the line and column from the original string, but the modified one.</p>
<p>As a final note I just want to point out that it would be straightforward to update any of the methods to accept a file object instead of a string.</p>
<p>BONUS: Apart from this, people usually likes to include C/C++ comments when json is used for
configuration files, in this case, you can either <a href="https://stackoverflow.com/questions/241327/python-snippet-to-remove-c-and-c-comments#answer-241506">remove comments using a regular expression</a>, or use the extended version and fix the json string in one pass:</p>
<pre class="lang-python prettyprint-override"><code>import tokenize
import token
from StringIO import StringIO

def fixLazyJsonWithComments (in_text):
  """ Same as fixLazyJson but removing comments as well
  """
  result = []
  tokengen = tokenize.generate_tokens(StringIO(in_text).readline)

  sline_comment = False
  mline_comment = False
  last_token = ''

  for tokid, tokval, _, _, _ in tokengen:

    # ignore single line and multi line comments
    if sline_comment:
      if (tokid == token.NEWLINE) or (tokid == tokenize.NL):
        sline_comment = False
      continue

    # ignore multi line comments
    if mline_comment:
      if (last_token == '*') and (tokval == '/'):
        mline_comment = False
      last_token = tokval
      continue

    # fix unquoted strings
    if (tokid == token.NAME):
      if tokval not in ['true', 'false', 'null', '-Infinity', 'Infinity', 'NaN']:
        tokid = token.STRING
        tokval = u'"%s"' % tokval

    # fix single-quoted strings
    elif (tokid == token.STRING):
      if tokval.startswith ("'"):
        tokval = u'"%s"' % tokval[1:-1].replace ('"', '\\"')

    # remove invalid commas
    elif (tokid == token.OP) and ((tokval == '}') or (tokval == ']')):
      if (len(result) &gt; 0) and (result[-1][1] == ','):
        result.pop()

    # detect single-line comments
    elif tokval == "//":
      sline_comment = True
      continue

    # detect multiline comments
    elif (last_token == '/') and (tokval == '*'):
      result.pop() # remove previous token
      mline_comment = True
      continue

    result.append((tokid, tokval))
    last_token = tokval

  return tokenize.untokenize(result)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Expanding on Ned's suggestion, the following has been helpful for me:</p>
<pre><code>j = re.sub(r"{\s*'?(\w)", r'{"\1', j)
j = re.sub(r",\s*'?(\w)", r',"\1', j)
j = re.sub(r"(\w)'?\s*:", r'\1":', j)
j = re.sub(r":\s*'(\w+)'\s*([,}])", r':"\1"\2', j)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In a similar case, I have used <a href="https://docs.python.org/3/library/ast.html#ast.literal_eval" rel="nofollow"><code>ast.literal_eval</code></a>. AFAIK, this won't work only when the constant <code>null</code> (corresponding to Python <code>None</code>) appears in the JSON.</p>
<p>Given that you know about the <code>null/None</code> predicament, you can:</p>
<pre><code>import ast
decoded_object= ast.literal_eval(json_encoded_text)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In addition to Neds and cheeseinvert suggestion, adding <code>(?!/)</code> should avoid the mentioned problem with urls  </p>
<pre><code>j = re.sub(r"{\s*'?(\w)", r'{"\1', j)
j = re.sub(r",\s*'?(\w)", r',"\1', j)
j = re.sub(r"(\w)'?\s*:(?!/)", r'\1":', j)
j = re.sub(r":\s*'(\w+)'\s*([,}])", r':"\1"\2', j) 
j = re.sub(r",\s*]", "]", j)
</code></pre>
</div>
<span class="comment-copy">Thanks for your input - I highly doubt the provider will respond but I'll try and contact them. I also gave REGEX a try. I've edited my question to reflect my findings with REGEX.</span>
<span class="comment-copy">I'm going to leave this open for a while to see if anyone else has any further suggestions - otherwise I'll accept your answer. Looking at the REGEX statements you added they do pretty much the same thing as mine.</span>
<span class="comment-copy">Beware that while this regex might work on some very specific scenarios, it will <b>not</b> work more complex stuff like: <code>{ location: 'http://www.google.com' }</code>, you'll end up with invalid JSON: <code>{"location": "http"://www.google.com"}</code></span>
<span class="comment-copy">Yes, demjson really support parsing non-strict json well.</span>
<span class="comment-copy">I tried it to load a 300kb+ file, it was too slow.</span>
<span class="comment-copy">I tested that 5.05 msec per loop for <code>demjson</code> and 15.2 usec per loop for <code>json</code>, json is much faster!</span>
<span class="comment-copy">Nice expansion on the previous answers - thanks :)</span>
<span class="comment-copy">Thanks, the explanation and functions are very elaborate....</span>
<span class="comment-copy">Indeed, thanks, although to get it to work I had to also add <code>import StringIO</code> and change the line using StringIO to:  <code>StringIO.StringIO(in_text)</code> from <code>StringIO(in_text)</code>  Then it worked a treat on a lazy json that Google finance uses for delayed option chain quotes.</span>
<span class="comment-copy">Thanks! I forgot to add the "from StringIO import StringIO" to the code that I pasted here. Now it is updated :)</span>
<span class="comment-copy">Dude, this is an absolute lifesaver.  Thank you for posting this.</span>
<span class="comment-copy">That last line the first (\w) needs to be (\w*) since you're trying to match the whole word.</span>
<span class="comment-copy">Thanks Chris, I updated to \w+ since 0 char match wouldn't make sense</span>
<span class="comment-copy">And, for those of us who accidentally create 'Pythonic' JSON with trailing comma: j = re.sub(r",\s*]", "]", j) ... I didn't edit the answer since there may well be drawbacks that I haven't thought about.</span>
