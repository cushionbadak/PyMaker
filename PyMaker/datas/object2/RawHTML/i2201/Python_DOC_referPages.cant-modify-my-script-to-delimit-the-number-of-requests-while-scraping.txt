<div class="post-text" itemprop="text">
<p>I've written a script in python using <code>Thread</code> to handl multiple requests at the same time and do the scraping process faster. The script is doing it's job accordingly.</p>
<blockquote>
<p>In short what the scraper does: It parses all the links from the landing page leading to its main page (where information are stored) and scrape <code>happy hours</code> and <code>featured special</code> from there. The scrapers keeps going on until all the 29 pages are crawled.</p>
</blockquote>
<p>As there may be numerous links to play with, I would like to limit the number of requests. However, as I don't have much idea on this I can't find any ideal way to modify my existing script to serve the purpose.</p>
<p>Any help will be vastly appreciated.</p>
<p>This is my attempt so far:</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import threading

url = "https://www.totalhappyhour.com/washington-dc-happy-hour/?page={}"

def get_info(link):
    for mlink in [link.format(page) for page in range(1,30)]:
        response = requests.get(mlink)
        soup = BeautifulSoup(response.text,"lxml")
        itemlinks = [urljoin(link,container.select_one("h2.name a").get("href")) for container in soup.select(".profile")]
        threads = []
        for ilink in itemlinks:
            thread = threading.Thread(target=fetch_info,args=(ilink,))
            thread.start()
            threads+=[thread]

        for thread in threads:
            thread.join()

def fetch_info(nlink):
    response = requests.get(nlink)
    soup = BeautifulSoup(response.text,"lxml")
    for container in soup.select(".specials"):
        try:
            hours = container.select_one("h3").text
        except Exception: hours = ""
        try:
            fspecial = ' '.join([item.text for item in container.select(".special")])
        except Exception: fspecial = ""
        print(f'{hours}---{fspecial}')

if __name__ == '__main__':
    get_info(url)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You should look at <a href="http://lucasb.eyer.be/snips/python-thread-pool.html" rel="nofollow noreferrer"><code>asyncio</code></a>, it's great simple and can help you do things faster!</p>
<p>Also <a href="http://lucasb.eyer.be/snips/python-thread-pool.html" rel="nofollow noreferrer"><code>multiprocessing.Pool</code></a> can simplified your code (in case you don't want to use asyncio).
<code>multiprocessing.pool</code> also have <a href="http://lucasb.eyer.be/snips/python-thread-pool.html" rel="nofollow noreferrer"><code>ThreadPool</code></a> equivalent if you prefer to to use threads.</p>
<p>About the requests limit, I recommend you to use <a href="https://docs.python.org/3/library/threading.html?highlight=semaphore#threading.Semaphore" rel="nofollow noreferrer"><code>threading.Semaphore</code></a> (or any other semaphore in case you switch from threading)</p>
<p><strong>threading approach:</strong></p>
<pre><code>from multiprocessing.pool import ThreadPool as Pool
from threading import Semaphore
from time import sleep


MAX_RUN_AT_ONCE = 5
NUMBER_OF_THREADS = 10

sm = Semaphore(MAX_RUN_AT_ONCE)


def do_task(number):
    with sm:
        print(f"run with {number}")
        sleep(3)
        return number * 2


def main():

    p = Pool(NUMBER_OF_THREADS)
    results = p.map(do_task, range(10))
    print(results)


if __name__ == '__main__':
    main()
</code></pre>
<p><strong>multiprocessing approach:</strong></p>
<pre><code>from multiprocessing import Pool
from multiprocessing import Semaphore
from time import sleep


MAX_RUN_AT_ONCE = 5
NUMBER_OF_PROCESS = 10

semaphore = None

def initializer(sm):
    """init the semaphore for the child process"""
    global semaphore
    semaphore = sm


def do_task(number):
    with semaphore:
        print(f"run with {number}\n")
        sleep(3)
        return number * 2


def main():
    sm = Semaphore(MAX_RUN_AT_ONCE)
    p = Pool(NUMBER_OF_PROCESS, initializer=initializer,
             initargs=[sm])

    results = p.map(do_task, range(10))
    print(results)


if __name__ == '__main__':
    main()
</code></pre>
<p><strong>asyncio approch:</strong></p>
<pre><code>import asyncio


MAX_RUN_AT_ONCE = 5
sm = asyncio.Semaphore(MAX_RUN_AT_ONCE)

async def do_task(number):
    async with sm:
        print(f"run with {number}\n")
        await asyncio.sleep(3)
        return number * 2

async def main():   
    coros = [do_task(number) for number in range(10)]
    finished, _  = await asyncio.wait(coros)
    print([fut.result() for fut in finished])

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
</code></pre>
<p>for conducting <code>http requests</code> with asyncio you should use <a href="https://aiohttp.readthedocs.io/en/stable/" rel="nofollow noreferrer"><code>aiohttp</code></a>, you can also use <code>requests</code> with <code>loop.run_in_executor</code> but then just don't use <code>asyncio</code> at all, because all your code is pretty much requests.</p>
<p><strong>output:</strong></p>
<blockquote>
<p>run with 0</p>
<p>run with 1</p>
<p>run with 2</p>
<p>run with 3</p>
<p>run with 4</p>
<p>(here there is a pause du to the semaphore and sleep)</p>
<p>run with 5</p>
<p>run with 6</p>
<p>run with 7</p>
<p>run with 8</p>
<p>run with 9</p>
<p>[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]</p>
</blockquote>
<p>you can also check <a href="https://docs.python.org/3/library/concurrent.futures.html?highlight=threadpoolexecutor" rel="nofollow noreferrer"><code>ThreadPoolExecutor</code></a> </p>
</div>
<div class="post-text" itemprop="text">
<p>As I'm very new to create any scraper using <strong><em>multiprocessing</em></strong>, I expected to have any real-life script in order to understand the logic very clearly. The site used within the script has some bot protection mechanism. However, i've found out a very similar webpage to apply <strong><em>multiprocessing</em></strong> within it.</p>
<pre><code>import requests
from multiprocessing import Pool
from urllib.parse import urljoin
from bs4 import BeautifulSoup

url = "http://srar.com/roster/index.php?agent_search={}"

def get_links(link):
    completelinks = []
    for ilink in [chr(i) for i in range(ord('a'),ord('d')+1)]:
        res = requests.get(link.format(ilink))  
        soup = BeautifulSoup(res.text,'lxml')
        for items in soup.select("table.border tr"):
            if not items.select("td a[href^='index.php?agent']"):continue
            data = [urljoin(link,item.get("href")) for item in items.select("td a[href^='index.php?agent']")]
            completelinks.extend(data)
    return completelinks

def get_info(nlink):
    req = requests.get(nlink)
    sauce = BeautifulSoup(req.text,"lxml")
    for tr in sauce.select("table[style$='1px;'] tr"):
        table = [td.get_text(strip=True) for td in tr.select("td")]
        print(table)

if __name__ == '__main__':
    allurls = get_links(url)
    with Pool(10) as p:  ##this is the number responsible for limiting the number of requests
        p.map(get_info,allurls)
        p.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Although I'm not sure I could implement the logic of <code>ThreadPool</code> within the following script which has already been described in SocketPlayer's answer, It seems to be working flawlessly. Feel free to rectify, If I went anywhere wrong.</p>
<pre><code>import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from multiprocessing.pool import ThreadPool as Pool
from threading import Semaphore

MAX_RUN_AT_ONCE = 5
NUMBER_OF_THREADS = 10

sm = Semaphore(MAX_RUN_AT_ONCE)

url = "http://srar.com/roster/index.php?agent_search={}"

def get_links(link):
    with sm:
        completelinks = []
        for ilink in [chr(i) for i in range(ord('a'),ord('d')+1)]:
            res = requests.get(link.format(ilink))  
            soup = BeautifulSoup(res.text,'lxml')
            for items in soup.select("table.border tr"):
                if not items.select("td a[href^='index.php?agent']"):continue
                data = [urljoin(link,item.get("href")) for item in items.select("td a[href^='index.php?agent']")]
                completelinks.extend(data)
        return completelinks

def get_info(nlink):
    req = requests.get(nlink)
    sauce = BeautifulSoup(req.text,"lxml")
    for tr in sauce.select("table[style$='1px;'] tr")[1:]:
        table = [td.get_text(strip=True) for td in tr.select("td")]
        print(table)

if __name__ == '__main__':
    p = Pool(NUMBER_OF_THREADS)
    p.map(get_info, get_links(url))
</code></pre>
</div>
<span class="comment-copy">If you want to set maximum number of parallel requests,  you might need to implement ThreadingPool. Check <a href="https://stackoverflow.com/questions/3033952/threading-pool-similar-to-the-multiprocessing-pool">this ticket</a></span>
<span class="comment-copy">This is exactly what I was looking for @sir Andersson. Could you help me implement that when you are free? I highly doubt I can do it myself. Thanks in advance.</span>
<span class="comment-copy">It seem that target website has some performance issue or some kind of bot-protection: when I try to run script server quickly becomes unresponsive, so it's hard to apply multi-threading web-scraping to this particular site...</span>
<span class="comment-copy">Sorry, I can't help much, they blocked me too. Andersson is right, they have a bot-protection mechanism - if you look at their Server header you'll see that they're using Sucuri/Cloudproxy. Of course you could use a proxy until it gets banned, and then a second proxy, and so on, but that's abusive. I'm afraid that there is not much we can do but leave them alone.</span>
<span class="comment-copy">That site is just a placeholder. However, I've already created one using <code>multiprocessing</code>. Thanks a lot both of you. Should i paste below the one I've created already?</span>
