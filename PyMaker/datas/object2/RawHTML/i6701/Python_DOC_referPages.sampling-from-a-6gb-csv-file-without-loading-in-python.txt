<div class="post-text" itemprop="text">
<p>I have a training data-set in CSV format of size 6 GB which I am required to analyze and implement machine learning on it. My system RAM is 6 GB so it is not possible for me to load the file in the memory. I need to perform random sampling and load the samples from the data-set. The number of samples may vary according to requirement. How to do this? </p>
</div>
<div class="post-text" itemprop="text">
<p>Something to start with:</p>
<pre><code>with open('dataset.csv') as f:
    for line in f:
        sample_foo(line.split(","))
</code></pre>
<p>This will load only one line at a time in memory and not the whole file.</p>
</div>
<span class="comment-copy">You can use Python CSV reader to load the file in chunks and sample from each chunk.</span>
<span class="comment-copy">Possible duplicate of <a href="https://stackoverflow.com/questions/17444679/reading-a-huge-csv-file">Reading a huge .csv file</a> . There are other simlar Q&amp;A's some with different answers.</span>
<span class="comment-copy">Yes I tried that but I dont know the actual size of the my dataset so I could not create chunks properly, ended up overloading my system</span>
<span class="comment-copy">There are a lot of solutions here on SO, some of them use itertools.islice to consume lines that aren't being sampled  - there is a <code>consume</code> function in the <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow noreferrer">Itertools Recipes</a>.   You should be able to make that approach work.</span>
<span class="comment-copy">I also like this answer <a href="https://stackoverflow.com/a/6347142/2823755">stackoverflow.com/a/6347142/2823755</a> - A single pass over the file to create a list of line indices/positions. Then you seek to the line you want to sample.</span>
<span class="comment-copy">This is the right answer and the pythonnic way to do it. Since python uses generator instead of loading the whole file to the memory no memory pressure happens.</span>
<span class="comment-copy">You may also want to mention using something like reservoir sampling (see <a href="https://en.wikipedia.org/wiki/Reservoir_sampling" rel="nofollow noreferrer">en.wikipedia.org/wiki/Reservoir_sampling</a>). While using iterators is a good way to save on memory, you still need a way to sample the entries. Also, is there is a header the first line should be saved and the iteration should begin with the second line.</span>
<span class="comment-copy">So I do not know the number of records in the dataset, and I want to have a sample size of say a fixed percentage of the dataset, "random samples"! Is it possible to make it happen?</span>
