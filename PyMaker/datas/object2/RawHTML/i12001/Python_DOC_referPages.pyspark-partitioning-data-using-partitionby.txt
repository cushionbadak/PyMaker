<div class="post-text" itemprop="text">
<p>I understand that <code>partitionBy</code> function partitions my data. If I use <code>rdd.partitionBy(100)</code> it will partition my data by key into 100 parts. i.e. data associated with similar keys will be grouped together</p>
<ol>
<li>Is my understanding correct?</li>
<li>Is it advisable to have number of partitions equal to number of
    available cores? Does that make processing more efficient?</li>
<li>what if my data is not in key,value format. Can i still use this function?</li>
<li>lets say my data is serial_number_of_student,student_name. In this
        case can i partition my data by student_name instead of the
        serial_number?</li>
</ol>
</div>
<div class="post-text" itemprop="text">
<ol>
<li>Not exactly. Spark, including PySpark, <a href="https://stackoverflow.com/q/31424396/1560062">is by default using <strong>hash partitioning</strong></a>. Excluding identical keys there is no practical similarity between keys assigned to a single partition. </li>
<li><p>There is no simple answer here. All depends on amount of data and available resources. <a href="https://stackoverflow.com/questions/31659404/spark-iteration-time-increasing-exponentially-when-using-join/31662127#31662127">Too large</a> or too low number of partitions will degrade the performance. </p>
<p><a href="https://youtu.be/7ooZ4S7Ay6Y" rel="nofollow noreferrer">Some resources</a> claim the number of partitions should around twice as large as the number of available cores. From the other hand a single partition typically shouldn't contain more than 128MB and a single shuffle block cannot be larger than 2GB (See <a href="https://issues.apache.org/jira/browse/SPARK-6235" rel="nofollow noreferrer">SPARK-6235</a>).</p>
<p>Finally you have to correct for potential data skews. If some keys are overrepresented in your dataset it can result in suboptimal resource usage and potential failure.</p></li>
<li><p>No, or at least not directly. You can use <code>keyBy</code> method to convert RDD to required format. Moreover any Python object can be treated as a <strong>key-value pair</strong> as long as it implements required methods which make it behave like an <code>Iterable</code> of length equal two. See <a href="https://stackoverflow.com/q/35703298/1560062">How to determine if object is a valid key-value pair in PySpark</a></p></li>
<li>It depends on the types. As long as key is <strong>hashable</strong>* then yes. Typically it means it has to be immutable structure and all values it contains have to be immutable as well. For example <a href="https://stackoverflow.com/q/31404238/1560062">a list is not a valid key</a> but a <code>tuple</code> of integers is.</li>
</ol>
<hr/>
<p>To quote <a href="https://docs.python.org/3/glossary.html#term-hashable" rel="nofollow noreferrer">Python glossary</a>:</p>
<blockquote>
<p>An object is hashable if it has a hash value which never changes during its lifetime (it needs a <code>__hash__()</code> method), and can be compared to other objects (it needs an <code>__eq__()</code> method). Hashable objects which compare equal must have the same hash value.</p>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<p>I recently used partitionby. What I did was restructure my data so that all those which I want to put in same partition have the same key, which in turn is a value from the data. my data was a list of dictionary, which I converted into tupples with key from dictionary.Initially the partitionby was not keeping same keys in same partition. But then I realized the keys were strings. I cast them to int. But the problem persisted. The numbers were very large. I then mapped these numbers to small numeric values and it worked. So my take away was that the keys need to be small integers. </p>
</div>
