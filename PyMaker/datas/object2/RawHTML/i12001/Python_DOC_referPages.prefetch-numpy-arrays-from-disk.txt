<div class="post-text" itemprop="text">
<p>I'm working with a bunch of <code>numpy</code> arrays that don't all fit in RAM, so I need to periodically save them to and load them from the disk.</p>
<p>Usually, I know which ones I'll need to read ahead of time, so I'd like to hide the latency by issuing something like a "prefetch" instruction in advance.</p>
<p>How should I do this?</p>
<hr/>
<p>(<a href="https://stackoverflow.com/questions/34594198/how-to-prefetch-data-using-a-custom-python-function-in-tensorflow">There is a similar question related to TensorFlow</a>:
 However, I am not using TensorFlow, and so I wouldn't want to create a dependency on it)</p>
</div>
<div class="post-text" itemprop="text">
<p>If you're using Python 3.3+ on a UNIX-like system, you can use <a href="https://docs.python.org/3/library/os.html#os.posix_fadvise" rel="nofollow"><code>os.posix_fadvise</code></a> to initiate a prefetch after opening a file. For example:</p>
<pre><code>with open(filepath, 'rb') as f:
    os.posix_fadvise(f.fileno(), 0, os.stat(f.fileno()).st_size, os.POSIX_FADV_WILLNEED)

    ... do other stuff ...

    # If you're lucky, OS has asynchronously prefetched file contents
    stuff = pickle.load(f)
</code></pre>
<p>Aside from that, Python doesn't directly offer any APIs for explicit prefetch, but you could use <code>ctypes</code> to manually load an OS appropriate prefetch function, or use a background thread that does nothing but read and discard blocks from the file to improve the odds that the data is in the system cache.</p>
</div>
<div class="post-text" itemprop="text">
<p>[disclaimer: shameless self-advertising here :-)]
I have written a library that should help on this, and it is compatible with python 2.7: <a href="https://seqtools-doc.readthedocs.io" rel="nofollow noreferrer">documentation</a>/<a href="https://github.com/nlgranger/SeqTools" rel="nofollow noreferrer">repository</a></p>
<p>You can use its <a href="https://seqtools-doc.readthedocs.io/en/stable/reference.html#seqtools.prefetch" rel="nofollow noreferrer"><code>prefetch</code></a> function which does what it says, prefetch some values:</p>
<pre><code>files = ['file1.npy', 'file2.npy', 'file3.npy']

def next_to_preload(current_idx):
    return (current_idx + 1) % 3

loaded = seqtools.smap(np.load, files)  # behaves like a list but elements are computed on-demand
preloaded = seqtool.prefetch(
    loaded, 
    max_buffered=10,
    direction=(0, next_to_preload))

for i in range(3):
    print(preloaded[i])
</code></pre>
<p>It has a few more options if you want to switch from threads to processes etc.</p>
<p>Note that fetching an item different from the one provisioned according to <code>next_to_preload</code> will reset the buffer.</p>
</div>
<span class="comment-copy">"almost all programming can be viewed as an exercise in caching" ...</span>
<span class="comment-copy">@ali_m I/O is of whole arrays, no memcpy, but I'm flexible in my choices regarding the rest</span>
<span class="comment-copy">Your question is still rather vague. Memory-mapped arrays (<code>numpy.memmap</code>) and HDF5 (PyTables, h5py) are two options you should probably consider, but you're going to have to get much more specific about your problem if you want a concrete answer.</span>
<span class="comment-copy">@ali_m I thought I answered your question. What's still vague?</span>
<span class="comment-copy">@ali_m edited anyway -- hope this clarifies the question</span>
<span class="comment-copy">I'm on 2.7. bummer..</span>
