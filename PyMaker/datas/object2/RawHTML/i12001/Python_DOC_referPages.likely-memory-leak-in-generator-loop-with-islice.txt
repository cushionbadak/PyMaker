<div class="post-text" itemprop="text">
<p>I am working with large files holding several million records each (approx 2GB unpacked, several hundred MBs gzip). </p>
<p>I iterate over the records with <code>islice</code>, which allows me to get either a small portion (for debug and development) or the whole thing when I want to test the code. I have noticed an absurdly large memory usage for my code and thus I am trying to find the memory leak in my code. </p>
<p>Below is the output from memory_profiler on a paired read (where I open two files and zip the records), for ONLY 10**5 values (the default value get overwritten). </p>
<pre><code>Line #    Mem usage    Increment   Line Contents
================================================
   137   27.488 MiB    0.000 MiB   @profile
   138                             def paired_read(read1, read2, nbrofitems = 10**8):
   139                              """ Procedure for reading both sequences and stitching them together """
   140   27.488 MiB    0.000 MiB    seqFreqs = Counter()
   141   27.488 MiB    0.000 MiB    linker_str = "~"
   142                              #for rec1, rec2 in izip(read1, read2):
   143 3013.402 MiB 2985.914 MiB    for rec1, rec2 in islice(izip(read1, read2), nbrofitems):
   144 3013.398 MiB   -0.004 MiB        rec1 = rec1[9:]                         # Trim the primer variable sequence
   145 3013.398 MiB    0.000 MiB        rec2 = rec2[:150].reverse_complement()  # Trim the low quality half of the 3' read AND take rev complement
   146                                  #aaSeq = Seq.translate(rec1 + rec2)
   147                             
   148                                  global nseqs 
   149 3013.398 MiB    0.000 MiB        nseqs += 1
   150                             
   151 3013.402 MiB    0.004 MiB        if filter_seq(rec1, direction=5) and filter_seq(rec2, direction=3):
   152 3013.395 MiB   -0.008 MiB            aakey = str(Seq.translate(rec1)) + linker_str + str(Seq.translate(rec2))
   153 3013.395 MiB    0.000 MiB            seqFreqs.update({ aakey : 1 })  
   154                                  
   155 3013.402 MiB    0.008 MiB    print "========================================"
   156 3013.402 MiB    0.000 MiB    print "# of total sequences: %d" % nseqs
   157 3013.402 MiB    0.000 MiB    print "# of filtered sequences: %d" % sum(seqFreqs.values())
   158 3013.461 MiB    0.059 MiB    print "# of repeated occurances: %d" % (sum(seqFreqs.values()) - len(list(seqFreqs)))
   159 3013.461 MiB    0.000 MiB    print "# of low-score sequences (&lt;20): %d" % lowQSeq
   160 3013.461 MiB    0.000 MiB    print "# of sequences with stop codon: %d" % starSeqs
   161 3013.461 MiB    0.000 MiB    print "========================================"
   162 3013.504 MiB    0.043 MiB    pprint(seqFreqs.most_common(100), width = 240)
</code></pre>
<p>The code, in short, does some filtering on the records and keeps track of how many times the strings occur in the file (zipped pair of strings in this particular case).</p>
<p>100 000 strings of 150 chars with integer values in a Counter should land around 100 MBs tops, which I checked using following function by <a href="https://stackoverflow.com/a/30316760/328725">@AaronHall</a>. </p>
<p>Given the memory_profiler output I suspect that islice doesn't let go of the previous entities over the course of the iteration. A google search landed me at <a href="https://bugs.python.org/issue21321" rel="nofollow noreferrer">this bug report</a> however it's marked as solved for Python 2.7 which is what I am running at the moment.</p>
<p>Any opinions?</p>
<p><strong>EDIT:</strong> I have tried to skip <code>islice</code> as per comment below and use a for loop like </p>
<pre><code>for rec in list(next(read1) for _ in xrange(10**5)):
</code></pre>
<p>which makes no significant difference. It is in the case of a single file, in order to avoid <code>izip</code> which also comes from <code>itertools</code>.</p>
<p>A secondary troubleshooting idea i had was to check if <code>gzip.open()</code> reads and expands the file to memory, and thus cause the issue here. However running the script on decompressed files doesn't make a difference.  </p>
</div>
<div class="post-text" itemprop="text">
<p>Note that memory_profiler only reports the maximum memory consumption for each line. For long loops this can be misleading as the first line of the loop always seem to report a disproportionate amount of memory. </p>
<p>That is because it compares the first line of the loop with respect to memory consumption of the line before, which would be out of the loop. It doesn't mean that the first line of the loop consumes 2985Mb but rather that the difference between the peak in memory within the loop is 2985Mb higher that out of the loop.</p>
</div>
<span class="comment-copy">Try using a <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow noreferrer">pure Python</a> version to validate the leak hypothesis. Or just drop <code>islice</code> and use a counter with an <code>if</code>.</span>
<span class="comment-copy">@SergeiLebedev how do you mean? is <code>islice</code> not pure python?</span>
<span class="comment-copy">No, the whole <code>itertools</code> module is implemented in C, <a href="https://hg.python.org/cpython/file/2.7/Modules/itertoolsmodule.c#l1123" rel="nofollow noreferrer">including</a> the <code>islice</code> function.</span>
<span class="comment-copy">@SergeiLebedev please see the edits above</span>
<span class="comment-copy">It is also possible that the there is a large number of unique elements in your input and the <code>Counter</code> is responsible for the memory bloat? Can you try running your script without the <code>Counter</code>? I can have a look at it myself if you link me to the sample inputs.</span>
<span class="comment-copy">Thanks for the pointer Fabian. As it turns out, the leak has nothing to do with islice at all, but a library call within Biopython.</span>
