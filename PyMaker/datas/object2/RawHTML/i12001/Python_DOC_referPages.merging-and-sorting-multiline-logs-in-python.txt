<div class="post-text" itemprop="text">
<p>I have a bunch of log files with the following format:</p>
<pre><code>[Timestamp1] Text1
Text2
Text3
[Timestamp2] Text4
Text5
...
...
</code></pre>
<p>Where the number of text lines following a timestamp can vary from 0 to many. All the lines following a timestamp until the next timestamp are part of the previous log statement.</p>
<p>Example:</p>
<pre><code>[2016-03-05T23:18:23.672Z] Some log text
[2016-03-05T23:18:23.672Z] Some other log text
[2016-03-05T23:18:23.672Z] Yet another log text
Some text
Some text
Some text
Some text
[2016-03-05T23:18:23.672Z] Log text
Log text
</code></pre>
<p>I am trying to create a log merge script for such types of log files and have been unsuccessful so far.</p>
<p>If the logs were in a standard format where each line is a separate log entry, it is straight forward to create a log merge script using fileinput and sorting.</p>
<p>I think am looking at a way to treat multiple lines as a single log entity that is sortable on the associated timestamp.</p>
<p>Any pointers?</p>
</div>
<div class="post-text" itemprop="text">
<p>You can write a generator that acts as an adapter for your log stream to do the chunking for you. Something like this:</p>
<pre><code>def log_chunker(log_lines):
    batch = []
    for line in log_lines:
        if batch and has_timestamp(line):
            # detected a new log statement, so yield the previous one
            yield batch
            batch = []
        batch.append(line)
    yield batch
</code></pre>
<p>This will turn your raw log lines into batches where each one is a list of lines, and the first line in each list has the timestamp. You can build the rest from there. It might make more sense to start <code>batch</code> as an empty string and tack on the rest of the message directly; whatever works for you.</p>
<p>Side-note, if you're merging multiple timestamped logs you shouldn't need to perform global sorting at all if you use a streaming merge-sort. </p>
</div>
<div class="post-text" itemprop="text">
<p>The following approach should work well.</p>
<pre><code>from heapq import merge
from itertools import groupby
import re
import glob

re_timestamp = re.compile(r'\[\d{4}-\d{2}-\d{2}')

def get_log_entry(f):
    entry = ''
    for timestamp, g in groupby(f, lambda x: re_timestamp.match(x) is not None):
        entries = [row.strip() + '\n' for row in g]

        if timestamp:
            if len(entries) &gt; 1:
                for entry in entries[:-1]:
                    yield entry
            entry = entries[-1]
        else:   
            yield entry + ''.join(entries)

files = [open(f) for f in glob.glob('*.log')]       # Open all log files

with open('output.txt', 'w') as f_output:     
    for entry in merge(*[get_log_entry(f) for f in files]):
        f_output.write(''.join(entry))

for f in files:
    f.close()
</code></pre>
<p>It makes use of the <a href="https://docs.python.org/2/library/heapq.html?highlight=merge#heapq.merge" rel="nofollow"><code>merge</code></a> function to combine a list of iterables in order. </p>
<p>As your timestamps are naturally ordered, all that is needed is a function to read whole entries at a time from each file. This is done using a regular expression to spot lines starting in each file with a timestamp, and groupby is used to read matching rows in at once.</p>
<p><a href="https://docs.python.org/2/library/glob.html?highlight=glob.glob#glob.glob" rel="nofollow"><code>glob</code></a> is used to first find all files in your folder with a <code>.log</code> extension.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can easily break it into chunks using <code>re.split()</code> with a capturing regexp:</p>
<pre><code>pieces = re.split(r"(^\[20\d\d-.*?\])", logtext, flags=re.M)
</code></pre>
<p>You can make the regexp as precise as you wish; I just require <code>[20\d\d-</code> at the start of a line. The result contains the matching and non-matching parts of <code>logtext</code>, as alternating pieces (starting with an empty non-matching part).</p>
<pre><code>&gt;&gt;&gt; print(pieces[:5])
['', '[2016-03-05T23:18:23.672Z] ', 'Some log text\n', '[2016-03-05T23:18:23.672Z] ', 'Some other log text\n']
</code></pre>
<p>It remains to reassemble the log parts, which you can do with this recipe from <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow"><code>itertools</code></a>:</p>
<pre><code>def pairwise(iterable):
    "s -&gt; (s0,s1), (s1,s2), (s2, s3), ..."
    a, b = itertools.tee(iterable)
    next(b, None)
    return zip(a, b)

log_entries = list( "".join(pair) for pair in pairwise(pieces[1:]) )
</code></pre>
<p>If you have several of these lists, you can indeed just combine and sort them, or use a fancier merge sort if you have lots of data. I understand your question to be about splitting up the log entries, so I won't go into this.</p>
</div>
<span class="comment-copy">I finally followed a very similar technique - chunking the individual lines around log entry boundaries and then merging/sorting them. Thanks for the input.</span>
