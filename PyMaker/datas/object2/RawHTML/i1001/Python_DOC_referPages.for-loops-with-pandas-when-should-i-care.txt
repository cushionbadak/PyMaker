<div class="post-text" itemprop="text">
<p>I am familiar with the concept of "vectorization", and how pandas employs vectorized techniques to speed up computation. Vectorized functions broadcast operations over the entire series or DataFrame to achieve speedups much greater than conventionally iterating over the data. </p>
<p>However, I am quite surprised to see a lot of code (including from answers on Stack Overflow) offering solutions to problems that involve looping through data using <code>for</code> loops and list comprehensions. Having read the documentation, and with a decent understanding of the API, I am given to believe that loops are "bad", and that one should "never" iterate over arrays, series, or DataFrames. So, how come I see users suggesting loopy solutions every now and then?</p>
<p>So, to summarise... my question is:<br/>
Are <code>for</code> loops really "bad"? If not, in what situation(s) would they be better than using a more conventional "vectorized" approach?<sup>1</sup></p>
<p><sub>1 - While it is true that the question sounds somewhat broad, the truth is that there are very specific situations when <code>for</code> loops are usually better than conventionally iterating over data. This post aims to capture this for posterity. </sub> </p>
</div>
<div class="post-text" itemprop="text">
<p>TLDR; No, <code>for</code> loops are not blanket "bad", at least, not always. It is probably <strong>more accurate to say that some vectorized operations are slower than iterating</strong>, versus saying that iteration is faster than some vectorized operations. Knowing when and why is key to getting the most performance out of your code. In a nutshell, these are the situations where it is worth considering an alternative to vectorized pandas functions:</p>
<ol>
<li>When your data is small (...depending on what you're doing),</li>
<li>When dealing with <code>object</code>/mixed dtypes</li>
<li>When using the <code>str</code>/regex accessor functions</li>
</ol>
<p>Let's examine these situations individually. </p>
<hr/>
<h3>Iteration v/s Vectorization on Small Data</h3>
<p>Pandas follows a <a href="https://en.wikipedia.org/wiki/Convention_over_configuration" rel="nofollow noreferrer">"Convention Over Configuration"</a> approach in its API design. This means that the same API has been fitted to cater to a broad range of data and use cases. </p>
<p>When a pandas function is called, the following things (among others) must internally be handled by the function, to ensure working </p>
<ol>
<li>Index/axis alignment</li>
<li>Handling mixed datatypes</li>
<li>Handling missing data</li>
</ol>
<p>Almost every function will have to deal with these to varying extents, and this presents an <strong>overhead</strong>. The overhead is less for numeric functions (for example, <a href="https://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/ops.py#L1371-L1388" rel="nofollow noreferrer"><code>Series.add</code></a>), while it is more pronounced for string functions (for example, <a href="https://github.com/pandas-dev/pandas/blob/v0.23.4/pandas/core/strings.py#L635-L669" rel="nofollow noreferrer"><code>Series.str.replace</code></a>).</p>
<p><a href="https://docs.python.org/3/tutorial/controlflow.html#for-statements" rel="nofollow noreferrer"><code>for</code></a> loops, on the other hand, are faster then you think. What's even better is <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" rel="nofollow noreferrer">list comprehensions</a> (which create lists through <code>for</code> loops) are even faster as they are optimized iterative mechanisms for list creation.</p>
<p>List comprehensions follow the pattern </p>
<pre><code>[f(x) for x in seq]
</code></pre>
<p>Where <code>seq</code> is a pandas series or DataFrame column. Or, when operating over multiple columns,</p>
<pre><code>[f(x, y) for x, y in zip(seq1, seq2)]
</code></pre>
<p>Where <code>seq1</code> and <code>seq2</code> are columns. </p>
<p><strong>Numeric Comparison</strong><br/>
Consider a simple boolean indexing operation. The list comprehension method has been timed against <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.ne.html" rel="nofollow noreferrer"><code>Series.ne</code></a> (<code>!=</code>) and <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html" rel="nofollow noreferrer"><code>query</code></a>. Here are the functions:</p>
<pre><code># Boolean indexing with Numeric value comparison.
df[df.A != df.B]                            # vectorized !=
df.query('A != B')                          # query (numexpr)
df[[x != y for x, y in zip(df.A, df.B)]]    # list comp
</code></pre>
<p>For simplicity, I have used the <a href="https://github.com/nschloe/perfplot" rel="nofollow noreferrer"><code>perfplot</code></a> package to run all the timeit tests in this post. The timings for the operations above are below:</p>
<p><a href="https://i.stack.imgur.com/sbtDB.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/sbtDB.png"/></a></p>
<p>The list comprehension outperforms <code>query</code> for moderately sized N, and even outperforms the vectorized not equals comparison for tiny N. Unfortunately, the list comprehension scales linearly, so it does not offer much performance gain for larger N. </p>
<blockquote>
<p><strong>Note</strong><br/>
  It is worth mentioning that much of the benefit of list comprehension come from not having to worry about the index alignment,
  but this means that if your code is dependent on indexing alignment,
  this will break. In some cases, vectorised operations over the
  underlying NumPy arrays can be considered as bringing in the "best of
  both worlds", allowing for vectorisation <em>without</em> all the unneeded overhead of the pandas functions. This means that you can rewrite the operation above as </p>
<pre><code>df[df.A.values != df.B.values]
</code></pre>
<p>Which outperforms both the pandas and list comprehension equivalents:<br/>
<a href="https://i.stack.imgur.com/m9hycb.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/m9hycb.png"/></a><br/>
  NumPy vectorization is out of the scope of this post, but it is definitely worth considering, if performance matters.</p>
</blockquote>
<p><strong>Value Counts</strong><br/>
Taking another example - this time, with another vanilla python construct that is <em>faster</em> than a for loop - <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="nofollow noreferrer"><code>collections.Counter</code></a>. A common requirement is to compute the value counts and return the result as a dictionary. This is done with <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html#pandas-series-value-counts" rel="nofollow noreferrer"><code>value_counts</code></a>, <a href="https://docs.scipy.org/doc/numpy-1.14.1/reference/generated/numpy.unique.html" rel="nofollow noreferrer"><code>np.unique</code></a>, and <code>Counter</code>:</p>
<pre><code># Value Counts comparison.
ser.value_counts(sort=False).to_dict()           # value_counts
dict(zip(*np.unique(ser, return_counts=True)))   # np.unique
Counter(ser)                                     # Counter
</code></pre>
<p><a href="https://i.stack.imgur.com/2i5qT.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/2i5qT.png"/></a></p>
<p>The results are more pronounced, <code>Counter</code> wins out over both vectorized methods for a larger range of small N (~3500). </p>
<blockquote>
<p><strong>Note</strong><br/>
   More trivia (courtesy @user2357112). The <code>Counter</code> is implemented with a <a href="https://github.com/python/cpython/blob/v3.7.0/Modules/_collectionsmodule.c#L2249-L2354" rel="nofollow noreferrer">C
  accelerator</a>,
  so while it still has to work with python objects instead of the
  underlying C datatypes, it is still faster than a <code>for</code> loop. Python
  power!</p>
</blockquote>
<p>Of course, the take away from here is that the performance depends on your data and use case. The point of these examples is to convince you not to rule out these solutions as legitimate options. If these still don't give you the performance you need, there is always <a href="https://pandas.pydata.org/pandas-docs/stable/enhancingperf.html#cython-writing-c-extensions-for-pandas" rel="nofollow noreferrer">cython</a> and <a href="http://numba.pydata.org/" rel="nofollow noreferrer">numba</a>. Let's add this test into the mix.</p>
<pre><code>from numba import njit, prange

@njit(parallel=True)
def get_mask(x, y):
    result = [False] * len(x)
    for i in prange(len(x)):
        result[i] = x[i] != y[i]

    return np.array(result)

df[get_mask(df.A.values, df.B.values)] # numba
</code></pre>
<p><a href="https://i.stack.imgur.com/nSpCH.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/nSpCH.png"/></a></p>
<p>Numba offers JIT compilation of loopy python code to very powerful vectorized code. Understanding how to make numba work involves a learning curve.</p>
<hr/>
<h3><strong>Operations with Mixed/<code>object</code> dtypes</strong></h3>
<p><strong>String-based Comparison</strong><br/>
Revisiting the filtering example from the first section, what if the columns being compared are strings? Consider the same 3 functions above, but with the input DataFrame cast to string.</p>
<pre><code># Boolean indexing with string value comparison.
df[df.A != df.B]                            # vectorized !=
df.query('A != B')                          # query (numexpr)
df[[x != y for x, y in zip(df.A, df.B)]]    # list comp
</code></pre>
<p><a href="https://i.stack.imgur.com/Lg2CE.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/Lg2CE.png"/></a></p>
<p>So, what changed? The thing to note here is that <strong>string operations are inherently difficult to vectorize.</strong> Pandas treats strings as objects, and all operations on objects fall back to a slow, loopy implementation.</p>
<p>Now, because this loopy implementation is surrounded by all the overhead mentioned above, there is a constant magnitude difference between these solutions, even though they scale the same. </p>
<p>When it comes to operations on mutable/complex objects, there is no comparison. List comprehension outperforms all operations involving dicts and lists. </p>
<p><strong>Accessing Dictionary Value(s) by Key</strong><br/>
Here are timings for two operations that extract a value from a column of dictionaries: <code>map</code> and the list comprehension. The setup is in the Appendix, under the heading "Code Snippets".</p>
<pre><code># Dictionary value extraction.
ser.map(operator.itemgetter('value'))     # map
pd.Series([x.get('value') for x in ser])  # list comprehension
</code></pre>
<p><a href="https://i.stack.imgur.com/QrbZO.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/QrbZO.png"/></a></p>
<p><strong>Positional List Indexing</strong><br/>
Timings for 3 operations that extract the 0th element from a list of columns (handling exceptions), <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html#pandas-series-map" rel="nofollow noreferrer"><code>map</code></a>, <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.get.html" rel="nofollow noreferrer"><code>str.get</code> accessor method</a>, and the list comprehension:</p>
<pre><code># List positional indexing. 
def get_0th(lst):
    try:
        return lst[0]
    # Handle empty lists and NaNs gracefully.
    except (IndexError, TypeError):
        return np.nan
</code></pre>
<p></p>
<pre><code>ser.map(get_0th)                                          # map
ser.str[0]                                                # str accessor
pd.Series([x[0] if len(x) &gt; 0 else np.nan for x in ser])  # list comp
pd.Series([get_0th(x) for x in ser])                      # list comp safe
</code></pre>
<blockquote>
<p><strong>Note</strong><br/>
  If the index matters, you would want to do:</p>
<pre><code>pd.Series([...], index=ser.index)
</code></pre>
<p>When reconstructing the series.</p>
</blockquote>
<p><a href="https://i.stack.imgur.com/Zzzez.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/Zzzez.png"/></a></p>
<p><strong>List Flattening</strong><br/>
A final example is flattening lists. This is another common problem, and demonstrates just how powerful pure python is here.</p>
<pre><code># Nested list flattening.
pd.DataFrame(ser.tolist()).stack().reset_index(drop=True)  # stack
pd.Series(list(chain.from_iterable(ser.tolist())))         # itertools.chain
pd.Series([y for x in ser for y in x])                     # nested list comp
</code></pre>
<p><a href="https://i.stack.imgur.com/eJ2iB.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/eJ2iB.png"/></a></p>
<p>Both <a href="https://docs.python.org/3/library/itertools.html#itertools.chain.from_iterable" rel="nofollow noreferrer"><code>itertools.chain.from_iterable</code></a> and the nested list comprehension are pure python constructs, and scale much better than the <code>stack</code> solution.</p>
<p>These timings are a strong indication of the fact that pandas is not equipped to work with mixed dtypes, and that you should probably refrain from using it to do so. Wherever possible, data should be present as scalar values (ints/floats/strings) in separate columns.</p>
<p>Lastly, the applicability of these solutions depend widely on your data. So, the best thing to do would be to test these operations on your data before deciding what to go with. Notice how I have not timed <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html#pandas-series-apply" rel="nofollow noreferrer"><code>apply</code></a> on these solutions, because it would skew the graph (yes, it's that slow).</p>
<hr/>
<h3>Regex Operations, and <code>.str</code> Accessor Methods</h3>
<p>Pandas can apply regex operations such as <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html" rel="nofollow noreferrer"><code>str.contains</code></a>, <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extract.html" rel="nofollow noreferrer"><code>str.extract</code></a>, and <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extractall.html#pandas.Series.str.extractall" rel="nofollow noreferrer"><code>str.extractall</code></a>, as well as other "vectorized" string operations (such as <code>str.split</code>, str.find<code>,</code>str.translate`, and so on) on string columns. These functions are slower than list comprehensions, and are meant to be more convenience functions than anything else.</p>
<p>It is usually much faster to pre-compile a regex pattern and iterate over your data with <a href="https://docs.python.org/3/library/re.html#re.compile" rel="nofollow noreferrer"><code>re.compile</code></a> (also see <a href="https://stackoverflow.com/questions/452104/is-it-worth-using-pythons-re-compile">Is it worth using Python's re.compile?</a>). The list comp equivalent to <code>str.contains</code> looks something like this:</p>
<pre><code>p = re.compile(...)
ser2 = pd.Series([x for x in ser if p.search(x)])
</code></pre>
<p>Or,</p>
<pre><code>ser2 = ser[[bool(p.search(x)) for x in ser]]
</code></pre>
<p>If you need to handle NaNs, you can do something like</p>
<pre><code>ser[[bool(p.search(x)) if pd.notnull(x) else False for x in ser]]
</code></pre>
<p>The list comp equivalent to <code>str.extract</code> (without groups) will look something like:</p>
<pre><code>df['col2'] = [p.search(x).group(0) for x in df['col']]
</code></pre>
<p>If you need to handle no-matches and NaNs, you can use a custom function (still faster!):</p>
<pre><code>def matcher(x):
    m = p.search(str(x))
    if m:
        return m.group(0)
    return np.nan

df['col2'] = [matcher(x) for x in df['col']]
</code></pre>
<p>The <code>matcher</code> function is very extensible. It can be fitted to return a list for each capture group, as needed. Just extract query the <code>group</code> or <code>groups</code> attribute of the matcher object.</p>
<p>For <code>str.extractall</code>, change <code>p.search</code> to <code>p.findall</code>.</p>
<p><strong>String Extraction</strong><br/>
Consider a simple filtering operation. The idea is to extract 4 digits if it is preceded by an upper case letter.</p>
<pre><code># Extracting strings.
p = re.compile(r'(?&lt;=[A-Z])(\d{4})')
def matcher(x):
    m = p.search(x)
    if m:
        return m.group(0)
    return np.nan

ser.str.extract(r'(?&lt;=[A-Z])(\d{4})', expand=False)   #  str.extract
pd.Series([matcher(x) for x in ser])                  #  list comprehension
</code></pre>
<p><a href="https://i.stack.imgur.com/XmXyX.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/XmXyX.png"/></a></p>
<p><strong>More Examples</strong><br/>
Full disclosure - I am the author (in part or whole) of these posts listed below.</p>
<ul>
<li><p><a href="https://stackoverflow.com/questions/50444346/fast-punctuation-removal-with-pandas">Fast punctuation removal with pandas</a></p></li>
<li><p><a href="https://stackoverflow.com/questions/11858472/string-concatenation-of-two-pandas-columns/54298586#54298586">String concatenation of two pandas columns</a> </p></li>
<li><p><a href="https://stackoverflow.com/questions/13682044/remove-unwanted-parts-from-strings-in-a-column/54302517#54302517">Remove unwanted parts from strings in a column</a></p></li>
<li><p><a href="https://stackoverflow.com/a/47813144/4909087">Replace all but the last occurrence of a character in a dataframe</a></p></li>
</ul>
<hr/>
<h3>Conclusion</h3>
<p>As shown from the examples above, iteration shines when working with small rows of DataFrames, mixed datatypes, and regular expressions.</p>
<p>The speedup you get depends on your data and your problem, so your mileage may vary. The best thing to do is to carefully run tests and see if the payout is worth the effort. </p>
<p>The "vectorized" functions shine in their simplicity and readability, so if performance is not critical, you should definitely prefer those. </p>
<p>Another side note, certain string operations deal with constraints that favour the use of NumPy. Here are two examples where careful NumPy vectorization outperforms python: </p>
<ul>
<li><p><a href="https://stackoverflow.com/a/49471005/4909087">Create new column with incremental values in a faster and efficient way - Answer by Divakar</a></p></li>
<li><p><a href="https://stackoverflow.com/a/50518852/4909087">Fast punctuation removal with pandas - Answer by Paul Panzer</a></p></li>
</ul>
<p>Additionally, sometimes just operating on the underlying arrays via <code>.values</code> as opposed to on the Series or DataFrames can offer a healthy enough speedup for most usual scenarios (see the <strong>Note</strong> in the <strong>Numeric Comparison</strong> section above). So, for example <code>df[df.A.values != df.B.values]</code> would show instant performance boosts over <code>df[df.A != df.B]</code>. Using <code>.values</code> may not be appropriate in every situation, but it is a useful hack to know. </p>
<p>As mentioned above, it's up to you to decide whether these solutions are worth the trouble of implementing.</p>
<hr/>
<h3>Appendix: Code Snippets</h3>
<pre><code>import perfplot  
import operator 
import pandas as pd
import numpy as np
import re

from collections import Counter
from itertools import chain
</code></pre>
<p></p>
<pre><code># Boolean indexing with Numeric value comparison.
perfplot.show(
    setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=['A','B']),
    kernels=[
        lambda df: df[df.A != df.B],
        lambda df: df.query('A != B'),
        lambda df: df[[x != y for x, y in zip(df.A, df.B)]],
        lambda df: df[get_mask(df.A.values, df.B.values)]
    ],
    labels=['vectorized !=', 'query (numexpr)', 'list comp', 'numba'],
    n_range=[2**k for k in range(0, 15)],
    xlabel='N'
)
</code></pre>
<p></p>
<pre><code># Value Counts comparison.
perfplot.show(
    setup=lambda n: pd.Series(np.random.choice(1000, n)),
    kernels=[
        lambda ser: ser.value_counts(sort=False).to_dict(),
        lambda ser: dict(zip(*np.unique(ser, return_counts=True))),
        lambda ser: Counter(ser),
    ],
    labels=['value_counts', 'np.unique', 'Counter'],
    n_range=[2**k for k in range(0, 15)],
    xlabel='N',
    equality_check=lambda x, y: dict(x) == dict(y)
)
</code></pre>
<p></p>
<pre><code># Boolean indexing with string value comparison.
perfplot.show(
    setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=['A','B'], dtype=str),
    kernels=[
        lambda df: df[df.A != df.B],
        lambda df: df.query('A != B'),
        lambda df: df[[x != y for x, y in zip(df.A, df.B)]],
    ],
    labels=['vectorized !=', 'query (numexpr)', 'list comp'],
    n_range=[2**k for k in range(0, 15)],
    xlabel='N',
    equality_check=None
)
</code></pre>
<p></p>
<pre><code># Dictionary value extraction.
ser1 = pd.Series([{'key': 'abc', 'value': 123}, {'key': 'xyz', 'value': 456}])
perfplot.show(
    setup=lambda n: pd.concat([ser1] * n, ignore_index=True),
    kernels=[
        lambda ser: ser.map(operator.itemgetter('value')),
        lambda ser: pd.Series([x.get('value') for x in ser]),
    ],
    labels=['map', 'list comprehension'],
    n_range=[2**k for k in range(0, 15)],
    xlabel='N',
    equality_check=None
)
</code></pre>
<p></p>
<pre><code># List positional indexing. 
ser2 = pd.Series([['a', 'b', 'c'], [1, 2], []])        
perfplot.show(
    setup=lambda n: pd.concat([ser2] * n, ignore_index=True),
    kernels=[
        lambda ser: ser.map(get_0th),
        lambda ser: ser.str[0],
        lambda ser: pd.Series([x[0] if len(x) &gt; 0 else np.nan for x in ser]),
        lambda ser: pd.Series([get_0th(x) for x in ser]),
    ],
    labels=['map', 'str accessor', 'list comprehension', 'list comp safe'],
    n_range=[2**k for k in range(0, 15)],
    xlabel='N',
    equality_check=None
)
</code></pre>
<p></p>
<pre><code># Nested list flattening.
ser3 = pd.Series([['a', 'b', 'c'], ['d', 'e'], ['f', 'g']])
perfplot.show(
    setup=lambda n: pd.concat([ser2] * n, ignore_index=True),
    kernels=[
        lambda ser: pd.DataFrame(ser.tolist()).stack().reset_index(drop=True),
        lambda ser: pd.Series(list(chain.from_iterable(ser.tolist()))),
        lambda ser: pd.Series([y for x in ser for y in x]),
    ],
    labels=['stack', 'itertools.chain', 'nested list comp'],
    n_range=[2**k for k in range(0, 15)],
    xlabel='N',    
    equality_check=None

)
</code></pre>
<p></p>
<pre><code># Extracting strings.
ser4 = pd.Series(['foo xyz', 'test A1234', 'D3345 xtz'])
perfplot.show(
    setup=lambda n: pd.concat([ser4] * n, ignore_index=True),
    kernels=[
        lambda ser: ser.str.extract(r'(?&lt;=[A-Z])(\d{4})', expand=False),
        lambda ser: pd.Series([matcher(x) for x in ser])
    ],
    labels=['str.extract', 'list comprehension'],
    n_range=[2**k for k in range(0, 15)],
    xlabel='N',
    equality_check=None
)
</code></pre>
</div>
<span class="comment-copy">PS. Some may think it's wrong that I'm answering my own question right after posting it. Before downvoting, please read <a href="http://blog.stackoverflow.com/2011/07/its-ok-to-ask-and-answer-your-own-questions/">It's OK to Ask and Answer Your Own Questions</a>. See also <a href="http://blog.stackoverflow.com/2014/04/putting-the-community-back-in-wiki/?cb=1">Putting the Community back in Wiki</a>, which says "Compiling a canonical reference" is "something wonderful". Questions like this one are asked often enough to justify writing a post that can concisely answer all of them.</span>
<span class="comment-copy">We shouldn't forget that many <i>vectorisable</i> operations are faster using a <code>for</code> loop <i>if</i> you utilise <code>numba</code>, e.g. <a href="https://stackoverflow.com/questions/53020764/efficiently-return-the-index-of-the-first-value-satisfying-condition-in-array">this NumPy example</a>. Relevant since you can assign to a series via an array.</span>
<span class="comment-copy">Very interesting, thanks!</span>
