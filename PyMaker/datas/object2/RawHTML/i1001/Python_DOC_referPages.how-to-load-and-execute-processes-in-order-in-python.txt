<div class="post-text" itemprop="text">
<p>Is there any map-like method which doesn't load all sub-processes in memory at once, instead, if total CPU threads is four, it firstly load four process and execute it, if one of four finishes, it will load another one and replace this.</p>
<p>The pool.map in standard library just loads all of jobs at once, and execute them in random order. If there was a large amount of jobs to execute, the memory will overflow.</p>
<p>I have read the <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">official documentation of Python3</a> , I don't find any related material yet. </p>
<p>The feature I required is too detailed, I don't think there is any third-part library which implement this feature on purpose. </p>
<p><strong>What I expected:</strong></p>
<p>There is four core computer.</p>
<pre><code>y = XXX.map(f,range(1,100))
</code></pre>
<p>if 1~4 doesn't finish, there is <strong>no f(5) in system memory</strong>. When one of these four tasks finishes, for example f(2), it will load f(5) at the position of f(2).</p>
<p>Let's talk about the feature of function 'f'. f is a heavy memory consuming function, its instance have to take up about huge amount of memory.</p>
</div>
<div class="post-text" itemprop="text">
<p>First of all the concept of map having all subprocesses in memory is incorrect, map has the entire iterable (input) in memory by turning it into a list. However it only has the number of workers (pool) you create, see example.</p>
<p>If the problem is that the iterable is long and consumes a lot of memory by itself then imap is a better choice as it doesn't keep the whole iterable in memory, it just takes the next one and hands it to a worker. An added benefit is that imap returns the results directly (but in order), so that it can be used by the main process.</p>
<p>After one worker completes it's task the memory is released as can be verified with the example code because of the time delays.</p>
<p><strong>Example:</strong></p>
<pre><code>import multiprocessing
import random
import time

def func(x):
    """
    1. Prints the process and input
    2. Waits a bit
    3. Uses a lot of memory
    4. Waits a random amount more
    """
    print(f'{multiprocessing.current_process()}: {x}')
    time.sleep(5)
    a = list(range(10000000))
    time.sleep(5 + random.randint(0, 5))

if __name__ == "__main__":
    pool = multiprocessing.Pool(processes=4)
    pool.map(func, range(10))
</code></pre>
<p>Output:</p>
<pre><code>&lt;ForkProcess(ForkPoolWorker-1, started daemon)&gt;: 0
&lt;ForkProcess(ForkPoolWorker-2, started daemon)&gt;: 1
&lt;ForkProcess(ForkPoolWorker-3, started daemon)&gt;: 2
&lt;ForkProcess(ForkPoolWorker-4, started daemon)&gt;: 3
&lt;ForkProcess(ForkPoolWorker-2, started daemon)&gt;: 4
&lt;ForkProcess(ForkPoolWorker-4, started daemon)&gt;: 5
&lt;ForkProcess(ForkPoolWorker-3, started daemon)&gt;: 6
&lt;ForkProcess(ForkPoolWorker-1, started daemon)&gt;: 7
&lt;ForkProcess(ForkPoolWorker-4, started daemon)&gt;: 8
&lt;ForkProcess(ForkPoolWorker-3, started daemon)&gt;: 9
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I'm borrowing from Treddy's answer <a href="https://stackoverflow.com/questions/20886565/using-multiprocessing-process-with-a-maximum-number-of-simultaneous-processes">here</a>:</p>
<p>You just need to edit the pool processes value. For example, like this:</p>
<pre><code>from multiprocessing import Pool

def f(x):
    return x*x

if __name__ == '__main__':
    pool = Pool(processes=4)              # start 4 worker processes
    result = pool.apply_async(f, [10])    # evaluate "f(10)" asynchronously
    print result.get(timeout=1)           # prints "100" unless your computer is *very* slow
    print pool.map(f, range(10))          # prints "[0, 1, 4,..., 81]"
</code></pre>
<p>In this example, there are 10 jobs to be done, but it maxes out at 4 processes. If you left the processes value blank, it would use as much resources as were available.</p>
<p>Does this help?</p>
</div>
<span class="comment-copy">Checkout imap in multiprocessing module</span>
<span class="comment-copy">Possible duplicate of <a href="https://stackoverflow.com/questions/20886565/using-multiprocessing-process-with-a-maximum-number-of-simultaneous-processes">Using multiprocessing.Process with a maximum number of simultaneous processes</a></span>
<span class="comment-copy">@UlrichEckhardt, I think they are different, please see what I added in the post.</span>
<span class="comment-copy">@SimonF, according to official document, the different between 'imap' and 'map' is " For very long iterables using a large value for chunksize can make the job complete much faster than using the default value of 1." . So I don't think it is what I am finding.</span>
<span class="comment-copy">@davmos I think you are missunderstanding the documententation, just as I missunderstand your question. Read this link for the difference of map/imap: <a href="https://stackoverflow.com/questions/26520781/multiprocessing-pool-whats-the-difference-between-map-async-and-imap" title="multiprocessing pool whats the difference between map async and imap">stackoverflow.com/questions/26520781/â€¦</a></span>
<span class="comment-copy">I'm trying to figure why my answer was downvoted. I'll work on improving my answers in the future.</span>
<span class="comment-copy">I could imagine that it was because it only repeats an existing answer. The proper reaction for duplicate questions is to close them as duplicate, though that option requires a certain reputation score.</span>
<span class="comment-copy">Thanks Ulrich. I'll keep that in mind for when I can mark as duplicate. Cheers.</span>
