<div class="post-text" itemprop="text">
<p>I want to sort values in a csv file based on timestamp and print it to another file, but for files with many lines python runs out of memory (when the file is being read). 
Is there something I can do to make this more efficient or should I use something else then csv.DictReader?</p>
<pre><code>import csv, sys
import datetime
from pathlib import Path

localPath = "C:/MyPath"


    # data variables 
dataDir = localPath + "data/" dataExtension = ".dat" 

    pathlistData = Path(dataDir).glob('**/*'+ dataExtension)

    # Generated filename as date, Format: YYYY-DDDTHH
    generatedDataDir = localPath + "result/"
    #generatedExtension = ".dat"
    errorlog = 'errorlog.csv'

    fieldnames = ['TimeStamp', 'A', 'B', 'C', 'C', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L','M', 'N', 'O', 'P', 'Q', 'R'] 

    for dataPath in pathlistData:
        #stores our data in a dictionary
        dataDictionary = {}

        dataFileName = str(dataPath).replace('\\', '/')
        newFilePathString = dataFileName.replace(dataDir,generatedDataDir)

        with open(dataPath, 'r') as readFile:
            print(str("Reading data from " + dataFileName))
            keysAsDate = []#[datetime.datetime.strptime(ts, "%Y-%m-%d") for ts in timestamps]
            reader = csv.DictReader(readFile, fieldnames=fieldnames) 

            for row in reader:

                try:
                    timestamp = row['TimeStamp']
                    #create a key based on the timestamp
                    timestampKey = datetime.datetime.strptime(timestamp[0:16], "%Y-%jT%H:%M:%S")
                    #save this key as a date, used later for sorting
                    keysAsDate.append(timestampKey)
                    #save the row data in a dictionary
                    dataDictionary[timestampKey] = row

                except csv.Error as e:
                    sys.exit('file %s, line %d: %s' % (errorlog, reader.line_num, e))

            #sort the keys
            keysAsDate.sort()
        readFile.close()

        with open(newFilePathString, 'w') as writeFile:
            writer = csv.DictWriter(writeFile, fieldnames=fieldnames, lineterminator='\n')
            print(str("Writing data to " + newFilePathString))
            #loop over the sorted keys
            for idx in range(0, len(keysAsDate)):

                #get the row from our data dictionary 
                writeRow = dataDictionary[keysAsDate[idx]]
                #print(dataDictionary[keysAsDate[key]])
                writer.writerow(writeRow)
                if idx%30000 == 0:
                    print("Writing to new file: " + str(int(idx/len(keysAsDate) * 100)) + "%")


        print(str("Finished writing to file: " + newFilePathString))

        writeFile.close()
</code></pre>
<p><strong>UPDATE:</strong> I have used pandas and divided the large file into smaller chunks that I could sort individually.
This currently doesn't solve the issue for wildly misplaced values if I append the files after each other.</p>
<pre><code>for dataPath in pathlistData:

dataFileName = str(dataPath).replace('\\', '/')
#newFilePathString = dataFileName.replace(dataDir,generatedDataDir)


print(str("Reading data from " + dataFileName))
#divide our large data frame into smaller data frame chunks
#so we can sort the content in memory
for df_chunk in pd.read_csv(dataFileName, header = None, chunksize = chunk_size, names = fieldnames):
    dataDictionary = {}
    dataDictionary.clear()

    for idx in range(0, chunk_size):
        #print(df_chunk[idx:idx+1])
        row = df_chunk[idx:idx+1]
        dataDictionary = df_chunk.sort_values(['TimeStamp'], ascending=True)
    firstTimeStampInChunk = dataDictionary[0:1]['TimeStamp']
    #print("first: " + firstTimeStampInChunk)
    lastTimeStampInChunk = dataDictionary[chunk_size-1:chunk_size]['TimeStamp']
    #print("last: " + lastTimeStampInChunk)

    timestampStr = str(firstTimeStampInChunk)[chunk_shift:timestamp_size+chunk_shift] + str(lastTimeStampInChunk)[chunk_shift:timestamp_size+chunk_shift]
    tempFilePathString = str(timestampStr + dataExtension).replace(':', '_').replace('\\', '/')
    dataDictionary.to_csv('temp/'+tempFilePathString, header = None, index=False)

# data variables
tempDataDir = localPath + "temp/"
tempPathlistData = Path(tempDataDir).glob('**/*'+ dataExtension)

tempPathList = list(tempPathlistData)
</code></pre>
<p>My algorithm theory (no code) to solve the issue with random values is: </p>
<p><strong>Step 1</strong> - divide into smaller chunks where "chunk_size = max lines to handle in memory divided by two"</p>
<p><strong>Step 2</strong> - Loop through files in order, merge two files at a time and sort them together, then split them up again so no file is larger than chunk_size. </p>
<p><strong>Step 3</strong> - Loop through backwards, merging two files at a time and sorting them, then split up again so no file is larger than chunk_size. </p>
<p><strong>Step 4</strong> - Now all wildly misplaced low values should have travelled to the lowest part, and all the wildly misplaced high values should have travelled to the highest part. Append the files in order!</p>
<p>Cons; The time complexity for this is not preferable at all, basically O(N^2) if I'm not mistaken</p>
</div>
<div class="post-text" itemprop="text">
<p>Try pandas csv reader, which is quite efficient. (<a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" rel="nofollow noreferrer">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a>). You can easily convert between pandas and dictionaries using <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html" rel="nofollow noreferrer">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You explained that an in-memory sort wouldn't work for you, as file size exceeds memory size. There are at least two ways out of this. Both rely on doing more file I/O.</p>
<ol>
<li>Compress long records down to a single memory-efficient file offset. Call <a href="https://docs.python.org/3/library/io.html?highlight=tell#io.TextIOBase.seek" rel="nofollow noreferrer"><code>tell()</code></a> as you read each record (or sum the line lengths), and retain just timestamps plus file offsets in memory. Sort the offsets by timestamp. Repeatedly call <code>seek()</code> as you go through the sorted tuples, do random read of a record, and append that to your output file.</li>
<li>A much better approach is to let <code>/usr/bin/sort</code> do an external mergesort. Windows users can obtain coreutils GNU sort from <a href="https://git-scm.com/download/" rel="nofollow noreferrer">https://git-scm.com/download/</a>. Use the <a href="https://docs.python.org/3/library/subprocess.html#subprocess.run" rel="nofollow noreferrer">subprocess</a> module to call it.</li>
</ol>
</div>
<span class="comment-copy">Even though pandas might be more efficient it does not solve this problem unfortunately. The memory simply can't read all the values in memory at once. However I could use the chunk_size to divide the file into smaller files and sort those. For wildly displaced values it does not fix the problem completely though.</span>
<span class="comment-copy">Have you tried increasing the size allocated for SWAP memory? It will be quite slow, but at least you won't run out of memory</span>
