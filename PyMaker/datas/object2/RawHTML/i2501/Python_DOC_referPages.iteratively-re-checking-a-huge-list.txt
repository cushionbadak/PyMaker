<div class="post-text" itemprop="text">
<p>I have a list of about 100,000 URLs saved in my computer. ( that 100,000 can very quickly multiply into several million.) For every url, i check that webpage and collect all additional urls on that page, but only if each additional link is not already in my large list. The issue here is reloading that huge list into memory iteratively so i can consistently have an accurate list. where the amount of memory used will probably very soon become much too much, and even more importantly, the time it takes inbetween reloading the list gets longer which is severely holding up the progress of the project.</p>
<p>My list is saved in several different formats. One format is by having all links contained in one single text file, where i open(filetext).readlines() to turn it straight into a list. Another format i have saved which seems more helpful, is by saving a folder tree with all the links, and turning that into a list by using os.walk(path).</p>
<p>im really unsure of any other way to do this recurring conditional check more efficiently, without the ridiculous use of memory and loadimg time. i tried using a queue as well, but It was such a benefit to be able to see the text output of these links that queueing became unneccesarily complicated. where else can i even start?</p>
</div>
<div class="post-text" itemprop="text">
<p>The main issue is not to load the list in memory. This should be done only once at the beginning, before scrapping the webpages. The issue is to find if an element is already in the list. The <code>in</code> operation will be too long for large list.</p>
<p>You should try to look into several thinks; among which <code>sets</code> and <code>pandas</code>. The first one will probably be the optimal solution.</p>
<p>Now, since you thought of using a folder tree with the urls as folder names, I can think of one way which could be faster. Instead of creating the list with <code>os.walk(path)</code>, try to look if the folder is already present. If not, it means you did not have that url yet. This is basically a fake graph database. To do so, you could use the function <code>os.path.isdir()</code>. If you want a true graph DB, you could look into OrientDB for instance.</p>
</div>
<div class="post-text" itemprop="text">
<p>Have you considered mapping a table of IP addresses to URL? Granted this would only work if you are seeking unique domains vs thousands of pages on the same domain. The advantage is you would be dealing with a 12 integer address. The downside is the need for additional tabulated data structures and additional processes to map the data.</p>
</div>
<div class="post-text" itemprop="text">
<p>Memory shouldn't be an issue. If each url takes 1KiB to store in memory (very generous), 1 million urls will be 1GiB. You say you have 8GiB of RAM.</p>
<p>You can keep known urls in memory in a <a href="https://docs.python.org/3/tutorial/datastructures.html#sets" rel="nofollow noreferrer"><strong><code>set</code></strong></a> and check for containment using <code>in</code> or <code>not in</code>. Python's <code>set</code> uses hashing, so searching for containment is O(1) which is a lot quicker in general than the linear search of a <code>list</code>.</p>
<p>You can recursively scrape the pages:</p>
<pre><code>def main():
    with open('urls.txt') as urls:
        known_urls = set(urls)

    for url in list(known_urls):
        scrape(url, known_urls)


def scrape(url, known_urls):
    new_urls = _parse_page_urls(url)
    for new_url in new_urls:
        if new_url not in known_urls:
            known_urls.add(new_url)
            scrape(new_url, known_urls)
</code></pre>
</div>
<span class="comment-copy">How much memory does a link use? How much RAM do you have? I can't think a few million links would be a memory problem. Put them in memory in a <code>set</code>.</span>
<span class="comment-copy">Being conservative, if a link uses 1KiB then 1 million links is 1GiB.</span>
<span class="comment-copy">im not sure honestly, maybe memory isn't as much an issue as i thought it would be. I have 8g ram but i haven't gotten that far yet because of how long it takes to even load the whole list.</span>
<span class="comment-copy">@JilliamWones A list of string should not be that long to load. Few seconds top. Could you share your code?</span>
<span class="comment-copy">Is really memory the issue here or that you try to repeatedly lookup whether a certain entry exists in a <code>list</code>? And that you re-read that list from file each time? Have you tried just keeping a <code>set</code> in memory? If memory is (also) a problem, and if many URLs share common parts (like sub-pages on a larger webpage) you might also consider using a prefix-tree / Trie.</span>
<span class="comment-copy">If you strip path, query, protocol, etc. from the URL you'd save almost as much (in some cases maybe more) as when converting to the IP, and that's probably much simpler.</span>
