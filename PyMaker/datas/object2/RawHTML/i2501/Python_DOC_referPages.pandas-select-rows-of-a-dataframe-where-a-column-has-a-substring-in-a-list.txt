<div class="post-text" itemprop="text">
<p>I need to filter rows in a <code>pandas</code> dataframe so that a specific string column contains at least one of a list of provided substrings. The substrings may have unusual / regex characters. The comparison should not involve regex and is case insensitive.</p>
<p>For example:</p>
<pre><code>lst = ['kdSj;af-!?', 'aBC+dsfa?\-', 'sdKaJg|dksaf-*']
</code></pre>
<p>I currently apply the mask like this:</p>
<pre><code>mask = np.logical_or.reduce([df[col].str.contains(i, regex=False, case=False) for i in lst])
df = df[mask]
</code></pre>
<p>My dataframe is large (~1mio rows) and <code>lst</code> has length 100. Is there a more efficient way? For example, if the first item in <code>lst</code> is found, we should not have to test any subsequent strings for that row.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you're sticking to using pure-pandas, for both performance and practicality I think you <em>should</em> use regex for this task. However, you will need to properly escape any special characters in the substrings first to ensure that they are matched literally (and not used as regex meta characters).</p>
<p>This is easy to do using <a href="https://docs.python.org/3/library/re.html#re.escape" rel="noreferrer"><code>re.escape</code></a>:</p>
<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; esc_lst = [re.escape(s) for s in lst]
</code></pre>
<p>These escaped substrings can then be joined using a regex pipe <code>|</code>. Each of the substrings can be checked against a string until one matches (or they have all been tested). </p>
<pre><code>&gt;&gt;&gt; pattern = '|'.join(esc_lst)
</code></pre>
<p>The masking stage then becomes a single low-level loop through the rows:</p>
<pre><code>df[col].str.contains(pattern, case=False)
</code></pre>
<hr/>
<p>Here's a simple setup to get a sense of performance:</p>
<pre><code>from random import randint, seed

seed(321)

# 100 substrings of 5 characters
lst = [''.join([chr(randint(0, 256)) for _ in range(5)]) for _ in range(100)]

# 50000 strings of 20 characters
strings = [''.join([chr(randint(0, 256)) for _ in range(20)]) for _ in range(50000)]

col = pd.Series(strings)
esc_lst = [re.escape(s) for s in lst]
pattern = '|'.join(esc_lst)
</code></pre>
<p>The proposed method takes about 1 second (so maybe up to 20 seconds for 1 million rows):</p>
<pre><code>%timeit col.str.contains(pattern, case=False)
1 loop, best of 3: 981 ms per loop
</code></pre>
<p>The method in the question took approximately 5 seconds using the same input data. </p>
<p>It's worth noting that these times are 'worst case' in the sense that there were no matches (so <em>all</em> substrings were checked). If there are matches than the timing will improve. </p>
</div>
<div class="post-text" itemprop="text">
<p>You could try using the <a href="https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm" rel="noreferrer">Aho-Corasick algorithm</a>. In the average case, it is <code>O(n+m+p)</code> where <code>n</code> is length of the search strings and <code>m</code> is the length of the searched text and <code>p</code> is the number of output matches.</p>
<p>The Aho-Corasick algorithm is <a href="https://stackoverflow.com/a/23341452/190597">often used</a> to find multiple patterns (needles) in an input text (the haystack).</p>
<p><a href="https://pypi.python.org/pypi/pyahocorasick" rel="noreferrer">pyahocorasick</a> is a Python wrapper around a C implementation of the algorithm.</p>
<hr/>
<p>Let's compare how fast it is versus some alternatives. Below is a benchmark
showing <code>using_aho_corasick</code> to be over 30x faster than the original method
(shown in the question) on a 50K-row DataFrame test case:</p>
<pre><code>|                    |     speed factor | ms per loop |
|                    | compared to orig |             |
|--------------------+------------------+-------------|
| using_aho_corasick |            30.7x |         140 |
| using_regex        |             2.7x |        1580 |
| orig               |             1.0x |        4300 |
</code></pre>
<hr/>
<pre><code>In [89]: %timeit using_ahocorasick(col, lst)
10 loops, best of 3: 140 ms per loop

In [88]: %timeit using_regex(col, lst)
1 loop, best of 3: 1.58 s per loop

In [91]: %timeit orig(col, lst)
1 loop, best of 3: 4.3 s per loop
</code></pre>
<hr/>
<p>Here the setup used for the benchmark. It also verifies that the output matches the result returned by <code>orig</code>:</p>
<pre><code>import numpy as np
import random
import pandas as pd
import ahocorasick
import re

random.seed(321)

def orig(col, lst):
    mask = np.logical_or.reduce([col.str.contains(i, regex=False, case=False) 
                                 for i in lst])
    return mask

def using_regex(col, lst):
    """https://stackoverflow.com/a/48590850/190597 (Alex Riley)"""
    esc_lst = [re.escape(s) for s in lst]
    pattern = '|'.join(esc_lst)
    mask = col.str.contains(pattern, case=False)
    return mask

def using_ahocorasick(col, lst):
    A = ahocorasick.Automaton(ahocorasick.STORE_INTS)
    for word in lst:
        A.add_word(word.lower())
    A.make_automaton() 
    col = col.str.lower()
    mask = col.apply(lambda x: bool(list(A.iter(x))))
    return mask

N = 50000
# 100 substrings of 5 characters
lst = [''.join([chr(random.randint(0, 256)) for _ in range(5)]) for _ in range(100)]

# N strings of 20 characters
strings = [''.join([chr(random.randint(0, 256)) for _ in range(20)]) for _ in range(N)]
# make about 10% of the strings match a string from lst; this helps check that our method works
strings = [_ if random.randint(0, 99) &lt; 10 else _+random.choice(lst) for _ in strings]

col = pd.Series(strings)

expected = orig(col, lst)
for name, result in [('using_regex', using_regex(col, lst)),
                     ('using_ahocorasick', using_ahocorasick(col, lst))]:
    status = 'pass' if np.allclose(expected, result) else 'fail'
    print('{}: {}'.format(name, status))
</code></pre>
</div>
<span class="comment-copy">very interesting. would it be possible to use this package in a pandas dataframe or that would reduce the performance (because of the loop I guess)?</span>
<span class="comment-copy">The benchmark shown above would still apply. Above, <code>A.iter</code> is called inside a call to <code>col.apply</code> where <code>col</code> is a pandas Series. That's not very different from (or maybe even exactly the same as) what you would be doing with a pandas DataFrame. Using <code>apply</code> has about the same performance as a simple Python loop, but you'd still be getting the benefit of using the Aho-Corasick algorithm.</span>
