<div class="post-text" itemprop="text">
<p>I have data in below format. </p>
<pre><code>abc, x1, x2, x3  
def, x1, x3, x4,x8,x9   
ghi, x7, x10, x11  
</code></pre>
<p>The output I want is</p>
<pre><code>0,abc, [x1, x2, x3]  
1,def, [x1, x3, x4,x8,x9]  
2,ghi, [x7, x10, x11]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Your data is not in CSV format. CSV means a comma-separated text file with a fixed schema. The CSV for your data would be:</p>
<pre><code>abc,x1,x2,x3,,
def,x1,x3,x4,x8,x9
ghi,x7,x10,x11,,
</code></pre>
<p>Note the trailing commas in lines 1 &amp; 3, which are not in your data.</p>
<p>Since you have a text file that is not a CSV, the way to get to the schema you want in Spark is to read the whole file in Python, parse into what you want and then use <code>spark.crateDataFrame()</code>. Alternatively, if you have more than one file like this in a directory, use <code>SparkContext.wholeTextFiles</code> and then <code>flatMap</code> your parsing function.</p>
<p>Assuming you've already done something like <code>open("Your File.txt").readlines</code>, the rest is simple:</p>
<pre><code>import re
from pyspark.sql import *

lines = [
  "abc, x1, x2, x3",
  "def, x1, x3, x4,x8,x9",
  "ghi, x7, x10, x11"
]

split = re.compile("\s*,\s*")
Line = Row("id", "first", "rest")

def parse_line(id, line):
  tokens = split.split(line.strip)
  return Line(id, tokens[0], tokens.pop(0))

def parse_lines(lines):
  return [parse_line(i, x) for i,x in enumerate(lines)]

spark.createDataFrame(parse_lines(lines))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>What you can do is to generate first the id using <code>zipWithIndex</code> and then inside the map function take the first part of the string with <code>r[0].split(",")[0]</code> and the second with <code>r[0].split(",")[1:]</code>.</p>
<p>Here is the code as described above:</p>
<pre><code>from pyspark.sql.types import StringType

lines = ["abc, x1, x2, x3",
        "def, x1, x3, x4,x8,x9",
        "ghi, x7, x10, x11"]

df = spark.createDataFrame(lines, StringType())
df = df.rdd.zipWithIndex() \
           .map(lambda (r, indx): (indx, r[0].split(",")[0], r[0].split(",")[1:])) \
           .toDF(["id", "name", "x_col"])

df.show(10, False)
</code></pre>
<p>And the output:</p>
<pre><code>+---+----+-----------------------+
|id |name|x_col                  |
+---+----+-----------------------+
|0  |abc |[ x1,  x2,  x3]        |
|1  |def |[ x1,  x3,  x4, x8, x9]|
|2  |ghi |[ x7,  x10,  x11]      |
+---+----+-----------------------+
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If data come in file, can implemented in such way:</p>
<ol>
<li>Read file as CSV;</li>
<li>Add index column with "monotonically_increasing_id"</li>
<li>Select first column, and all remaining columns as array.</li>
</ol>
<p>On Scala can be implemented in this way:</p>
<pre><code>val df = spark.read.option("header", "false").csv("non-csv.txt")
val remainingColumns = df.columns.tail
df.withColumn("id", monotonically_increasing_id).
  select(
    col("id"),
    col(df.columns(0)),
    array(remainingColumns.head, remainingColumns.tail: _*)
  ).show(false)
</code></pre>
<p>Output:</p>
<pre><code>+---+---+--------------------+
|id |_c0|array(_c1, _c2, _c3)|
+---+---+--------------------+
|0  |abc|[ x1,  x2,  x3]     |
|1  |def|[ x1,  x3,  x4]     |
|2  |ghi|[ x7,  x10,  x11]   |
+---+---+--------------------+
</code></pre>
</div>
<span class="comment-copy">Have you tried anything ?</span>
<span class="comment-copy">I have tried 2 solutions provided and done the problem with little modifications. what I have done is by converting into rdd and then using map function.    rdd = spark.read.text(filename).rdd     rdd = rdd.map(lambda x: Row(number=str(x['value'].split(',')[0]), count=str(x['value'].split(',')[1:])))</span>
<span class="comment-copy">Sorry data is not csv but the data is separated by commas and I want it separated by name and parameters. Could me help me by how to use flatmap for parsing it.</span>
<span class="comment-copy"><code>flatMap</code> is only for the cases where you have lots of files. I updated the answer with example code.</span>
