<div class="post-text" itemprop="text">
<p>My main question is; is averaging the loss the same thing as averaging the gradient and how do i accumulate my loss over mini batches then calculate my gradient?</p>
<p>I have been trying to implement policy gradient in Tensorflow and run into the issue where i can not feed all my game states into my network at once and then update. The problem is if i lower my network size then train on all frames at once and take the mean of the loss then it begins to converge nicely. But if I accumulate the gradients over mini batches then average them, my gradients explode and i overflow my weights.</p>
<p>Any help or insight will be very appreciated.</p>
<p>Keep in mind also, this is my first time asking a question here.</p>
</div>
<div class="post-text" itemprop="text">
<p>What you can do is to accumulate gradients after each mini-batch and then update the weights based on gradient averages. Consider following simple case for fitting 50 Gaussian blobs with a single-layered perceptron:</p>
<pre><code>from sklearn.datasets import make_blobs
import tensorflow as tf
import numpy as np

x_train, y_train = make_blobs(n_samples=50,
                              n_features=2,
                              centers=[[1, 1], [-1, -1]],
                              cluster_std=0.5)

with tf.name_scope('x'):
    x = tf.placeholder(tf.float32, [None, 2])
    y = tf.placeholder(tf.int32, [None])

with tf.name_scope('layer'):
    logits = tf.layers.dense(x,
                             units=2,
                             kernel_initializer=tf.contrib.layers.xavier_initializer())
with tf.name_scope('loss'):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
    loss_op = tf.reduce_mean(xentropy)
</code></pre>
<p>The <code>minimize()</code> method of the tensorflow optimizers calls <code>compute_gradients()</code> and then <code>apply_gradients()</code>. Instead of calling the <code>minimize()</code>, I'm going to call both methods directly. First, to get the gradients we call <code>compute_gradients()</code> (which returns a list of tuples <code>grads_and_vars</code>) and for <code>apply_gradients()</code> instead of gradients I'm going to feed placeholders for future gradient's averages:</p>
<pre><code>optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
grads_and_vars = optimizer.compute_gradients(loss_op)
grads = [g for g, v in grads_and_vars]

# placeholders for gradients averages
placeholder_grads = [tf.placeholder(tf.float32, [None] + g.get_shape().as_list())
                     for g in grads]

new_grads_and_vars = [(tf.reduce_mean(p, axis=0), gv[1])
                      for p, gv in zip(placeholder_grads, grads_and_vars)]

apply_grads_op = optimizer.apply_gradients(new_grads_and_vars)
</code></pre>
<p>During mini-batches we only compute losses (you can accumulate losses as well - append to some list and then compute average) and gradients, without applying gradients to weights. At the end of each epoch we execute <code>apply_grads_op</code> operation while feeding accumulated gradients to its placeholders:</p>
<pre><code>data = tf.data.Dataset.from_tensor_slices({'x':x_train, 'y':y_train}).batch(10)
iterator = data.make_initializable_iterator()
n_epochs = 2
with tf.Session() as sess:
    _ = sess.run([tf.global_variables_initializer(), iterator.initializer])
    next_batch = iterator.get_next()
    for epoch in range(n_epochs):
        epoch_grads = []
        while True:
            try:
                batch = sess.run(next_batch)
                evaled = sess.run([loss_op] + grads,
                                  feed_dict={x:batch['x'], y:batch['y']})
                epoch_grads.append(evaled[1:])
                print('batch loss:', evaled[0])
            except tf.errors.OutOfRangeError:
                _ = sess.run(iterator.initializer)
                feed_dict = {p:[g[i] for g in epoch_grads]
                             for i, p in enumerate(placeholder_grads)}
                _ = sess.run(apply_grads_op, feed_dict=feed_dict)

                break
</code></pre>
</div>
<span class="comment-copy">Welcome to SO. Consider adding some code, please</span>
