<div class="post-text" itemprop="text">
<p>I have been trying to scrape a table from <a href="https://www.basketball-reference.com/leagues/NBA_2019.html" rel="nofollow noreferrer">here</a> for quite some time but have been unsuccessful. The table I am trying to scrape is titled "Team Per Game Stats". I am confident that once I am able to scrape one element of that table that I can iterate through the columns I want from the list and eventually end up with a pandas data frame.</p>
<p>Here is my code so far:</p>
<pre><code>from bs4 import BeautifulSoup
import requests

# url that we are scraping
r = requests.get('https://www.basketball-reference.com/leagues/NBA_2019.html')
# Lets look at what the request content looks like
print(r.content)

# use Beautifulsoup on content from request
c = r.content
soup = BeautifulSoup(c)
print(soup)

# using prettify() in Beautiful soup indents HTML like it should be in the web page
# This can make reading the HTML a little be easier
print(soup.prettify())

# get elements within the 'main-content' tag
team_per_game = soup.find(id="all_team-stats-per_game")
print(team_per_game)
</code></pre>
<p>Any help would be greatly appreciated.</p>
</div>
<div class="post-text" itemprop="text">
<p>That webpage employs a trick to try to stop search engines and other automated web clients (including scrapers) from finding the table data: the tables are stored in HTML comments:</p>
<pre class="lang-html prettyprint-override"><code>&lt;div id="all_team-stats-per_game" class="table_wrapper setup_commented commented"&gt;

&lt;div class="section_heading"&gt;
  &lt;span class="section_anchor" id="team-stats-per_game_link" data-label="Team Per Game Stats"&gt;&lt;/span&gt;&lt;h2&gt;Team Per Game Stats&lt;/h2&gt;    &lt;div class="section_heading_text"&gt;
      &lt;ul&gt; &lt;li&gt;&lt;small&gt;* Playoff teams&lt;/small&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/div&gt;      
&lt;/div&gt;
&lt;div class="placeholder"&gt;&lt;/div&gt;
&lt;!--
   &lt;div class="table_outer_container"&gt;
      &lt;div class="overthrow table_container" id="div_team-stats-per_game"&gt;
  &lt;table class="sortable stats_table" id="team-stats-per_game" data-cols-to-freeze=2&gt;&lt;caption&gt;Team Per Game Stats Table&lt;/caption&gt;

...

&lt;/table&gt;

      &lt;/div&gt;
   &lt;/div&gt;
--&gt;
&lt;/div&gt;
</code></pre>
<p>I note that the opening <code>div</code> has <code>setup_commented</code> and <code>commented</code> classes. Javascript code included in the page is then executed by your browser that then loads the text from those comments and replaces the <code>placeholder</code> div with the contents as new HTML for the browser to display.</p>
<p>You can extract the comment text here:</p>
<pre><code>from bs4 import BeautifulSoup, Comment

soup = BeautifulSoup(r.content, 'lxml')
placeholder = soup.select_one('#all_team-stats-per_game .placeholder')
comment = next(elem for elem in placeholder.next_siblings if isinstance(elem, Comment))
table_soup = BeautifulSoup(comment, 'lxml')
</code></pre>
<p>then continue to parse the table HTML.</p>
<p>This specific site has published both <a href="https://www.sports-reference.com/termsofuse.html" rel="nofollow noreferrer">terms of use</a>, and <a href="https://www.sports-reference.com/data_use.html" rel="nofollow noreferrer">a page on data use</a> you should probably read if you are going to use their data. Specifically, their terms state, under section 6. <em>Site Content</em>:</p>
<blockquote>
<p>You may not frame, capture, harvest, or collect any part of the Site or Content without SRL's advance written consent.</p>
</blockquote>
<p>Scraping the data would fall under that heading.</p>
</div>
<div class="post-text" itemprop="text">
<p>Just to complete Martijn Pieters's answer (and without lxml)</p>
<pre><code>from bs4 import BeautifulSoup, Comment
import requests

r = requests.get('https://www.basketball-reference.com/leagues/NBA_2019.html')
soup = BeautifulSoup(r.content, 'html.parser')
placeholder = soup.select_one('#all_team-stats-per_game .placeholder')
comment = next(elem for elem in placeholder.next_siblings if isinstance(elem, Comment))
table = BeautifulSoup(comment, 'html.parser')
rows = table.find_all('tr')
for row in rows:
    cells = row.find_all('td')
    if cells:
        print([cell.text for cell in cells])
</code></pre>
<p>Partial output</p>
<pre><code>[u'New Orleans Pelicans', u'71', u'240.0', u'43.6', u'91.7', u'.476', u'10.1', u'29.4', u'.344', u'33.5', u'62.4', u'.537', u'18.1', u'23.9', u'.760', u'11.0', u'36.0', u'47.0', u'27.0', u'7.5', u'5.5', u'14.5', u'21.4', u'115.5']
[u'Milwaukee Bucks*', u'69', u'241.1', u'43.3', u'90.8', u'.477', u'13.3', u'37.9', u'.351', u'30.0', u'52.9', u'.567', u'17.6', u'22.8', u'.773', u'9.3', u'40.1', u'49.4', u'26.0', u'7.4', u'6.0', u'14.0', u'19.8', u'117.6']
[u'Los Angeles Clippers', u'70', u'241.8', u'41.0', u'87.6', u'.469', u'9.8', u'25.2', u'.387', u'31.3', u'62.3', u'.502', u'22.8', u'28.8', u'.792', u'9.9', u'35.7', u'45.6', u'23.4', u'6.6', u'4.7', u'14.5', u'23.5', u'114.6']
</code></pre>
</div>
<span class="comment-copy">That page cheats, the HTML source for the tables is stored in <i>HTML comments</i>, that <a href="https://stackoverflow.com/questions/6027830/is-it-possible-to-get-reference-to-comment-element-block-by-javascript">Javascript then extracts</a> and turns back in to HTML...</span>
<span class="comment-copy">Is this a preventative method to prevent scraping?</span>
<span class="comment-copy">The more likely reason is to keep the tables from showing up in the Google results page.</span>
<span class="comment-copy">what does the word <i>next</i> do in this context please?</span>
<span class="comment-copy">@QHarr: itâ€™s a <a href="https://docs.python.org/3/library/functions.html#next" rel="nofollow noreferrer">function</a>, it takes the next item from an iterator.  It is passed a generator expression that filters the <code>Element.next_siblings</code> iterator. I used this to skip the text node that directly follows the placeholder div</span>
<span class="comment-copy">Many thanks for taking the time to explain</span>
<span class="comment-copy">Both are incredible answers. Thank you both!</span>
