<div class="post-text" itemprop="text">
<p>I am fairly new to the internals of TensorFlow.  Towards trying to understand TensorFlow's implementation of AdamOptimizer, I checked the corresponding subgraph in TensorBoard.  There seems to be a duplicate subgraph named <code>name + '_1'</code>, where <code>name='Adam'</code> by default.  </p>
<p>The following MWE produces the graph below.  (<strong>Note that I have expanded the <code>x</code> node!</strong>)</p>
<pre><code>import tensorflow as tf

tf.reset_default_graph()
x = tf.Variable(1.0, name='x')
train_step = tf.train.AdamOptimizer(1e-1, name='MyAdam').minimize(x)

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    with tf.summary.FileWriter('./logs/mwe') as writer:
        writer.add_graph(sess.graph)
</code></pre>
<p><a href="https://i.stack.imgur.com/pAhPL.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/pAhPL.png"/></a></p>
<p>I am confused because I would expect the above code to produce just a single namespace inside the graph.  Even after examining the relevant source files (namely <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py" rel="nofollow noreferrer"><code>adam.py</code></a>, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py" rel="nofollow noreferrer"><code>optimizer.py</code></a> and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc" rel="nofollow noreferrer"><code>training_ops.cc</code></a>), it's not clear to me how/why/where the duplicate is created.</p>
<p><strong>Question: What is the source of the duplicate <code>AdamOptimizer</code> subgraph?</strong></p>
<p>I can think of the following possibilities:</p>
<ul>
<li>A bug in my code</li>
<li>Some sort of artifact generated in TensorBoard</li>
<li>This is expected behavior (if so, then why?)</li>
<li>A bug in TensorFlow</li>
</ul>
<h2>Edit: Cleanup and clarification</h2>
<p>Due to some initial confusion, I cluttered my original question with detailed instructions for how to set up a reproducible environment with TensorFlow/TensorBoard which reproduces this graph.  I have now replaced all that with the clarification about expanding the <code>x</code> node.</p>
</div>
<div class="post-text" itemprop="text">
<p>This is not a bug, just a perhaps questionable way of leaking outside of your own scope.</p>
<p>First, not a bug: The Adam optimizer is not duplicated. As can be seen in your graph, there is a single <code>/MyAdam</code> scope, not two. No problem here.</p>
<p>However, there <em>are</em> two <code>MyAdam</code> and <code>MyAdam_1</code> <em>subscopes</em> added to your variable scope. They correspond respectively to the <code>m</code> and <code>v</code> variables (and their initialization operations) of the Adam optimizer for this variable.</p>
<p>This is where choices made by the optimizer are debatable. You could indeed reasonably expect the Adam optimizer operations and variables to be strictly defined within its assigned scope. Instead, they choose to creep in the optimized variables' scope to locate the statistics variables.</p>
<p>So, debatable choice to say the least, but not a bug, in the sense that the Adam optimizer is indeed not duplicated.</p>
<p><strong>EDIT</strong></p>
<p>Note that this way of locating variables is common across optimizers -- you can observe the same effect with a <code>MomentumOptimizer</code> for example. Indeed, this is the standard way of creating <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#slots" rel="nofollow noreferrer">slots</a> for optimizers -- see <a href="https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/python/training/slot_creator.py#L141" rel="nofollow noreferrer">here</a>:</p>
<pre><code># Scope the slot name in the namespace of the primary variable.
# Set "primary.op.name + '/' + name" as default name, so the scope name of
# optimizer can be shared when reuse is True. Meanwhile when reuse is False
# and the same name has been previously used, the scope name will add '_N'
# as suffix for unique identifications.
</code></pre>
<p>So as I understand it, they chose to locate the statistics of a variable within a subscope of the scope of the variable itself, so that if the variable is shared/reused, then its statistics are also shared/reused and do not need to be recomputed. This is indeed a reasonable thing to do, even if again, creeping outside of your scope is somewhat unsettling.</p>
</div>
<span class="comment-copy">@P-Gn, I can reproduce the issue in 1.13.1.  See details in my edit.  Any ideas?</span>
<span class="comment-copy">I now wonder if this would be more appropriate as a GitHub issue.  If so advised, I could copy-paste this into GitHub and delete the SO question.  (I don't have a good sense for where one should draw the distinction between GitHub issue and SO question.)</span>
<span class="comment-copy">Also can't reproduce your error, both TF 1.12 and 1.13</span>
<span class="comment-copy">Thanks for bearing with me.  I just provided <i>exact</i> steps to reproduce on GCP.  I'm more confused than ever about why others can't reproduce this.</span>
<span class="comment-copy">Yes,  you're right,  how could i miss it.  Seems that it is the case with all optimizers,  that create additional values.  But it seems that it is mostly visualization issue,  cause i dont see any additional values in graph or savrd checkpoint</span>
<span class="comment-copy">Ah!  So <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/adam.py#L131-L132" rel="nofollow noreferrer"><code>AdamOptimizer</code></a> calls <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L1121-L1140" rel="nofollow noreferrer"><code>_zeros_slot</code></a> which in turn <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L1135" rel="nofollow noreferrer">calls</a> <code>slot_creator.create_zeros_slot</code>.  However, it fails to pass on <code>slot_name</code> (which would be <code>m</code> or <code>v</code>), and so the corresponding slots are misleadingly named only after <code>AdamOptimizer</code> instead of <code>m</code> or <code>v</code>.</span>
<span class="comment-copy">Yes, using <code>m</code> and <code>v</code> -- perhaps in combination with the optimizer name -- would probably have been clearer.</span>
