<div class="post-text" itemprop="text">
<p>I have a question about <a href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="nofollow noreferrer">Tensorflows Object Detection API</a>. I trained  <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" rel="nofollow noreferrer">Faster R-CNN Inception v2 model</a> with my own dataset for Traffic Sign Classification and I want to deploy it to Android but <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android" rel="nofollow noreferrer">Tensorflows Object Detection API for Android</a> and/or <a href="https://www.tensorflow.org/lite" rel="nofollow noreferrer">Tensorflow Lite</a> seems only supporting SSD models. </p>
<p>Is there any way to deploy a Faster R-CNN model to Android? I mean how can I put my frozen inference graph of Faster R-CNN to android API instead of SSDs frozen inference graph ?</p>
</div>
<div class="post-text" itemprop="text">
<p>For SSD models, it must be possible using the <code>export_tflite_ssd_graph.py</code> tool. But:</p>
<blockquote>
<p>At this time only SSD models are supported. Models like faster_rcnn
  are not supported at this time</p>
</blockquote>
<p>See <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md" rel="nofollow noreferrer">this guide</a> for more information.</p>
</div>
