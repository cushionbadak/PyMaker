<div class="post-text" itemprop="text">
<p>From <a href="https://stackoverflow.com/questions/40817665/whats-the-difference-between-variable-and-resourcevariable-in-tensorflow">here</a></p>
<blockquote>
<p>Unlike tf.Variable, a tf.ResourceVariable has well-defined semantics. Each usage of a ResourceVariable in a TensorFlow graph adds a read_value operation to the graph. The Tensors returned by a read_value operation are guaranteed to see all modifications to the value of the variable which happen in any operation on which the read_value depends on (either directly, indirectly, or via a control dependency) and guaranteed to not see any modification to the value of the variable on which the read_value operation does not depend on. For example, if there is more than one assignment to a ResourceVariable in a single session.run call there is a well-defined value for each operation which uses the variable's value if the assignments and the read are connected by edges in the graph.</p>
</blockquote>
<p>So i tried to test the behavior. My code:</p>
<pre><code>tf.reset_default_graph()
a = tf.placeholder(dtype=tf.float32,shape=(), name='a')
d = tf.placeholder(dtype=tf.float32,shape=(), name='d')
b = tf.get_variable(name='b', initializer=tf.zeros_like(d), use_resource=True)
c=a+b
b_init = tf.assign(b, d)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())   
    print(sess.run([c,b_init,b], feed_dict={a:5.,d:10.})) 
</code></pre>
<p>This prints [15.,10.,10.]. As per my understanding of resource variables in tensorflow variable <code>c</code> should not have access to the value of <code>b</code> that was assigned to it in <code>b_init</code> which, would mean the output instead should be [5.,10.,0.]. Please, help me understand where i am going wrong</p>
</div>
<div class="post-text" itemprop="text">
<p>Two remarks:</p>
<ol>
<li><p>The order in which you write variables/ops in the first argument of <code>sess.run</code> does not mean that is the order of execution.</p></li>
<li><p>If something worked in one step it does not mean it will work if you add loads of parallelism.</p></li>
</ol>
<p>The answer to the question:</p>
<p>The key in the definition is <code>depends on</code> : <code>a read_value operation are guaranteed to see all modifications on which the read_value depends on</code>. If you look at the graph below, the add operation actually contains a <code>ReadVariableOp</code> operation for <code>b</code>, and then <code>ReadVariableOp</code> also depends on <code>AssignVariableOp</code>. Hence, <code>c</code> should take into account all modifications to <code>b</code>. </p>
<p>Unless I am mixing something, but I sound convincing to myself. :)
<a href="https://i.stack.imgur.com/zvsou.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/zvsou.png"/></a></p>
<p>If you want to see [10.0, 5.0, 0.0] you have to add <code>tf.control_dependency</code> like below</p>
<pre><code>tf.reset_default_graph()
a = tf.placeholder(dtype=tf.float32,shape=(), name='a')
d = tf.placeholder(dtype=tf.float32,shape=(), name='d')
b = tf.get_variable(name='b', initializer=tf.zeros_like(d), use_resource=True)
c=a+b
with tf.control_dependencies([c]):
  b_init = tf.assign(b, d)


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())   
    print(sess.run([b_init,c,b], feed_dict={a:5.,d:10.})) 
</code></pre>
<p>Then the graph will change a bit
<a href="https://i.stack.imgur.com/z858K.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/z858K.png"/></a></p>
</div>
