<div class="post-text" itemprop="text">
<p>I'm using Keras' built-in <code>inception_resnet_v2</code> to train a CNN to recognize images. When training the model, I have a numpy array of data as inputs, with input shape (1000, 299, 299, 3),</p>
<pre><code> model.fit(x=X, y=Y, batch_size=16, ...) # Output shape `Y` is (1000, 6), for 6 classes
</code></pre>
<p>At first, When trying to predict, I passed in a single image of shape (299, 299, 3), but got the error</p>
<blockquote>
<p>ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (299, 299, 3)</p>
</blockquote>
<p>I reshaped my input with:</p>
<pre><code>x = np.reshape(x, ((1, 299, 299, 3)))
</code></pre>
<p>Now, when I predict, </p>
<pre><code>y = model.predict(x, batch_size=1, verbose=0)
</code></pre>
<p>I don't get an error.</p>
<p>I want to make sure I understand <code>batch_size</code> correctly in both training and predicting. My assumptions are:</p>
<p>1) With <code>model.fit</code>, Keras takes <code>batch_size</code> elements from the input array (in this case, it works through my 1000 examples 16 samples at a time)</p>
<p>2) With <code>model.predict</code>, I should reshape my input to be a single 3D array, and I should explicitly set <code>batch_size</code> to 1.</p>
<p>Are these correct assumptions? </p>
<p>Also, would it be better (possible even) to provide training data to the model so that this sort of reshape before prediction was not necessary? Thank you for helping me learn this.</p>
</div>
<div class="post-text" itemprop="text">
<p>No, you got the idea wrong. <code>batch_size</code> specifies how many data examples are "forwarded" through the network at once (using GPU usually).</p>
<p>By default, this value is set to <code>32</code> inside <code>model.predict</code> method, but you may specify otherwise (as you did with <code>batch_size=1</code>). Because of this default value you got an error:</p>
<blockquote>
<p>ValueError: Error when checking input: expected input_1 to have 4
  dimensions, but got array with shape (299, 299, 3)</p>
</blockquote>
<p>You <strong>should not</strong> reshape your input this way, rather you would provide it with the correct batch size.</p>
<p>Say, for the default case you would pass an array of shape <code>(32, 299, 299, 3)</code>, analogous for different <code>batch_size</code>, e.g. with <code>batch_size=64</code> this function requires you to pass an input of shape <code>(64, 299, 299, 3</code>.</p>
<p><strong>EDIT:</strong></p>
<p>It seems you need to reshape your single sample into a batch. I would advise you to use <code>np.expand_dims</code> for improved readability and portability of your code, like this:</p>
<pre><code>y = model.predict(np.expand_dims(x, axis=0), batch_size=1)
</code></pre>
</div>
<span class="comment-copy">Thank you--I almost understand, but are you saying since I trained my network with shape (1000, 299, 299, 3), it expects a batch size on <code>model.predict</code> of 1000? Since I am only predicting one image at a time, should I retrain with a different <code>model.fit</code> batch_size of 1?</span>
<span class="comment-copy">No, by default it expects batch size of <code>32</code>. It has nothing to do with what batch size model has been trained on. Just specify <code>batch_size</code> equal to <code>1</code>, in your <code>predict</code> method, from what I have gathered you need to perform a reshape like you do unfortunately.</span>
<span class="comment-copy">Updated my answer, it  should be fine now, sorry for the noise.</span>
