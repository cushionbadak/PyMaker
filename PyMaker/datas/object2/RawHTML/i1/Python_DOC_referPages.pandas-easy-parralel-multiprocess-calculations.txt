<div class="post-text" itemprop="text">
<p>I am looking for a fast and easy-to-use solution to make parallel calculations with pandas. I know it is a very important topic for data science but I did not find something <strong>easy</strong>, <strong>much more faster</strong> than standard pandas <code>df.apply</code> funct, and overall <strong>fast to implement</strong>!</p>
<p>so...</p>
<p>Let's have a quick overview of available tools/frameworks out there. Of course I do assume not to talk about  <code>asyncio</code> which not directly deals with my topic.</p>
<p><strong>Dask</strong></p>
<p>Please find a good article on <a href="https://towardsdatascience.com/how-i-learned-to-love-parallelized-applies-with-python-pandas-dask-and-numba-f06b0b367138" rel="nofollow noreferrer">https://towardsdatascience.com/how-i-learned-to-love-parallelized-applies-with-python-pandas-dask-and-numba-f06b0b367138</a>
or directly on the Dask web site : <a href="http://docs.dask.org/en/latest/use-cases.html" rel="nofollow noreferrer">http://docs.dask.org/en/latest/use-cases.html</a></p>
<p>Find below a snippet which currently <strong>do not work</strong> but give us a pretty good idea of the implementation : </p>
<pre><code>from dask import dataframe as dd
from dask.multiprocessing import get
from multiprocessing import cpu_count

cores = cpu_count()

dd.from_pandas(my_df,npartitions=cores).\
   map_partitions(
      lambda df : df.apply(
         lambda x : nearest_street(x.lat,x.lon),axis=1)).\
   compute(get=get)
</code></pre>
<p>Personally, I find this implementation very painful (ok, mabybe I'm a lazy man), but overall, I found this implementation not very fast, sometimes slower than the old fashion <code>df[feature] = df.feature.apply(my_funct)</code></p>
<p><br/></p>
<p><strong>MultiProcessing</strong></p>
<p>Find below a snippet of code to run easily a multi-process task, but ... with HDD IO. This implementation <strong>could or could not work</strong>, but give us a very good idea of the code implementation</p>
<pre><code>import os
from multiprocessing import Process, cpu_count
from math import ceil
from tqdm import tqdm
import numpy as np


def chunks(l, n) :
    numbs =  [ceil(i) for i in np.linspace(0,len(l)+1, n+1)]    
    pairs = list()
    for i, val in enumerate(numbs) : 
        try : 
            pairs.append((numbs[i], numbs[i+1]))
        except : 
            return pairs

def my_funct(i0=0, i1=10000000) : 
    for n in tqdm(features[i0:i1]) :
        _df = df.loc[df.feature == n, :]
        _df = do_something_complex(_df)
        _df.to_csv(f"{my_path}/feat-{n}.csv", index=False)


# multiprocessing
cores = cpu_count()
features = df.feature.unique()
if cores &lt; 2 : 
    my_funct(i0=0, i1=100000000)
else : 
    chks  = chunks(features, cores)
    process_list = [Process(target=my_funct, args=chk) \
                    for chk in chks]
    [i.start() for i in process_list]
    [i.join()  for i in process_list]

# join files and 'merge' in our new_df 
new_df = pd.DataFrame(columns=df.columns)
for filename in os.listdir(my_path) : 
    new_df = new_df.append(pd.read_csv(f'{my_path}/{filename}'),\
                           axis=0, ignore_index=True)
    os.remove(f'{my_path}/{filename}')
</code></pre>
<p>Ok this implementation is overkilled but 1/ it works most of times, 2/ it is easily understandable, and 3/ it is faster than df = df.apply(my_funct) and -- sometimes -- faster than Dask</p>
<p><strong>BUT</strong> ... assuming that I could not statistically be the only/first one to deal with such a topic...</p>
<p>Could you please help me? 
Is there any solution out there? 
Is there something like : </p>
<ul>
<li><strong>df.multi_process_apply(my_funct)</strong> or </li>
<li><strong>df.parralel_apply(my_func)</strong></li>
</ul>
<p>Thanks a Lot ! </p>
</div>
<div class="post-text" itemprop="text">
<p>You can try <a href="https://github.com/nalepae/pandarallel" rel="nofollow noreferrer">Pandarallel</a>.</p>
<p><strong>DISCLAIMER:</strong> I am the author of this lib (which is still under development, but you can already achieve good results with it).</p>
<p><strong>Without parallelisation:</strong>
<a href="https://i.stack.imgur.com/KPLGg.gif" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/KPLGg.gif"/></a></p>
<p><strong>With parallelisation:</strong>
<a href="https://i.stack.imgur.com/xtaxp.gif" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/xtaxp.gif"/></a></p>
<p>Just replace <code>df.apply(func)</code> by <code>df.parallel_apply(func)</code> and all your CPUs will be used.</p>
</div>
<span class="comment-copy">You would just use <code>pool.map</code> no? No need to manually chunk and it maps the function you want already.</span>
<span class="comment-copy">hi, very good option, do you have a snippet or some pseudo code?</span>
<span class="comment-copy">You will find many examples of how to use pool.map() via Google search and in the Python docs.</span>
<span class="comment-copy">Hi, thanks a lot ! It is exactly what I was looking for ! easy to use, fast and just with one line of code!!! thanks a lot...</span>
