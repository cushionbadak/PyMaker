<div class="post-text" itemprop="text">
<p>I am new to PySpark and just use it to process data.</p>
<p>I have a file of 120GB containing over 1.05Billion rows. I am able to process aggregation and filtering on the file and output the result to a CSV file with the coalesce() function with no issues.</p>
<p>My challenge is when I try to read through each row in the file to perform some computation my spark job fails using either the .collect() or .toLocalIterator() functions. When I limit the number of rows I read, it works fine. </p>
<p>Please, how can I solve this challenge? is it possible to read the rows in bits e.g. a row at a time or a chunk at a time?</p>
<p>I run Spark locally on a 64GB RAM Computer.</p>
<p>Below is a sample of my python code that works:</p>
<pre><code>sql = "select * from table limit 1000"
details = sparkSession.sql(sql).collect()
for detail in details:
    #do some computation
</code></pre>
<p>Below is a sample of my python code that fails:</p>
<pre><code>sql = "select * from table"
details = sparkSession.sql(sql).collect()
for detail in details:
    #do some computation
</code></pre>
<p>Here is how I submit my spark job</p>
<pre><code>spark-submit --driver-memory 16G --executor-memory 16G python_file.py
</code></pre>
<p>Thanks a lot.</p>
</div>
<div class="post-text" itemprop="text">
<p>Your approach to the problem is wrong. the <code>collect</code> method loads complete file (may actually take more than 120GB due to deserialization)  into driver memory (single pyspark process) causing out of memory.<br/>
<strong>As a rule of thumb if you use <code>collect()</code> method in spark code it is not good, and should be changed.</strong> </p>
<p>When used properly spark will read only part of the input data (input split) at a time to process and produces (much smaller) intermediate results stored in executor memory. Thus it (depending on the kind of processing) can process 120GB of file with 16GB memory.</p>
</div>
<span class="comment-copy">Collecting your data is as good as loading the whole dataset into memory in the first place. Or even worse if you account for all the duplication.</span>
<span class="comment-copy">I don't understand why you have to use <code>collect()</code> for processing. You can apply transformations on each row. With <code>collect()</code> you put everything to the driver, i.e. you load everything into memory. That's clearly wrong, in that case you could just do it without Spark at all. do it properly via Spark or do it without Spark. But not like you did above ... any <code>map()</code> process is obviously the way to go. You should read again about Spark and maybe also the docs</span>
