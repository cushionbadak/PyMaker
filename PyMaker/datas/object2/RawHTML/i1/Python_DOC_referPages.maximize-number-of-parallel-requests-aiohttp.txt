<div class="post-text" itemprop="text">
<p><strong>tl;dr</strong>: how do I maximize number of http requests I can send in parallel?</p>
<p>I am fetching data from multiple urls with <code>aiohttp</code> library. I'm testing its performance and I've observed that somewhere in the process there is a bottleneck, where running more urls at once just doesn't help.</p>
<p>I am using this code:</p>
<pre><code>import asyncio
import aiohttp

async def fetch(url, session):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0'}
    try:
        async with session.get(
            url, headers=headers, 
            ssl = False, 
            timeout = aiohttp.ClientTimeout(
                total=None, 
                sock_connect = 10, 
                sock_read = 10
            )
        ) as response:
            content = await response.read()
            return (url, 'OK', content)
    except Exception as e:
        print(e)
        return (url, 'ERROR', str(e))

async def run(url_list):
    tasks = []
    async with aiohttp.ClientSession() as session:
        for url in url_list:
            task = asyncio.ensure_future(fetch(url, session))
            tasks.append(task)
        responses = asyncio.gather(*tasks)
        await responses
    return responses

loop = asyncio.get_event_loop()
asyncio.set_event_loop(loop)
task = asyncio.ensure_future(run(url_list))
loop.run_until_complete(task)
result = task.result().result()
</code></pre>
<p>Running this with <code>url_list</code> of varying length (tests against <em><a href="https://httpbin.org/delay/2" rel="nofollow noreferrer">https://httpbin.org/delay/2</a></em>) I see that adding more urls to be run at once helps only up to ~100 urls and then total time starts to grow proportionally to number of urls (or in other words, time per one url does not decrease). This suggests that something fails when trying to process these at once. In addition, with more urls in 'one batch' I am occasionally receiving connection timeout errors. </p>
<p><a href="https://i.stack.imgur.com/bk0UR.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/bk0UR.png"/></a></p>
<ul>
<li>Why is it happening? What exactly limits the <em>speed</em> here?</li>
<li>How can I check what is the <em>maximum</em> number of parallel requests I can send on a given computer? (I mean an exact number - not approx by 'trial and error' as above)</li>
<li>What can I do to <em>increase</em> the number of requests processed at once?</li>
</ul>
<p>I am runnig this on Windows.</p>
<p><strong>EDIT</strong> in response to comment:</p>
<p>This is the same data with limit set to <code>None</code>. Only slight improvement in the end and there are many connection timeout errors with 400 urls sent at once. I ended up using <code>limit = 200</code> on my actual data.</p>
<p><a href="https://i.stack.imgur.com/4oinp.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/4oinp.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>By default <code>aiohttp</code> limits number of simultaneous connections to <code>100</code>. It achieves by setting default <code>limit</code> to <code>TCPConnector</code> <a href="https://aiohttp.readthedocs.io/en/stable/client_reference.html#tcpconnector" rel="nofollow noreferrer">object</a> that is used by <code>ClientSession</code>. You can bypass it by creating and passing custom connector to session:</p>
<pre><code>connector = aiohttp.TCPConnector(limit=None)
async with aiohttp.ClientSession(connector=connector) as session:
    # ...
</code></pre>
<p>Note however that you probably don't want to set this number too high: your network capacity, CPU, RAM and target server have their own limits and try to make enormous amount of connection can lead to increasing failures.</p>
<p>Optimal number can probably be found only through experiments on concrete machine.</p>
<hr/>
<p><em>Unrelated:</em></p>
<p>You don't have to create tasks without <a href="https://stackoverflow.com/a/37345564/1113207">reason</a>. Most asyncio api accept regular coroutines. For example, your last lines of code can be altered this way:</p>
<pre><code>loop = asyncio.get_event_loop()
loop.run_until_complete(run(url_list))
</code></pre>
<p>Or even to just <code>asyncio.run(run(url_list))</code> (<a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.run" rel="nofollow noreferrer">doc</a>) if you're using Python 3.7</p>
</div>
<span class="comment-copy">It'd be really interesting to see the updated graph with the artificial limit removed. Could you perhaps edit the question to include it?</span>
<span class="comment-copy">@user4815162342 updated</span>
<span class="comment-copy">@pieca I'm not sure when aiohttp starts timeout timer, so instead of limiting connections you may want to leave it <code>=None</code> and use semaphore to limit silmuntanious requests number instead. <a href="https://stackoverflow.com/a/55270554/1113207">Here's example</a> of how it can be done. It may improve performance and reduce errors.</span>
<span class="comment-copy">@MikhailGerasimov thanks for the link, I'll try to run it that way</span>
<span class="comment-copy">Thanks! This is good to know.</span>
