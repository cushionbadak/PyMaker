<div class="post-text" itemprop="text">
<p>I have a small utility that I use to download a MP3 from a website on a schedule and then builds/updates a podcast XML file which I've obviously added to iTunes.</p>
<p>The text processing that creates/updates the XML file is written in Python. I use wget inside a Windows <code>.bat</code> file to download the actual MP3 however. I would prefer to have the entire utility written in Python though.</p>
<p>I struggled though to find a way to actually down load the file in Python, thus why I resorted to <code>wget</code>.</p>
<p>So, how do I download the file using Python?</p>
</div>
<div class="post-text" itemprop="text">
<p>In Python 2, use urllib2 which comes with the standard library.</p>
<pre><code>import urllib2
response = urllib2.urlopen('http://www.example.com/')
html = response.read()
</code></pre>
<p>This is the most basic way to use the library, minus any error handling.  You can also do more complex stuff such as changing headers.  The documentation can be found <a href="http://docs.python.org/2/library/urllib2.html" rel="noreferrer">here.</a></p>
</div>
<div class="post-text" itemprop="text">
<p>One more, using <a href="http://docs.python.org/2/library/urllib.html#urllib.urlretrieve" rel="noreferrer"><code>urlretrieve</code></a>:</p>
<pre><code>import urllib
urllib.urlretrieve ("http://www.example.com/songs/mp3.mp3", "mp3.mp3")
</code></pre>
<p>(for Python 3+ use 'import urllib.request' and urllib.request.urlretrieve)</p>
<p>Yet another one, with a "progressbar"</p>
<pre><code>import urllib2

url = "http://download.thinkbroadband.com/10MB.zip"

file_name = url.split('/')[-1]
u = urllib2.urlopen(url)
f = open(file_name, 'wb')
meta = u.info()
file_size = int(meta.getheaders("Content-Length")[0])
print "Downloading: %s Bytes: %s" % (file_name, file_size)

file_size_dl = 0
block_sz = 8192
while True:
    buffer = u.read(block_sz)
    if not buffer:
        break

    file_size_dl += len(buffer)
    f.write(buffer)
    status = r"%10d  [%3.2f%%]" % (file_size_dl, file_size_dl * 100. / file_size)
    status = status + chr(8)*(len(status)+1)
    print status,

f.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In 2012, use the <a href="http://docs.python-requests.org/en/latest/index.html" rel="noreferrer">python requests library</a></p>
<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; 
&gt;&gt;&gt; url = "http://download.thinkbroadband.com/10MB.zip"
&gt;&gt;&gt; r = requests.get(url)
&gt;&gt;&gt; print len(r.content)
10485760
</code></pre>
<p>You can run <code>pip install requests</code> to get it.</p>
<p>Requests has many advantages over the alternatives because the API is much simpler. This is especially true if you have to do authentication. urllib and urllib2 are pretty unintuitive and painful in this case.</p>
<hr/>
<p>2015-12-30</p>
<p>People have expressed admiration for the progress bar. It's cool, sure. There are several off-the-shelf solutions now, including <code>tqdm</code>:</p>
<pre><code>from tqdm import tqdm
import requests

url = "http://download.thinkbroadband.com/10MB.zip"
response = requests.get(url, stream=True)

with open("10MB", "wb") as handle:
    for data in tqdm(response.iter_content()):
        handle.write(data)
</code></pre>
<p>This is essentially the implementation @kvance described 30 months ago.</p>
</div>
<div class="post-text" itemprop="text">
<pre class="lang-py prettyprint-override"><code>import urllib2
mp3file = urllib2.urlopen("http://www.example.com/songs/mp3.mp3")
with open('test.mp3','wb') as output:
  output.write(mp3file.read())
</code></pre>
<p>The <code>wb</code> in <code>open('test.mp3','wb')</code> opens a file (and erases any existing file) in binary mode so you can save data with it instead of just text.</p>
</div>
<div class="post-text" itemprop="text">
<h3>Python 3</h3>
<ul>
<li><p><a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.urlopen" rel="noreferrer"><code>urllib.request.urlopen</code></a></p>
<pre><code>import urllib.request
response = urllib.request.urlopen('http://www.example.com/')
html = response.read()
</code></pre></li>
<li><p><a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve" rel="noreferrer"><code>urllib.request.urlretrieve</code></a></p>
<pre><code>import urllib.request
urllib.request.urlretrieve('http://www.example.com/songs/mp3.mp3', 'mp3.mp3')
</code></pre></li>
</ul>
<h3>Python 2</h3>
<ul>
<li><p><a href="https://docs.python.org/2/library/urllib2.html#urllib2.urlopen" rel="noreferrer"><code>urllib2.urlopen</code></a> (thanks <a href="https://stackoverflow.com/a/22682/399105">Corey</a>)</p>
<pre><code>import urllib2
response = urllib2.urlopen('http://www.example.com/')
html = response.read()
</code></pre></li>
<li><p><a href="https://docs.python.org/2/library/urllib.html#urllib.urlretrieve" rel="noreferrer"><code>urllib.urlretrieve</code></a> (thanks <a href="https://stackoverflow.com/a/22776/399105">PabloG</a>)</p>
<pre><code>import urllib
urllib.urlretrieve('http://www.example.com/songs/mp3.mp3', 'mp3.mp3')
</code></pre></li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>use wget module:</p>
<pre><code>import wget
wget.download('url')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>An improved version of the PabloG code for Python 2/3:</p>
<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import ( division, absolute_import, print_function, unicode_literals )

import sys, os, tempfile, logging

if sys.version_info &gt;= (3,):
    import urllib.request as urllib2
    import urllib.parse as urlparse
else:
    import urllib2
    import urlparse

def download_file(url, dest=None):
    """ 
    Download and save a file specified by url to dest directory,
    """
    u = urllib2.urlopen(url)

    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)
    filename = os.path.basename(path)
    if not filename:
        filename = 'downloaded.file'
    if dest:
        filename = os.path.join(dest, filename)

    with open(filename, 'wb') as f:
        meta = u.info()
        meta_func = meta.getheaders if hasattr(meta, 'getheaders') else meta.get_all
        meta_length = meta_func("Content-Length")
        file_size = None
        if meta_length:
            file_size = int(meta_length[0])
        print("Downloading: {0} Bytes: {1}".format(url, file_size))

        file_size_dl = 0
        block_sz = 8192
        while True:
            buffer = u.read(block_sz)
            if not buffer:
                break

            file_size_dl += len(buffer)
            f.write(buffer)

            status = "{0:16}".format(file_size_dl)
            if file_size:
                status += "   [{0:6.2f}%]".format(file_size_dl * 100 / file_size)
            status += chr(13)
            print(status, end="")
        print()

    return filename

if __name__ == "__main__":  # Only run if this file is called directly
    print("Testing with 10MB download")
    url = "http://download.thinkbroadband.com/10MB.zip"
    filename = download_file(url)
    print(filename)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Wrote <a href="https://pypi.python.org/pypi/wget" rel="noreferrer">wget</a> library in pure Python just for this purpose. It is pumped up <code>urlretrieve</code> with <a href="https://bitbucket.org/techtonik/python-wget/src/6859e7b4aba37cef57616111be890fb59631bc4c/wget.py?at=default#cl-330" rel="noreferrer">these features</a> as of version 2.0.</p>
</div>
<div class="post-text" itemprop="text">
<p>Simple yet <code>Python 2 &amp; Python 3</code> compatible way comes with <code>six</code> library:</p>
<pre><code>from six.moves import urllib
urllib.request.urlretrieve("http://www.example.com/songs/mp3.mp3", "mp3.mp3")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I agree with Corey, urllib2 is more complete than <a href="http://docs.python.org/lib/module-urllib.html" rel="noreferrer">urllib</a> and should likely be the module used if you want to do more complex things, but to make the answers more complete, urllib is a simpler module if you want just the basics:</p>
<pre><code>import urllib
response = urllib.urlopen('http://www.example.com/sound.mp3')
mp3 = response.read()
</code></pre>
<p>Will work fine. Or, if you don't want to deal with the "response" object you can call <strong>read()</strong> directly:</p>
<pre><code>import urllib
mp3 = urllib.urlopen('http://www.example.com/sound.mp3').read()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Following are the most commonly used calls for downloading files in python:</p>
<ol>
<li><p><code>urllib.urlretrieve ('url_to_file', file_name)</code></p></li>
<li><p><code>urllib2.urlopen('url_to_file')</code></p></li>
<li><p><code>requests.get(url)</code></p></li>
<li><p><code>wget.download('url', file_name)</code></p></li>
</ol>
<p>Note: <code>urlopen</code> and <code>urlretrieve</code> are found to perform relatively bad with downloading large files (size &gt; 500 MB). <code>requests.get</code> stores the file in-memory until download is complete.  </p>
</div>
<div class="post-text" itemprop="text">
<p>You can get the progress feedback with urlretrieve as well:</p>
<pre><code>def report(blocknr, blocksize, size):
    current = blocknr*blocksize
    sys.stdout.write("\r{0:.2f}%".format(100.0*current/size))

def downloadFile(url):
    print "\n",url
    fname = url.split('/')[-1]
    print fname
    urllib.urlretrieve(url, fname, report)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you have wget installed, you can use parallel_sync.</p>
<p>pip install parallel_sync</p>
<pre><code>from parallel_sync import wget
urls = ['http://something.png', 'http://somthing.tar.gz', 'http://somthing.zip']
wget.download('/tmp', urls)
# or a single file:
wget.download('/tmp', urls[0], filenames='x.zip', extract=True)
</code></pre>
<p>Doc:
<a href="https://pythonhosted.org/parallel_sync/pages/examples.html" rel="noreferrer">https://pythonhosted.org/parallel_sync/pages/examples.html</a></p>
<p>This is pretty powerful. It can download files in parallel, retry upon failure , and it can even download files on a remote machine.</p>
</div>
<div class="post-text" itemprop="text">
<pre><code>import os,requests
def download(url):
    get_response = requests.get(url,stream=True)
    file_name  = url.split("/")[-1]
    with open(file_name, 'wb') as f:
        for chunk in get_response.iter_content(chunk_size=1024):
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)


download("https://example.com/example.jpg")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In python3 you can use urllib3 and shutil libraires.
Download them by using pip or pip3 (Depending whether python3 is default or not)</p>
<pre><code>pip3 install urllib3 shutil
</code></pre>
<p>Then run this code</p>
<pre><code>import urllib.request
import shutil

url = "http://www.somewebsite.com/something.pdf"
output_file = "save_this_name.pdf"
with urllib.request.urlopen(url) as response, open(output_file, 'wb') as out_file:
    shutil.copyfileobj(response, out_file)
</code></pre>
<p>Note that you download <code>urllib3</code> but use <code>urllib</code> in code</p>
</div>
<div class="post-text" itemprop="text">
<p>If speed matters to you, I made a small performance test for the modules <code>urllib</code> and <code>wget</code>, and regarding <code>wget</code> I tried once with status bar and once without. I took three different 500MB files to test with (different files- to eliminate the chance that there is some caching going on under the hood). Tested on debian machine, with python2.</p>
<p>First, these are the results (they are similar in different runs):</p>
<pre><code>$ python wget_test.py 
urlretrive_test : starting
urlretrive_test : 6.56
==============
wget_no_bar_test : starting
wget_no_bar_test : 7.20
==============
wget_with_bar_test : starting
100% [......................................................................] 541335552 / 541335552
wget_with_bar_test : 50.49
==============
</code></pre>
<p>The way I performed the test is using "profile" decorator. This is the full code:</p>
<pre><code>import wget
import urllib
import time
from functools import wraps

def profile(func):
    @wraps(func)
    def inner(*args):
        print func.__name__, ": starting"
        start = time.time()
        ret = func(*args)
        end = time.time()
        print func.__name__, ": {:.2f}".format(end - start)
        return ret
    return inner

url1 = 'http://host.com/500a.iso'
url2 = 'http://host.com/500b.iso'
url3 = 'http://host.com/500c.iso'

def do_nothing(*args):
    pass

@profile
def urlretrive_test(url):
    return urllib.urlretrieve(url)

@profile
def wget_no_bar_test(url):
    return wget.download(url, out='/tmp/', bar=do_nothing)

@profile
def wget_with_bar_test(url):
    return wget.download(url, out='/tmp/')

urlretrive_test(url1)
print '=============='
time.sleep(1)

wget_no_bar_test(url2)
print '=============='
time.sleep(1)

wget_with_bar_test(url3)
print '=============='
time.sleep(1)
</code></pre>
<p><code>urllib</code> seems to be the fastest</p>
</div>
<div class="post-text" itemprop="text">
<p>Source code can be:</p>
<pre><code>import urllib
sock = urllib.urlopen("http://diveintopython.org/")
htmlSource = sock.read()                            
sock.close()                                        
print htmlSource  
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Just for the sake of completeness, it is also possible to call any program for retrieving files using the <code>subprocess</code> package. Programs dedicated to retrieving files are more powerful than Python functions like <code>urlretrieve</code>. For example, <a href="https://www.gnu.org/software/wget/" rel="nofollow noreferrer"><code>wget</code></a> can download directories recursively (<code>-R</code>), can deal with FTP, redirects, HTTP proxies, can avoid re-downloading existing files (<code>-nc</code>), and <a href="https://aria2.github.io/" rel="nofollow noreferrer"><code>aria2</code></a> can do multi-connection downloads which can potentially speed up your downloads.</p>
<pre><code>import subprocess
subprocess.check_output(['wget', '-O', 'example_output_file.html', 'https://example.com'])
</code></pre>
<p>In Jupyter Notebook, one can also call programs directly with the <code>!</code> syntax:</p>
<pre><code>!wget -O example_output_file.html https://example.com
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I wrote the following, which works in vanilla Python 2 or Python 3.</p>
<hr/>
<pre><code>import sys
try:
    import urllib.request
    python3 = True
except ImportError:
    import urllib2
    python3 = False


def progress_callback_simple(downloaded,total):
    sys.stdout.write(
        "\r" +
        (len(str(total))-len(str(downloaded)))*" " + str(downloaded) + "/%d"%total +
        " [%3.2f%%]"%(100.0*float(downloaded)/float(total))
    )
    sys.stdout.flush()

def download(srcurl, dstfilepath, progress_callback=None, block_size=8192):
    def _download_helper(response, out_file, file_size):
        if progress_callback!=None: progress_callback(0,file_size)
        if block_size == None:
            buffer = response.read()
            out_file.write(buffer)

            if progress_callback!=None: progress_callback(file_size,file_size)
        else:
            file_size_dl = 0
            while True:
                buffer = response.read(block_size)
                if not buffer: break

                file_size_dl += len(buffer)
                out_file.write(buffer)

                if progress_callback!=None: progress_callback(file_size_dl,file_size)
    with open(dstfilepath,"wb") as out_file:
        if python3:
            with urllib.request.urlopen(srcurl) as response:
                file_size = int(response.getheader("Content-Length"))
                _download_helper(response,out_file,file_size)
        else:
            response = urllib2.urlopen(srcurl)
            meta = response.info()
            file_size = int(meta.getheaders("Content-Length")[0])
            _download_helper(response,out_file,file_size)

import traceback
try:
    download(
        "https://geometrian.com/data/programming/projects/glLib/glLib%20Reloaded%200.5.9/0.5.9.zip",
        "output.zip",
        progress_callback_simple
    )
except:
    traceback.print_exc()
    input()
</code></pre>
<hr/>
<p>Notes:</p>
<ul>
<li>Supports a "progress bar" callback.</li>
<li>Download is a 4 MB test .zip from my website.</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>You can use <a href="http://pycurl.io/" rel="nofollow noreferrer">PycURL</a> on Python 2 and 3.</p>
<pre><code>import pycurl

FILE_DEST = 'pycurl.html'
FILE_SRC = 'http://pycurl.io/'

with open(FILE_DEST, 'wb') as f:
    c = pycurl.Curl()
    c.setopt(c.URL, FILE_SRC)
    c.setopt(c.WRITEDATA, f)
    c.perform()
    c.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>urlretrieve and requests.get is simple, however the reality not.
I have fetched data for couple sites, including text and images, the above two probably solve most of the tasks. but for a more universal solution I suggest the use of urlopen. As it is included in Python 3 standard library, your code could run on any machine that run Python 3 without pre-installing site-par</p>
<pre><code>import urllib.request
url_request = urllib.request.Request(url, headers=headers)
url_connect = urllib.request.urlopen(url_request)
len_content = url_content.length

#remember to open file in bytes mode
with open(filename, 'wb') as f:
    while True:
        buffer = url_connect.read(buffer_size)
        if not buffer: break

        #an integer value of size of written data
        data_wrote = f.write(buffer)

#you could probably use with-open-as manner
url_connect.close()
</code></pre>
<p>This answer provides a solution to HTTP 403 Forbidden when downloading file over http using Python. I have tried only requests and urllib modules, the other module may provide something better, but this is the one I used to solve most of the problems.</p>
</div>
<div class="post-text" itemprop="text">
<p>This may be a little late, But I saw pabloG's code and couldn't help adding a os.system('cls') to make it look AWESOME! Check it out : </p>
<pre><code>    import urllib2,os

    url = "http://download.thinkbroadband.com/10MB.zip"

    file_name = url.split('/')[-1]
    u = urllib2.urlopen(url)
    f = open(file_name, 'wb')
    meta = u.info()
    file_size = int(meta.getheaders("Content-Length")[0])
    print "Downloading: %s Bytes: %s" % (file_name, file_size)
    os.system('cls')
    file_size_dl = 0
    block_sz = 8192
    while True:
        buffer = u.read(block_sz)
        if not buffer:
            break

        file_size_dl += len(buffer)
        f.write(buffer)
        status = r"%10d  [%3.2f%%]" % (file_size_dl, file_size_dl * 100. / file_size)
        status = status + chr(8)*(len(status)+1)
        print status,

    f.close()
</code></pre>
<p>If running in an environment other than Windows, you will have to use something other then 'cls'. In MAC OS X and Linux it should be 'clear'.</p>
</div>
<span class="comment-copy">See also: <a href="http://stackoverflow.com/q/8286352/562769">How to save an image locally using Python whose URL address I already know?</a></span>
<span class="comment-copy">Many of the answers below are not a satisfactory replacement for <code>wget</code>. Among other things, <code>wget</code> (1) preserves timestamps (2) auto-determines filename from url, appending <code>.1</code> (etc.) if the file already exists (3) has many other options, some of which you may have put in your <code>.wgetrc</code>. If you want any of those, you have to implement them yourself in Python, but it's simpler to just invoke <code>wget</code> from Python.</span>
<span class="comment-copy">This won't work if there are spaces in the url you provide. In that case, you'll need to parse the url and urlencode the path.</span>
<span class="comment-copy">Here is the Python 3 solution: <a href="http://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3" title="download file from web in python 3">stackoverflow.com/questions/7243750/…</a></span>
<span class="comment-copy">Just for reference. The way to urlencode the path is <code>urllib2.quote</code></span>
<span class="comment-copy">@JasonSundram: If there are spaces in it, it isn't a URI.</span>
<span class="comment-copy">This does not work on windows with larger files. You need to read all blocks!</span>
<span class="comment-copy">Oddly enough, this worked for me on Windows when the urllib2 method wouldn't. The urllib2 method worked on Mac, though.</span>
<span class="comment-copy">Bug: file_size_dl += block_sz should be += len(buffer) since the last read is often not a full block_sz.  Also on windows you need to open the output file as "wb" if it isn't a text file.</span>
<span class="comment-copy">Me too urllib and urllib2 didn't work but urlretrieve worked well, was getting frustrated - thanks :)</span>
<span class="comment-copy">Wrap the whole thing (except the definition of file_name) with <code>if not os.path.isfile(file_name):</code> to avoid overwriting podcasts! useful when running it as a cronjob with the urls found in a .html file</span>
<span class="comment-copy">@PabloG it's a tiny bit more than 31 votes now ;)  Anyway, status bar was fun so i'll +1</span>
<span class="comment-copy">how do I save or extract if the zip file is actually a folder with many files in it?</span>
<span class="comment-copy">How does this handle large files, does everything get stored into memory or can this be written to a file without large memory requirement?</span>
<span class="comment-copy">It is possible to stream large files by setting stream=True in the request.  You can then call iter_content() on the response to read a chunk at a time.</span>
<span class="comment-copy">Why would a url library need to have a file unzip facility? Read the file from the url, save it and then unzip it in whatever way floats your boat. Also a zip file is not a 'folder' like it shows in windows, Its a file.</span>
<span class="comment-copy">@Ali: <code>r.text</code>: For text or unicode content. Returned as unicode. <code>r.content</code>: For binary content. Returned as bytes. Read about it here: <a href="http://docs.python-requests.org/en/latest/user/quickstart/" rel="nofollow noreferrer">docs.python-requests.org/en/latest/user/quickstart</a></span>
<span class="comment-copy">The disadvantage of this solution is, that the entire file is loaded into ram before saved to disk, just something to keep in mind if using this for large files on a small system like a router with limited ram.</span>
<span class="comment-copy">@tripplet so how would we fix that?</span>
<span class="comment-copy">To avoid reading the whole file into memory, try passing an argument to <code>file.read</code> that is the number of bytes to read. See: <a href="https://gist.github.com/hughdbrown/c145b8385a2afa6570e2" rel="nofollow noreferrer">gist.github.com/hughdbrown/c145b8385a2afa6570e2</a></span>
<span class="comment-copy">@hughdbrown I found your script useful, but have one question: can I use the file for post-processing? suppose I download a jpg file that I want to process with OpenCV, can I use the 'data' variable to keep working? or do I have to read it again from the downloaded file?</span>
<span class="comment-copy">Use <code>shutil.copyfileobj(mp3file, output)</code> instead.</span>
<span class="comment-copy">It sure took a while, but there, finally is the easy straightforward api I expect from a python stdlib :)</span>
<span class="comment-copy">I would remove the parentheses from the first line, because it is not too old feature.</span>
<span class="comment-copy">No option to save with custom filename ?</span>
<span class="comment-copy">@Alex added -o FILENAME option to version 2.1</span>
<span class="comment-copy">The progress bar does not appear when I use this module under Cygwin.</span>
<span class="comment-copy">You should change from <code>-o</code> to <code>-O</code> to avoid confusion, as it is in GNU wget. Or at least both options should be valid.</span>
<span class="comment-copy">@eric I am not sure that I want to make <code>wget.py</code> an in-place replacement for real <code>wget</code>. The <code>-o</code> already behaves differently - it is compatible with <code>curl</code> this way. Would a note in documentation help to resolve the issue? Or it is the essential feature for an utility with such name to be command line compatible?</span>
<span class="comment-copy">This is the best way to do it for 2+3 compatibility.</span>
<span class="comment-copy">Note this is for Linux only</span>
<span class="comment-copy">There must be something completely horrible going on under the hood to make the bar increase the time so much.</span>
<span class="comment-copy">works great, run it through jupyter got what i want  :-)</span>
<span class="comment-copy"><code>cls</code> doesn't do anything on my OS X or nor on an Ubuntu server of mine. Some clarification could be good.</span>
<span class="comment-copy">I think you should use <code>clear</code> for linux, or even better replace the print line instead of clearing the whole command line output.</span>
<span class="comment-copy">this answer just copies another answer and adds a call to a deprecated function (<code>os.system()</code>) that launches a subprocess to clear the screen using a  platform specific command (<code>cls</code>).  How does this have <i>any</i> upvotes?? Utterly worthless "answer" IMHO.</span>
