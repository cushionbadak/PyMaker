<div class="post-text" itemprop="text">
<p>I trained an image classifier with tensorflow and deployed it to google cloud and now I'm trying to make online predictions using the following code :</p>
<pre><code>    service = googleapiclient.discovery.build('ml','v1')
name = 'projects/{}/models/{}'.format("project_name","model_name")


image = img_to_array(load_img('path/to/image/image.jpg', target_size=(299,299))) / 255.


payload = {
  "instances": [{'image': image.tolist()}]
}


response = service.projects().predict(
    name=name,
    body=payload).execute()

if 'error' in response:
    raise RuntimeError(response['error'])

print(response['predictions'])
</code></pre>
<p>I saw in couple of posts that I need to save my request as json file in cloud storage and call it from there to make the prediction and avoid  the exceeds the limit problem. I also read that this is only possible with batch prediction.</p>
<p>Is there a workaround for this or should i just give up and use batch prediction ? any information is much appreciated.</p>
</div>
<div class="post-text" itemprop="text">
<p>There is no workaround unfortunately. You will have to use the batch prediction. </p>
<p>If you found another way though, please share it here!</p>
</div>
<span class="comment-copy">If you use case is strictly then you should use the online prediction. If batch then you should be able to use batch prediction which doesn't have the payload limit.  If you write to us at cloudml-feedback@google.com, we can have our team suggest alternatives on online prediction.</span>
