<div class="post-text" itemprop="text">
<p>Gunicorn: <code>19.9.0</code></p>
<p>Flask: <code>1.0.2</code></p>
<p>Python: <code>3.6.7</code></p>
<p>We have a bunch of internal APIs server data science models with multiple thousands of req./sec. We recently introduced a new one and for whatever reason, when served with multiple processes (Gunicorn is our default), it'll server a few hundred requests and just lock up. </p>
<p>If I run the API as a bare file without Gunicorn, the following works ok:</p>
<pre><code>app.run(ip, port=port, threaded=True)
</code></pre>
<p>If I run with multiple processes, it locks up shortly after starting:</p>
<pre><code>app.run(ip, port=port, threaded=False, processes=2)
</code></pre>
<p>If I use Gunicorn with <code>workers=1</code>, it locks up too, here's the config:</p>
<pre><code>preload_app = False
bind = "0.0.0.0:{}".format(8889)
workers = 1
debug = False
timeout = 120
</code></pre>
<p>I've commented out all code in the endpoint and that's had no effect on it locking up. It feels like some kind of conflict with a dependency, but I'm having trouble pinpointing it.</p>
<p>If I try to attach using <code>strace</code> while it's locked, I get a tight loop with the following output on the master gunicorn process:</p>
<pre><code>strace: Process 4387 attached
select(4, [3], [], [], {tv_sec=0, tv_usec=832486}) = 0 (Timeout)
fstat(6, {st_mode=S_IFREG|001, st_size=0, ...}) = 0
select(4, [3], [], [], {tv_sec=1, tv_usec=0}) = 0 (Timeout)
fstat(6, {st_mode=S_IFREG|001, st_size=0, ...}) = 0
select(4, [3], [], [], {tv_sec=1, tv_usec=0}) = 0 (Timeout)
fstat(6, {st_mode=S_IFREG|001, st_size=0, ...}) = 0
select(4, [3], [], [], {tv_sec=1, tv_usec=0}) = 0 (Timeout)
fstat(6, {st_mode=S_IFREG|001, st_size=0, ...}) = 0
</code></pre>
<p>Any suggestions on where to go or what to try at this point?</p>
</div>
<div class="post-text" itemprop="text">
<p>It appears it was due to the combinations of number of clients and lack of reverse proxy (e.g. nginx) in front of it. There wasn't enough workers available to start queueing requests as compared to the number clients, which overwhelmed the workers to the point where they would stop responding. I bumped the workers to 60 and there's much more consistent throughput.</p>
</div>
<span class="comment-copy">since your api got thousand request/sec. , I am not sure, maybe celery can help you to avoid this issue, <a href="http://flask.pocoo.org/docs/1.0/patterns/celery/" rel="nofollow noreferrer">flask.pocoo.org/docs/1.0/patterns/celery</a></span>
<span class="comment-copy">We're not looking for a job queue. This is an HTTP endpoint that does single-digit millisecond responses when it's not locked up.</span>
<span class="comment-copy">Have you tried reducing the gunicorn worker timeout? I see you have it set to 120s but since your app's response time is in milliseconds it might be worth reducing the timeout to something lower than 30s (the default)</span>
<span class="comment-copy">We have, yeah. It's pretty conservation b/c there are some outliers at times, but it's a very small minority.</span>
