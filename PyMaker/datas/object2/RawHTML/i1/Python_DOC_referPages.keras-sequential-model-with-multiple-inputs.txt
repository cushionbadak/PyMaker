<div class="post-text" itemprop="text">
<p>I am making a MLP model which takes two inputs and produces a single output.</p>
<p>I have two input arrays (one for each input) and 1 output array. The neural network has 1 hidden layer with 2 neurons. Each array has 336 elements.</p>
<pre><code>model0 = keras.Sequential([
keras.layers.Dense(2, input_dim=2, activation=keras.activations.sigmoid, use_bias=True),
keras.layers.Dense(1, activation=keras.activations.relu, use_bias=True),
])

# Compile the neural network #
model0.compile(
    optimizer = keras.optimizers.RMSprop(lr=0.02,rho=0.9,epsilon=None,decay=0),
    loss = 'mean_squared_error',
    metrics=['accuracy']
)
</code></pre>
<p>I tried two ways, both of them are giving errors.</p>
<pre><code>model0.fit(numpy.array([array_1, array_2]),output, batch_size=16, epochs=100)
</code></pre>
<blockquote>
<p>ValueError: Error when checking input: expected dense_input to have shape (2,) but got array with shape (336,)</p>
</blockquote>
<p>The second way:</p>
<pre><code>model0.fit([array_1, array_2],output, batch_size=16, epochs=100)
</code></pre>
<blockquote>
<p>ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays:</p>
</blockquote>
<p><a href="https://stackoverflow.com/questions/53017177/multiple-inputs-to-keras-sequential-model">Similar question</a>. But not using sequential model.</p>
</div>
<div class="post-text" itemprop="text">
<p>To solve this problem you have two options. </p>
<p><strong>1. Using a sequential model</strong> </p>
<p>You can concatenate both arrays into one before feeding to the network. Let's assume the two arrays have a shape of (Number_data_points, ), now the arrays can be merged using <code>numpy.stack</code> method.</p>
<pre class="lang-py prettyprint-override"><code>merged_array = np.stack([array_1, array_2], axis=1)

</code></pre>
<pre class="lang-py prettyprint-override"><code>model0 = keras.Sequential([
keras.layers.Dense(2, input_dim=2, activation=keras.activations.sigmoid, use_bias=True),
keras.layers.Dense(1, activation=keras.activations.relu, use_bias=True),
])

model0.fit(merged_array,output, batch_size=16, epochs=100)

</code></pre>
<p><strong>2. Using Functional API.</strong></p>
<p>This is the most recommened way to use when there are multiple inputs to the model.</p>
<pre class="lang-py prettyprint-override"><code>input1 = keras.layers.Input(shape=(1, ))
input2 = keras.layers.Input(shape=(1,))
merged = keras.layers.Concatenate(axis=1)([input1, input2])
dense1 = keras.layers.Dense(2, input_dim=2, activation=keras.activations.sigmoid, use_bias=True)(merged)
output = keras.layers.Dense(1, activation=keras.activations.relu, use_bias=True)(dense1)
model10 = keras.models.Model(inputs=[input1, input2], output=output)
</code></pre>
<p>Now you can use the second method you have trying to fit to the model</p>
<pre class="lang-py prettyprint-override"><code>model0.fit([array_1, array_2],output, batch_size=16, epochs=100)

</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>As in the answer you've linked, you cant be using the <code>Sequential</code> API for the stated reason. You should use <code>Model</code> API which is also called the functional API. Architecturally, you need to define to the model how you'll combine the inputs with the Dense layer ie how you want to create the intermediate layer viz. merge/add or subtract etc/construct a embedding layer etc), or maybe you want to have 2 neural networks, 1 for each input and only want to combine the output in the last layer. The code for each of the above will vary.</p>
<p>Here's a working solution assuming you want to merge the inputs into a vector of shape 672 and then construct a neural network on that input:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam, RMSprop
import numpy as np

input1 = Input(shape=(336,))
input2 = Input(shape=(336,))
input = Concatenate()([input1, input2])
x = Dense(2)(input)
x = Dense(1)(x)
model = Model(inputs=[input1, input2], outputs=x)
model.summary()
</code></pre>
<p>You'll notice that this model merges or concatenates the two inputs and then constructs a neural network on top of that:</p>
<pre><code>Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 336)          0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 336)          0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 672)          0           input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            1346        concatenate[0][0]                
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            3           dense[0][0]                      
==================================================================================================
Total params: 1,349
Trainable params: 1,349
Non-trainable params: 0
</code></pre>
<p>If you have some other preferred way to create the intermediate layer, you should replace the <code>Concatenate</code> line with that in the code.</p>
<p>You can then compile and fit the model:</p>
<pre><code>model.compile(
    optimizer = RMSprop(lr=0.02,rho=0.9,epsilon=None,decay=0),
    loss = 'mean_squared_error',
    metrics=['accuracy']
)


x1, x2 = np.random.randn(100, 336),np.random.randn(100, 336,)
y = np.random.randn(100, 1)
model.fit([x1, x2], y)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The above solutions contain the recommended approach but I still got some errors.
So I tried the following.</p>
<pre><code>for count in range (len(array_1)):
    input_array[count][0] = array_1[count]
    input_array[count][1] = array_2[count]
</code></pre>
<p>Both Array_1 and Array_2 were the same length.</p>
<p>And then created and compiled the model as before.</p>
<p>Finally for training, I used:</p>
<pre><code>model0.fit(input_array, output_array, batch_size=16, epochs=100, verbose=0)
</code></pre>
<p>This approach worked for me.</p>
</div>
<span class="comment-copy">When I try the first method I get the error <code>ValueError: Error when checking input: expected dense_input to have shape (672,) but got array with shape (1,)</code>. This is what the merged array looks like: <code>length 672 shape (672,)</code></span>
<span class="comment-copy">When I try the second method, I get the error <code>ValueError: Error when checking input: expected input_1 to have shape (336,) but got array with shape (1,)</code>. The shapes of each individual array are like this: <code>Array_1 Type &amp; Shape: &lt;class 'numpy.ndarray'&gt; (336,) Array_2 Type &amp; Shape: &lt;class 'numpy.ndarray'&gt; (336,)</code></span>
<span class="comment-copy">what is the result of <code>print(array_1.shape)</code> and <code>print(array_2.shape)</code>? What is number features for each array?</span>
<span class="comment-copy">Shape of <code>Array_1 is (336,)</code> and the same with <code>Array_2</code>.</span>
<span class="comment-copy">@ghostwar88 I've updated the code.</span>
