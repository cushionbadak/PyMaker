<div class="post-text" itemprop="text">
<p>I am using dask-jobqueue to launch many 2-5 min jobs (using subprocess) on a small SLURM cluster. I am running several 1000s of jobs total, and I would like to occasionally let my workers die and get shuffled back through the SLURM to be kind to other users.  On the dask-jobqueue documentation site there is a passage:</p>
<blockquote>
<p>So, to get a large cluster quickly, we recommend allocating a dask-scheduler process on one node with a modest wall time (the intended time of your session) and then allocating many small single-node dask-worker jobs with shorter wall times (perhaps 30 minutes) that can easily squeeze into extra space in the job scheduler. As you need more computation, you can add more of these single-node jobs or let them expire.</p>
</blockquote>
<p>This sounds very much like what I want to do, but my question is: how do you configure this?</p>
<p>If I setup my cluster with the total walltime required for my full run (~24hrs) I can't figure out how to launch workers with shorter walltimes:</p>
<pre><code># setup cluster, launch one worker with 24hr walltime
In [1]:from dask_jobqueue import SLURMCluster
   ...:cluster = SLURMCluster(memory='8g',cores=4,walltime='24:00:00')
   ...:cluster.start_workers(1)
Out[1]:SLURMCluster(cores=0, memory=0 B, workers=0/1, jobs=0/1)

# try to launch a worker with shorter walltime???
In [2]:cluster.start_workers(100,walltime='00:30:00')
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-16-77ae6b0ed75d&gt; in &lt;module&gt;
----&gt; 1 cluster.start_workers(100,walltime='00:30:00')

TypeError: start_workers() got an unexpected keyword argument 'walltime'
</code></pre>
<p>If I try to just use a walltime of 30min to begin with, all of the workers die simultaneously (usually) and cause DASK to crash.</p>
<p>I have found one <a href="https://jobqueue.dask.org/en/latest/examples.html#slurm-deployment-providing-additional-arguments-to-the-dask-workers" rel="nofollow noreferrer">example</a> where additional arguements were passed to individual workers, but these were resources and they were passed <em>after</em> starting the workers (at which time I assume the walltime limit is already set).</p>
<p>Is there any way to assign properties like walltime for each work when they are initialized?</p>
</div>
<div class="post-text" itemprop="text">
<p>The walltime that you specify here is for the workers, not the scheduler.  </p>
<pre><code>SLURMCluster(memory='8g',cores=4,walltime='24:00:00')
</code></pre>
<p>The scheduler is run wherever you're running the <code>SLURMCluster</code> object (which given that you haven't mentioned it, is perhaps on an interactive node?)</p>
<p>You're right that all of your workers will die at the same time if you start them all at the same time.  If you want more workers to arrive to take their place you might consider using the <code>adapt</code> method to make sure that new workers come to take their place.</p>
<pre><code>cluster.adapt(minimum=100, maximum=100)
</code></pre>
</div>
<span class="comment-copy">I get it now.  Yes, I am initializing my SLURMCluster in a script running on the login node of our cluster, so no time limit.  I'm trying the <code>cluster.adapt</code> method, and it seems to be working.  I think I was originally having timeout issues once all of the workers died because I wasn't doing anything to re-launch them.</span>
<span class="comment-copy">One more followup question: if I am launching the python script that hosts the <code>SLURMCluster()</code> in an <code>sbatch</code> (instead of just on the head node) I can't seem to get the workers to spawn.  How do I need to configure the node hosting the scheduler?  Currently I am using <code>sbatch -N1 -n1 -c1 script.py</code>.</span>
<span class="comment-copy">You'll need to make sure that your jobs can also launch SLURM jobs.  You should ask your cluster administrator.</span>
