<div class="post-text" itemprop="text">
<p>I am trying to import a pretrained Model from tensorflow to PyTorch. It takes a single input and maps it onto a single output. 
Confusion comes up, when I try to import the LSTM weights</p>
<p>I read the weights and their variables from the file with the following function:
</p>
<pre class="lang-python prettyprint-override"><code>def load_tf_model_weights():        

    modelpath = 'models/model1.ckpt.meta'

    with tf.Session() as sess:        
        tf.train.import_meta_graph(modelpath) 
        init = tf.global_variables_initializer()
        sess.run(init)  
        vars = tf.trainable_variables()        
        W = sess.run(vars)

    return W,vars

W,V = load_tf_model_weights()
</code></pre>
<p>Then I am inspecting the shapes of the weights</p>
<pre class="lang-python prettyprint-override"><code>In [33]:  [w.shape for w in W]
Out[33]: [(51, 200), (200,), (100, 200), (200,), (50, 1), (1,)]
</code></pre>
<p>furthermore the variables are defined as</p>
<pre class="lang-python prettyprint-override"><code>In [34]:    V
Out[34]: 
[&lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(51, 200) dtype=float32_ref&gt;,
&lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(200,) dtype=float32_ref&gt;,
&lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0' shape=(100, 200) dtype=float32_ref&gt;,
&lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0' shape=(200,) dtype=float32_ref&gt;,
&lt;tf.Variable 'weight:0' shape=(50, 1) dtype=float32_ref&gt;,
&lt;tf.Variable 'FCLayer/Variable:0' shape=(1,) dtype=float32_ref&gt;]
</code></pre>
<p>So I can say that the first element of  <code>W</code> defines the  Kernel of an LSTM and the second element define its bias. According to <a href="https://stackoverflow.com/questions/48212694/in-what-order-are-weights-saved-in-a-lstm-kernel-in-tensorflow">this post</a>, the shape for the Kernel is defined as
<code>[input_depth + h_depth, 4 * self._num_units]</code>
and the bias as <code>[4 * self._num_units]</code>. We already know that <code>input_depth</code> is <code>1</code>. So we get, that <code>h_depth</code> and <code>_num_units</code> both have the value <code>50</code>.</p>
<p>In pytorch my LSTMCell, to which I want to assign the weights, looks like this:</p>
<pre class="lang-python prettyprint-override"><code>In [38]: cell = nn.LSTMCell(1,50)
In [39]: [p.shape for p in cell.parameters()]
Out[39]: 
[torch.Size([200, 1]),
torch.Size([200, 50]),
torch.Size([200]),
torch.Size([200])]
</code></pre>
<p>The first two entries can be covered by the first value of <code>W</code> which has the shape <code>(51,200)</code>. But the LSTMCell from Tensorflow yields only one bias of shape <code>(200)</code> while pytorch wants two of them</p>
<p>And by leaving the bias out I have weights left over:</p>
<pre class="lang-python prettyprint-override"><code>cell2 = nn.LSTMCell(1,50,bias=False)
[p.shape for p in cell2.parameters()]
Out[43]: [torch.Size([200, 1]), torch.Size([200, 50])]
</code></pre>
<p>Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>pytorch uses CuDNN's LSTM underlayer(even when you don't have CUDA, it still uses something compatible) thus it has one extra bias term. </p>
<p>So you can pick two numbers with their sum equal to 1(0 and 1, 1/2 and 1/2 or anything else) and set your pytorch biases as those numbers times TF's bias.</p>
<pre><code>pytorch_bias_1 = torch.from_numpy(alpha * tf_bias_data)
pytorch_bias_2 = torch.from_numpy((1.0-alpha) * tf_bias_data)
</code></pre>
</div>
