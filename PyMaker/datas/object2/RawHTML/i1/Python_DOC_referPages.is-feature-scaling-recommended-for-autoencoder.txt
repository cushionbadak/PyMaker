<div class="post-text" itemprop="text">
<h3>Problem:</h3>
<p>The Staked Auto Encoder is being applied to a dataset with 25K rows and 18 columns, all float values.
SAE is used for feature extraction with encoding &amp; decoding. </p>
<p>When I train the model without feature scaling, the loss is around 50K, even after 200 epochs. But, when scaling is applied the loss is around 3 from the first epoch.</p>
<h3>My questions:</h3>
<ol>
<li><p>Is it recommended to apply feature scaling when SAE is used for feature extraction </p></li>
<li><p>Does it impact accuracy during decoding?</p></li>
</ol>
</div>
<div class="post-text" itemprop="text">
<ol>
<li>With a few exceptions, you should always apply feature scaling in machine learning, especially when working with gradient descent as in your SAE. Scaling your features will ensure a much smoother cost function and thus faster convergence to global (hopefully) minima. </li>
</ol>
<p>Also worth noting that your much smaller loss after 1 epoch with scaling should be a result of much smaller values used to compute the loss.</p>
<ol start="2">
<li>No</li>
</ol>
</div>
<span class="comment-copy">Thank you JimmyOnThePage for your answer..  on your comments "Also worth noting that your much smaller loss after 1 epoch with scaling should be a result of much smaller values used to compute the loss" .. yes after scaling applied values became small.. what could be the alternative to improve model to avoid it</span>
<span class="comment-copy">I'm not sure I quite understand the question. Feature scaling will always lead to faster convergence and increase the probability of finding the global minimum. There's no need to avoid it, since inverting the scaling of the model output will take the features back to their original ranges. If you're asking how to even further decrease the loss, there's a few options such as changing the SAE architecture, better initial feature selection etc. But feature scaling will always improve your results, or at the least converge faster</span>
