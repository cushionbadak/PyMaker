<div class="post-text" itemprop="text">
<p>I am running docker+python+spyder</p>
<p>My spyder run just as much as my concurency limit, idk, can someone help me to understand it ?</p>
<p>my docker-compose.py</p>
<pre><code>celery:
    build:
      context: .
      dockerfile: ./celery-queue/Dockerfile
    entrypoint: celery
    command: -A tasksSpider worker --loglevel=info  --concurrency=5 -n myuser@%n
    env_file:
    - .env
    depends_on:
    - redis
</code></pre>
<p>My spider code :</p>
<pre><code>def spider_results_group():
    results = []

    def crawler_results(signal, sender, item, response, spider):
        results.append(item)

    dispatcher.connect(crawler_results, signal=signals.item_passed)

    process = CrawlerProcess(get_project_settings())
    process.crawl(groupSpider)
    process.start()  # the script will block here until the crawling is finished
    process.stop()
    return results
</code></pre>
<p>With this code, i could run spider multiple times, but only 5 times, when i check it, i think this is because my concurency is only 5, and when this run again(6th), it stuck..</p>
<p>if need other code, please ask</p>
</div>
<div class="post-text" itemprop="text">
<p>solved by using this command :</p>
<pre><code>    command: -A tasksSpider worker --loglevel=info  --concurrency=5 --max-tasks-per-child=1 -n myuser@%n
</code></pre>
<p>got answer from :
<a href="https://stackoverflow.com/questions/11528739/running-scrapy-spiders-in-a-celery-task">Running Scrapy spiders in a Celery task</a></p>
</div>
<span class="comment-copy">note this "maybe" different issue because if iam trying outside docker, my code perfectly run, but when it run in docker+celery it got only maximum limit of celery concurency..</span>
<span class="comment-copy">i mean about duplicate thing.. XD</span>
