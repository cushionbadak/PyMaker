<div class="post-text" itemprop="text">
<p>I'm having problem with the size of file after unpickling. The size of folder </p>
<p>before pickling is about 307 MB (shape= 357,227,227), when I pad the images in </p>
<p>the folder to a  desired size (6000,227,227) )and then pickle it using gzip </p>
<p>compression  ,  the pickle  file size becoames 14 mb. However, when I try to </p>
<p>load the pickle file it loaded  into the memory as 1.25 GB while it is on the</p>
<p>disk 307 MB  why this happening , is there a way to load the pickle file with it </p>
<p>is same size into memory ? below is sample output .</p>
<pre><code>import gzip


 f=gzip.open('C:/Users/Documents/data/folder_030.pklz','rb')
 img_array = pickle.load(f)
 img_array.shape
 print('size of unpickle images',sys.getsizeof(img_array) )


 X = np.empty((1,6106 ,227, 227), dtype='float32')

 print('empty X shape',X.shape)

for i in range(1):

  X[i] = img_array

 print(X.shape)


 print('size of unpickle file',sys.getsizeof(X))
</code></pre>
<p>here is the output :</p>
<pre><code> (6106, 227, 227)
 size of unpickled images 128

 X.shape (1, 6106, 227, 227)
 size of x  1258544440
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Any data stored on disk will have a very different size from the same data in your script. Data stored on your disk will almost always have a smaller footprint then the data in memory(pickles, h5 or whatever it is you are using). When you are loading your data, it is converted from bytes(what pickles essentially are) into a numpy object, which contains many properties and attributes that won't be stored on disk for an obvious reason - they are not needed. On disk you'll only need the values of your array, nothing more. That is without adding the effects on compression. When you load your data into memory however, all those attributes are initialized in order to be able to have fast access to slices, mathematical operations and transformations. Think of the following slice:</p>
<pre><code>im[:,:2]
</code></pre>
<p>Numpy cannot possibly have it's ability or performance if it didn't use a number of pointers in order to have access to that information fast and efficiently. And needless to say, those pointers will have a significant effect on memory. So no, you won't be able to load a pickle into memory and not pay the additional price with memory I'm afraid.</p>
</div>
<span class="comment-copy">Thanks @Alexander Ejbekov , so there is no other techniques to solve this issue rather than pickle ?</span>
<span class="comment-copy">@N.zay no, not in a conventional way at least. But there are ways to overcome it depending on what you are trying to do, that is distribute your tasks to multiple machines or work with chunks of your data or resort to solutions such as dask but your performance will inevitably suffer from the slow io operations or networking and so on. There are always trade offs.</span>
