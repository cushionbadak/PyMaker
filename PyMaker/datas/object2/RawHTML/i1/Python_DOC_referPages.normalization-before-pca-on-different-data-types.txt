<div class="post-text" itemprop="text">
<p>Prior to running principal component analysis you should normalize the data as to not have the results skewed. Under normal situations, this is a fairly simple task. I am curious how I should go about normalizing my data, which contains multiple data types within the data set. Some I know (strongly believe) are very important. Others I am not so sure, but that is why I wanted to run PCA on my data set.</p>
<pre><code>    0       1       2       3       4    ...
  0.112   'Bob'   68.47   'Right'  9493  ...
</code></pre>
<p>Something like this, where there could be a string that has no categorical backing, such as a name. While 'Right' could be enumerated to a category. </p>
<p>I am not sure this is even necessary but I would appreciate some suggestions.</p>
</div>
<div class="post-text" itemprop="text">
<p>First you should be very careful when running PCA on variables that have no inherent order.  Such as categorical data.  </p>
<p>Second,  think what does even mean to apply PCA to things like names.  PCA works on vectors which are lengths that have a direction.  What is the length of bob and which direction would it be pointing?  </p>
<p>One thing you can try is to convert your string data to N-Grams which would be perfect vectors.  Another thing to try is to apply TF-IDF conversion, which again would give you a vector.  </p>
<p>Once you applied one of this conversions.  You've got a problem of having vectors embedded within vectors.  You can try combining those into one vector by concatenation and normalization.   Or you can abandon PCA and treat your dataset as  collection of tensors and apply something like  <a href="https://en.wikipedia.org/wiki/Multilinear_principal_component_analysis" rel="nofollow noreferrer">multilinear component analysis</a> which is an extension of PCA to tensors.</p>
<p>Note either of those aproaches require will produce giant vectors,  so you need to have a lot of data instances to get anything meaningful out your analysis.</p>
</div>
<span class="comment-copy">I currently have millions of rows and WAY TOO many columns...so I am working on reducing.    Thank you Vlad for the quick response.</span>
<span class="comment-copy">Millions of rows is good, many columns is bad.  The techniques I mentioned above are going to produce even more columns.   General rule of a thumb is to have about two more orders of magnitude rows than columns.  So with millions of rows you can support about 10,000 columns.  Which might sound like a lot,  but when you start going into N-Grams and TF-IDF you'll be generating columns by thousand.  One technique to minimize the columns is to throw away columns which have very small numeric value across the board.  You should probably do it after normalization.</span>
<span class="comment-copy">Look at this MOOC.  You can learn tons of good techniques that are relevant to your task <a href="https://open.hpi.de/courses/semanticweb2017/" rel="nofollow noreferrer">open.hpi.de/courses/semanticweb2017</a></span>
<span class="comment-copy">Thank you Vlad.</span>
