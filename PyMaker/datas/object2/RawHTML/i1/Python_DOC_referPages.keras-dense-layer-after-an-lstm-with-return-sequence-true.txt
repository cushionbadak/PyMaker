<div class="post-text" itemprop="text">
<p>I am trying to reimplement this paper <a href="http://aclweb.org/anthology/D18-1060" rel="nofollow noreferrer">1</a> in Keras as the authors used PyTorch <a href="https://github.com/gao-g/metaphor-in-context/" rel="nofollow noreferrer">2</a>. Here is the network architecture: 
<a href="https://i.stack.imgur.com/XNiys.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/XNiys.png"/></a>
What I have done so far is:</p>
<pre><code>number_of_output_classes = 1
hidden_size = 100
direc = 2
lstm_layer=Bidirectional(LSTM(hidden_size, dropout=0.2, return_sequences=True))(combined) #shape after this step (None, 200) 
#weighted sum and attention should be here
attention = Dense(hidden_size*direc, activation='linear')(lstm_layer) #failed trial
drop_out_layer = Dropout(0.2)(attention)    
output_layer=Dense(1,activation='sigmoid')(drop_out_layer) #shape after this step (None, 1)
</code></pre>
<p>I want to include the attention layer and the final FF layer after the LSTM but I am running into errors due to the dimensions and the return_sequence= True option.</p>
</div>
<div class="post-text" itemprop="text">
<p>This is a sequence classification task. Sequence classification is many to one mapping, where you have input from multiple timesteps labeled to a single class. In this case, you inputs should have a shape of (batch_size, time_steps, channels) and outputs should have a shape of (batch_size, channels). If the <code>return_sequences</code> argument of LSTM class is True, the output will have a shape of <code>(batch_size, time_steps, channels)</code>. Feeding this to dense layers and dropout layer wouldn't reduce the number of dimensions. To reduce the number of dimension into two you have to set <code>return_sequences</code> argument of last LSTM layer to <code>True</code>. In your case </p>
<pre class="lang-py prettyprint-override"><code>lstm_layer=Bidirectional(LSTM(hidden_size, dropout=0.2, return_sequences=False))(combined)

</code></pre>
</div>
<span class="comment-copy">Thanks Mitiku for your reply. But as I understood from the figure that the output is a sequence as well. Then combine it, right?</span>
<span class="comment-copy">Not really. In the paper figure 2 is sentence labeling, i.e sequence to label mapping and the output is not sequence rather it is a single word. For sequence to sequence mapping look figure 1 of the paper you mentioned.</span>
