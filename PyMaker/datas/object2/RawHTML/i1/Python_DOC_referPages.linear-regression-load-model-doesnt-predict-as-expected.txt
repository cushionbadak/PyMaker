<div class="post-text" itemprop="text">
<p>I have trained a linear regression model, with sklearn, for a 5 star rating and it's good enough. I have used Doc2vec to create my vectors, and saved that model. Then I save the linear regression model to another file. What I'm trying to do is load the Doc2vec model and linear regression model and try to predict another review. </p>
<p>There is something very strange about this prediction: whatever the input it always predicts around 2.1-3.0. </p>
<p>Thing is, I have a suggestion that it predicts around the average of 5 (which is 2.5 +/-) but this is not the case. I have printed when training the model the prediction value and the actual value of the test data and they range normally 1-5. So my idea is, that there is something wrong with the loading part of the code. This is my load code:</p>
<pre><code>from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from bs4 import BeautifulSoup
from joblib import dump, load
import pickle
import re

model = Doc2Vec.load('../vectors/750000/doc2vec_model')

def cleanText(text):
    text = BeautifulSoup(text, "lxml").text
    text = re.sub(r'\|\|\|', r' ', text) 
    text = re.sub(r'http\S+', r'&lt;URL&gt;', text)
    text = re.sub(r'[^\w\s]','',text)
    text = text.lower()
    text = text.replace('x', '')
    return text

review = cleanText("Horrible movie! I don't recommend it to anyone!").split()
vector = model.infer_vector(review)

pkl_filename = "../vectors/750000/linear_regression_model.joblib"
with open(pkl_filename, 'rb') as file:  
    linreg = pickle.load(file)

review_vector = vector.reshape(1,-1)
predict_star = linreg.predict(review_vector)
print(predict_star)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Your example code shows imports of both <code>joblib.dump</code> and <code>joblib.load</code> – even though neither is used in this excerpt. And, the suffix of your file is suggestive that the model may have originally been saved with <code>joblib.dump()</code>, not vanilla pickle. </p>
<p>But, this code shows the file being loaded only via plain <code>pickle.load()</code> – which may be the source of the error. </p>
<p><a href="https://joblib.readthedocs.io/en/latest/generated/joblib.load.html#joblib.load" rel="nofollow noreferrer">The <code>joblib.load()</code> docs</a> suggest that its <code>load()</code> may do things like load numpy arrays from multiple separate files created by its own <code>dump()</code>. (Oddly, the <code>dump()</code> docs are less clear on this, but supposedly <code>dump()</code> has a return-value that may be a <em>list</em> of filenames.) </p>
<p>You can check where the file was saved for extra files that appear to be related, and try using <code>joblib.load()</code> rather than plain-pickle, to see if that loads a more-functional/more-complete version of your <code>linreg</code> object.</p>
</div>
<div class="post-text" itemprop="text">
<p>(<strong>Update:</strong> I overlooked the <code>.split()</code> tokenization being done in the question code after <code>.cleanText()</code>, so this isn't the real problem. But keeping answer up for reference &amp; because the real issue was discovered in the comments.) </p>
<p>Very commonly, users get mysteriously-weak results from <code>Doc2Vec</code> when they provide a plain string to <code>infer_vector()</code>. <code>Doc2Vec</code> <code>infer_vector()</code> requires a list-of-words, <em>not</em> a string. </p>
<p>If providing a string, the function will see it as a list-of-one-character words – per Python's modeling of strings as lists-of-characters, and type-conflation of characters and one-character-strings. Most of these one-character words probably aren't known by the model, and those that might be – <code>'i'</code>, <code>'a'</code>, etc – aren't very meaningful. So the inferred doc-vector will be weak &amp; meaningless. (And, it isn't surprising such a vector, fed to your linear regression, always gives a middling predicted value.)</p>
<p>If you break the text into the expected list-of-words, your results should improve. </p>
<p>But more generally, the words provided to <code>infer_vector()</code> should be preprocessed and tokenized <strong>exactly</strong> however the training documents were.</p>
<p>(A fair sanity test of whether you're doing inference properly is to infer vectors for some of your training documents, then ask the <code>Doc2Vec</code> model for the doc-tags closest to these re-inferred vectors. In general, the same document's training-time tag/ID should be the top result, or at least one of the top few. If it isn't, there may be other problems in the data, model parameters, or inference.)</p>
</div>
<span class="comment-copy">this perhaps belongs more on the data science stack exchange</span>
<span class="comment-copy">Your predictions around mean response may mean 2 things: (1) your model does not have enough data to learn from; (2) your model is too rigid (highly biased). For 1st case, you need more data. For the second, instead of linreg, you may try more flexible models like Random Forest.</span>
<span class="comment-copy">@Sergey Bushmanov The data are 750000 reviews (from 150000 reviews each star) so I think there are enough data. Thing is that when I predict the test data, I actually print the actual rating and the predicted rating and they are not around mean response. If this happens (my model is too rigid) then my last predictions would be around 2.5. But they are not. The only problem (I think) is my loading of the model or perhaps the reshape. How do I figure out if my model is too rigid as you said?</span>
<span class="comment-copy">I'm not quite getting you. First you said: "whatever the input it always predicts around 2.1-3.0." And then you continue: "when I predict the test data, I actually print the actual rating and the predicted rating and they are not around mean response". These two contradict each other, do they not? If you want something different from mean, see what produced different results from your test and feed to your prediction algo. As far as checking if your model is too biased, just try RF and see if you get more sensible ("accurate"?) results.</span>
<span class="comment-copy">"whatever the input" refers to when loading the model.  "When  I predict the test data" refers to after training the model and predicting the test data.  sorry :D</span>
<span class="comment-copy">This text is preprocessed with the SAME method as my training data. As for infer_vector, I know it requires a list of words, that's why I split it in the end of that sentence, after preprocessing. Please read the code carefully next time. The problem here is my loading of the model OR the reshape of the vector.</span>
<span class="comment-copy">Sorry I hadn't noticed that. It's a very common error, your training code isn't shown, and your <code>split()</code> was trimmed-out-of-scroll-region in my display of your code. Good luck figuring it out!</span>
<span class="comment-copy">It's probably not your loading or reshaping - but something in your <code>Doc2Vec</code> or regressor training. You may want to add that code to the question.</span>
<span class="comment-copy">Hmm, actually the way you're using generic <code>pickle.load()</code>, but the file-extension for your file is <code>.joblib</code> is a little fishy. If <code>joblib.dump()</code> was used to save, you should use <code>joblib.load()</code> to load (and be sure to keep any other files dumped alongside the main file together).</span>
<span class="comment-copy">Thaaaaaaank you! It worked by just using save/load joblib library! But I want you to put here another answer about this with joblib and pickle so I can mark it as a proper answer.... Thanks saved my sanity :P</span>
