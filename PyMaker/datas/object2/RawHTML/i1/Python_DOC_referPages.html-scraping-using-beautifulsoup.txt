<div class="post-text" itemprop="text">
<p>I would like to perform scraping on the following website, a repository of cases: <a href="https://engagements.ceres.org/?_ga=2.157917299.852607976.1552678391-697747477.1552678391" rel="nofollow noreferrer">https://engagements.ceres.org/?_ga=2.157917299.852607976.1552678391-697747477.1552678391</a></p>
<p>The features intend to extract are:  </p>
<blockquote>
<p>'Organization', "Industry","Title", "Filed_By", 'Status, Year','Summary'(main body text)</p>
</blockquote>
<p>My question is how do I scrape by each case and have the program loop through all pages ?</p>
<p>the URL in my code is only the first case but I need to loop through all the pages in the repository (88pages) and write them into CSV</p>
<p>I am wondering if using lambda would work in this case </p>
<p>Also can someone kindly shed some lights on how to understand and identify patterns in the html tags for future use because I am new to this field.</p>
<p>The following code is what I have at this moment: </p>
<pre><code>url = "https://engagements.ceres.org/ceres_engagementdetailpage?recID=a0l1H00000CDy78QAD"

page = requests.get(url, verify=False)

soup = BeautifulSoup(page.text, 'html.parser')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I think you need to combine bs with selenium as some content is a little slower to load. You can use bs to grab the initial links and then use selenium and waits to ensure content on each page is loaded. You need to handle the certificate problem initially.</p>
<p>I am not sure what summary is so I provide all the p tags. This means some duplicated info. You can refine this.</p>
<pre><code>import requests
from bs4 import BeautifulSoup as bs
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd

baseUrl = 'https://engagements.ceres.org'
results = []
driver = webdriver.Chrome()

r = requests.get('https://engagements.ceres.org/?_ga=2.157917299.852607976.1552678391-697747477.1552678391', verify=False)
soup = bs(r.content, 'lxml')
items =  [baseUrl + item['href'] for item in soup.select("[href*='ceres_engagementdetailpage?recID=']")]

for item in items:
    driver.get(item)
    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "p")))
    title = driver.find_element_by_css_selector('.resolutionsTitle').text
    organisation = driver.find_element_by_css_selector('#description p').text
    year = driver.find_element_by_css_selector('#description p + p').text
    aList = driver.find_elements_by_css_selector('.td2')
    industry = aList[0].text
    filedBy = aList[2].text
    status = aList[5].text
    summary = [item.text for item in driver.find_elements_by_css_selector('#description p')]
    results.append([organization, industry, title, filedBy, status, year, summary])
df = pd.DataFrame(results, headers = ['Organization', 'Industry', 'Title', 'Filed By', 'Status', 'Year', 'Summary'])
print(results)
</code></pre>
</div>
<span class="comment-copy">Only "category" and "industry" appear on the page accessible by the link provided. Can you clarify?</span>
<span class="comment-copy">@Ajax1234 UI is not user friendly there. if you just use ctrl+f and search what OP asked you will find all desired coloumn there</span>
<span class="comment-copy">thank you for your comment. I am currently not familiar with selenium but I will look into it</span>
<span class="comment-copy">ok. let me know how it goes. There are a  lot of links so you might want to try with a list slice of for item in items[:10]:   first</span>
<span class="comment-copy">thank you for your suggestion, it works perfectly fine. appreciate !</span>
<span class="comment-copy">no worries. Please consider hitting the accept check mark next to the answer.  <a href="https://stackoverflow.com/help/someone-answers">stackoverflow.com/help/someone-answers</a></span>
