<div class="post-text" itemprop="text">
<p>I'm having trouble with this autoencoder I'm building using Keras. The input's shape is dependent on the screen size, and the output is going to be a prediction of the next screen size...  However there seems to be an error that I cannot figure out... Please excuse my awful formatting on this website...</p>
<p>Code:</p>
<pre><code>def model_build():
input_img = InputLayer(shape=(1, env_size()[1], env_size()[0]))
x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = MaxPooling2D((2, 2), padding='same')(x)
x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
x = MaxPooling2D((2, 2), padding='same')(x)
x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)
encoded = MaxPooling2D((2, 2), padding='same')(x)
x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
x = UpSampling2D((2, 2))(x)
x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
x = UpSampling2D((2, 2))(x)
x = Conv2D(32, (3, 3), activation='relu')(x)
x = UpSampling2D((2, 2))(x)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
model = Model(input_img, decoded)
return model
if __name__ == '__main__':
    model = model_build()
    model.compile('adam', 'mean_squared_error')
    y = np.array([env()])
    print(y.shape)
    print(y.ndim)
    debug = model.fit(np.array([[env()]]), np.array([[env()]]))
</code></pre>
<p>Error:</p>
<blockquote>
<p>Traceback (most recent call last):
    File "/home/ai/Desktop/algernon-test/rewarders.py", line 46, in 
      debug = model.fit(np.array([[env()]]), np.array([[env()]]))
    File "/home/ai/.local/lib/python3.6/site-packages/keras/engine/training.py", line 952, in fit
      batch_size=batch_size)
    File "/home/ai/.local/lib/python3.6/site-packages/keras/engine/training.py", line 789, in _standardize_user_data
      exception_prefix='target')
    File "/home/ai/.local/lib/python3.6/site-packages/keras/engine/training_utils.py", line 138, in standardize_input_data
      str(data_shape))
  ValueError: Error when checking target: expected conv2d_7 to have shape (4, 268, 1) but got array with shape (1, 270, 480)</p>
</blockquote>
<p>EDIT:</p>
<p>Code for get_screen imported as env():</p>
<pre><code>def get_screen():
    img = screen.grab()
    img = img.resize(screen_size())
    img = img.convert('L')
    img = np.array(img)
    return img
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Looks like <code>env_size()</code> and <code>env()</code> mess image dimensions somehow. Consider this example:</p>
<pre><code>image1 = np.random.rand(1, 1, 270, 480) #First dimension is batch size for test purpose
image2 = np.random.rand(1, 4, 268, 1) #Or any other arbitrary dimensions

input_img = layers.Input(shape=image1[0].shape)
x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)
encoded = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(32, (3, 3), activation='relu')(x)
x = layers.UpSampling2D((2, 2))(x)
decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
model = tf.keras.Model(input_img, decoded)
model.compile('adam', 'mean_squared_error')
model.summary()
</code></pre>
<p>This line will work:</p>
<pre><code>model.fit(image1, nb_epoch=1, batch_size=1)
</code></pre>
<p>But this doesn't </p>
<pre><code>model.fit(image2, nb_epoch=1, batch_size=1)
</code></pre>
<p>Edit:
In order to get output of the same size as input you need to calculate convolution kernel size carefully.
image1 = np.random.rand(1, 1920, 1080, 1) </p>
<pre><code>input_img = layers.Input(shape=image1[0].shape)
x = layers.Conv2D(32, 3, activation='relu', padding='same')(input_img)
x = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(16, 3, activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(8, 3, activation='relu', padding='same')(x)
encoded = layers.MaxPooling2D((2, 2), padding='same')(x)
x = layers.Conv2D(8, 3, activation='relu', padding='same')(encoded)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(16, 3, activation='relu', padding='same')(x)
x = layers.UpSampling2D((2, 2))(x)
x = layers.Conv2D(32, 1, activation='relu')(x) # set kernel size to 1 for example
x = layers.UpSampling2D((2, 2))(x)
decoded = layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)
model = tf.keras.Model(input_img, decoded)
model.compile('adam', 'mean_squared_error')
model.summary()
</code></pre>
<p>This will output same dimensions. </p>
<p>As per this guide <a href="http://cs231n.github.io/convolutional-networks/" rel="nofollow noreferrer">http://cs231n.github.io/convolutional-networks/</a></p>
<blockquote>
<p>We can compute the spatial size of the output volume as a function of
  the input volume size (W), the receptive field size of the Conv Layer
  neurons (F), the stride with which they are applied (S), and the
  amount of zero padding used (P) on the border. You can convince
  yourself that the correct formula for calculating how many neurons
  “fit” is given by <strong>(W−F+2P)/S+1</strong>. For example for a 7x7 input and a 3x3
  filter with stride 1 and pad 0 we would get a 5x5 output. With stride
  2 we would get a 3x3 output.</p>
</blockquote>
</div>
<span class="comment-copy">What is the original shape of your data?  Add code for <code>env()</code> Error occurs in line <code>decoded = </code>?</span>
<span class="comment-copy">@Sharky. There is no supposed "original shape of the data'... Basically I'm just wanting the loss from the autoencoder (to serve as a reward for my rl agent...) The current screen and it's resolution divided by 4 is the shape of the data I guess, as well as being turned to greyscale...</span>
<span class="comment-copy">(1920, height)  ?</span>
<span class="comment-copy">Kept accidentally hitting the enter key, sorry. Eh, really don't understand this... Let's suppose the screen is 1920 by 1080... Ignore any prior resizing, and feed the screen greyscaled into the network... The network should expect the same shape to be both the input and output's shape... The expected output should be the same shape as the input. Hope this makes sense...</span>
<span class="comment-copy">So you need your net to return the following shape (1920, 1080, num_cov_filters) ?</span>
<span class="comment-copy">Yes, I think I want that...</span>
<span class="comment-copy">Updated answer, hope it helps</span>
