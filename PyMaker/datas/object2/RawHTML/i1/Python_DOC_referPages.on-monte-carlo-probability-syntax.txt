<div class="post-text" itemprop="text">
<p>Let 20 people, including exactly 3 women, seat themselves randomly at 4 tables (denoted (A,B,C,D)) of 5 persons each, with all arrangements equally likely. Let X be the number of tables at which no women sit.  Write a numpy Monte Carlo simulation to estimate the expectation of X and also estimate the probability <em>p</em> that no women sit at table A. Run the simulation for 3 cases (100,1000,10000)</p>
<p>I would like to define a function that utilizes numpy's random.permutation function to calculate the expected value of X given an a variable number of trials I understand how to do this on pen and paper, iterating through my collection of probabilities and multiplying them by each other such that I can calculate the total probability of the event.  This is what I have so far </p>
<pre><code>T = 4       # number of tables
N = 20      # number of persons. Assumption: N is a multiple of T.
K = 5       # capacity per table
W = 3       # number of women. Assumption: first W of N persons are women.
M =100      #number of trials

collection = []

for i in range(K):


    x = (((N-W)-i)/(N-i))

    collection.append(x)
</code></pre>
<p>If I examine my collection, this is my output :[0.85, 0.8421052631578947, 0.8333333333333334, 0.8235294117647058, 0.8125]</p>
</div>
<div class="post-text" itemprop="text">
<h2>Implementation</h2>
<p>Here is na√Øve implementation of your Monte-Carlo simulation. It is not designed to be performant, instead it allows you to cross check setup and see details:</p>
<pre><code>import collections
import numpy as np

def runMonteCarlo(nw=3, nh=20, nt=4, N=20):
    """
    Run Monte Carlo Simulation
    """

    def countWomen(c, nt=4):
        """
        Count Number of Women per Table
        """
        x = np.array(c).reshape(nt, -1).T  # Split permutation into tables
        return np.sum(x, axis=0)           # Sum woman per table

    # Initialization:
    comp = np.array([1]*nw + [0]*(nh-nw)) # Composition: 1=woman, 0=man
    x = []                                # Counts of tables without any woman
    p = 0                                 # Probability of there is no woman at table A  

    for k in range(N):
        c = np.random.permutation(comp)   # Random permutation, table composition
        w = countWomen(c, nt=nt)          # Count Woman per table
        nc = np.sum(w!=0)                 # Count how many tables with women 
        x.append(nt - nc)                 # Store count of tables without any woman
        p += int(w[0]==0)                 # Is table A empty?
        #if k % 100 == 0:
            #print(c, w, nc, nt-nc, p)

    # Rationalize (count-&gt;frequency)
    r = collections.Counter(x)
    r = {k:r.get(k, 0)/N for k in range(nt+1)}
    p /= N
    return r, p
</code></pre>
<p>Performing the job:</p>
<pre><code>for n in [100, 1000, 10000]:
    s = runMonteCarlo(N=n)
    E = sum([k*v for k,v in s[0].items()])
    print('N=%d, P(X=k) = %s, p=%s, E[X]=%s' % (n, *s, E))
</code></pre>
<p>Returns:</p>
<pre><code>N=100, P(X=k) = {0: 0.0, 1: 0.43, 2: 0.54, 3: 0.03, 4: 0.0}, p=0.38, E[X]=1.6
N=1000, P(X=k) = {0: 0.0, 1: 0.428, 2: 0.543, 3: 0.029, 4: 0.0}, p=0.376, E[X]=1.601
N=10000, P(X=k) = {0: 0.0, 1: 0.442, 2: 0.5235, 3: 0.0345, 4: 0.0}, p=0.4011, E[X]=1.5924999999999998
</code></pre>
<p>Plotting the distribution, it leads to:</p>
<pre><code>import pandas as pd
axe = pd.DataFrame.from_dict(s[0], orient='index').plot(kind='bar')
axe.set_title("Monte Carlo Simulation")
axe.set_xlabel('Random Variable, $X$')
axe.set_ylabel('Frequency, $F(X=k)$')
axe.grid()
</code></pre>
<p><a href="https://i.stack.imgur.com/NlzsM.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/NlzsM.png"/></a></p>
<h2>Divergence with alternative version</h2>
<blockquote>
<p><strong>Caution:</strong> this method does not answer the stated problem!</p>
</blockquote>
<p>If we implement an another version of the simulation where we change the way random experiment is performed as follow:</p>
<pre><code>import random
import collections

def runMonteCarlo2(nw=3, nh=20, nt=4, N=20):
    """
    Run Monte Carlo Simulation
    """

    def one_experiment(nt, nw):
        """
        Table setup (suggested by @Inon Peled)
        """
        return set(random.randint(0, nt-1) for _ in range(nw)) # Sample nw times from 0 &lt;= k &lt;= nt-1

    c = collections.Counter()             # Empty Table counter
    p = 0                                 # Probability of there is no woman at table A  

    for k in range(N):
        exp = one_experiment(nt, nw)      # Select table with at least one woman
        c.update([nt - len(exp)])         # Update Counter X distribution
        p += int(0 not in exp)            # There is no woman at table A (table 0)

    # Rationalize:
    r = {k:c.get(k, 0)/N for k in range(nt+1)}
    p /= N

    return r, p
</code></pre>
<p>It returns:</p>
<pre><code>N=100, P(X=k) = {0: 0.0, 1: 0.41, 2: 0.51, 3: 0.08, 4: 0.0}, p=0.4, E[X]=1.67
N=1000, P(X=k) = {0: 0.0, 1: 0.366, 2: 0.577, 3: 0.057, 4: 0.0}, p=0.426, E[X]=1.691
N=1000000, P(X=k) = {0: 0.0, 1: 0.37462, 2: 0.562787, 3: 0.062593, 4: 0.0}, p=0.42231, E[X]=1.687973
</code></pre>
<p>This second version converges towards another values, and it is clearly not equivalent to the first version, it does not answer the same question.</p>
<p><a href="https://i.stack.imgur.com/AIZSb.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/AIZSb.png"/></a>
<a href="https://i.stack.imgur.com/53A28.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/53A28.png"/></a>
<a href="https://i.stack.imgur.com/gb5DK.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/gb5DK.png"/></a></p>
<h2>Discussion</h2>
<p>To discriminate which implementation is the correct one I have <a href="https://math.stackexchange.com/q/3158013/113708">computed sampled spaces and probabilities</a> for both implementations. It seems the first version is the correct one because it takes into account that probability of a woman to seat at a table is dependent of who have been selected before. The second version does not take it into account, this is why it does not need to know about how many humans there are and how many people can seat per table.</p>
<p>This is a nice problem to ask because both answers provide close results. A important part of the work is to well setup the Monte Carlo inputs.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can multiply items inside a collection using <code>functools.reduce</code> in <strong>Python 3.x</strong>.</p>
<pre><code>from functools import reduce
event_probability = reduce(lambda x, y: x*y, collection)
</code></pre>
<p>So in your code:</p>
<pre><code>from functools import reduce

T = 4       # number of tables
N = 20      # number of persons. Assumption: N is a multiple of T.
K = 5       # capacity per table
W = 3       # number of women. Assumption: first W of N persons are women.
M = 100      #number of trials

collection = []

for i in range(K):
    x = (((N-W)-i)/(N-i))
    collection.append(x)

event_probability = reduce(lambda x, y: x*y, collection)

print(collection)
print(event_probability)
</code></pre>
<p>Output:</p>
<pre><code>[0.85, 0.8421052631578947, 0.8333333333333334, 0.8235294117647058, 0.8125] # collection
0.3991228070175438 # event_probability
</code></pre>
<p>Then you can use the result to complete your code.</p>
</div>
<div class="post-text" itemprop="text">
<p>Do you have to explicitly simulate the sittings? If not, then simply draw 3 times at random with replacement from 1..4 to simulate one sitting, that is:</p>
<pre><code>def one_experiment():
    return set(random.randint(1, 4) for _ in range(3))  # Distinct tables with women.
</code></pre>
<p>The desired values are then obtained as follows, where N is the number of experiments for any case.</p>
<pre><code>expectation_of_X = sum(4 - len(one_experiment()) for _ in range(N)) / float(N)
probability_no_women_table_1 = sum(1 not in one_experiment() for _ in range(N)) / float(N)
</code></pre>
<p>For large N, the values you get should be approximately p = (3 / 4)^3 and E[X] = (3^3) / (4^2).</p>
</div>
<span class="comment-copy">You are clearing your collection every cycle. <code>collection = []</code> should be outside the loop. Same for <code>count = 0</code>. Why you're using count instead of <code>i</code>?</span>
<span class="comment-copy">I am fairly new to python and wanted to make sure that my count stars at 0.  I was not sure that i was 0 indexed. But your suggestion greatly improved my code.</span>
<span class="comment-copy">Yes python is 0-indexed</span>
<span class="comment-copy">Furthermore, now that I I have my independent probabilities, how do I go through my collection and multiply each value by each other?</span>
<span class="comment-copy">Update your question and add your new code and your problem with the collection. It would be great if you show us an example output!</span>
<span class="comment-copy">I do need to explicitly simulate the sittings, where I can pass in the number of trials</span>
<span class="comment-copy">Are you sure about your formulas for <code>E[x]</code> and <code>p</code>? There are 4 tables, not 5.</span>
<span class="comment-copy">inonPeled , I don't think the output of the code below is correct. My calculations show that the probabilty of 5 men sitting at a table is (91/228) and that E(x) is (91/57) . I am having trouble drafting a function that takes in a variable number of trials and spits out this output</span>
<span class="comment-copy">Yes this kind of typo happens. Anyway I have been thinking about your implementation and I found that it diverges with mine. Doing some math, I know why and it might point out that your answer does not exactly address the stated problem. Details are <a href="https://math.stackexchange.com/questions/3158013/how-to-explain-this-monte-carlo-divergence">here</a>.</span>
<span class="comment-copy">Correct. I liked to investigate this problem. The funny fact about it is that answers are relatively close together, so it is easy to convince yourself it might be correct, until you discover the fundamental reason. Nice exercise.</span>
