<div class="post-text" itemprop="text">
<p>I've been trying to research model/weight saving for a while, but I still can't fully grasp it. I feel what I'd like to do should be simple enough, but I've not found a solution.</p>
<p>The final goal is to do transfer laerning with a collection of pretrained networks. I write my models/layers as classes, so class method(s) for saving the weights and restoring would be ideal.</p>
<p>Example:
If I have a graph, features &gt; A &gt; B &gt; labels, where A and B are sub-networks, I'd like to save and/or restore weights for these sections. Say I already have the weights for A trained, but the variable scope is now different, how would I restore the weights I've trained for A from a different training session? At the end of training this new graph i'd like 1 directory for my new A weights, 1 directory for my new B weights, and 1 directory for the full graph (I can handle the full graph bit). </p>
<p>It's very possible I keep overlooking the solution, but model saving is so poorly documented.</p>
<p>Hope I've explained the scenario well.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can do this with <code>tf.train.init_from_checkpoint</code> </p>
<p>Define your model</p>
<pre><code>def model_fn():
    with tf.variable_scope('One'):
        layer = any_tf_layer
    with tf.variable_scope('Two'):
        layer = any_tf_layer 
</code></pre>
<p>Output variable names in checkpoint file </p>
<pre><code>vars = [i[0] for i in tf.train.list_variables(ckpt_file)]
</code></pre>
<p>Then you can create assignment map to load only variables, defined in your model.
You can also assign new names to restored variables</p>
<pre><code> map = {variable.op.name: variable for variable in tf.global_variables() if variable.op.name in vars}
</code></pre>
<p>This line is placed before session or outside model function for Estimator API</p>
<pre><code>tf.train.init_from_checkpoint(ckpt_file, map)
</code></pre>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/train/init_from_checkpoint" rel="nofollow noreferrer">https://www.tensorflow.org/api_docs/python/tf/train/init_from_checkpoint</a></p>
<p>You also can do it with <code>tf.train.Saver</code>
First you need to know the names of variables</p>
<pre><code>vars_dict = {}
for var_current in tf.global_variables():
    print(var_current)
    print(var_current.op.name) # this gets only name

for var_ckpt in tf.train.list_variables(ckpt):
    print(var_ckpt[0]) this gets only name
</code></pre>
<p>When you know exact names of all variables you can assign whatever value you need, provided variables have same shape and dtype. So to get a dictionary</p>
<pre><code>vars_dict[var_ckpt[0]) = tf.get_variable(var_current.op.name, shape) # remember to specify shape, you can always get it from var_current 

saver = tf.train.Saver(vars_dict)
</code></pre>
<p>Take a look at my other answer to similar question
<a href="https://stackoverflow.com/questions/55275221/how-to-restore-pretrained-checkpoint-for-current-model-in-tensorflow/55275854#55275854">How to restore pretrained checkpoint for current model in Tensorflow?</a></p>
</div>
<span class="comment-copy">Updated answer, hope this will make it clear</span>
<span class="comment-copy">It actually all made sense to me soon after I posted. But this is a good answer, hopefully it will help others in finding tf.init_from_checkpoint.</span>
