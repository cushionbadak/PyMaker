<div class="post-text" itemprop="text">
<h3>Problem</h3>
<p>I'm trying to classify some <code>64x64</code> images as a black box exercise. The NN I have written doesn't change my weights. First time writing something like this, the same code, but on <strong>MNIST</strong> letters input works just fine, but on this code it does not train like it should:</p>
<pre><code>import tensorflow as tf
import numpy as np


path = ""

# x is a holder for the 64x64 image
x = tf.placeholder(tf.float32, shape=[None, 4096])

# y_ is a 1 element vector, containing the predicted probability of the label
y_ = tf.placeholder(tf.float32, [None, 1])

# define weights and balances
W = tf.Variable(tf.zeros([4096, 1]))
b = tf.Variable(tf.zeros([1]))

# define our model
y = tf.nn.softmax(tf.matmul(x, W) + b)

# loss is cross entropy
cross_entropy = tf.reduce_mean(
                tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))

# each training step in gradient decent we want to minimize cross entropy
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

train_labels = np.reshape(np.genfromtxt(path + "train_labels.csv", delimiter=',', skip_header=1), (14999, 1))
train_data = np.genfromtxt(path + "train_samples.csv", delimiter=',', skip_header=1)

# perform 150 training steps with each taking 100 train data
for i in range(0, 15000, 100):
    sess.run(train_step, feed_dict={x: train_data[i:i+100], y_: train_labels[i:i+100]})
    if i % 500 == 0:
        print(sess.run(cross_entropy, feed_dict={x: train_data[i:i+100], y_: train_labels[i:i+100]}))
        print(sess.run(b), sess.run(W))

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


sess.close()
</code></pre>
<p>How do I solve this problem?</p>
</div>
<div class="post-text" itemprop="text">
<p>The key to the problem is that the class number of you output <code>y_</code> and <code>y</code> is <code>1</code>.You should adopt <code>one-hot</code> mode when you use <code>tf.nn.softmax_cross_entropy_with_logits</code> on classification problems in tensorflow. <code>tf.nn.softmax_cross_entropy_with_logits</code> will first compute <code>tf.nn.softmax</code>. When your class number  is <code>1</code>, your results are all the same. For example:</p>
<pre><code>import tensorflow as tf

y = tf.constant([[1],[0],[1]],dtype=tf.float32)
y_ = tf.constant([[1],[2],[3]],dtype=tf.float32)

softmax_var = tf.nn.softmax(logits=y_)
cross_entropy = tf.multiply(y, tf.log(softmax_var))

errors = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)

with tf.Session() as sess:
    print(sess.run(softmax_var))
    print(sess.run(cross_entropy))
    print(sess.run(errors))

[[1.]
 [1.]
 [1.]]
[[0.]
 [0.]
 [0.]]
[0. 0. 0.]
</code></pre>
<p>This means that no matter what your output <code>y_</code>, your loss will be zero. So your <code>weights</code> and <code>bias</code> haven't been updated.</p>
<p><strong>The solution is to modify the class number of <code>y_</code> and <code>y</code>.</strong></p>
<p>I suppose your class number is <code>n</code>. </p>
<p>First approch:You can change data to <code>one-hot</code> before feed data.Then use the following code.</p>
<pre><code>y_ = tf.placeholder(tf.float32, [None, n])
W = tf.Variable(tf.zeros([4096, n]))
b = tf.Variable(tf.zeros([n]))
</code></pre>
<p>Second approchï¼šchange data to <code>one-hot</code> after feed data.</p>
<pre><code>y_ = tf.placeholder(tf.int32, [None, 1])
y_ = tf.one_hot(y_,n) # your dtype of y_ need to be tf.int32
W = tf.Variable(tf.zeros([4096, n]))
b = tf.Variable(tf.zeros([n]))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>All your initial weights are zeros. When you have that way, the NN doesn't learn well. You need to initialize all the initial weights with random values.</p>
<p>See <a href="https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers">Why should weights of Neural Networks be initialized to random numbers?</a></p>
<p>"<strong>Why Not Set Weights to Zero?</strong>
We can use the same set of weights each time we train the network; for example, you could use the values of 0.0 for all weights.</p>
<p>In this case, the equations of the learning algorithm would fail to make any changes to the network weights, and the model will be stuck. It is important to note that the bias weight in each neuron is set to zero by default, not a small random value.
"</p>
<p>See 
<a href="https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/" rel="nofollow noreferrer">https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/</a></p>
</div>
<span class="comment-copy">Could you define? "does not train like it should".  What exactly are you expecting that you can't fix?</span>
<span class="comment-copy">it should change the weights and bias to minimize the loss function</span>
<span class="comment-copy">How do I modify the class? Or how do I use the one hot approach?</span>
<span class="comment-copy">@Samuel I have added it to the answer.</span>
<span class="comment-copy">If I do the first then it can't read the data since the labels have (?, 1) shape (forgot to mention in the labels.csv files there is one label on each line) and y_ now is (?, 8) If I do the second, it says "Cannot feed value of shape (100, 1) for Tensor 'one_hot:0', which has shape '(?, 8, 8)' "</span>
<span class="comment-copy">@Samuel You need change data to (?,8) before feed data and after read files when you use the first method. You need keep the shape of labels is (?,1) when you use second method.</span>
<span class="comment-copy">I can't figure out how to code this...and what the thinking process is</span>
<span class="comment-copy">Already tried that, they just stay the same</span>
