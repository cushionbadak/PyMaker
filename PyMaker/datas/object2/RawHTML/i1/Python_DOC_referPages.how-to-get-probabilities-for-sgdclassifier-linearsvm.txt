<div class="post-text" itemprop="text">
<p>I'm using <code>SGDClassifier</code> with <code>loss function = "hinge"</code>. But hinge loss does not support probability estimates for class labels. </p>
<p>I need probabilities for calculating <code>roc_curve</code>. How can I get probabilities for hinge loss in SGDClassifier without using SVC from svm?</p>
<p>I've seen people mention about using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html]" rel="nofollow noreferrer">CalibratedClassifierCV</a> to get the probabilities but I've never used it and I don't know how it works.</p>
<p>I really appreciate the help. Thanks</p>
</div>
<div class="post-text" itemprop="text">
<p>In the strict sense, that's not possible.</p>
<p>Support vector machine classifiers are non-probabilistic: they use a hyperplane (a line in 2D, a plane in 3D and so on) to separate points into one of two classes. Points are only defined by which side of the hyperplane they are on., which forms the prediction directly.</p>
<p>This is in contrast with probabilistic classifiers like logistic regression and decision trees, which generate a probability for every point that is <em>then</em> converted to a prediction.</p>
<p><code>CalibratedClassifierCV</code> is a sort of meta-estimator; to use it, you simply pass your instance of a base estimator to its constructor, so this will work:</p>
<pre><code>base_model = SGDClassifier()
model = CalibratedClassifierCV(base_model)

model.fit(X, y)
model.predict_proba(X)
</code></pre>
<p>What it does is perform internal cross-validation to create a probability estimate. Note that this is equivalent to what <code>sklearn.SVM.SVC</code> does anyway.</p>
</div>
<span class="comment-copy">I've split my data into X_train and X_test so I've fitted SGDClassifier on X_train and y_train after that what should I use.</span>
<span class="comment-copy">Do what I said in the question - pass a <code>SGDClassifier</code> instance into <code>CalibratedClassifierCV()</code> and then fit and score the <code>CalibratedClassifierCV</code> instance as normal on <code>X_train</code> and <code>X_test</code> respectively.</span>
<span class="comment-copy">So rather than using <code>base_model.predict</code> we should use <code>model.predict</code> for predicting class labels and also probabilities right?</span>
<span class="comment-copy">@user214 Yes, see also the example code in the answer.</span>
