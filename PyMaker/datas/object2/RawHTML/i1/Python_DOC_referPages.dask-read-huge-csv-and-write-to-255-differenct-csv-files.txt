<div class="post-text" itemprop="text">
<p>I am using <a href="http://docs.dask.org/en/latest/" rel="nofollow noreferrer">DASK</a> to read CSV file which sizes around 2GB.
I want to write each row of it to separate 255 number CSV files based on some hash function as below.</p>
<h3>My naive solution:</h3>
<pre><code>from dask import dataframe as dd

if __name__ == '__main__':
    df = dd.read_csv('train.csv', header=None, dtype='str')
    df = df.fillna()
    for _, line in df.iterrows():
        number = hash(line[2]) % 256
        with open("{}.csv".format(number), 'a+') as f:
            f.write(', '.join(line))
</code></pre>
<p>This way takes around 15 minutes. Is there any way we can do it faster.</p>
</div>
<div class="post-text" itemprop="text">
<p>Since your procedure is dominated by IO, it is very unlikely that Dask would do anything but add overhead in this case, <em>unless</em> your hash function is really really slow. I assume that is not the case.</p>
<p>@zwer 's solution would look something like</p>
<pre><code>files = [open("{}.csv".format(number), 'a+') for number in range(255)]
for _, line in df.iterrows():
    number = hash(line[2]) % 256
    files[number].write(', '.join(line))
[f.close() for f in files]
</code></pre>
<p>However, your data appears to fit in memory, so you may find much better performance</p>
<pre><code>for (number, group) in df.groupby(df.iloc[:, 2].map(hash)):
    group.to_csv("{}.csv".format(number))
</code></pre>
<p>because you write to each file continuously rather than jumping between them. Depending on your IO device and buffering, the difference can be none or huge.</p>
</div>
<span class="comment-copy">Since you're not doing anything fancy with your CSV, why not just use the built-in <a href="https://docs.python.org/3/library/csv.html" rel="nofollow noreferrer"><code>csv</code></a> module? Also, you should cache your file handlers so you don't keep opening and closing the files on each row of your CSV - it's ok in your case to keep 256 open file handles and close them in the end.</span>
<span class="comment-copy">Thanks @zwer so should I open all files before and try to loop over data?</span>
<span class="comment-copy">You can open them on demand... Make a dictionary to hold your map, say <code>file_map</code>, then just do something like <code>f = file_map.get(number)</code> and <code>if f is None: f = file_map[number] = open('{}.csv'.format(number), 'a+')</code> instead of your <code>with ...</code> line. Then just go through all of them and close the handles once you've iterated over the full length of your CSV.</span>
<span class="comment-copy">Ok right now trying with csv reader and writing to only one file but it is still taking more time.</span>
<span class="comment-copy">According to you what is the ideal time for this kind of task?</span>
<span class="comment-copy">Thanks mdurant I would definitely try this.</span>
