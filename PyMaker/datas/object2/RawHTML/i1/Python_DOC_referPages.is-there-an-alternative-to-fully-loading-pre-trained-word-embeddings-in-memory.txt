<div class="post-text" itemprop="text">
<p>I want to use pre-trained word embeddings in my machine learning model. The word embedings file I have is about 4GB. I currently read the entire file into memory in a dictionary and whenever I want to map a word to its vector representation I perform a lookup in that dictionary.</p>
<p>The memory usage is very high and I would like to know if there is another way of using word embeddings without loading the entire data into memory.</p>
<p>I have recently come across generators in Python. Could they help me reduce the memory usage?</p>
<p>Thank you!</p>
</div>
<div class="post-text" itemprop="text">
<p>What task do you have in mind? If this is a similarity based task, you could simply use the <code>load_word2vec_format</code> method in gensim, this allows you to pass in a limit to the number of vectors loaded. The vectors in something like the Googlenews set are ordered by frequency, this will give you the critical vectors. 
This also makes sense theoretically as the words with low frequency will usually have relatively bad representations.</p>
</div>
<span class="comment-copy">Thank you. I will use Googlenews and extract only the words with the highest frequency(eg. first 40k words).</span>
