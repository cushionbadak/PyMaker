<div class="post-text" itemprop="text">
<p>I have to productionize a PyTorch BERT Question Answer model. The CPU inference is very slow for me as for every query the model needs to evaluate 30 samples. Out of the result of these 30 samples, I pick the answer with the maximum score. GPU would be too costly for me to use for inference.</p>
<p>Can I leverage multiprocessing / parallel CPU inference for this?
If Yes, what is the best practice to do so?
If No, is there a cloud option that bills me only for the GPU queries I make and not for continuously running the GPU instance?</p>
</div>
<div class="post-text" itemprop="text">
<p>You can try out the following:</p>
<p>Make use of Intel Pytorch for better performance. Refer <a href="https://github.com/intel/pytorch" rel="nofollow noreferrer">https://github.com/intel/pytorch</a> to install pytorch with Intel Optimized functionalities.</p>
<p>You can use Intel Optimized Python libraries. i.e while creating your environment make sure you pass the intel channel. Eg: conda create -n <strong>env_name</strong> python=3.6 <strong>-c intel</strong>. Also, you can install specific libraries with Intel channel.</p>
<p>Another option is to try out using Multi-Node distributed training using Horovod for PyTorch. Refer the link <a href="https://github.com/horovod/horovod#pytorch" rel="nofollow noreferrer">https://github.com/horovod/horovod#pytorch</a> for further details.</p>
</div>
<div class="post-text" itemprop="text">
<p>Another possible way to get better performance would be to reduce the model as much as possible.</p>
<p>One of the most promising techniques is quantized and binarized neural networks. Here are some references:</p>
<ol>
<li><a href="https://arxiv.org/abs/1603.05279" rel="nofollow noreferrer">https://arxiv.org/abs/1603.05279</a></li>
<li><a href="https://arxiv.org/abs/1602.02505" rel="nofollow noreferrer">https://arxiv.org/abs/1602.02505</a></li>
</ol>
</div>
