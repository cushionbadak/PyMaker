<div class="post-text" itemprop="text">
<p>I have implemented unit tests for my Flask app. I use pytest. My goal is to make sure that a number of pre-defined queries always return the same output (some json).</p>
<p>In order to do that, I use a fixture to start an app with test settings:</p>
<pre><code># Session for scope, otherwise server is reloaded everytime
@pytest.fixture(scope="session")
def app():

    os.environ["FLASK_ENV"] = "development"
    os.environ["TEST_SETTINGS"] = os.path.join(
        ds.WORK_DIR, "..", "tests", "settings.py"
    )

    app = create_app()

    # http://flask.pocoo.org/docs/1.0/api/#flask.Flask.test_client
    app.testing = True

    return app
</code></pre>
<p>And then I run a test function, which is not really a test function, with pytest:</p>
<pre><code># Freeze time for consistent output
@pytest.mark.usefixtures("live_server")
@pytest.mark.freeze_time("2018-01-01")
class TestLiveServer:

    @pytest.mark.skip(reason="should only be used to update the test data")
    def test_export(self):
        """Not a test function.

        Used to export questions outputs in json file to freeze the expected output

        """

        for q in TESTED_QUESTIONS:
            r = requests.post(
                url_for("query_direct", _external=True), json={"query": q}
            )
            print(r.text)

            filename = (
                "".join(filter(lambda c: str.isalnum(c) or c == " ", q))
                .lower()
                .replace(" ", "_")
                + ".json"
            )
            export_path = os.path.join("tests", "fake_responses", filename)

            data = {"query": q, "response": r.json()}

            with open(export_path, "w") as outfile:
                json.dump(data, outfile, indent=4, sort_keys=True)
                outfile.write("\n")
</code></pre>
<p>To generate my frozen outputs,  I uncomment the pytest marker and I run this particular test. As you can see, it's not very elegant and error prone. Ssometimes I  forget to re-enable the marker, annd if I run all the tests at once, it first re-generates the fake outputs before running unit tests on them. If this  happens, my tests don't fail and I don't catch the potential mistakes (whhich kills the point of these tests).</p>
<p>Is there a way to run this particular function alone, maybe with some pytest flag or something?</p>
</div>
<div class="post-text" itemprop="text">
<p>This should probably be a standalone thing and not part of the pytest test suite (since it's not a test), but you can get around it by exploiting pytest's <a href="https://docs.pytest.org/en/latest/skipping.html#id1" rel="nofollow noreferrer">skipping facilities</a>.</p>
<pre class="lang-py prettyprint-override"><code># redefine test_export to feature a conditional skip
@pytest.mark.skipif(not os.getenv('FREEZE_OUTPUTS'), reason='')
def test_export(self):
</code></pre>
<p>Here we skip this test, unless the <code>FREEZE_OUTPUTS</code> environment variable is set.</p>
<p>You can then run this test by calling the following from the command line (define the environment variable for this invocation only):</p>
<pre class="lang-sh prettyprint-override"><code>$ FREEZE_OUTPUTS=1 py.test &lt;your_test_file&gt;::TestLiveServer::test_export
</code></pre>
<p>Which will only run that test. In all other cases it will be skipped.</p>
<p>You can even follow the above approach and declare this as a fixture that you include on a <a href="https://docs.pytest.org/en/latest/fixture.html#scope-sharing-a-fixture-instance-across-tests-in-a-class-module-or-session" rel="nofollow noreferrer">session level</a> with <code>autouse=True</code> <a href="https://docs.pytest.org/en/latest/fixture.html#autouse-fixtures-xunit-setup-on-steroids" rel="nofollow noreferrer">so it's always included</a> and then in the fixture itself add some logic to check if you've defined <code>FREEZE_OUTPUTS</code> and if so, run the logic in question. Something like:</p>
<pre class="lang-py prettyprint-override"><code>@pytest.fixture(scope='session', autouse=True)
def frozen_outputs():
    if os.getenv('FREEZE_OUTPUTS'):
        # logic for generating the outputs goes here
    return  # do nothing otherwise
</code></pre>
</div>
<span class="comment-copy">This is not what I'm asking for. I can check my responses already. I need a way to generate/export them without running that as a test.</span>
<span class="comment-copy">I've edited my reply.</span>
<span class="comment-copy">Ah, I think I like the second part with the fixture autouse. How do I run it though?</span>
<span class="comment-copy">It's always run/included (because of <code>autouse=True</code>) when you run <code>py.test</code> (and it lasts for the entire session because of <code>scope='session'</code>) but the logic will be executed only if you add the <code>FREEZE_OUTPUTS=1</code> before the command invocation. Otherwise it's skipped. Put something innocent in there and give it a go.</span>
