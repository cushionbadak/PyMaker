<div class="post-text" itemprop="text">
<p>I've got a problem where I want to predict one time series with many time series.  My input is <code>(batch_size, time_steps, features)</code> and my output should be <code>(1, time_steps, features)</code></p>
<p>I can't figure out how to average over N.  </p>
<p>Here's a dummy example.  First, dummy data where the output is a linear function of 200 time series:</p>
<pre><code>import numpy as np
time = 100
N = 2000

dat = np.zeros((N, time))
for i in range(time):
    dat[i,:] = np.sin(list(range(time)))*np.random.normal(size =1) + np.random.normal(size = 1)

y = dat.T @ np.random.normal(size = N)
</code></pre>
<p>Now I'll define a time series model (using 1-D conv nets):</p>
<pre><code>from keras.models import Model
from keras.layers import Input, Conv1D, Dense, Lambda
from keras.optimizers import Adam
from keras import backend as K

n_filters = 2
filter_width = 3
dilation_rates = [2**i for i in range(5)] 
inp = Input(shape=(None, 1))
x = inp
for dilation_rate in dilation_rates:
    x = Conv1D(filters=n_filters,
               kernel_size=filter_width, 
               padding='causal',
               activation = "relu",
               dilation_rate=dilation_rate)(x)
x = Dense(1)(x)

model = Model(inputs = inp, outputs = x)
model.compile(optimizer = Adam(), loss='mean_squared_error')
model.predict(dat.reshape(N, time, 1)).shape

Out[43]: (2000, 100, 1)
</code></pre>
<p>The output is the wrong shape!  Next, I tried using an averaging layer, but I get this weird error:</p>
<pre><code>def av_over_batches(x):
    x = K.mean(x, axis = 0)
    return(x)

x = Lambda(av_over_batches)(x)

model = Model(inputs = inp, outputs = x)
model.compile(optimizer = Adam(), loss='mean_squared_error')
model.predict(dat.reshape(N, time, 1)).shape

Traceback (most recent call last):

  File "&lt;ipython-input-3-d43ccd8afa69&gt;", line 4, in &lt;module&gt;
    model.predict(dat.reshape(N, time, 1)).shape

  File "/home/me/.local/lib/python3.6/site-packages/keras/engine/training.py", line 1169, in predict
    steps=steps)

  File "/home/me/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py", line 302, in predict_loop
    outs[i][batch_start:batch_end] = batch_out

ValueError: could not broadcast input array from shape (100,1) into shape (32,1)
</code></pre>
<p>Where does <code>32</code> come from?  (Incidentally, I got the same number in my real data, not just in the MWE).   </p>
<p>But the main question is: <strong>how can I build a network that averages over the input batch dimension?</strong></p>
</div>
<div class="post-text" itemprop="text">
<p>I would approach the problem in a different way </p>
<p>Problem: You want to predict a time series from a set of time series. so lets say you have 3 time series value <code>TS1, TS2, TS3</code> each of 100 time steps you want to predict a time series <code>y1, y2, y3</code>.</p>
<p>My approach for this problem will be as below</p>
<p><a href="https://i.stack.imgur.com/10Jy4.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/10Jy4.png"/></a></p>
<p>i.e group the times series each time step together and feed it to an LSTM. If some time steps are shorter then others them you can pad them. Similarly if some sets have fewer time series then again pad them. </p>
<h2>Example:</h2>
<pre><code>import numpy as np
np.random.seed(33)

time = 100
N = 5000
k = 5

magic = np.random.normal(size = k)

x = list()
y = list()
for i in range(N):
    dat = np.zeros((k, time))
    for i in range(k):
        dat[i,:] = np.sin(list(range(time)))*np.random.normal(size =1) + np.random.normal(size = 1)
    x.append(dat)
    y.append(dat.T @ magic)
</code></pre>
<p>So I want to predict a timeseries of 100 steps from a set of 3 times steps. We want to the model to learn the <code>magic</code>.</p>
<pre><code>from keras.models import Model
from keras.layers import Input, Conv1D, Dense, Lambda, LSTM
from keras.optimizers import Adam
from keras import backend as K
import matplotlib.pyplot as plt

input = Input(shape=(time, k))
lstm = LSTM(32, return_sequences=True)(input)
output = Dense(1,activation='sigmoid')(lstm)

model = Model(inputs = input, outputs = output)
model.compile(optimizer = Adam(), loss='mean_squared_error')

data_x = np.zeros((N,100,5))
data_y = np.zeros((N,100,1))

for i in range(N):
    data_x[i] = x[i].T.reshape(100,5)
    data_y[i] = y[i].reshape(100,1)

from sklearn.preprocessing import StandardScaler

ss_x = StandardScaler()
ss_y = StandardScaler()

data_x = ss_x.fit_transform(data_x.reshape(N,-1)).reshape(N,100,5)
data_y = ss_y.fit_transform(data_y.reshape(N,-1)).reshape(N,100,1)

# Lets leave the last one sample for testing rest split into train and validation
model.fit(data_x[:-1],data_y[:-1], batch_size=64, nb_epoch=100, validation_split=.25)
</code></pre>
<p>The val loss was going down still but I stoped it. Lets see how good our prediction is</p>
<pre><code>y_hat = model.predict(data_x[-1].reshape(-1,100,5))
plt.plot(data_y[-1], label='y')
plt.plot(y_hat.reshape(100), label='y_hat')
plt.legend(loc='upper left')
</code></pre>
<p><a href="https://i.stack.imgur.com/f87Ut.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/f87Ut.png"/></a></p>
<p>The results are promising. Running it for more epochs and also hyper parameter tuning should further bring us close the the <code>magic</code>. One can also try stacked LSTM and bi-directional LSTM.</p>
<p>I feel RNNs are better suited for time series data rather then CNN's</p>
<p>Data Format:
Lets say time steps = 3
Time series 1 = <code>[1,2,3]</code></p>
<p>Time series 2 = <code>[4,5,6]</code></p>
<p>Time series 3 = <code>[7,8,9]</code></p>
<p>Time series 3 = <code>[10,11,12]</code></p>
<p>Y = <code>[100,200,300]</code></p>
<p>For a batch size of 1 </p>
<p><code>[[1,4,7,10],[2,5,8,11],[3,6,9,12]] -&gt; LSTM -&gt; [100,200,300]</code></p>
</div>
<span class="comment-copy">This is an interesting answer, but I don't see how to take it to either learn the <code>magic</code> parameter (nothing that your parameterization is different than mine -- you have one parameter per variable, rather than one parameter per time series), or the single aggregate function.</span>
<span class="comment-copy">What I tried to do above is to train a model to learn the function (<code>magic</code> in this case) which transforms a bunch of data points (corresponding to different time series) to a single data point (resulting in one time series)</span>
<span class="comment-copy">Hmm. Apologies if I'm being dense, but how does your approach yield a <code>(1, time, 1)</code> time series from an input of <code>(N, time, 1)</code> time series?</span>
<span class="comment-copy">@generic_user Yes you are correct about the dimensions, except we have to do reshaping to match how LSTMS take the input</span>
<span class="comment-copy">Correct about what?</span>
