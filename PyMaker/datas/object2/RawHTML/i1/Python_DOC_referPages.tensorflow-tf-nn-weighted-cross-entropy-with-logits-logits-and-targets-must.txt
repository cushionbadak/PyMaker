<div class="post-text" itemprop="text">
<p>I've just started using tensorflow for a project I'm working on. The program aims to be a binary classifier with input being 12 features. The output is either normal patient or patient with a disease. The prevalence of the disease is quite low and so my dataset is very imbalanced, with 502 examples of normal controls and only 38 diseased patients. For this reason, I'm trying to use <code>tf.nn.weighted_cross_entropy_with_logits</code> as my cost function.</p>
<p>The code is based on the iris custom estimator from the official tensorflow documentation, and works with <code>tf.losses.sparse_softmax_cross_entropy</code> as the cost function. However, when I change to <code>weighted_cross_entropy_with_logits</code>, I get a shape error and I'm not sure how to fix this.</p>
<pre><code>ValueError: logits and targets must have the same shape ((?, 2) vs (?,))
</code></pre>
<p>I have searched and similar problems have been solved by just reshaping the labels - I have tried to do this unsuccessfully (and don't understand why <code>tf.losses.sparse_softmax_cross_entropy</code> works fine and the weighted version does not). </p>
<p>My full code is here
<a href="https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7" rel="nofollow noreferrer">https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7</a></p>
<p>Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>With non-sparse cross-entropy functions, you need to one-hot encode your labels so they have the same shape as your logits:</p>
<pre><code>loss = tf.nn.weighted_cross_entropy_with_logits(tf.one_hot(labels, 2), logits, pos_weight)
</code></pre>
<p>Note <a href="https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy" rel="nofollow noreferrer"><code>tf.losses.sparse_softmax_cross_entropy</code></a> also admits a <code>weights</code> parameter, although it has a slightly different meaning (it is just a sample-wise weight). The equivalent formulation should be:</p>
<pre><code>loss = tf.losses.sparse_softmax_cross_entropy(labels, logits,
                                              weights=pos_weight * labels + (1 - labels))
</code></pre>
</div>
<span class="comment-copy">Ah I see. I thought that might be the case but I was one-hot encoding as part of the data preparation which wasn't working.    After using your sparse_softmax_cross_entropy with the added weight parameter it seems to work, but using the weighted_cross_entropy_with_logits generates another error.     <code>ValueError: Dimension size must be evenly divisible by 2 but is 1 for 'Reshape' (op: 'Reshape') with input shapes: [?,2], [0].</code></span>
