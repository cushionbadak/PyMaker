<div class="post-text" itemprop="text">
<p>I am having upto 500k rows of data in mysql database table.I have to process that data with some queries and insert the resultant query data into 5 different tables.</p>
<p>My code snippet is like as below:</p>
<pre><code>def jobsFunction(values):
    unique_values = []
    ref_value = {}
    for value in values:
        if value not in unique_values:
            unique_values.append(value[0])
            # some select queries with other tables
            # from the result insert into table1
            for query_vals in select_query:
                ref_val[id] = some_val
                # Insert into table2 with query_vals
                # Update table3 with query_vals
        # insert into table4 for each iteration with some process
        # insert into table5 based on ref_val[id]

if __name__ == '__main__':
    query = "SELECT roll_no, user_id, tenant_item_id FROM table_name"
    cursor.execute(query)
    vals = cursor.fetchall()
    values = list(vals)
    jobFunction(values)
</code></pre>
<p>The problem is it takes more than 12 hours to complete the entire process.
So I decided to complete the process with <code>multiprocessing.Pool</code> with the code like as below:</p>
<pre><code>import multiprocessing as mp

def jobsFunction(values):
    # jobs function code

if __name__ == '__main__':
    # values fetching
    lock = mp.Lock()
    p = mp.Pool()
    p.map(jobsFunction, values)
    p.close()
    p.join()
</code></pre>
<p>But in this case the flow of data from main function to <code>jobsFunction</code> is not in the order.</p>
<p>My question is: Am I using the right method for my requirement
and how can I achieve my requirement effectively using multiprocessing or multithreading?</p>
</div>
<div class="post-text" itemprop="text">
<p>Fetching data out of a database and then writing is back is slow. Try to avoid it. Some numbers: If every query just takes 100 ms, just executing them takes more than 13 hours.</p>
<p>Consider using this design: Instead of transferring all the data to Python to process it there, use a series or SQL queries to do everything inside the database. So instead of reading the data into a Python list, use SQL queries like</p>
<pre><code>insert into table1 (...)
select ... from table_name
</code></pre>
<p>or</p>
<pre><code>update table1 out
set out.col1 = source.col2,
    out.col2 = source.col3 ...
from table_name source
where out.pk = source.pk
  and ...
</code></pre>
<p>Databases are optimized to copy data around. Those queries will run very fast, especially when you have set up your indexes right.</p>
<p>Consider using helper tables to make your queries more simple or efficient since you can create them, truncate them, fill them with data and then create the perfect indexes for your case.</p>
<p>Only do the really complicated stuff in Python and make sure it only processes a few rows.</p>
</div>
<span class="comment-copy">The unordered results is just a side-effect of concurrent processing (whether you're using multi-threading or multi-processing). Since that's important, then you will need serialize the database updates yourself somehow. Note that the way you're doing the multiprocessing is only passing a single <code>value</code> to the <code>jobsFunction</code>, not a list of them â€” so the code in the function would need to change from the way shown above it to accommodate this. That also means it's creating a total 500k subprocesses which will involve significant overhead, so it may be slower than not using it.</span>
<span class="comment-copy">It was a mistake.Updated the question.Check it now</span>
<span class="comment-copy">Your edit made no changes to the code, so my previous comments all still apply.</span>
<span class="comment-copy">I not suppose to use this.Because there is multiple values for each primary key from <code>table_name</code>.So, I am inserting multiple rows of data in table2 , table3 for each primary key from <code>table_name</code></span>
<span class="comment-copy">Then your code will be slow. 500K queries where each query takes 100ms = 50'000 seconds = 13 hours.</span>
<span class="comment-copy">@DineshKumar How do you know how many values? It's always 5? Then repeat <code>insert into ... select ...</code> five times. That should just take a few seconds even for 500K rows.</span>
<span class="comment-copy">For each primary key in <code>table_name</code> has multiple rows as foreign key from other tables.So I have to insert all the rows from resultant query to table1, table2 and table3.In my case for each id from <code>table_name</code>, I am inserting upto 10 to 15 rows in table1 and table2 in for loop</span>
<span class="comment-copy">What stops you from sending 10 to 15 queries which create all the rows instead of a million queries in a loop?</span>
