<div class="post-text" itemprop="text">
<p>There are lots of examples online that show how to use TimeSeriesSplit to create multiple training/test sets.  However, they don't show how to actually aggregate these in practice.</p>
<p>For example, this is provided from the scikit-learn documentation:</p>
<pre><code>from sklearn.model_selection import TimeSeriesSplit
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4, 5, 6])
tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(X):
   print("TRAIN:", train_index, "TEST:", test_index)
   X_train, X_test = X[train_index], X[test_index]
   y_train, y_test = y[train_index], y[test_index]
</code></pre>
<p>Which produces the results:</p>
<blockquote>
<p>TRAIN: [0] TEST: [1]</p>
<p>TRAIN: [0 1] TEST: [2]</p>
<p>TRAIN: [0 1 2] TEST: [3]</p>
<p>TRAIN: [0 1 2 3] TEST: [4]</p>
<p>TRAIN: [0 1 2 3 4] TEST: [5]</p>
</blockquote>
<p>However, it is not clear on how to actually utilize these multiple splits in a training regime.  I can use each individually, but then future trainers are not benefitting from the previous split.  Right now my best guess would be combining all splits together work?  So I'm left with:</p>
<blockquote>
<p>TRAIN: [0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4] TEST [1, 2, 3, 4, 5]</p>
</blockquote>
<p>Or is there something else I'm missing?</p>
</div>
<div class="post-text" itemprop="text">
<p>Lets have a look at the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html" rel="nofollow noreferrer">documentation</a>, where it is stated that:</p>
<blockquote>
<p>Note that unlike standard cross-validation methods, successive
  training sets are supersets of those that come before them.</p>
</blockquote>
<p>If I understand correctly, they are using a windowing approach for this time series split, where each following split is one element larger than the previous.
That explains the content of the test train data for each fold. </p>
<p>Yet your question implies a general misunderstanding about test-train split and crossvalidation: </p>
<p>You do not want to aggregate or unsilo the splits. </p>
<p>The idea of TimeSeriesSplit is based on <a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation" rel="nofollow noreferrer">crossvalidation</a> where you seperate a dataset into multiple subsets for e.g. hyperparameter selection.</p>
<p>Therefore you would train a model with a given set of hyperparameters s1 on each of the 5 splits seperately - training the model only on the train data, evaluating on test. </p>
<p>Then finally you will e.g. average the test metric and choose the model with the best average performance. </p>
<p>This data is only used for model selection - a final hold-out set should be available for evaluation.</p>
<p><a href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="nofollow noreferrer">This</a> could help you</p>
</div>
<span class="comment-copy">Thanks, this helps.  To confirm my understanding - when n_splits=N, I would train and test my model N times, and "average" each split performance (calcualted on that split's test set) to get a more representative idea of how the model is performing?</span>
<span class="comment-copy">And a quick follow-up question - once hyper parameters are tuned/selected, I'm assuming you do a final train on your model using the entire dataset as the training set, without needing a split to test/validate on?</span>
<span class="comment-copy">(1) Yes, (2) for the final evaluation you train on all development data that you used before and evaluate on another part of the dataset, that you have never used before.</span>
<span class="comment-copy">Thanks for your help - I have a better general understanding of train/test splitting and the final evaluation.</span>
