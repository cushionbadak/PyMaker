<div class="post-text" itemprop="text">
<p>I have a very large (100 GB+) distance matrix containing origin, destination, and distance columns. The structure is n<sup>2</sup>x3 like so:</p>
<pre><code>origin destination distance
1      2           0.5
2      3           0.7
3      4           0.8
4      5           0.5
1      3           1.1
</code></pre>
<p>I need to transform this matrix into an nxn matrix with the following form:</p>
<pre><code>destination 2    3    4    5
origin
1           0.5  1.1  NA   NA
2           NA   0.7  NA   NA
3           NA   NA   0.8  NA
4           NA   NA   NA   0.5
</code></pre>
<p>The problem here is the size of the matrix. It is basically impossible to read the entire 100 GB matrix into memory to pivot it, so I've been looking for ways to chunk and parallelize this process. Using python, I've figured something like this might work:</p>
<pre><code>chunksize = 10 ** 7
dtypes = {"origin":np.int, "destination":np.int, "agg_cost":np.float32}
col_names = ["origin", "destination", "distance"]

def get_chunk(chunk):
    return chunk.pivot(index='origin', columns='destination', values='agg_cost')

results = pool.map(get_chunk, pd.read_csv("matrix.csv", usecols=col_names, dtype=dtypes, chunksize=chunksize))

pd.concat(results).to_csv("finished_matrix.csv")
</code></pre>
<p>But this still requires reading a huge amount into memory. Additionally, since the chunksize doesn't take into account where sections of repeating origin IDs start and stop, there are repeating row indices in the final concatenated result.</p>
<p>Is there a way to effectively parallelize this operation such that it can be run with a normal (16 GB) amount of memory?</p>
</div>
<div class="post-text" itemprop="text">
<p>Since the input file is too large for memory, the transformed output will also be too large. So I'm assuming the goal is to produce a new output file, not to figure out a way to hold all of the information in memory at one time (the latter question might involve sparse matrices or some other technique).</p>
<p>For example, suppose we start with this data.</p>
<pre><code>1   2   0.5
3   4   0.8
5   6   2.7
2   3   0.7
1   3   1.1
3   6   3.1
4   5   0.5
1   6   4.6
</code></pre>
<p>First split the input file apart into a bunch of intermediate input files, one per ORIGIN. In our example, we end up with 5 files.</p>
<pre><code>1   2   0.5
1   3   1.1
1   6   4.6

2   3   0.7

3   4   0.8
3   6   3.1

4   5   0.5

5   6   2.7
</code></pre>
<p>Then use multiple processes to transform the intermediate input files into intermediate output files, each having the new matrix structure. Here are the resulting files based on the example.</p>
<pre><code>1   .   0.5   1.1   .     .     4.6

2   .   .     0.7   .     .     .

3   .   .     .     0.8   .     3.1

4   .   .     .     .     0.5   .

5   .   .     .     .     .     2.7
</code></pre>
<p>Then concatenate the intermediate output files to produce the final output.</p>
<p>The general strategy described above can probably be optimized for speed in various ways by skipping some of the intermediate files. For example, you could probably avoid having a bunch of intermediate files by doing the following: (A) create a single intermediate input file, merge-sorted by ORIGIN; (B) while doing that also keep track of the file-seek (START, END) locations for each ORIGIN; then (C) use multiple processes to produce the final output, based on the merge-sorted file and the seek metadata. That approach <em>might</em> be speedier (it also might not), but it requires some more bookkeeping. My first instinct would be to start simple and evolve from there.</p>
</div>
<div class="post-text" itemprop="text">
<p>Based on everyone's suggestions I wrote the following script. I fixed my input matrix so that it has origins grouped together in sequence. The script runs very quickly. It processes a 50 GB matrix in about 2 minutes using 48 cores on a server.</p>
<pre><code>import csv
import itertools
import os
import operator
import shutil

import pandas as pd
import multiprocessing as mp

dir_path = "temp/"
dtypes = {0:str, 1:str, 2:np.float64}
col_names = ["origin", "destination", "distance"]

os.makedirs(os.path.dirname(dir_path), exist_ok=True)

for key, rows in itertools.groupby(csv.reader(open("temp.csv")), operator.itemgetter(0)):
    with open(dir_path + "%s.csv" % key, "w") as output:
        for row in rows:
            output.write(",".join(row[0:3]) + "\n")

if os.path.isfile(dir_path + "origin.csv"):
    os.remove(dir_path + "origin.csv")
files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if \
             os.path.isfile(os.path.join(dir_path, f)) and f != "origin.csv"]

destinations = pd.read_csv("temp.csv", usecols=["destination"], dtype=dtypes, squeeze=True).unique()

def convert_row(file):
    row = pd.read_csv(file, dtype=dtypes, names=col_names) \
    .pivot(index="origin", columns="destination", values="distance") \
    .reindex(columns=destinations) \
    .to_csv(file)

pool = mp.Pool(mp.cpu_count())
results = pool.map(convert_row, files)

with open('output.csv', 'wb') as outfile:
    for i, file in enumerate(files):
        with open(file, 'rb') as infile:
            if i != 0:
                infile.readline()  
            shutil.copyfileobj(infile, outfile)
</code></pre>
</div>
<span class="comment-copy">Is there any specific ordering of the origins / destinations in the original csv? One approach would be to construct the new distance matrix one row at a time by reading the old csv (in chunks as large as you can manage), selecting only those distances corresponding to origin <code>i</code> (for <code>i</code> in <code>1, 2, ...</code>), and then appending to your csv the distance matrix row for origin <code>i</code>. This should be simple to implement and not take too much memory, but you'd have to make <code>n</code> passes over the old csv.</span>
<span class="comment-copy">You could improve this a bit by adding <code>k</code> rows at a time instead of just one, where <code>k</code> is as large as your memory permits.</span>
<span class="comment-copy">@Nathan Unfortunately the rows aren't in any specific order. An additional complication here is that it's actually not a nxn matrix, ie. there are more destination than origins. So I could do it one row at a time but the rows would be different lengths and I'd need to look up the rows for each origin.</span>
<span class="comment-copy">You can easily find all origins / destinations by scanning through once, though. Then when you construct the rows of the distance matrix, you'll know which origins are missing to insert <code>NaN</code>s for them so that all of your rows are of the appropriate length.</span>
<span class="comment-copy">It is very inefficient memory-wise to represent your data like that, many places will have <code>nans</code> (second matrix representation) or many origins will be repeated (first and original case). Why do you want to represent it as matrix? What operations are you going to perform on it? Have you tried sparse matrices representation using <code>scipy</code>?</span>
