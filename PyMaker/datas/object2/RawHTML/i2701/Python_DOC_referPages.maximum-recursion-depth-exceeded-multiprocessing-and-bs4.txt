<div class="post-text" itemprop="text">
<p>I'm trying to make a parser use beautifulSoup and multiprocessing. I have an error: </p>
<blockquote>
<p>RecursionError: maximum recursion depth exceeded</p>
</blockquote>
<p>My code is:</p>
<pre><code>import bs4, requests, time
from multiprocessing.pool import Pool

html = requests.get('https://www.avito.ru/moskva/avtomobili/bmw/x6?sgtd=5&amp;radius=0')
soup = bs4.BeautifulSoup(html.text, "html.parser")

divList = soup.find_all("div", {'class': 'item_table-header'})


def new_check():
    with Pool() as pool:
        pool.map(get_info, divList)

def get_info(each):
   pass

if __name__ == '__main__':
    new_check()
</code></pre>
<p>Why I get this error and how I can fix it?</p>
<p><strong>UPDATE:</strong>
All text of error is</p>
<pre><code>Traceback (most recent call last):
  File "C:/Users/eugen/PycharmProjects/avito/main.py", line 73, in &lt;module&gt; new_check()
  File "C:/Users/eugen/PycharmProjects/avito/main.py", line 67, in new_check
    pool.map(get_info, divList)
  File "C:\Users\eugen\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 266, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "C:\Users\eugen\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 644, in get
    raise self._value
  File "C:\Users\eugen\AppData\Local\Programs\Python\Python36\lib\multiprocessing\pool.py", line 424, in _handle_tasks
    put(task)
  File "C:\Users\eugen\AppData\Local\Programs\Python\Python36\lib\multiprocessing\connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "C:\Users\eugen\AppData\Local\Programs\Python\Python36\lib\multiprocessing\reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
RecursionError: maximum recursion depth exceeded
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>When you use <code>multiprocessing</code>, everything you pass to a worker has to be <a href="https://docs.python.org/3/library/pickle.html" rel="noreferrer">pickled</a>.</p>
<p>Unfortunately, many <code>BeautifulSoup</code> trees can't be pickled.</p>
<hr/>
<p>There are a few different reasons for this. Some of them are bugs that have since been fixed, so you <em>could</em> try making sure you have the latest bs4 version, and some are specific to different parsers or tree builders… but there's a good chance nothing like this will help.</p>
<p>But the fundamental problem is that many elements in the tree contain references to the rest of the tree.</p>
<p>Occasionally, this leads to an actual infinite loop, because the circular references are too indirect for its circular reference detection. But that's usually a bug that gets fixed.</p>
<p>But, even more importantly, even when the loop isn't <em>infinite</em>, it can still drag in more than 1000 elements from all over the rest of the tree, and that's already enough to cause a <code>RecursionError</code>.</p>
<p>And I think the latter is what's happening here. If I take your code and try to pickle <code>divList[0]</code>, it fails. (If I bump the recursion limit way up and count the frames, it needs a depth of 23080, which is way, way past the default of 1000.) But if I take that exact same <code>div</code> and parse it separately, it succeeds with no problem.</p>
<hr/>
<p>So, one possibility is to just do <code>sys.setrecursionlimit(25000)</code>. That will solve the problem for this exact page, but a slightly different page might need even more than that. (Plus, it's usually not a great idea to set the recursion limit that high—not so much because of the wasted memory, but because it means actual infinite recursion takes 25x as long, and 25x as much wasted resources, to detect.)</p>
<hr/>
<p>Another trick is to write code that "prunes the tree", eliminating any upward links from the div before/as you pickle it. This is a great solution, except that it might be a lot of work, and requires diving into the internals of how BeautifulSoup works, which I doubt you want to do.</p>
<hr/>
<p>The easiest workaround is a bit clunky, but… you can convert the soup to a string, pass that to the child, and have the child re-parse it:</p>
<pre><code>def new_check():
    divTexts = [str(div) for div in divList]
    with Pool() as pool:
        pool.map(get_info, divTexts)

def get_info(each):
    div = BeautifulSoup(each, 'html.parser')

if __name__ == '__main__':
    new_check()
</code></pre>
<p>The performance cost for doing this is probably not going to matter; the bigger worry is that if you had imperfect HTML, converting to a string and re-parsing it might not be a perfect round trip. So, I'd suggest that you do some tests without multiprocessing first to make sure this doesn't affect the results.</p>
</div>
<span class="comment-copy">As a side note, you have a <code>__main__</code> guard, but you didn't put the first three lines of code inside it. This means that if you're on Windows or otherwise needed that guard, every child process in the pool is going to download and parse the file all over again, which is at the very least wasteful, even if it doesn't cause any other problems. Why not move that code into the guard, or even into the <code>new_check</code> function?</span>
<span class="comment-copy"><a href="http://idownvotedbecau.se/noexceptiondetails/" rel="nofollow noreferrer">idownvotedbecau.se/noexceptiondetails</a> , <a href="http://idownvotedbecau.se/nomcve/" rel="nofollow noreferrer">idownvotedbecau.se/nomcve</a></span>
<span class="comment-copy">@abarnert, yes, this code repriduce my problem. I added full text of my error.</span>
<span class="comment-copy">@abarnert, sorry, instead of <code>'link.com'</code> should be <code>'https://www.avito.ru/moskva/avtomobili/bmw/x6?sgtd=5&amp;radius=0'</code></span>
<span class="comment-copy">OK, now we're getting somewhere. If I download that page and try to pickle the soup, or just <code>divList[0]</code>, I get a RecursionError. In multiprocessing, when you use <code>Pool</code>, it has to pickle each value to send it to the child workers, so that's why your code isn't working. The question is what makes the soup unpicklable. I'll look into the details.</span>
<span class="comment-copy">I tried your code, but I have another error: <code>slice indices must be integers or None or have an __index__ method</code>. What does it mean?</span>
<span class="comment-copy">@SBrain Probably a different bug in some other part of your code. It means that you're trying to do something like <code>spam[eggs:cheese]</code> somewhere and <code>eggs</code> or <code>cheese</code> is a float or a string or something instead of an int.</span>
<span class="comment-copy">I think it's due to code into get_info(each): <code>name = each.find("span", {'itemprop': 'name'}).text</code> `</span>
<span class="comment-copy">@SBrain OK, that makes sense. If you copied <code>div = BeautifulSoup(each, 'html.parser')</code> exactly as I wrote it, then <code>div</code> is the soup tag you want, but <code>each</code> is just a string, so you can't call soup methods on it. (<code>str.find</code> will get confused by you trying to use a dictionary as a start value.) Just change that line to <code>each = BeautifulSoup(each, 'html.parser')</code> and it should fix that.</span>
<span class="comment-copy">Great thanks for advices, it works</span>
