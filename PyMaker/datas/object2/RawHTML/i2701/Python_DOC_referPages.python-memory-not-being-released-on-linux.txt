<div class="post-text" itemprop="text">
<p>I am trying to load a large json object into memory and then perform some operations with the data. However, I am noticing a large increase in RAM after the json file is read -<strong>EVEN AFTER the object is out of scope.</strong></p>
<p>Here is the code </p>
<pre><code>import json
import objgraph
import gc
from memory_profiler import profile
@profile
def open_stuff():
    with open("bigjson.json", 'r') as jsonfile:
        d= jsonfile.read()
        jsonobj = json.loads(d)
        objgraph.show_most_common_types()
        del jsonobj
        del d
    print ('d')
    gc.collect()

open_stuff()
</code></pre>
<p>I tried running this script in Windows with Python version 2.7.12 and Debian 9 with Python version 2.7.13, and I am seeing an issue with the Python in Linux.</p>
<p>In Windows, when I run the script, it uses up a lot of RAM while the json object is being read and in scope (as expected), but it is released after the operation is done (as expected).</p>
<pre><code>list                       3039184
dict                       413840
function                   2200
wrapper_descriptor         1199
builtin_function_or_method 819
method_descriptor          651
tuple                      617
weakref                    554
getset_descriptor          362
member_descriptor          250
d
Filename: testjson.py

Line #    Mem usage    Increment   Line Contents
================================================
     5     16.9 MiB     16.9 MiB   @profile
     6                             def open_stuff():
     7     16.9 MiB      0.0 MiB       with open("bigjson.json", 'r') as jsonfile:
     8    197.9 MiB    181.0 MiB           d= jsonfile.read()
     9   1393.4 MiB   1195.5 MiB           jsonobj = json.loads(d)
    10   1397.0 MiB      3.6 MiB           objgraph.show_most_common_types()
    11    402.8 MiB   -994.2 MiB           del jsonobj
    12    221.8 MiB   -181.0 MiB           del d
    13    221.8 MiB      0.0 MiB       print ('d')
    14     23.3 MiB   -198.5 MiB       gc.collect()
</code></pre>
<p>However in the LINUX environment, over 500MB of RAM is still used even though all references to the JSON object has been deleted. </p>
<pre><code>list                       3039186
dict                       413836
function                   2336
wrapper_descriptor         1193
builtin_function_or_method 765
method_descriptor          651
tuple                      514
weakref                    480
property                   273
member_descriptor          250
d
Filename: testjson.py

Line #    Mem usage    Increment   Line Contents
================================================
     5     14.2 MiB     14.2 MiB   @profile
     6                             def open_stuff():
     7     14.2 MiB      0.0 MiB       with open("bigjson.json", 'r') as jsonfile:
     8    195.1 MiB    181.0 MiB           d= jsonfile.read()
     9   1466.4 MiB   1271.3 MiB           jsonobj = json.loads(d)
    10   1466.8 MiB      0.4 MiB           objgraph.show_most_common_types()
    11    694.8 MiB   -772.1 MiB           del jsonobj
    12    513.8 MiB   -181.0 MiB           del d
    13    513.8 MiB      0.0 MiB       print ('d')
    14    513.0 MiB     -0.8 MiB       gc.collect()
</code></pre>
<p>The same script run in Debian 9 with Python 3.5.3 uses less RAM but leaks a proportionate amount of RAM.</p>
<pre><code>list                       3039266
dict                       414638
function                   3374
tuple                      1254
wrapper_descriptor         1076
weakref                    944
builtin_function_or_method 780
method_descriptor          780
getset_descriptor          477
type                       431
d
Filename: testjson.py

Line #    Mem usage    Increment   Line Contents
================================================
     5     17.2 MiB     17.2 MiB   @profile
     6                             def open_stuff():
     7     17.2 MiB      0.0 MiB       with open("bigjson.json", 'r') as jsonfile:
     8    198.3 MiB    181.1 MiB           d= jsonfile.read()
     9   1057.7 MiB    859.4 MiB           jsonobj = json.loads(d)
    10   1058.1 MiB      0.4 MiB           objgraph.show_most_common_types()
    11    537.5 MiB   -520.6 MiB           del jsonobj
    12    356.5 MiB   -181.0 MiB           del d
    13    356.5 MiB      0.0 MiB       print ('d')
    14    355.8 MiB     -0.8 MiB       gc.collect()
</code></pre>
<p>What is causing this issue?
Both versions of Python are running 64bit versions.</p>
<p><strong>EDIT - calling that function several times in a row leads to even stranger data, the json.loads function uses less RAM each time it's called, after the 3rd try the RAM usage stabilizes, but the earlier leaked RAM does not get released..</strong></p>
<pre><code>list                       3039189
dict                       413840
function                   2339
wrapper_descriptor         1193
builtin_function_or_method 765
method_descriptor          651
tuple                      517
weakref                    480
property                   273
member_descriptor          250
d
Filename: testjson.py

Line #    Mem usage    Increment   Line Contents
================================================
     5     14.5 MiB     14.5 MiB   @profile
     6                             def open_stuff():
     7     14.5 MiB      0.0 MiB       with open("bigjson.json", 'r') as jsonfile:
     8    195.4 MiB    180.9 MiB           d= jsonfile.read()
     9   1466.5 MiB   1271.1 MiB           jsonobj = json.loads(d)
    10   1466.9 MiB      0.4 MiB           objgraph.show_most_common_types()
    11    694.8 MiB   -772.1 MiB           del jsonobj
    12    513.9 MiB   -181.0 MiB           del d
    13    513.9 MiB      0.0 MiB       print ('d')
    14    513.1 MiB     -0.8 MiB       gc.collect()


list                       3039189
dict                       413842
function                   2339
wrapper_descriptor         1202
builtin_function_or_method 765
method_descriptor          651
tuple                      517
weakref                    482
property                   273
member_descriptor          253
d
Filename: testjson.py

Line #    Mem usage    Increment   Line Contents
================================================
     5    513.1 MiB    513.1 MiB   @profile
     6                             def open_stuff():
     7    513.1 MiB      0.0 MiB       with open("bigjson.json", 'r') as jsonfile:
     8    513.1 MiB      0.0 MiB           d= jsonfile.read()
     9   1466.8 MiB    953.7 MiB           jsonobj = json.loads(d)
    10   1493.3 MiB     26.6 MiB           objgraph.show_most_common_types()
    11    723.9 MiB   -769.4 MiB           del jsonobj
    12    723.9 MiB      0.0 MiB           del d
    13    723.9 MiB      0.0 MiB       print ('d')
    14    722.4 MiB     -1.5 MiB       gc.collect()


list                       3039189
dict                       413842
function                   2339
wrapper_descriptor         1202
builtin_function_or_method 765
method_descriptor          651
tuple                      517
weakref                    482
property                   273
member_descriptor          253
d
Filename: testjson.py

Line #    Mem usage    Increment   Line Contents
================================================
     5    722.4 MiB    722.4 MiB   @profile
     6                             def open_stuff():
     7    722.4 MiB      0.0 MiB       with open("bigjson.json", 'r') as jsonfile:
     8    722.4 MiB      0.0 MiB           d= jsonfile.read()
     9   1493.1 MiB    770.8 MiB           jsonobj = json.loads(d)
    10   1493.4 MiB      0.3 MiB           objgraph.show_most_common_types()
    11    724.4 MiB   -769.0 MiB           del jsonobj
    12    724.4 MiB      0.0 MiB           del d
    13    724.4 MiB      0.0 MiB       print ('d')
    14    722.9 MiB     -1.5 MiB       gc.collect()


Filename: testjson.py

Line #    Mem usage    Increment   Line Contents
================================================
    17     14.2 MiB     14.2 MiB   @profile
    18                             def wow():
    19    513.1 MiB    498.9 MiB       open_stuff()
    20    722.4 MiB    209.3 MiB       open_stuff()
    21    722.9 MiB      0.6 MiB       open_stuff()
</code></pre>
<p><strong>EDIT 2: Someone suggested this is a duplicate of <a href="https://stackoverflow.com/questions/29529135/why-does-my-programs-memory-not-release">Why does my program's memory not release?</a> , but the amount of memory in question is far from the "small pages" discussed in the other question.</strong></p>
</div>
<div class="post-text" itemprop="text">
<p>The linked duplicate likely hints at what your problem is, but let's go into a bit more detail.</p>
<p><strike>First, you should use <a href="https://docs.python.org/3/library/json.html#json.load" rel="nofollow noreferrer"><code>json.load</code></a> instead of loading the file entirely into memory and then doing <code>json.loads</code> on that:</strike></p>
<pre><code>with open('bigjson.json') as f:
    data = json.load(f)
</code></pre>
<p>This allows the decoder to read the file in at its own leisure and will most likely reduce memory usage. In your original version, you had to at least store the entire original file in memory before you could even start parsing the JSON. This allows the file to be streamed as the decoder needs it.</p></div>
<div class="post-text" itemprop="text">
<p>while python freed memory back to glibc, glibc will not release back to OS immediately every time, since the user may request memory later. you could call glibc's <a href="http://man7.org/linux/man-pages/man3/malloc_trim.3.html" rel="nofollow noreferrer"><code>malloc_trim(3)</code></a> to make an attempt to release memory back:</p>
<pre><code>import ctypes

def malloc_trim():
    ctypes.CDLL('libc.so.6').malloc_trim(0) 

@profile
def load():
    with open('big.json') as f:
        d = json.load(f)
    del d
    malloc_trim()
</code></pre>
<p>result:</p>
<pre><code>Line #    Mem usage    Increment   Line Contents
================================================
    27     11.6 MiB     11.6 MiB   @profile
    28                             def load():
    29     11.6 MiB      0.0 MiB       with open('big.json') as f:
    30    166.5 MiB    154.9 MiB           d = json.load(f)
    31     44.1 MiB   -122.4 MiB       del d
    32     12.7 MiB    -31.4 MiB       malloc_trim()
</code></pre>
</div>
<span class="comment-copy">Does memory use keep growing if you call <code>open_stuff</code> repeatedly?</span>
<span class="comment-copy">@melpomene yes, it looks like it increases by about 200MB per call after the initial 513MB is taken up by the first call</span>
<span class="comment-copy">Actually it looks like the first call bumps it to 513MB, the second to 722MB, and the third keeps it at 723MB. The json.loads call uses less RAM everytime it's called.</span>
<span class="comment-copy">Possible duplicate of <a href="https://stackoverflow.com/questions/29529135/why-does-my-programs-memory-not-release">Why does my program's memory not release?</a></span>
<span class="comment-copy">@Miguel the point of the context manager is that you don't have to close the resource manually after you are done. <a href="https://docs.python.org/2.5/whatsnew/pep-343.html" rel="nofollow noreferrer">docs.python.org/2.5/whatsnew/pep-343.html</a>   Nevertheless, I tried that as well but there was no difference.</span>
<span class="comment-copy">The JSON decoder is not a streaming decoder so your initial advice will help at all as you should have noticed if you had ran some benchmarks to support you claim. It  can easily be checked by looking for a few seconds at the source code: the only thing that <code>json.load()</code> does is call <code>json.loads(fp.read(),....)</code> (<a href="https://github.com/python/cpython/blob/master/Lib/json/__init__.py#L274" rel="nofollow noreferrer">repo</a>).</span>
<span class="comment-copy">I stand corrected, updated</span>
<span class="comment-copy">despite your answer is lengthy, but the infomation is very thin, not directly linked with the question...</span>
