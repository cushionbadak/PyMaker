<div class="post-text" itemprop="text">
<p>I know there are plenty of topics about finding indices of given keywords in strings, but my case is a bit different</p>
<p>I have 2 inputs, one is a string and another is a mapping list (or whatever you wanna call it)</p>
<pre><code>s = "I am awesome and I love you"
mapping_list = "1 1 2 3 1 2 3"
</code></pre>
<p>each word will always map onto a digit in the mapping list. Now I want to find all indices of a given number, say 1, when matching the string.</p>
<p>In the above case, it will return [0, 2, 17] (Thakns @rahlf23)</p>
<p>My current approach would be zipping each word with a digit by doing</p>
<pre><code>zip(mapping_list.split(' '), s.split(' '))
</code></pre>
<p>which gives me</p>
<pre><code>('1', 'I')
('1', 'am')
('2', 'awesome')
('3', 'and')
('1', 'I')
('2', 'love')
('3', 'you')
</code></pre>
<p>and then iterate through the list, find "1", use the word to generate a regex, and then search for indices and append it to a list or something. Rinse and repeat.</p>
<p>However this seems really inefficient especially if the <code>s</code> gets really long</p>
<p>I'm wondering if there's a better way to deal with it.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could <code>map</code> the words to their <code>len</code> and use <a href="https://docs.python.org/3/library/itertools.html#itertools.accumulate" rel="nofollow noreferrer"><code>itertools.accumulate</code></a>, although you have to add <code>1</code> to each length (for the spaces) and add an initial <code>0</code> for the start of the first word.</p>
<pre><code>&gt;&gt;&gt; words = "I am awesome and I love you".split()
&gt;&gt;&gt; mapping = list(map(int, "1 1 2 3 1 2 3".split()))
&gt;&gt;&gt; start_indices = list(itertools.accumulate([0] + [len(w)+1 for w in words]))
&gt;&gt;&gt; start_indices
[0, 2, 5, 13, 17, 19, 24, 28]
</code></pre>
<p>The last element is not used. Then, <code>zip</code> and iterate the pairs and collect them in a dictionary.</p>
<pre><code>&gt;&gt;&gt; d = collections.defaultdict(list)
&gt;&gt;&gt; for x, y in zip(mapping, start_indices):
...     d[x].append(y)
&gt;&gt;&gt; dict(d)
&gt;&gt;&gt; {1: [0, 2, 17], 2: [5, 19], 3: [13, 24]}
</code></pre>
<p>Alternatively, you could also use a <a href="https://docs.python.org/3/library/re.html" rel="nofollow noreferrer">regular expression</a> like <code>\b\w</code> (word-boundary followed by word-character) to find each position a word starts, then proceed as above.</p>
<pre><code>&gt;&gt;&gt; s = "I am awesome and I love you"
&gt;&gt;&gt; [m.start() for m in re.finditer(r"\b\w", s)]
[0, 2, 5, 13, 17, 19, 24]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code># Find the indices of all the word starts
word_starts = [0] + [m.start()+1 for m in re.finditer(' ', s)]

# Break the mapping list into an actual list
mapping = mapping_list.split(' ')

# Find the indices in the mapping list we care about
word_indices = [i for i, e in enumerate(mapping) if e == '1']

# Map those indices onto the word start indices
word_starts_at_indices = [word_starts[i] for i in word_indices]
# Or you can do the last line the fancy way:
# word_starts_at_indices = operator.itemgetter(*word_indices)(word_starts)
</code></pre>
</div>
<span class="comment-copy">Are you doing this many times? Or are you just concerned with <code>s</code> getting long? This is probably close to optimal, the only improvement I see is turning <code>s.split</code> into a generator for constant memory. tbh though, I kind of doubt this is a bottleneck.</span>
<span class="comment-copy">I believe it should return <code>[0,2,17]</code>.</span>
<span class="comment-copy">Or use a default dict</span>
<span class="comment-copy">@JChao Did you try to time your solution with a really long string?</span>
<span class="comment-copy">let's say there will be 1k of these <code>s</code>, and each <code>s</code> will be 50 to 100 chara long, after <code>split</code>, a typical length of the list would be 7 to 10</span>
<span class="comment-copy">Thanks! these approaches look much cleaner than mine. I'm gonna do some quick benchmark though.</span>
