<div class="post-text" itemprop="text">
<p>I try to build a list of urls out of an online-forum. It is necessary to use BeautifulSoup in my case. The goal is a list of URLs containing every page of a thread, e.g.</p>
<pre><code>[http://forum.pcgames.de/stellt-euch-vor/9331721-update-im-out-bitches.html, 
http://forum.pcgames.de/stellt-euch-vor/9331721-update-im-out-bitches-2.html, 
http://forum.pcgames.de/stellt-euch-vor/9331721-update-im-out-bitches-3.html]
</code></pre>
<p>What works is this:</p>
<pre><code>#import modules
import requests
from bs4 import BeautifulSoup
#define main-url
url = 'http://forum.pcgames.de/stellt-euch-vor/9331721-update-im-out-bitches.html'
#create a list of urls
urls=[url]
#load url
page = requests.get(url)
#parse it using BeautifulSoup
soup = BeautifulSoup(page.text, 'html.parser')
#search for the url of the next page
nextpage = soup.find("a", ["pag next"]).get('href')
#append the urls of the next page to the list of urls
urls.append(nextpage)
print(urls)
</code></pre>
<p>When I try to build a if loop for the next pages as follows it will not work. Why?</p>
<pre><code>for url in urls:
    urls.append(soup.find("a", ["pag next"]).get('href'))
</code></pre>
<p>(<code>"a", ["pag next"]).get('href')</code> identifies the url for the next page)</p>
<p>It's not an option to use the pagination of the url because there will be many other threads to crawl.
I'm using Jupyter Notebook server 5.7.4 on a macbook pro
Python 3.7.1 
IPython 7.2.0</p>
<p>I know about the existence of <a href="https://stackoverflow.com/questions/42557071/web-scraping-every-forum-post-python-beautifulsoup">this</a> post. For my beginners knowledge the code is written too complicated but maybe your experience allows you to apply it on my use case.</p>
</div>
<div class="post-text" itemprop="text">
<p>The url pattern for pagination is always consistent with this site, therefore you don't need to make requests to grab page urls. Instead you can parse the the text in the button that says "Page 1 of 10" and construct the page urls after knowing the final page number. </p>
<pre><code>import re

import requests
from bs4 import BeautifulSoup

thread_url = "http://forum.pcgames.de/stellt-euch-vor/9331721-update-im-out-bitches.html"
r = requests.get(thread_url)
soup = BeautifulSoup(r.content, 'lxml')
pattern = re.compile(r'Seite\s\d+\svon\s(\d+)', re.I)
pages = soup.find('a', text=pattern).text.strip()
pages = int(pattern.match(pages).group(1))
page_urls = [f"{thread_url[:-5]}-{p}.html" for p in range(1, pages + 1)]
for url in page_urls:
    print(url)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You are looping through the urls and adding it to itself so the size of the list continues increasing indefinitely. </p>
<p>You are adding each url from urls to url - do you see the problem? urls continues to grow... it also already has each url that you are iterating through inside of it. Do you mean to call a function that executes the prior code on each next url and then adds it to the list?</p>
</div>
<span class="comment-copy">Your pattern matching works perfectly on my Jupyter Notebook (Python 3.7.1, IPython 7.2.0). Have many thanks. I downloaded it the .py file and tried to run it on my Synology NAS using Python 3.5.1 on the command line via SSH. It says <b><code>SyntaxError: invalid syntax</code></b> and the second quotation mark in <code>[f"{thread_url[:-5]}-{p}.html"</code> is highlighted. Do you see why? I tried using apostrophes instead of quotation marks which caused the same problem. Note that I was not able to install module 're' using <code>sudo python3.5 -m pip install re</code>, because there was no matching distribution found.</span>
<span class="comment-copy">The syntax is using the new python <code>fstrings</code>,  only available in 3.6+. If you're using an older version of python, you'll need to use string formatting appropriate to that specific version.</span>
