<div class="post-text" itemprop="text">
<p>I'm creating a <code>tf.data.Dataset</code> inside a for loop and I noticed that the memory was not freed as one would expect after each iteration.</p>
<p>Is there a way to request from TensorFlow to free the memory?</p>
<p>I tried using <code>tf.reset_default_graph()</code>, I tried calling <code>del</code> on the relevant python objects but this does not work.</p>
<p>The only thing that seems to work is <code>gc.collect()</code>. Unfortunately, <code>gc.collect</code> does not work on some more complex examples.</p>
<p>Fully reproducible code:</p>
<pre><code>import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import psutil
%matplotlib inline

memory_used = []
for i in range(500):
    data = tf.data.Dataset.from_tensor_slices(
                    np.random.uniform(size=(10, 500, 500)))\
                    .prefetch(64)\
                    .repeat(-1)\
                    .batch(3)
    data_it = data.make_initializable_iterator()
    next_element = data_it.get_next()

    with tf.Session() as sess:
        sess.run(data_it.initializer)
        sess.run(next_element)
    memory_used.append(psutil.virtual_memory().used / 2 ** 30)
    tf.reset_default_graph()

plt.plot(memory_used)
plt.title('Evolution of memory')
plt.xlabel('iteration')
plt.ylabel('memory used (GB)')
</code></pre>
<p><a href="https://i.stack.imgur.com/gkXE7.png" rel="nofollow noreferrer"><img alt="Evolution of memory usage" src="https://i.stack.imgur.com/gkXE7.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>You are creating new python object (dataset) ever iteration of a loop and looks like garbage collector is not being invoked. Add impplicit garbage collection call and the memory usage should be fine. </p>
<p>Other than that, as mentioned in other answer, keep building data obect and session outside of the loop.</p>
<pre><code>import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import psutil
import gc

%matplotlib inline

memory_used = []
for i in range(100):
    data = tf.data.Dataset.from_tensor_slices(
                    np.random.uniform(size=(10, 500, 500)))\
                    .prefetch(64)\
                    .repeat(-1)\
                    .batch(3)
    data_it = data.make_initializable_iterator()
    next_element = data_it.get_next()

    with tf.Session() as sess:
        sess.run(data_it.initializer)
        sess.run(next_element)
    memory_used.append(psutil.virtual_memory().used / 2 ** 30)
    tf.reset_default_graph()
    gc.collect()

plt.plot(memory_used)
plt.title('Evolution of memory')
plt.xlabel('iteration')
plt.ylabel('memory used (GB)')
</code></pre>
<p><a href="https://i.stack.imgur.com/ecwDF.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/ecwDF.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>Dataset API handles iteration via built-in iterator, at least while eager mode is off or TF version is not 2.0. 
So, there's simply no need to create dataset object from numpy array inside for loop, as it writes values in the graph as <code>tf.constant</code>. This is not the case with data = <code>tf.data.TFRecordDataset()</code>, so if you transform your data to tfrecords format and run it inside for loop it won't leak memory.</p>
<pre><code>for i in range(500):
    data = tf.data.TFRecordDataset('file.tfrecords')\
        .prefetch(64)\
        .repeat(-1)\
        .batch(1)
    data_it = data.make_initializable_iterator()
    next_element = data_it.get_next()
    with tf.Session() as sess:
        sess.run(data_it.initializer)
        sess.run(next_element)
    memory_used.append(psutil.virtual_memory().used / 2 ** 30)
    tf.reset_default_graph()
</code></pre>
<p>But as I said, there's no need to create dataset inside a loop.</p>
<pre><code>data = tf.data.Dataset.from_tensor_slices(
                    np.random.uniform(size=(10, 500, 500)))\
                    .prefetch(64)\
                    .repeat(-1)\
                    .batch(3)
data_it = data.make_initializable_iterator()
next_element = data_it.get_next()

for i in range(500):
    with tf.Session() as sess:
        ...
</code></pre>
</div>
<span class="comment-copy">Sorry, I've just noticed that you did write that you tried gc.collect(). But what would be the more complex usecase?</span>
<span class="comment-copy">Yes, I tried <code>gc.collect()</code>. My more complex use case involves many <code>*.tfrecord</code> files with a quite complex data pipeline. In this more complex use case, <code>gc.collect()</code> does not work. Thus my question: how to explicitly free the memory allocated by TensorFlow.</span>
<span class="comment-copy">I read somewhere about it and in general: with a GPU memory it is not possible to free it, with Python it has to be <code>gc.collect()</code> but then you have numerous typical issues with Python that does not want to free the memory - there is a big threat about it on stackoverflow.</span>
<span class="comment-copy">Here, Iâ€™m referring to the general RAM, not the memory of the GPU.</span>
<span class="comment-copy">I think you need to describe your usecase more precisely. Like do you need to run through all those different TFRecord files at once or not?</span>
<span class="comment-copy">I'm familiar with the <code>tf.data</code> API, my question is on a different point, i.e. explicitly freeing memory allocated by TensorFlow with <code>tf.data</code>.</span>
<span class="comment-copy">Take a loop at this, it specifically points out why it's not a good idea to try to free memory <a href="https://github.com/tensorflow/tensorflow/issues/14181" rel="nofollow noreferrer">github.com/tensorflow/tensorflow/issues/14181</a></span>
