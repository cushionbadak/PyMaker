<div class="post-text" itemprop="text">
<p>I have recently been doing a project in which one of the aims is to use as little memory as possible to store a series of files using Python 3. Almost all of the files take up very little space, apart from one list of integers that is roughly <code>333,000</code> integers long and has integers up to about <code>8000</code> in size. </p>
<p>I'm currently using <code>pickle</code> to store the list, which takes up around <code>7mb</code>, but I feel like there must be a more memory efficient way to do this.</p>
<p>I have tried storing it as a text file and <code>csv</code>, bur both of these used in excess of <code>10mb</code> of space.</p>
</div>
<div class="post-text" itemprop="text">
<p>One <code>stdlib</code> solution you could use is arrays from <a href="https://docs.python.org/3/library/array.html" rel="nofollow"><code>array</code></a>, from the docs:</p>
<blockquote>
<p>This module defines an object type which can compactly represent an array of basic values: characters, integers, floating point numbers. Arrays are sequence types and behave very much like lists, except that the type of objects stored in them is constrained. </p>
</blockquote>
<p>This generally sheds a bit of memory of large lists, for example, with a 10 million element a list, the array trims up <code>11mb</code>:</p>
<pre><code>import pickle    
from array import array

l = [i for i in range(10000000)]
a = array('i', l)

# tofile can also be used.
with open('arrfile', 'wb') as f:  
    pickle.dump(a, f)

with open('lstfile', 'wb') as f:
    pickle.dump(l, f)
</code></pre>
<p>Sizes:    </p>
<pre><code>!du -sh ./*
39M     arrfile
48M     lstfile
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here is a small demo, which uses Pandas module:</p>
<pre><code>import numpy as np
import pandas as pd
import feather

# let's generate an array of 1M int64 elements...
df = pd.DataFrame({'num_col':np.random.randint(0, 10**9, 10**6)}, dtype=np.int64)
df.info()

%timeit -n 1 -r 1 df.to_pickle('d:/temp/a.pickle')

%timeit -n 1 -r 1 df.to_hdf('d:/temp/a.h5', 'df_key', complib='blosc', complevel=5)
%timeit -n 1 -r 1 df.to_hdf('d:/temp/a_blosc.h5', 'df_key', complib='blosc', complevel=5)
%timeit -n 1 -r 1 df.to_hdf('d:/temp/a_zlib.h5', 'df_key', complib='zlib', complevel=5)
%timeit -n 1 -r 1 df.to_hdf('d:/temp/a_bzip2.h5', 'df_key', complib='bzip2', complevel=5)
%timeit -n 1 -r 1 df.to_hdf('d:/temp/a_lzo.h5', 'df_key', complib='lzo', complevel=5)

%timeit -n 1 -r 1 feather.write_dataframe(df, 'd:/temp/a.feather')
</code></pre>
<p><strong>DataFrame info:</strong></p>
<pre><code>In [56]: df.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 1 columns):
num_col    1000000 non-null int64
dtypes: int64(1)
memory usage: 7.6 MB
</code></pre>
<p><strong>Results (speed):</strong></p>
<pre><code>In [49]: %timeit -n 1 -r 1 df.to_pickle('d:/temp/a.pickle')
1 loop, best of 1: 16.2 ms per loop

In [50]: %timeit -n 1 -r 1 df.to_hdf('d:/temp/a.h5', 'df_key', complib='blosc', complevel=5)
1 loop, best of 1: 39.7 ms per loop

In [51]: %timeit -n 1 -r 1 df.to_hdf('d:/temp/a_blosc.h5', 'df_key', complib='blosc', complevel=5)
1 loop, best of 1: 40.6 ms per loop

In [52]: %timeit -n 1 -r 1 df.to_hdf('d:/temp/a_zlib.h5', 'df_key', complib='zlib', complevel=5)
1 loop, best of 1: 213 ms per loop

In [53]: %timeit -n 1 -r 1 df.to_hdf('d:/temp/a_bzip2.h5', 'df_key', complib='bzip2', complevel=5)
1 loop, best of 1: 1.09 s per loop

In [54]: %timeit -n 1 -r 1 df.to_hdf('d:/temp/a_lzo.h5', 'df_key', complib='lzo', complevel=5)
1 loop, best of 1: 32.1 ms per loop

In [55]: %timeit -n 1 -r 1 feather.write_dataframe(df, 'd:/temp/a.feather')
1 loop, best of 1: 3.49 ms per loop
</code></pre>
<p><strong>Results (size):</strong></p>
<pre><code>{ temp }  » ls -lh a*                                                                                         /d/temp
-rw-r--r-- 1 Max None 7.7M Sep 20 23:15 a.feather
-rw-r--r-- 1 Max None 4.1M Sep 20 23:15 a.h5
-rw-r--r-- 1 Max None 7.7M Sep 20 23:15 a.pickle
-rw-r--r-- 1 Max None 4.1M Sep 20 23:15 a_blosc.h5
-rw-r--r-- 1 Max None 4.0M Sep 20 23:15 a_bzip2.h5
-rw-r--r-- 1 Max None 4.1M Sep 20 23:15 a_lzo.h5
-rw-r--r-- 1 Max None 3.9M Sep 20 23:15 a_zlib.h5
</code></pre>
<p><strong>Conclusion:</strong> pay attention at HDF5 (+ <code>blosc</code> or <code>lzo</code> compression) if you need both speed and a reasonable size or at <a href="https://blog.rstudio.org/2016/03/29/feather/" rel="nofollow">Feather-format</a> if you only care of speed - it's 4 times faster compared to Pickle!</p>
</div>
<div class="post-text" itemprop="text">
<p>I like <a href="https://stackoverflow.com/a/39603768/1392132">Jim's suggestion</a> of using the <a href="https://docs.python.org/3/library/array.html" rel="nofollow noreferrer"><code>array</code></a> module. If your numeric values are small enough to fit into the machine's native <code>int</code> type, then this is a fine solution. (I'd prefer to serialize the array with the <code>array.tofile</code> method instead of using <code>pickle</code>, though.) If an <code>int</code> is 32 bits, then this uses 4 bytes per number.</p>
<p>I would like to question how you did your text file, though. If I create a file with 333 000 integers in the range [0, 8 000] with one number per line,</p>
<pre><code>import random

with open('numbers.txt', 'w') as ostr:
    for i in range(333000):
        r = random.randint(0, 8000)
        print(r, file=ostr)
</code></pre>
<p>it comes out to a size of only 1.6 MiB which isn't all that bad compared to the 1.3 MiB that the binary representation would use. And if you do happen to have a value outside the range of the native <code>int</code> type one day, the text file will handle it happily without overflow.</p>
<p>Furthermore, if I <em>compress</em> the file using gzip, the file size shrinks down to 686 KiB. That's better than gzipping the binary data! When using bzip2, the file size is only 562 KiB. Python's standard library has support for both <a href="https://docs.python.org/3/library/gzip.html" rel="nofollow noreferrer"><code>gzip</code></a> and <a href="https://docs.python.org/3/library/bz2.html" rel="nofollow noreferrer"><code>bz2</code></a> so you might want to give the plain-text format plus compression another try.</p>
</div>
<span class="comment-copy">you may want to read about Pandas and HDF5 format (+ <code>blosc</code> compression)</span>
<span class="comment-copy">how many bytes do you need for your largest integer?</span>
<span class="comment-copy"><code>integers up to about 8000 in size</code> what do you mean?</span>
<span class="comment-copy">do you mean that <code>max(of_all_integers) &lt;= 8000</code>?</span>
<span class="comment-copy">How large are the pickle/text/csv files after you compress them (with zip, gz, lzma or bzip2, all of which are supported by Python's standard library)?</span>
<span class="comment-copy">This might be a decent solution for the OP's problem. It is important to know, however, that the array will store values using the platform's native C-<code>int</code> type – not Python's arbitrary precision integers.</span>
<span class="comment-copy">This crashes for <code>array('i', [10**7999])</code>.</span>
<span class="comment-copy">Of course it does @StefanPochmann, anything over <code>array('i', [2**31-1])</code> isn't allowed with <code>'i'</code> :-). I'll make sure to note how large int's are subject to the limitations of the underlying <code>C</code> type if OP makes it clear that <code>size</code> is <code>8000</code> actually means <code>8000</code> digits (which I'm seriously doubting).</span>
<span class="comment-copy">Like I said elsewhere: Divide the over 10mb of their text file by 333000. That's over 30 bytes per number. How stupid would they have to be to take over 30 bytes per four-digit number in a text file? Looks pretty certain they mean up to 8000 digits (or maybe bits).</span>
<span class="comment-copy">Your numbers only have up to 4 digits, not up to 8000.</span>
<span class="comment-copy">@StefanPochmann That's how I've interpreted the question; 8000 has 4 digits, too.</span>
<span class="comment-copy">Come on. Divide the over 10mb of their text file by 333000. That's over 30 bytes per number. How stupid would they have to be to take over 30 bytes per four-digit number in a text file?</span>
<span class="comment-copy">You say <i>"That's better than gzipping the binary data"</i> but I don't see the size of that anywhere. What is it?</span>
<span class="comment-copy">@StefanPochmann It was 721 KiB for my test.</span>
