<div class="post-text" itemprop="text">
<p>I would like to know is there a method that can read multiple lines from a file batch by batch. For example:</p>
<pre><code>with open(filename, 'rb') as f:
    for n_lines in f:
        process(n_lines)
</code></pre>
<p>In this function, what I would like to do is: for every iteration, next n lines will be read from the file, batch by batch. </p>
<p>Because one single file is too big. What I want to do is to read it part by part.</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow"><code>itertools.islice</code></a> and two arg <code>iter</code> can be used to accomplish this, but it's a little funny:</p>
<pre><code>from itertools import islice

n = 5  # Or whatever chunk size you want
with open(filename, 'rb') as f:
    for n_lines in iter(lambda: tuple(islice(f, n)), ()):
        process(n_lines)
</code></pre>
<p>This will keep <code>islice</code>ing off <code>n</code> lines at a time (using <code>tuple</code> to actually force the whole chunk to be read in) until the <code>f</code> is exhausted, at which point it will stop. The final chunk will be less than <code>n</code> lines if the number of lines in the file isn't an even multiple of <code>n</code>. If you want all the lines to be a single string, change the <code>for</code> loop to be:</p>
<pre><code>    # The b prefixes are ignored on 2.7, and necessary on 3.x since you opened
    # the file in binary mode
    for n_lines in iter(lambda: b''.join(islice(f, n)), b''):
</code></pre>
<p>Another approach is to use <code>izip_longest</code> for the purpose, which avoids <code>lambda</code> functions:</p>
<pre><code>from future_builtins import map  # Only on Py2
from itertools import izip_longest  # zip_longest on Py3

    # gets tuples possibly padded with empty strings at end of file
    for n_lines in izip_longest(*[f]*n, fillvalue=b''):

    # Or to combine into a single string:
    for n_lines in map(b''.join, izip_longest(*[f]*n, fillvalue=b'')):
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can actually just iterate over lines in a file (see <a href="https://docs.python.org/2/library/stdtypes.html#file.next" rel="nofollow noreferrer">file.next</a> docs - this also works on Python 3) like</p>
<pre><code>with open(filename) as f:
    for line in f:
        something(line)
</code></pre>
<p>so your code can be rewritten to</p>
<pre><code>n=5 # your batch size
with open(filename) as f:
    batch=[]
    for line in f:
        batch.append(line)
        if len(batch)==n:
            process(batch)
            batch=[]
process(batch) # this batch might be smaller or even empty
</code></pre>
<p>but normally just processing line-by-line is more convenient (first example)</p>
<p>If you dont care about how many lines are read exactly for each batch but just that it is not too much memory then use <a href="https://docs.python.org/2/library/stdtypes.html#file.readlines" rel="nofollow noreferrer">file.readlines</a> with <code>sizehint</code> like</p>
<pre><code>size_hint=2&lt;&lt;24 # 16MB
with open(filename) as f:
    while f: # not sure if this check works
        process(f.readlines(size_hint))
</code></pre>
</div>
<span class="comment-copy">You could read all the lines with <code>readlines</code>, and then pass successive ten-line slices into <code>process</code>.</span>
<span class="comment-copy">No. Because the file is too big. What I want to do is to read it batch by batch.</span>
<span class="comment-copy">try <code>f.read(byte_size)</code> where byte_size is the number of byte chars you wanna read, if that's what you want.</span>
<span class="comment-copy">I want to do it lines-by-lines, since I am not sure the size of each line and the size of each of a single line is not fixed. But I have to read them always with entire line not partial of it.</span>
<span class="comment-copy">I am wondering, in the solution of using <code>islice</code>, are the n lines read at one time, or there are actually read one by one and grouped together as n-line chunk?</span>
<span class="comment-copy">@ChangLiu: In all cases, they're read one by one, but there is block buffering occurring, so odds are there are only 0-2 reads needed for any given block. There is no magical way to read <code>n</code> lines as a single read; heck, at the lower layers, there is no way to read <i>one</i> line as a single read, it's either buffering (fast, but overreading) or pulling a character at a time (no overread, but much slower).</span>
<span class="comment-copy">In this way, you are actually still reading a single line every time, what I want is reading n lines every time.</span>
<span class="comment-copy">There is no function for reading n lines directly - actually because of buffering (buffer size can be passed  to <code>open</code> as an arg) when you read a file line by line there normally is only one disk read for many lines.</span>
<span class="comment-copy">extended my answer to include <code>readlines</code> which actually is multiline-read</span>
