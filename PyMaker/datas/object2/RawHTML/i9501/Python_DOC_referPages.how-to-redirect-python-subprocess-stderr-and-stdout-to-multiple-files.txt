<div class="post-text" itemprop="text">
<div class="question-status question-originals-of-duplicate">
<p>This question already has an answer here:</p>
<ul>
<li>
<a dir="ltr" href="/questions/4984428/python-subprocess-get-childrens-output-to-file-and-terminal">Python subprocess get children's output to file and terminal?</a>
<span class="question-originals-answer-count">
                    1 answer
                </span>
</li>
</ul>
</div>
<p>I just want to redirect stderr and stdout to multiple files.
For example: 
stderr should redirected to file_1 and file_2.</p>
<p>I am using below to redirect output to single file.</p>
<pre><code>subprocess.Popen("my_commands",shell=True,stdout=log_file,stderr=err_file,executable="/bin/bash")
</code></pre>
<p>Above thing redirects <code>stdout</code> and <code>stderr</code> to a single file.<br/>
Can anybody tell the way to do same(redirect output to both files log_file and err_file e.g. <code>stdout</code> should redirect to both log_file and err_file and <code>stderr</code> should redirect to err_file and new_file)</p>
</div>
<div class="post-text" itemprop="text">
<p>You can create your own file-like class that writes to multiple file handles. Here's a simple example, with a test that redirects <code>sys.stdout</code> and <code>sys.stderr</code>.</p>
<pre><code>import sys

class MultiOut(object):
    def __init__(self, *args):
        self.handles = args

    def write(self, s):
        for f in self.handles:
            f.write(s)

with open('q1', 'w') as f1, open('q2', 'w') as f2, open('q3', 'w') as f3:
    sys.stdout = MultiOut(f1, f2)
    sys.stderr = MultiOut(f3, f2)
    for i, c in enumerate('abcde'):
        print(c, 'out')
        print(i, 'err', file=sys.stderr)
</code></pre>
<p>After running that code, here's what those files contain:</p>
<p><strong>q1</strong></p>
<pre><code>a out
b out
c out
d out
e out    
</code></pre>
<p><strong>q3</strong></p>
<pre><code>0 err
1 err
2 err
3 err
4 err    
</code></pre>
<p><strong>q2</strong></p>
<pre><code>a out
0 err
b out
1 err
c out
2 err
d out
3 err
e out
4 err
</code></pre>
<hr/>
<p>FWIW, you can even do this, if you like:</p>
<pre><code>sys.stdout = MultiOut(f1, f2, sys.stdout)
sys.stderr = MultiOut(f3, f2, sys.stderr)
</code></pre>
<hr/>
<p>Unfortunately, file-like objects like <code>MultiOut</code> can't be used with <code>Popen</code> because <code>Popen</code> accesses files via the underlying OS file descriptor, i.e., it wants something that the OS considers to be a file, so only Python objects that supply a valid <code>fileno</code> method can be used for <code>Popen</code>'s file arguments.</p>
<p>Instead, we can use  Python 3's <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" rel="nofollow noreferrer"><code>asyncio</code></a> features to execute the shell command and to copy its stdout and stderr output concurrently.</p>
<p>Firstly, here's a simple Bash script that I used to test the following Python code. It simply loops over an array, echoing the array contents to stdout and the array indices to stderr, like the previous Python example.</p>
<p><strong>multitest.bsh</strong></p>
<pre class="lang-bash prettyprint-override"><code>#!/usr/bin/env bash

a=(a b c d e)
for((i=0; i&lt;${#a[@]}; i++))
do 
    echo "OUT: ${a[i]}"
    echo "ERR: $i" &gt;&amp;2
    sleep 0.01
done
</code></pre>
<p><strong>output</strong></p>
<pre><code>OUT: a
ERR: 0
OUT: b
ERR: 1
OUT: c
ERR: 2
OUT: d
ERR: 3
OUT: e
ERR: 4
</code></pre>
<p>And here's Python 3 code that runs multitest.bsh, piping its stdout output to files q1 and q2, and its stderr output to q3 and q2.</p>
<pre class="lang-python prettyprint-override"><code>import asyncio
from asyncio.subprocess import PIPE

class MultiOut(object):
    def __init__(self, *args):
        self.handles = args

    def write(self, s):
        for f in self.handles:
            f.write(s)

    def close(self):
        pass

@asyncio.coroutine
def copy_stream(stream, outfile):
    """ Read from stream line by line until EOF, copying it to outfile. """
    while True:
        line = yield from stream.readline()
        if not line:
            break
        outfile.write(line) # assume it doesn't block

@asyncio.coroutine
def run_and_pipe(cmd, fout, ferr):
    # start process
    process = yield from asyncio.create_subprocess_shell(cmd,
        stdout=PIPE, stderr=PIPE, executable="/bin/bash")

    # read child's stdout/stderr concurrently
    try:
        yield from asyncio.gather(
            copy_stream(process.stdout, fout),
            copy_stream(process.stderr, ferr))
    except Exception:
        process.kill()
        raise
    finally:
        # wait for the process to exit
        rc = yield from process.wait()
    return rc

# run the event loop
loop = asyncio.get_event_loop()

with open('q1', 'wb') as f1, open('q2', 'wb') as f2, open('q3', 'wb') as f3:
    fout = MultiOut(f1, f2)
    ferr = MultiOut(f3, f2)
    rc = loop.run_until_complete(run_and_pipe("./multitest.bsh", fout, ferr))
loop.close()
print('Return code:', rc)    
</code></pre>
<p>After running the code, here's what those files contain:</p>
<p><strong>q1</strong></p>
<pre><code>OUT: a
OUT: b
OUT: c
OUT: d
OUT: e
</code></pre>
<p><strong>q3</strong></p>
<pre><code>ERR: 0
ERR: 1
ERR: 2
ERR: 3
ERR: 4
</code></pre>
<p><strong>q2</strong></p>
<pre><code>OUT: a
ERR: 0
OUT: b
ERR: 1
OUT: c
ERR: 2
OUT: d
ERR: 3
OUT: e
ERR: 4
</code></pre>
<p>The asyncio code was lifted from <a href="https://stackoverflow.com/a/25960956/4014959">J.F. Sebastian's answer</a> to the question <a href="https://stackoverflow.com/q/17190221/4014959">Subprocess.Popen: cloning stdout and stderr both to terminal and variables</a>. Thanks, J.F!</p>
<p>Note that data is written to the files when it becomes available to the scheduled coroutines; exactly <em>when</em> that happens depends on the current system load. So I put the <code>sleep 0.01</code> command in multitest.bsh to keep the processing of stdout and stderr lines synchronised. Without that delay the stdout and stderr lines in q2 generally won't be nicely interleaved. There may be a better way to achieve that synchronisation, but I'm still very much a novice with asyncio programming.</p>
</div>
<span class="comment-copy">You may find some of the answers here helpful: <a href="http://stackoverflow.com/questions/2996887/how-to-replicate-tee-behavior-in-python-when-using-subprocess" title="how to replicate tee behavior in python when using subprocess">stackoverflow.com/questions/2996887/â€¦</a></span>
<span class="comment-copy">You are telling me the same what I was using. But it should redirect to multiple file descriptors. Not only to single file.</span>
<span class="comment-copy">Not usefull. Not working</span>
<span class="comment-copy">With the code in my answer you can redirect to as many files as you can open. :)</span>
<span class="comment-copy">What Python version are you using? Can you wait for "my_commands" to complete before writing the output data to the files? Or do you need to write to the files while "my_commands" is still running? If you can wait, what you want is easy. If you need to write to the 3 files while "my_commands" is still running it's a bit trickier.</span>
<span class="comment-copy">This will not work for me. Can you please suggest same with subprocess.Popen. Thanks.</span>
<span class="comment-copy">@Swapnil Sorry about that! I've just learned that this doesn't work with <code>Popen</code> because <code>Popen</code> accesses files via the underlying OS file, which is why you get the <code>AttributeError: 'MultiOut' object has no attribute 'fileno'</code> error. What you want <i>is</i> possible, but a little tricky, and I'm currently looking into various solutions.</span>
<span class="comment-copy">@Swapnil Please see my updated answer.</span>
