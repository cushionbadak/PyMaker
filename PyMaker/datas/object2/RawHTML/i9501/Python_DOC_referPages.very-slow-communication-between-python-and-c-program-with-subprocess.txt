<div class="post-text" itemprop="text">
<p>I am trying to speed up a Python program by outsourcing some repetitive calculations to a C++ program by using subprocess Python module.</p>
<p>To illustrate my problem, I took a simple C++ code that return the double of the input. It takes 16 seconds for a million of integers, which seems very slow.</p>
<p>Here is the C++ program (double.exe) :</p>
<pre><code>#include &lt;iostream&gt;

using namespace std;

int main()
{
    int a;
    bool goon = true;
    while (goon)
    {
        cin &gt;&gt; a;
        cout &lt;&lt; 2 * a &lt;&lt; endl;
        if (a == 0)
            goon= false;
    }
}
</code></pre>
<p>And here the Python 3 code :</p>
<pre><code>from time import time
from subprocess import PIPE,Popen

cmd = ["double"]
process = Popen(cmd, stdin=PIPE,stdout=PIPE, bufsize=32,universal_newlines=True, shell=True)

t0 = time()
for i in range(1,int(1e6)):
    print(i, file=process.stdin, flush=True)
    output = int(process.stdout.readline())
dt = time() - t0
print("Time to communicate : %fs" % dt)
print(0,file=process.stdin,flush=True) # close 'double' program
</code></pre>
<blockquote>
<p>Time to communicate : 16.029137s</p>
</blockquote>
<p>To me the reason why it's so slow can only be the communication between the Python process and the C++ program through the pipes, but I haven't find how to accelerate it. Any solution to speed up this communication, with subprocess or an other library ?</p>
<p>I'm using Python 3.5.2 on Windows.</p>
</div>
<div class="post-text" itemprop="text">
<p>The problem is not stdin communication per se, but rather massive context switching. You do a very small "task" in the C++ code, but for each such "task" the python code should write data to pipe, flush, go asleep, the C++ part wakes up, parses the input, calculates the result, prints the output, flushes and goes asleep. Then the python code wakes up, etc.</p>
<p>Going to sleep and waking up (and associated context switching) is not free. With the size of a "task" (multiplying the input by two) this overhead consumes most of the time.</p>
<p>You can "fix" that by either supplying work to the C++ program in batches, or having bigger tasks. Or both.</p>
<p>For example, the same job with million of numbers, but done using batches of 10 numbers runs 2 times faster on my box if the pipe is flushed after each write. The code:</p>
<pre><code>for i in range(1,int(1e5)):
    for j in range(1, 10):
        print(i*10 + j, file=process.stdin, flush=True)
    for j in range(1, 10):
        output = int(process.stdout.readline())
</code></pre>
<p>If the flush is done only once per 10 numbers it runs 1.5 times faster than the previous example (or 3 times faster than the original code):</p>
<pre><code>for i in range(1,int(1e5)):
    for j in range(1, 10):
        print(i*10 + j, file=process.stdin)
    process.stdin.flush()
    for j in range(1, 10):
        output = int(process.stdout.readline())
</code></pre>
<p>If the "task" is bigger then the price you have to pay for the context switch is the same. But it's not as big compared to the size of the task. For example, let's imagine that the context switch takes 0.1 second (it's way smaller in the real life, this is just an example). If the task is a multiplication which is done in say 1ms (again, just for example) then the context switch overhead compared to the task is 10000%. But if your task is heavy and takes 1s to be performed, then the overhead is just 10%. 1000 times difference in relative value.</p>
</div>
<div class="post-text" itemprop="text">
<p>Just a guess, but it may be due to the fact that <a href="http://en.cppreference.com/w/cpp/io/manip/endl" rel="nofollow noreferrer">std::endl</a> not only writes a new line character but also flushes the output stream. The flush miight be the part that takes the most time. Therefore it might be faster if you just write</p>
<pre><code>std::cout &lt;&lt; 2 * a &lt;&lt; "\n"; //Unix style line break
</code></pre>
<p>or</p>
<pre><code>std::cout &lt;&lt; 2 * a &lt;&lt; "\r\n"; //Windows style line break
</code></pre>
<p>(Note: Untested whether this works or the implicit flush is actually required to be there.)</p>
</div>
<span class="comment-copy">Why not just expose the C++ code to Python as a module with something like boost::python?</span>
<span class="comment-copy">a) just starting and closing a process takes quite some time b) I'm not sure whether your calculation would actually be substantially faster in C++. Have you profiled that?</span>
<span class="comment-copy">In addition to the module approach you might alternatively consider things like <a href="https://docs.scipy.org/doc/scipy/reference/weave.html" rel="nofollow noreferrer">scipy.weave</a> or <a href="http://cffi.readthedocs.io/en/latest/index.html" rel="nofollow noreferrer">cffi</a> as well (which may or may not be suited for your use case).</span>
<span class="comment-copy">It's been my experience also that stdin/stdout communication is very slow under Windows.  I'm not sure that anything can be done about it other than using some other communications mechanism besides stdin/stdout, or running under a different OS :(</span>
<span class="comment-copy">@user45891 I haven't tested yet, but from what I know C/C++ is always faster than Python, especially for simple numeric operations (which is the case of the task I want to compute on C++), that's why I wanted to try that to see what gain I could obtain.</span>
<span class="comment-copy">I took the example of a simple task as double to illustrate my problem, but the task I want to speed up is actually bigger, but unfortunately not enough to get a real time gain with a C++ code called with subprocess compared to a Python implementation of the task. On my PC, your both codes run in 5s, 3 times faster than mine. Nevertheless, in my real problem, I need the answer before compute the next calculation, so a such implementation can't be done.</span>
<span class="comment-copy">@Aral in this case you <i>probably</i> want to implement a C/C++ extension module for your python code - assuming the calculations you have to do are really heavy and implementing them in python is going to be very slow. See <a href="https://docs.python.org/3/extending/extending.html" rel="nofollow noreferrer">docs.python.org/3/extending/extending.html</a></span>
<span class="comment-copy">@aral, Using IPC on fine grained tasks is invariably a bad idea for performance improvement, irrespective of languages, and stdin for ipc is about the slowest and worst method possible. Even with extensions, while in process, you still suffer from (automatic) data types translation, stack conversion and the like. A memory mapped file might help, but you should ask yourself whether you've already exhausted your ways to profile and optimize your python algorithm.</span>
<span class="comment-copy">I tested it, It doesn't accelerate the communication compared to std::endl. Even if I am on Windows, the proper way is the Unix style line break. With '\r\n', it produces a two output lines for each integer sent : " *doublea*\n " and "\n".</span>
