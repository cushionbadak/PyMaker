<div class="post-text" itemprop="text">
<p>I am reading a large csv file 25GB into pandas.DataFrame. My pc specifications are:</p>
<ul>
<li>Intel core i7-8700 3.2GHz</li>
<li>RAM 16G</li>
<li>windows 10</li>
<li>DataFrame.shape =144,000,000 rows by 13 cols</li>
<li>csv file size on disk says 24GB</li>
</ul>
<p>reading this file takes a long time like 20 minutes sometimes. Is there any recommendation, code wise, that I can do better? </p>
<p>*note: This DF is needed in whole, since I am going to Join(Merge) with another one.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could use a <a href="http://docs.dask.org/en/latest/dataframe.html" rel="nofollow noreferrer">dask.dataframe</a>:</p>
<pre><code>import dask.dataframe as dd # import dask.dataframe
df = dd.read_csv('filename.csv') # read csv
</code></pre>
<p>or you could use <a href="https://towardsdatascience.com/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c" rel="nofollow noreferrer">chunking</a>:</p>
<pre><code>def chunk_processing(): # define a function that you will use on chunks
    ## Do Something # your function code here


chunk_list = [] # create an empty list to hold chunks
chunksize = 10 ** 6 # set chunk size
for chunk in pd.read_csv('filename.csv', chunksize=chunksize): # read in csv in chunks of chunksize
    processed_chunk = chunk_processing(chunk) # process the chunks with chunk_processing() function
    chunk_list.append(processed_chunk) # append the chunks to a list
df_concat = pd.concat(chunk_list) # concatenate the list to a dataframe
</code></pre>
</div>
<span class="comment-copy">Look at the <code>chunksize</code> argument in <code>pd.read_csv()</code>: <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html" rel="nofollow noreferrer">pandas.pydata.org/pandas-docs/stable/reference/api/â€¦</a></span>
<span class="comment-copy">You can try to load it by chunks as shown <a href="https://stackoverflow.com/questions/39384539/how-to-read-data-in-chunks-in-python-dataframe">here</a>. Also if you can refactor your logic to process dataframe chunk-wise, you can do make an algorithm to work as shown <a href="https://stackoverflow.com/questions/25962114/how-to-read-a-6-gb-csv-file-with-pandas">here</a>. Maybe this will help.</span>
<span class="comment-copy">Maybe I have to edit my questions. This DF is needed in whole, since I am going to Join(Merge) with another one. if I use chunk, do I have all at once?</span>
<span class="comment-copy">You might check out <a href="https://dask.org/" rel="nofollow noreferrer">dask</a>. It's pretty good for these situations where data is larger than memory.</span>
<span class="comment-copy">You should considere changing the whole design. Pandas does a great job when everything fits into memory. But when the size of data if greater than available memory, it has to use some form of swapping to let its data reside on the disk. Joining a 25 Gb dataframe on a 16 Gb machine will be painfully slow, full stop. <b>You</b> know what you want to do, <b>I</b> don't, so I cannot guess whether a more efficient design is possible or not.</span>
<span class="comment-copy">in the case of using chunks. if DF1(100M) wants to LEFT JOIN DF2(144M) and I did split(chunk) it into 10M from DF1 each time, I have to run the join 10times, each time joining 10M with 144M. You think this improves my code? to me looks like the same as joining 100M with 144M at once. No?</span>
<span class="comment-copy">It's a bit difficult to offer optimization advice without the seeing the code you are using. If the merge() is what is taking a long time, chunking probably won't speed it up that much, though it may still speed it up some. If it's the initial reading in of the CSV that is taking forever you should notice a significant improvement by using chunking.</span>
<span class="comment-copy">the "dask" library is taking longer time compare to pandas. I have no idea why.</span>
