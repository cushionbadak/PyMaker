<div class="post-text" itemprop="text">
<p>I am running python 3.6 on Windows 10 (both 64 bit) with a fast SSD, 32GB RAM and a very fast processor (AMD Threadripper). I also get similar results running on an Intel i7. I am opening XML text files about 5MB in size and noticed very slow performance even using lxml. I boiled the problem down to disk read performance. If I load as binary, it is over 8x faster. The following tests were performed with the file already in disk cache. The disk was never hit during these runs. Similar results were obtained with a cold read.</p>
<pre><code>%%timeit
with open(xmlpath + fn, 'rb') as f:
    r = f.read()
1.66 ms ± 31.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%%timeit
with open(xmlpath + fn, 'r', encoding='UTF-8') as f:
    r = f.read()
13.7 ms ± 32.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<p>I understand the encoding has to be processed for the text version, but 8x slower seems excessive running on a fast processor. Also, 1.6ms seems slow to me to read such a small file already in cache. Disk performance measured with Passmark is as expected at &gt; 2GB/s. </p>
<p>Any advice/suggestions would be appreciated. Thanks.</p>
</div>
<div class="post-text" itemprop="text">
<p>When you're using <code>rb</code>, the file can be read with an unlimited/very big block size directly in the final buffer memory (<code>r</code>).</p>
<p>When you're using <code>r</code>, the file must be read with a smaller block size in order to be able to post-process the blocks &amp; remove the <code>\r</code> chars before the <code>\n</code> chars (<em>if</em> it's not done char by char, which would be the easiest but the slowest way)</p>
<p>if you can afford to duplicate the memory (which <code>r</code> mode doesn't do), I would suggest:</p>
<pre><code>with open(xmlpath + fn, 'rb') as f:
    r = f.read().decode('UTF-8').replace('\r','')
</code></pre>
<p>also to save decoding &amp; replacing as comments stated:</p>
<pre><code>with open(xmlpath + fn, 'r', newline="") as f:
    r = f.read()
</code></pre>
</div>
<span class="comment-copy">Thanks for the quick reply. I tried your suggestion and got: 12.4 ms ± 160 µs per loop (mean ± std. dev. of 7 runs, 100 loops each). This still seems excessive.</span>
<span class="comment-copy">UTF-8 is a superset of ASCII, so you can safely do the replace on the <code>bytes</code> object instead of the decoded string. That should shave a couple ms off the time. But the replace operation is still the performance bottleneck -- much more so than the decoding. Optimizing this at the pure Python level isn't possible. If you can live with in-memory CRLF line endings, then <code>newline=''</code> is a good tradeoff between performance and memory.</span>
<span class="comment-copy">@eryksun good point. Added. My personal experience with this kind of binary/not binary was in writing to networked disks. The difference of performance was from 10 seconds to 3 minutes, probably because of the block size difference.</span>
<span class="comment-copy">If the buffer size is the issue, why not use the third argument <code>buffering</code> for <a href="https://docs.python.org/3/library/functions.html#open" rel="nofollow noreferrer">open</a>? Also, on Python 3 it's definitely not just newlines getting converted, it must convert into the in-core string type, which depending on the characters encountered will have varying formats (see <a href="https://www.python.org/dev/peps/pep-0393/" rel="nofollow noreferrer">PEP 393</a>). Perhaps it's optimized if an internal format matches the file encoding.</span>
<span class="comment-copy">buffering, worth a try.</span>
