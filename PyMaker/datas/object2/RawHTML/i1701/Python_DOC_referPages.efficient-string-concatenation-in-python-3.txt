<div class="post-text" itemprop="text">
<div class="question-status question-originals-of-duplicate">
<p>This question already has an answer here:</p>
<ul>
<li>
<a dir="ltr" href="/questions/3055477/how-slow-is-pythons-string-concatenation-vs-str-join">How slow is Python's string concatenation vs. str.join?</a>
<span class="question-originals-answer-count">
                    5 answers
                </span>
</li>
</ul>
</div>
<p>I am writing a Python 3 code where the task is to open about 550 files in a directory, read their contents and append it to a string variable 'all_text' which will be say around millions of line long as a single line.</p>
<p>The inefficient code I was using till now is as follows-</p>
<pre><code>all_text += str(file_content)
</code></pre>
<p>But then I read that using 'join()' method is efficient, so I tried the following code-</p>
<pre><code>all_text = ''.join(file_content)
</code></pre>
<p>The problem with this code is that this is removing the previously held contents of 'all_text' variable and writing the current file's content only!</p>
<p>How do I get around this problem?</p>
<p>Thanks for your help!</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://docs.python.org/3/library/stdtypes.html?highlight=join#str.join" rel="nofollow noreferrer">join</a>() has a definition str.join(iterable) where <a href="https://docs.python.org/3/glossary.html#term-iterable" rel="nofollow noreferrer">iterable</a> is a generator or a list or a set and so on. So it is helpful if you already have a list of strings read from the files and you are concatenating them using join.
 For example</p>
<pre><code>numList = ['1', '2', '3', '4']
seperator = ', '
print(seperator.join(numList))

numTuple = ('1', '2', '3', '4')
print(seperator.join(numTuple))

s1 = 'abc'
s2 = '123'

""" Each character of s2 is concatenated to the front of s1""" 
print('s1.join(s2):', s1.join(s2))

""" Each character of s1 is concatenated to the front of s2""" 
print('s2.join(s1):', s2.join(s1))
</code></pre>
<p>You can get all lines in a file using join like <code>''.join(readlines(f))</code></p>
<p>Now you can accomplish your task using join as follows using <a href="https://docs.python.org/3/library/fileinput.html" rel="nofollow noreferrer">fileinput</a> module</p>
<pre><code>import fileinput
files= ['package-lock.json', 'sqldump.sql', 'felony.json', 'maindata.csv']
allfiles = fileinput.input(files)
all_text = ''.join(allfiles)
</code></pre>
<p>Refer to <a href="https://stackoverflow.com/questions/46237182/concatenate-multiple-files-into-a-single-file-object-without-creating-a-new-file">this answer</a> to know the most efficient way to concat files into a string.</p>
<p><strong>Suggestion</strong>: As you mentioned there would be millions of lines, did you consider the memory it is going to consume to store it in a variable? So it is better you do what you are planning to do on the fly while reading the lines instead of storing it in a variable.</p>
</div>
<span class="comment-copy"><code>all_text += ''.join(file_content)</code>?</span>
<span class="comment-copy">I tried your code using 'fileinput' module but it gives me a MemoryError. My system has 8GB RAM and the combined size of the 550 files in the directory is around 2.3 GB. Any ideas how to avoid this?</span>
<span class="comment-copy">Could be that your machine has not enough free RAM or your OS has limited Python to a certain amount of RAM only. Nevertheless, as I suggested you better do whatever you are planning to do with those files on-the-fly rather than adding them up because opening file consumes memory and the string again consumes memory. So you're using a lot of memory which is inefficient in the first place.</span>
