<div class="post-text" itemprop="text">
<p>I have many files that each have several million rows; each row is a dumped data entry and is several hundred characters long. The rows come in groups and the first two characters tell me the type of row it is, and I use that to parse it. This structure prohibits me from loading the rows to a dataframe, for example, or anything else that does not go through the rows one at a time.</p>
<p>For each row, I currently create a dictionary vals = {}, and then sequentially run through about fifty keys along the lines of </p>
<p>vals{'name'} = row[2:24]</p>
<p>vals{'state'} = row[24:26]</p>
<p>Instead of doing fifty assignments sequentially, can I do this simultaneously or in parallel in some simple manner?</p>
<p>Is</p>
<p>vals{'name'},vals{'state'} = row[2:24],row[24:26]</p>
<p>faster if I do this simultaneous assignment for many entries? I could also reformulate this as a list comprehension. Would that be faster than running through sequentially?</p>
</div>
<div class="post-text" itemprop="text">
<p>To answer your question, no, doing multiple assignment will not speed up your program. This is because the multiple assignment syntax is just a different way of writing multiple assignments on different lines.</p>
<p>For example</p>
<pre><code>vals{'name'},vals{'state'} = row[2:24],row[24:26]
</code></pre>
<p>is equivalent to </p>
<pre><code>vals{'name'}= row[2:24]
vals{'state'} = row[2:24]
</code></pre>
<p>If you want to optimize your code, your should start by <a href="https://docs.python.org/3.7/library/profile.html" rel="nofollow noreferrer">profiling it</a> to determine the parts that are taking the largest amount of time. I would also check to ensure that you are not doing multiple reads from the same file, as these are very slow compared to reading from memory. If possible, you should <a href="https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files" rel="nofollow noreferrer">read the entire file into memory</a> first, and then process it.</p>
</div>
