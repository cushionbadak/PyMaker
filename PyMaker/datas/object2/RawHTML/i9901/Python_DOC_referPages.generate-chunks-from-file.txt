<div class="post-text" itemprop="text">
<p>I have a JSON file and would like to write a function to return a list of <strong>the next 10 objects</strong> in the file.  I've started with a class, <code>FileProcessor</code>, and the method <code>get_row()</code> which returns a generator that yields a single JSON object from the file.  Another method, <code>get_chunk()</code>, should return the next 10 objects. </p>
<p>Here is what I have so far: </p>
<pre><code>class FileProcessor(object):

    def __init__(self, filename):
        self.FILENAME = filename

    def get_row(self):
        with open( os.path.join('path/to/file', self.FILENAME), 'r') as f:
            for i in f:
                yield json.loads(i)

    def get_chunk(self):
        pass
</code></pre>
<p>I've tried like this, but it only returns the <em>first 10</em> rows every time.</p>
<pre><code>    def get_chunk(self):
        chunk = []
        consumer = self.consume()
        for i in self.get_row():
            chunk.append(i)
        return chunk     
</code></pre>
<p>So what is the correct way to write <code>get_chunk()</code>?</p>
</div>
<div class="post-text" itemprop="text">
<p>Here's a simple generator that gets values from another generator and puts them into a list. It should work with your <code>FileProcessor.get_row</code> method.</p>
<pre><code>def count(n):
    for v in range(n):
        yield str(v)

def chunks(it, n):
    while True:
        yield [next(it) for _ in range(n)]

for u in chunks(count(100), 12):
    print(u)
</code></pre>
<p><strong>output</strong></p>
<pre><code>['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
['12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']
['24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35']
['36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']
['48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59']
['60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71']
['72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83']
['84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95']
</code></pre>
<p>Note that this only yields complete chunks. If that's a problem, you can do this:</p>
<pre><code>def chunks(it, n):
    while True:
        chunk = []
        for _ in range(n):
            try:
                chunk.append(next(it))
            except StopIteration: 
                yield chunk
                return
        yield chunk
</code></pre>
<p>which will print</p>
<pre><code>['96', '97', '98', '99']
</code></pre>
<p>after the previous output.</p>
<hr/>
<p>A better way to do this is to use <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow noreferrer"><code>itertools.islice</code></a>, which will handle a partial final chunk:</p>
<pre><code>from itertools import islice

def chunks(it, n):
    while True:
        a = list(islice(it, n))
        if not a:
            return
        yield a
</code></pre>
<p>Thanks to <a href="https://stackoverflow.com/users/918959/antti-haapala">Antti Haapala</a> for reminding me about <code>islice</code>. :)</p>
</div>
<div class="post-text" itemprop="text">
<p>(note: PM 2Ring beat me to it!)</p>
<p>Your <code>get_row</code> method doesn't return a row - it returns a generator that will produce rows as you iterate through it. You can see that in the <code>get_chunk</code> method that does <code>for i in self.get_row...</code>. The annoying thing is that every time you call <code>get_row</code> it will open the file again and return the first object. The problem with <code>get_chunk</code> is that you don't pass in the number of rows you want and you don't limit the <code>for</code> loop to that number. <code>get_chunk</code> gets all of the rows in the file.</p>
<p>How about a rethink? All you really need is a generator that reads lines and deserializes the json. The <code>map</code> function is already built to do that. You can get a single row with python's <code>next</code> function and multiple rows with <code>itertools.islice</code>. Your class is just a thin wrapper around stuff that's already implemented so just use the native tools and skip writing your own class completely.</p>
<p>Fist I generate a test file</p>
<pre><code>&gt;&gt;&gt; with open('test.json', 'w') as fp:
...     for obj in 'foo', 'bar', 'baz':
...         fp.write(json.dumps(obj) + '\n')
</code></pre>
<p>... </p>
<p>Now I can create an iterator that can be used to get a row or list of rows. In cpython, you can open the file in the <code>map</code> function safely, but you can also do your work in a <code>with</code> clause.</p>
<pre><code>&gt;&gt;&gt; json_iter = map(json.loads, open('test.json')) 
&gt;&gt;&gt; next(json_iter)
'foo'
&gt;&gt;&gt;
&gt;&gt;&gt; with open('test.json') as fp:
&gt;&gt;&gt;     json_iter = map(json.loads, open('test.json'))
&gt;&gt;&gt;     next(json_iter)
'foo'
</code></pre>
<p>I can get all of the objects in a loop</p>
<pre><code>&gt;&gt;&gt; for obj in map(json.loads, open('test.json')):
...     print(obj)
... 
foo
bar
baz
</code></pre>
<p>Or put some of them in a list</p>
<pre><code>&gt;&gt;&gt; list(itertools.islice(json_iter, 2))
['foo', 'bar']
</code></pre>
<p>or combine operations</p>
<pre><code>&gt;&gt;&gt; json_iter = map(json.loads, open('test.json'))
&gt;&gt;&gt; for obj in json_iter:
...     if obj == 'foo':
...         list(itertools.islice(json_iter,2))
... 
['bar', 'baz']
</code></pre>
<p>The point is, the simple <code>map</code> based iterator can do what you want, without having to update a wrapper class every time you think of a new use case.</p>
</div>
<span class="comment-copy">Are you sure the json parser that is included in the standard library doesn't support incremental loading? Or can't be extended to do so?</span>
<span class="comment-copy">Does the <code>FileProcessor.get_row</code> method work correctly? IOW, is each row in the text file <i>guaranteed</i> to be a complete JSON object?</span>
<span class="comment-copy">@PM2Ring yes, it's returning a complete JSON object</span>
<span class="comment-copy">@SwiftsNamesake not sure, but I read through the docs and did not see how this is possible using the JSON library</span>
<span class="comment-copy"><a href="http://stackoverflow.com/questions/10715628/opening-a-large-json-file-in-python" title="opening a large json file in python">stackoverflow.com/questions/10715628/â€¦</a></span>
<span class="comment-copy">I was busy writing my one answer and you beat me to it! I'm using <code>map</code> to create the generator so I think I'll post anyway to show some more options. But I like your answer.</span>
<span class="comment-copy">Thanks for helping me work through this!  I think the map function is great, and I'm glad to have learned about it, but I'm wondering if it's OK to use with large data sets.  Our JSON files are on average 2-3 GB with anywhere between 1k and 1mm objects in each file.  While testing the map function on some of these files it seems very slow, which I imagine is because its processing the entire data set - is that correct?</span>
<span class="comment-copy">@JoeFusaro - I assume you use python 3.x otherwise this would have blown up (you would need to use <code>itertools.imap</code> on 2.x). As long as you used the file like I've shown (didn't do something like a <code>read</code> or <code>readlines</code> on the file), for each iteration it reads 1 line from the file, deserializes that 1 line and returns the python object. If the json objects themselves are large, this will take some time reading chunks of data, scanning for a new line, building a new line string and then decoding into python objects. But still, only one json object is built at a time.</span>
<span class="comment-copy">Ah, I am using 2.7, I should have specified - sorry about that. Will try out the imap function. Thanks!</span>
