<div class="post-text" itemprop="text">
<p>I am building a simple webcrawler in Python. I will have to go through ~50k websites and I want to speed up the process with some multithreading.</p>
<p>I have defined a crawler class to crawl through each website as a meta-object of Thread:</p>
<pre><code>Crawler(Thread):
     def __init__(self, url, depth, wait):
...
</code></pre>
<p>Then in a main function, I iterate through batches of 10 URLs from the full list of URLs and create a Crawler object for each URL:</p>
<pre><code>    for i in range(index, math.ceil(len(urls) / 10)):
        jobs = []
        for url in urls[i * 10:(i + 1) * 10]:
            s = Crawler(url)
            s.setDaemon(True)
            s.start()
            jobs.append(s)

        for j in jobs:
            j.join()
</code></pre>
<p>The problem is that, for each batch, I have to wait for all threads to finish. This is unefficient as when I have, say 9 websites with 100 pages and only 1 website with 10,000 pages, the 9 websites will be done a in a matter of minutes, but I will have to wait an hour for the 10,000 pages large website to finish before I can proceed to the next batch.</p>
<p>To optimize things, it would be better to start with 10 Crawler threads, then, every time a Crawler thread is finished, create a new Crawler with the next url in the list, until the list is done. </p>
<p>I'm thinking I could get rid of the join() and have a while loop over the length of <code>threading.enumerate</code>, adding a new thread everytime the length drops below 10, but that sounds a bit hackish. </p>
<p>I was looking into python's <code>Queue</code> but judging from the examples at <a href="https://docs.python.org/3/library/queue.html" rel="nofollow noreferrer">https://docs.python.org/3/library/queue.html</a>, I would still have to rely on <code>.join()</code> and therefore wait for all the threads in the queue to have executed.</p>
<p>Is there any way to add something like an "event listener" to a thread, so that whenever a thread finishes I can update the thread list with a new thread ? </p>
</div>
<div class="post-text" itemprop="text">
<p>Perhaps have a look at the Queue again, you don't need a join per batch or at all.</p>
<p>You can put all of the 50K websites into the queue. I would that perhaps call <code>jobs</code> and the limited number of threads are usually called something like <code>workers</code>. Each worker then picks up an item from the queue, processes it and continues to pick up items from the queue until done. What done means varies. One suggestion is to put <code>None</code> on the queue for each worker and each worker will stop once it sees a <code>None</code>. But there are other signals you could use. You could then use <code>join</code> to wait for all worker threads to finish. In that case a worker doesn't need to be a daemon. (You wouldn't want to create a separate Thread per URL)</p>
<p>For example:</p>
<pre><code>from threading import Queue, Thread

def crawl_worker(q):
  while True:
    url = q.get()
    if url is None:
      break
    # do something with url

url_queue = Queue()

# populate the queue
for url in urls:
  url_queue.put(url)

num_workers = 10

workers = [
  Thread(target=crawl_worker, args=(url_queue))
  for _ in range(num_workers)
]

# add a None signal for each worker
for worker in workers:
  url_queue.put(None)

# start all workers
for worker in workers:
  worker.start()

# wait for all workers to finish
for worker in workers:
  worker.join()

# we're done now
</code></pre>
<p>There are alternatives to that. If you are not actually looking at it as an exercise but want to get something done, then I would recommend <a href="https://scrapy.org/" rel="nofollow noreferrer">https://scrapy.org/</a></p>
</div>
