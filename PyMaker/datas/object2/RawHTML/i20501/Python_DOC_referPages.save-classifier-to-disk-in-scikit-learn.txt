<div class="post-text" itemprop="text">
<p>How do I save a trained <strong>Naive Bayes classifier</strong> to <strong>disk</strong> and use it to <strong>predict</strong> data?</p>
<p>I have the following sample program from the scikit-learn website:</p>
<pre><code>from sklearn import datasets
iris = datasets.load_iris()
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)
print "Number of mislabeled points : %d" % (iris.target != y_pred).sum()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Classifiers are just objects that can be pickled and dumped like any other. To continue your example:</p>
<pre><code>import cPickle
# save the classifier
with open('my_dumped_classifier.pkl', 'wb') as fid:
    cPickle.dump(gnb, fid)    

# load it again
with open('my_dumped_classifier.pkl', 'rb') as fid:
    gnb_loaded = cPickle.load(fid)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can also use <a href="http://packages.python.org/joblib/generated/joblib.dump.html">joblib.dump</a> and <a href="http://packages.python.org/joblib/generated/joblib.load.html">joblib.load</a> which is much more efficient at handling numerical arrays than the default python pickler.</p>
<p>Joblib is included in scikit-learn:</p>
<pre><code>&gt;&gt;&gt; from sklearn.externals import joblib
&gt;&gt;&gt; from sklearn.datasets import load_digits
&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier

&gt;&gt;&gt; digits = load_digits()
&gt;&gt;&gt; clf = SGDClassifier().fit(digits.data, digits.target)
&gt;&gt;&gt; clf.score(digits.data, digits.target)  # evaluate training error
0.9526989426822482

&gt;&gt;&gt; filename = '/tmp/digits_classifier.joblib.pkl'
&gt;&gt;&gt; _ = joblib.dump(clf, filename, compress=9)

&gt;&gt;&gt; clf2 = joblib.load(filename)
&gt;&gt;&gt; clf2
SGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,
       fit_intercept=True, learning_rate='optimal', loss='hinge', n_iter=5,
       n_jobs=1, penalty='l2', power_t=0.5, rho=0.85, seed=0,
       shuffle=False, verbose=0, warm_start=False)
&gt;&gt;&gt; clf2.score(digits.data, digits.target)
0.9526989426822482
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>What you are looking for is called <strong>Model persistence</strong> in sklearn words and it is documented in <a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html#model-persistence">introduction</a> and in <a href="http://scikit-learn.org/stable/modules/model_persistence.html">model persistence</a> sections.</p>
<p>So you have initialized your classifier and trained it for a long time with</p>
<pre><code>clf = some.classifier()
clf.fit(X, y)
</code></pre>
<p>After this you have two options:</p>
<p><strong>1) Using Pickle</strong></p>
<pre><code>import pickle
# now you can save it to a file
with open('filename.pkl', 'wb') as f:
    pickle.dump(clf, f)

# and later you can load it
with open('filename.pkl', 'rb') as f:
    clf = pickle.load(f)
</code></pre>
<p><strong>2) Using Joblib</strong></p>
<pre><code>from sklearn.externals import joblib
# now you can save it to a file
joblib.dump(clf, 'filename.pkl') 
# and later you can load it
clf = joblib.load('filename.pkl')
</code></pre>
<p>One more time it is helpful to read the above-mentioned links </p>
</div>
<div class="post-text" itemprop="text">
<p>In many cases, particularly with text classification it is not enough just to store the classifier but you'll need to store the vectorizer as well so that you can vectorize your input in future.</p>
<pre><code>import pickle
with open('model.pkl', 'wb') as fout:
  pickle.dump((vectorizer, clf), fout)
</code></pre>
<p>future use case:</p>
<pre><code>with open('model.pkl', 'rb') as fin:
  vectorizer, clf = pickle.load(fin)

X_new = vectorizer.transform(new_samples)
X_new_preds = clf.predict(X_new)
</code></pre>
<p>Before dumping the vectorizer, one can delete the stop_words_ property of vectorizer by:</p>
<pre><code>vectorizer.stop_words_ = None
</code></pre>
<p>to make dumping more efficient.
Also if your classifier parameters is sparse (as in most text classification examples) you can convert the parameters from dense to sparse which will make a huge difference in terms of memory consumption, loading and dumping. <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.sparsify" rel="noreferrer">Sparsify</a> the model by:</p>
<pre><code>clf.sparsify()
</code></pre>
<p>Which will automatically work for <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" rel="noreferrer">SGDClassifier</a> but in case you know your model is sparse (lots of zeros in clf.coef_) then you can manually convert clf.coef_ into a <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html" rel="noreferrer">csr scipy sparse matrix</a> by:</p>
<pre><code>clf.coef_ = scipy.sparse.csr_matrix(clf.coef_)
</code></pre>
<p>and then you can store it more efficiently.</p>
</div>
<div class="post-text" itemprop="text">
<p><code>sklearn</code> estimators implement methods to make it easy for you to save relevant trained properties of an estimator. Some estimators implement <code>__getstate__</code> methods themselves, but others, like the <code>GMM</code> just use the <a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py" rel="nofollow noreferrer">base implementation</a> which simply saves the objects inner dictionary:</p>
<pre><code>def __getstate__(self):
    try:
        state = super(BaseEstimator, self).__getstate__()
    except AttributeError:
        state = self.__dict__.copy()

    if type(self).__module__.startswith('sklearn.'):
        return dict(state.items(), _sklearn_version=__version__)
    else:
        return state
</code></pre>
<p>The recommended method to save your model to disc is to use the <a href="https://docs.python.org/3/library/pickle.html" rel="nofollow noreferrer"><code>pickle</code></a> module:</p>
<pre><code>from sklearn import datasets
from sklearn.svm import SVC
iris = datasets.load_iris()
X = iris.data[:100, :2]
y = iris.target[:100]
model = SVC()
model.fit(X,y)
import pickle
with open('mymodel','wb') as f:
    pickle.dump(model,f)
</code></pre>
<p>However, you should save additional data so you can retrain your model in the future, or suffer dire consequences <strong>(such as being locked into an old version of sklearn)</strong>.</p>
<p>From the <a href="http://scikit-learn.org/stable/modules/model_persistence.html" rel="nofollow noreferrer">documentation</a>:</p>
<blockquote>
<p>In order to rebuild a similar model with future versions of
  scikit-learn, additional metadata should be saved along the pickled
  model: </p>
<p>The training data, e.g. a reference to a immutable snapshot </p>
<p>The python source code used to generate the model </p>
<p>The versions of scikit-learn and its dependencies </p>
<p>The cross validation score obtained on the training data</p>
</blockquote>
<p><strong>This is especially true for Ensemble estimators</strong> that rely on the <a href="https://github.com/scikit-learn/scikit-learn/blob/f3320a6f1ab2d060fd49c94a6e3386291a684ec7/sklearn/tree/_tree.pyx" rel="nofollow noreferrer"><code>tree.pyx</code></a> module written in Cython(such as <code>IsolationForest</code>), since it creates a coupling to the implementation, which is not guaranteed to be stable between versions of sklearn. It has seen backwards incompatible changes in the past.</p>
<p>If your models become very large and loading becomes a nuisance, you can also use the more efficient <code>joblib</code>. From the documentation:</p>
<blockquote>
<p>In the specific case of the scikit, it may be more interesting to use
  joblib’s replacement of <code>pickle</code> (<code>joblib.dump</code> &amp; <code>joblib.load</code>), which is
  more efficient on objects that carry large numpy arrays internally as
  is often the case for fitted scikit-learn estimators, but can only
  pickle to the disk and not to a string:</p>
</blockquote>
</div>
<span class="comment-copy">Works like a charm! I was trying to use np.savez and load it back all along and that never helped. Thanks a lot.</span>
<span class="comment-copy">in python3, use the pickle module, which works exactly like this.</span>
<span class="comment-copy">But from my understanding pipelining works if its part of a single work flow. If I want to build the model store it on disk and stop the the execution there. Then I come back a week later and try to load the  model from disk it throws me an error :</span>
<span class="comment-copy">There is no way to stop and resume the execution of the <code>fit</code> method if this is what you are looking for. That being said, <code>joblib.load</code> should not raise an exception after a successful <code>joblib.dump</code> if you call it from a Python with the same version of the scikit-learn library.</span>
<span class="comment-copy">If you are using IPython, do not use the <code>--pylab</code> command line flag or the <code>%pylab</code> magic as the implicit namespace overloading is known to break the pickling process. Use explicit imports and the <code>%matplotlib inline</code> magic instead.</span>
<span class="comment-copy">see the scikit-learn documentation for reference: <a href="http://scikit-learn.org/stable/tutorial/basic/tutorial.html#model-persistence" rel="nofollow noreferrer">scikit-learn.org/stable/tutorial/basic/…</a></span>
<span class="comment-copy">Is it possible to retrain previously saved model? Specifically SVC models?</span>
<span class="comment-copy">Thank You Salvador. This works perfectly</span>
<span class="comment-copy">I wish this is was added to the documentation! Particularly your first sentence since this is the only place I've ever seen it being mentioned.</span>
<span class="comment-copy">this was much needed, couldn't find it anywhere else. thanks!</span>
