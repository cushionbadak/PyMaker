<div class="post-text" itemprop="text">
<p>I have two versions of Python. When I launch a spark application using spark-submit, the application uses the default version of Python. But, I want to use the other one.
How to specify the version of Python for spark-submit to use?</p>
</div>
<div class="post-text" itemprop="text">
<p>You can set the <code>PYSPARK_PYTHON</code> variable in <code>conf/spark-env.sh</code> (in Spark's installation directory) to the absolute path of the desired Python executable.</p>
<p>Spark distribution contains <code>spark-env.sh.template</code> (<code>spark-env.cmd.template</code> on Windows) by default. It must be renamed to <code>spark-env.sh</code> (<code>spark-env.cmd</code>) first.</p>
<p>For example, if Python executable is installed under <code>/opt/anaconda3/bin/python3</code>:</p>
<pre><code>PYSPARK_PYTHON='/opt/anaconda3/bin/python3'
</code></pre>
<p>Check out the <a href="https://spark.apache.org/docs/latest/configuration.html#environment-variables" rel="nofollow noreferrer">configuration documentation</a> for more information.</p>
</div>
<div class="post-text" itemprop="text">
<p>In my environment I simply used</p>
<pre><code>export PYSPARK_PYTHON=python2.7
</code></pre>
<p>It worked for me</p>
</div>
<div class="post-text" itemprop="text">
<p>You can either specify the version of Python by listing the path to your install in a shebang line in your script:</p>
<p>myfile.py:</p>
<pre><code>#!/full/path/to/specific/python2.7
</code></pre>
<p>or by calling it on the command line without a shebang line in your script:</p>
<pre><code>/full/path/to/specific/python2.7 myfile.py
</code></pre>
<p>However, I'd recommend looking into Python's excellent virtual environments that will allow you to create separate "environments" for each version of Python.  Virtual environments more or less work by handling all the path specification after you activate them, alllowing you to just type <code>python myfile.py</code> without worrying about conflicting dependencies or knowing the full path to a specific version of python.</p>
<p><a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/" rel="nofollow">Click here for an excellent guide to getting started with Virtual Environments</a> or <a href="https://docs.python.org/3/library/venv.html" rel="nofollow">[here]</a> for the Python3 official documentation.</p>
<p>If you do not have access to the nodes and you're running this using PySpark, you can specify the <a href="https://spark.apache.org/docs/latest/configuration.html#environment-variables" rel="nofollow">Python version in your <code>spark-env.sh</code></a>:</p>
<p>Spark_Install_Dir/conf/spark-env.sh:</p>
<pre><code>PYSPARK_PYTHON = /full/path/to/python_executable/eg/python2.7
</code></pre>
</div>
<span class="comment-copy">I tried your solution : I set <code>PYSPARK_PYTHON=/path/to/python</code> in<code>spark-env.sh</code>but it doesn't work. I use spark 1.3.1</span>
<span class="comment-copy">As far as I know, any environment variables should be set in the location I posted. Other parameters are added to the conf/spark-defaults.conf as key value pairs. I've changed the link in the answer to the 1.3.1 docs, which state the same as older versions. The submission script should load in both configs by default.</span>
<span class="comment-copy">Hi, I used Anaconda Python distribution with Pyspark and also set PYSPARK_PYTHON in spark-env.sh and everything worked fine. Also I set it similar way in my virtualenv configuration for local tests. I used a command like this <code>export PYSPARK_PYTHON="/usr/local/ml/anaconda/bin/python"</code>, maybe you have mistyped something?</span>
<span class="comment-copy">It works for <a href="https://spark.apache.org/docs/latest/configuration.html#environment-variables" rel="nofollow noreferrer">Pyspark 2.3.1</a> too.</span>
<span class="comment-copy">In the <a href="https://spark.apache.org/docs/latest/configuration.html#environment-variables" rel="nofollow noreferrer">documentation</a> linked in the post it says that "When running Spark on YARN in cluster mode, environment variables need to be set using the spark.yarn.appMasterEnv.[EnvironmentVariableName] property in your conf/spark-defaults.conf file". What I did was pass <code>--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=python2</code> and <code>--conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=python2</code> as options when calling <code>spark-submit</code>. That sorted it out for me.</span>
<span class="comment-copy">It will work only if the path to <code>python2.7</code> is in the <code>PATH</code> environment variable. It is better to use an absolute path to the Python executable.</span>
<span class="comment-copy">Yes I totally agree with that, actually forget to mention that in my answer since the PATH had already been setup for me. Thanks!</span>
<span class="comment-copy">actually I am working on hadoop cluster and I don't have access to DataNodes, So I cannot use virtaul env.</span>
<span class="comment-copy">I have already tried your first suggestion with shebang but it doesn't work. Actually, I am launching spark application not just a python file so I cannot do your second suggestion</span>
<span class="comment-copy">@user4851438 Ah, sorry about that - check up the update -- you can use a shell script to modify which python is used when running submitted scripts.</span>
<span class="comment-copy">I tried your suggestion but it doesn't work for me :/</span>
<span class="comment-copy">this doesnt work for me either. Still looking for a solution after a month.</span>
