<div class="post-text" itemprop="text">
<p>I am trying to avoid duplicates in my mp3 collection (quite large). I want to check for duplicates by checking file contents, instead of looking for same file name. I have written the code below to do this but it throws a MemoryError after about a minute. Any suggestions on how I can get this to work?</p>
<pre><code>import os
import hashlib

walk = os.walk('H:\MUSIC NEXT GEN')

mySet = set()
dupe  = []

hasher = hashlib.md5()

for dirpath, subdirs, files in walk:
    for f in files:
        fileName =  os.path.join(dirpath, f)
        with open(fileName, 'rb') as mp3:
            buf = mp3.read()
            hasher.update(buf)
            hashKey = hasher.hexdigest()
            print hashKey
            if hashKey in mySet:
                dupe.append(fileName)
            else:
                mySet.add(hashKey)


print 'Dupes: ' + str(dupe)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You probably have a huge file that can't be read at once like you try with <code>mp3.read()</code>. Read smaller parts instead. Putting it into a nice little function also helps keeping your main program clean. Here's a function I've been using myself for a while now (just slightly polished it now) for a tool probably similar to yours:</p>
<pre><code>import hashlib

def filehash(filename):
    with open(filename, mode='rb') as file:
        hasher = hashlib.md5()
        while True:
            buffer = file.read(1 &lt;&lt; 20)
            if not buffer:
                return hasher.hexdigest()
            hasher.update(buffer)
</code></pre>
<p><em>Update</em>: A <a href="https://docs.python.org/3/library/io.html#io.RawIOBase.readinto" rel="nofollow"><code>readinto</code></a> version:</p>
<pre><code>buffer = bytearray(1 &lt;&lt; 20)
def filehash(filename):
    with open(filename, mode='rb') as file:
        hasher = hashlib.md5()
        while True:
            n = file.readinto(buffer)
            if not n:
                return hasher.hexdigest()
            hasher.update(buffer if n == len(buffer) else buffer[:n])
</code></pre>
<p>With a 1GB file already cached in memory and ten attempts, this took on average 5.35 seconds. The <code>read</code> version took on average 6.07 seconds. In both versions, the Python process occupied about 10MB of RAM during the run.</p>
<p>I'll probably stick with the <code>read</code> version, as I prefer its simplicity and because in my real use cases, the data isn't already cached in RAM and I use sha256 (so the overall time goes up significantly and makes the little advantage of <code>readinto</code> even more irrelevant).</p>
</div>
<div class="post-text" itemprop="text">
<p><code>hasher.update</code> appends the content to the previous.  You may want to create a new <code>hasher</code> for each file</p>
</div>
<span class="comment-copy">use a database and store the filename in primary key field might be one way to do it</span>
<span class="comment-copy">One question is, does the MemoryError occur during the filesystem walk, or when you try to create a string from <code>dupe</code>? You might try <code>for name in dupe: print name</code> instead. Or even better, output dupes as you find them rather than storing them until the end.</span>
<span class="comment-copy">instead of <code>mp3.read()</code> read in smaller chunks, say 1 meg.</span>
<span class="comment-copy">Hashing MP3 files will only find <i>exact</i> binary duplicates. The same song but a couple of seconds shorter or encoded with different parameters or volume will give a different hash value.</span>
<span class="comment-copy">How many mp3 files are we talking about here?  Like more than 4 billion?</span>
<span class="comment-copy">Note also that this fixes OP's other bug where a single hasher is updated instead of creating a new one each time.</span>
<span class="comment-copy">a great observation, but doesn't answer the question.</span>
<span class="comment-copy">Why not?  the hasher is accumulating the contents of all the files and concatinating them together, which is causing the out of memory error, if he resets the hasher each time that won't happen and the memory problem will be solved</span>
<span class="comment-copy">No, the hasher keeps a fixed size hash and discards hashed content.</span>
<span class="comment-copy">@Malvolio - that's not how hashers work.</span>
<span class="comment-copy">@EricRenouf - the job of a hasher is to condense a large blob of data into a small hash. It does this by shifting around small chunks of data, blending them into the existing hash, discarding the data and repeating with the next chunk.</span>
