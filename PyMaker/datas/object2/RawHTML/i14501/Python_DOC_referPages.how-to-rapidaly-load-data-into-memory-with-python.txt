<div class="post-text" itemprop="text">
<p>I have a large csv file (5 GB) and I can read it with <code>pandas.read_csv()</code>. This operation takes a lot of time 10-20 minutes. </p>
<p>How can I speed it up? </p>
<p>Would it be useful to transform the data in a <code>sqllite</code> format? In case what should I do?</p>
<p>EDIT: More information:
The data contains 1852 columns and 350000 rows. Most of the columns are float65 and contain numbers. Some other contains string or dates (that I suppose are considered as string)</p>
<p>I am using a laptop with 16 GB of RAM and SSD hard drive. The data should fit fine in memory (but I know that python tends to increase the data size)</p>
<p>EDIT 2 : </p>
<p>During the loading I receive this message</p>
<pre><code>/usr/local/lib/python3.4/dist-packages/pandas/io/parsers.py:1164: DtypeWarning: Columns (1841,1842,1844) have mixed types. Specify dtype option on import or set low_memory=False.
  data = self._reader.read(nrows)
</code></pre>
<p>EDIT: SOLUTION</p>
<p>Read one time the csv file and save it as 
<code>data.to_hdf('data.h5', 'table')</code>
This format is incredibly efficient</p>
</div>
<div class="post-text" itemprop="text">
<p>This actually depends on which part of reading it is taking 10 minutes.</p>
<ul>
<li>If it's actually reading from disk, then obviously any more compact form of the data will be better.</li>
<li>If it's processing the CSV format (you can tell this because your CPU is at near 100% on one core while reading; it'll be very low for the other two), then you want a form that's already preprocessed.</li>
<li>If it's swapping memory, e.g., because you only have 2GB of physical RAM, then nothing is going to help except splitting the data.</li>
</ul>
<p>It's important to know which one you have. For example, stream-compressing the data (e.g., with <code>gzip</code>) will make the first problem a lot better, but the second one even worse.</p>
<p>It sounds like you probably have the second problem, which is good to know. (However, there are things you can do that will probably be better no matter what the problem.)</p>
<hr/>
<p>Your idea of storing it in a sqlite database is nice because it can at least potentially solve all three at once; you only read the data in from disk as-needed, and it's stored in a reasonably compact and easy-to-process form. But it's not the best possible solution for the first two, just a "pretty good" one.</p>
<p>In particular, if you actually do need to do array-wide work across all 350000 rows, and can't translate that work into SQL queries, you're not going to get much benefit out of sqlite. Ultimately, you're going to be doing a giant <code>SELECT</code> to pull in all the data and then process it all into one big frame.</p>
<hr/>
<p>Writing out the shape and structure information, then writing the underlying arrays in NumPy binary form. Then, for reading, you have to reverse that. NumPy's binary form just stores the raw data as compactly as possible, and it's a format that can be written blindingly quickly (it's basically just dumping the raw in-memory storage to disk). That will improve both the first and second problems.</p>
<hr/>
<p>Similarly, storing the data in HDF5 (either using Pandas IO or an external library like PyTables or h5py) will improve both the first and second problems. HDF5 is designed to be a reasonably compact and simple format for storing the same kind of data you usually store in Pandas. (And it includes optional compression as a built-in feature, so if you know which of the two you have, you can tune it.) It won't solve the second problem quite as well as the last option, but probably well enough, and it's much simpler (once you get past setting up your HDF5 libraries).</p>
<hr/>
<p>Finally, pickling the data may sometimes be faster. <a href="https://docs.python.org/3/library/pickle.html" rel="nofollow"><code>pickle</code></a> is Python's native serialization format, and it's hookable by third-party modules—and NumPy and Pandas have both hooked it to do a reasonably good job of pickling their data.</p>
<p>(Although this doesn't apply to the question, it may help someone searching later: If you're using Python 2.x, make sure to explicitly use <a href="https://docs.python.org/2.7/library/pickle.html#data-stream-format" rel="nofollow">pickle format 2</a>; IIRC, NumPy is very bad at the default pickle format 0. In Python 3.0+, this isn't relevant, because the default format is at least 3.)</p>
</div>
<div class="post-text" itemprop="text">
<p>Python has <a href="https://docs.python.org/2/library/pickle.html" rel="nofollow">two built-in libraries</a> called <code>pickle</code> and <code>cPickle</code> that can store any Python data structure. 
<code>cPickle</code> is identical to <code>pickle</code> except that <code>cPickle</code> has trouble with Unicode stuff and is 1000x faster. 
Both are really convenient for saving stuff that's going to be re-loaded into Python in some form, since you don't have to worry about some kind of error popping up in your file I/O.</p>
<p>Having worked with a number of XML files, I've found some performance gains from loading pickles instead of raw XML. I'm not entirely sure how the performance compares with CSVs, but it's worth a shot, especially if you don't have to worry about Unicode stuff and can use <code>cPickle</code>. It's also simple, so if it's not a good enough boost, you can move on to other methods with minimal time lost.</p>
<p>A simple example of usage:</p>
<pre><code>&gt;&gt;&gt; import pickle
&gt;&gt;&gt; stuff = ["Here's", "a", "list", "of", "tokens"]
&gt;&gt;&gt; fstream = open("test.pkl", "wb")
&gt;&gt;&gt; pickle.dump(stuff,fstream)
&gt;&gt;&gt; fstream.close()
&gt;&gt;&gt; 
&gt;&gt;&gt; fstream2 = open("test.pkl", "rb")
&gt;&gt;&gt; old_stuff = pickle.load(fstream2)
&gt;&gt;&gt; fstream2.close()
&gt;&gt;&gt; old_stuff
["Here's", 'a', 'list', 'of', 'tokens']
&gt;&gt;&gt; 
</code></pre>
<p>Notice the "b" in the file stream openers. This is important--it preserves cross-platform compatibility of the pickles. I've failed to do this before and had it come back to haunt me. </p>
<p>For your stuff, I recommend writing a first script that parses the CSV and saves it as a pickle; when you do your analysis, the script associated with that loads the pickle like in the second block of code up there.</p>
<p>I've tried this with XML; I'm curious how much of a boost you will get with CSVs.</p>
</div>
<div class="post-text" itemprop="text">
<p>If the problem is in the processing overhead, then you can divide the file into smaller files and handle them in different CPU cores or threads. Also for some algorithms the python time will increase non-linearly and the dividing method will help in these cases.</p>
</div>
<span class="comment-copy">With a vague question that tells us nothing about the format other than that it's "CSV" and reading it is slow, it's hard to give you an answer that isn't just as vague. For example, if sqlite is the answer, I can't show you what SQL code to generate without knowing what the columns are, and what kinds of searches you need to do.</span>
<span class="comment-copy">It's really not that vague of a question. They have to load a file over-and-over again, so how can that be done faster?</span>
<span class="comment-copy">From EDIT 2: The warning is actually giving you some useful information. You're making it guess at the dtypes for the columns; if you actually know them, you may get better performance <i>and</i> more correct or useful results. And I don't know what that <code>low_memory=False</code> flag means, but it certainly sounds like something work looking up in the docs, because it could be helpful here.</span>
<span class="comment-copy">@Dan: It's not vague enough to close it instead of just commenting and writing an answer. But if he gave more information, I (or someone else) might be able to write a better answer, which is why I commented.</span>
<span class="comment-copy">I think the delay is due to the processing  because only one cpu works at 100%.  What can I do in this case?</span>
<span class="comment-copy">OK, then you just want a format that requires less processing. As the answer explains, NumPy's binary array format (plus wrapper information storing the array shape and Pandas structure) and HDF5 both qualify.</span>
<span class="comment-copy">First, <code>cPickle</code> is Python 2-specific, and the OP is on 3.4, which instead just has one <code>pickle</code> library and automatically uses the C accelerator if appropriate. And it actually doesn't usually help much with NumPy (and therefore Pandas) data, because NumPy basically does most of the work itself. Using format 2 vs. 0 is much more important—but again, that's not relevant to 3.4, where the default format is 3, not 0.</span>
<span class="comment-copy">Second, XML is actually expensive to parse; CSV really isn't. But you're definitely right that this is worth trying; the worst possible thing that can happen is he wastes 30 minutes testing it (or, if he's smart, he wastes 5 minutes preparing a smaller data set and testing that), and surely that's worth the potential long-term savings if it helps and is good enough.</span>
