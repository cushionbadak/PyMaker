<div class="post-text" itemprop="text">
<p>I'd really like to create a <code>numpy</code> array from a csv file, however, I'm having issues when the file is ~50k lines long (like the MNIST training set). The file I'm trying to import looks something like this: </p>
<pre><code>0.0,0.0,0.0,0.5,0.34,0.24,0.0,0.0,0.0
0.0,0.0,0.0,0.4,0.34,0.2,0.34,0.0,0.0
0.0,0.0,0.0,0.34,0.43,0.44,0.0,0.0,0.0
0.0,0.0,0.0,0.23,0.64,0.4,0.0,0.0,0.0
</code></pre>
<p>It works fine for something thats 10k lines long, like the validation set: </p>
<pre><code>import numpy as np
csv = np.genfromtxt("MNIST_valid_set_data.csv",delimiter = ",")
</code></pre>
<p>If I do the same with the training data (larger file), I'll get a c-style segmentation fault. Does anyone know any better ways besides breaking the file up and then piecing it together?</p>
<p>The end result is that I'd like to pickle the arrays into a similar <code>mnist.pkl.gz</code> file but I can't do that if I can't read in the data. </p>
<p>Any help would be greatly appreciated. </p>
</div>
<div class="post-text" itemprop="text">
<p>I think you really want to track down the actual problem and solve it, rather than just work around it, because I'll bet you have other problems with your NumPy installation that you're going to have to deal with eventually.</p>
<p>But, since you asked for a workaround that's better than manually splitting the files, reading them, and merging them, here are two:</p>
<hr/>
<p>First, you can split the files programmatically and dynamically, instead of manually. This avoids wasting a lot of your own human effort, and also saves the disk space needed for those copies, even though it's conceptually the same thing you already know how to do.</p>
<p>As the <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html" rel="nofollow"><code>genfromtxt</code></a> docs make clear, the <code>fname</code> argument can be a pathname, or a file object (open in <code>'rb'</code> mode), or just a generator of lines (as <code>bytes</code>). Of course a file object is itself a generator of lines, but so is, say, an <code>islice</code> of a file object, or a group from a <code>grouper</code>. So:</p>
<pre><code>import numpy as np
from more_itertools import grouper

def getfrombigtxt(fname, *args, **kwargs):
    with open(fname, 'rb') as f:
        return np.vstack(np.genfromtxt(group, *args, **kwargs) 
                         for group in grouper(f, 5000, b''))
</code></pre>
<p>If you don't want to install <code>more_itertools</code>, you can also just copy the 2-line <code>grouper</code> implementation from the <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow">Recipes</a> section of the <code>itertools</code> docs, or even inline zipping the iterators straight into your code.</p>
<hr/>
<p>Alternatively, you can parse the CSV file with the stdlib's <a href="https://docs.python.org/3/library/csv.html" rel="nofollow"><code>csv</code></a> module instead of with NumPy:</p>
<pre><code>import csv
import numpy as np

def getfrombigtxt(fname, delimiter=','):
    with open(fname, 'r') as f: # note text mode, not binary
        rows = (list(map(float, row)) for row in csv.reader(f))
        return np.vstack(rows)
</code></pre>
<p>This is obviously going to be a lot slower… but if we're talking about turning 50ms of processing into 1000ms, and you only do it once, who cares?</p>
</div>
<span class="comment-copy">This is only about 450000 floats, which should take only ~3.6MB, which shouldn't be a problem on any platform that can run NumPy in the first place which makes me think maybe there's a problem with your NumPy installation. What Python and NumPy versions do you have, what platform are you on, and how did you install them?</span>
<span class="comment-copy">If you just want a workaround, breaking up the file can be done dynamically rather than on-disk; just <code>genfromtxt</code> against file-like objects holding each batch 10K lines instead of against a filename (and then you can just <code>stack</code> them together in a one-liner). But you shouldn't need one.</span>
<span class="comment-copy">Another workaround is to use the stdlib <code>csv</code> module to read the file into an iterable of rows, and use <code>fromiter</code> to convert it into an arrow. That will be a lot slower… but who cares?</span>
