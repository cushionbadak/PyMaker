<div class="post-text" itemprop="text">
<p>I have a weird problem. 
I'm loading a huge file (3.5G) and making a dictionary out of it and do some processing. 
After everything is finished, my script doesn't terminate immediately, it terminates after some time. 
I think it might be due to memory freeing , what can be other reasons ?? I'd appreciate any opinion. And how can I make my script run faster? </p>
<p>Here's the corresponding code:</p>
<p>class file_processor:</p>
<pre><code>    def __init__(self):
            self.huge_file_dict = self.upload_huge_file()


    def upload_huge_file(self):
            d = defaultdict(list)
            f=  codecs.open('huge_file', 'r',  encoding='utf-8').readlines()
            for line in f:
                    l = line.strip()
                    x,y,z,rb,t = l.split()
                    d[rb].append((x,y,z,t))
            return d

    def do_some_processing(self, word):
           if word in self.huge_file_dict:
                    do sth with  self.huge_file_dict[word]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>My guess is that your horrible slowdown, which doesn't recover until after your program is finished, is caused by using more memory than you actually have, which causes your OS to start swapping VM pages in and out to disk. Once you get enough swapping happening, you end up in "swap hell", where a large percentage of your memory accesses involve a disk read and even a disk write, which takes orders of magnitude more time, and your system won't recover until a few seconds after you finally free up all that memory.</p>
<p>The obvious solution is to not use so much memory.</p>
<p><a href="https://stackoverflow.com/a/30083236/908494">tzaman's answer</a>, avoiding <code>readlines()</code>, will eliminate some of that memory. A giant list of all the lines in a 3.5GB file has to take at least 3.5GB on Python 3.4 or 2.7 (but realistically at least 20% more than that) and maybe 2x or 4x on 3.0-3.3.</p>
<p>But the dict is going to be even bigger than the list, and you need that, right?</p>
<p>Well, no, you probably don't. Keeping the dict on-disk and fetching the values as-needed may sound slow, but it may still be a lot faster than keeping it in virtual memory, if that virtual memory has to keep swapping back and forth to disk.</p>
<p>You may want to consider using a simple <a href="https://docs.python.org/3/library/dbm.html" rel="nofollow noreferrer"><code>dbm</code></a>, or a more powerful key-value database (google "NoSQL key value" for some options), or a <a href="https://docs.python.org/3/library/sqlite3.html" rel="nofollow noreferrer"><code>sqlite3</code></a> database, or even a server-based SQL database like MySQL.</p>
<p>Alternatively, if you can keep everything in memory, but in a more compact form, that's the best of both worlds.</p>
<p>I notice that in your example code, the only thing you're doing with the dict is checking <code>word in self.huge_file_dict</code>. If that's true, then you can use a <code>set</code> instead of a <code>dict</code> and not keep all those values around in memory. That should cut your memory use by about 80%.</p>
<p>If you frequently need the keys, but occasionally need the values, you might want to consider a dict that just maps the keys to indices into something you can read off disk as needed (e.g., a file with fixed-length strings, which you can then <code>mmap</code> and slice).</p>
<p>Or you could stick the values in a Pandas frame, which will be a little more compact than native Python storage—maybe enough to make the difference—and use a dict mapping keys to indices.</p>
<p>Finally, you may be able to reduce the amount of swapping without actually reducing the amount of memory. Bisecting a giant sorted list, instead of accessing a giant dict, may—depending on the pattern of your words—give much better memory locality.</p>
</div>
<div class="post-text" itemprop="text">
<p>Don't call <code>.readlines()</code> -- that loads the entire file into memory beforehand. You can just iterate over <code>f</code> directly and it'll work fine.</p>
<pre><code>with codecs.open('huge_file', 'r',  encoding='utf-8') as f:
    for line in f:
        ...
</code></pre>
</div>
<span class="comment-copy">How do you know it's done? Maybe it's still processing the file since it's so large.</span>
<span class="comment-copy">How much physical RAM do you have? Reading a 3.5GB file into a dict will probably take somewhere on the order of 7-14GB. If you've only got, say, 8GB, that means your OS will go into swap hell, and it won't recover until a short time after you free up that memory… which sounds like exactly what you're seeing.</span>
<span class="comment-copy">@user1823  , I put a print "loaded"  in upload_huge_file() just before return. Also do_some_processing reads words from a line and print some stuff, so everything is printed.</span>
<span class="comment-copy">@abarnert  here's the output of cat /proc/meminfo/  MemTotal:       264106712 kB  MemFree:        80306872 kB</span>
<span class="comment-copy">Does your system really have 250GB of RAM?</span>
<span class="comment-copy">Comprehensive and excellent. +1!</span>
<span class="comment-copy">Thanks for the answer, I tried that too, but  in that case  making the dictionary took so much time. I want to make the dict fast and terminate fast:))</span>
<span class="comment-copy">@tzaman - it will work a little better. The contents of the file are still in memory, now it is in dictionary form as opposed to dictionary and string.</span>
<span class="comment-copy">@MartinKonecny well, yeah. I'd suggest retooling the whole thing but that's beyond the scope of this question.</span>
<span class="comment-copy">Can you explain how to retool ? I tried pickling the dictionary, but that takes a lot of time too. (I don't know why). I'd love to hear the ideas:))</span>
<span class="comment-copy">This probably won't actually <i>fix</i> your problem, only cut it roughly in half… but that's still worth doing, isn't it?</span>
