<div class="post-text" itemprop="text">
<p>I am making a program the downloads a large file, and I have added in a feature with which the program determines what percentage has been downloaded and informs the user each time another 10% was downloaded and at what time (ie, <code>print (str(percent) + " downloaded at " + str(time))</code>) When I was testing the program on smaller files, however, I noticed it was far less accurate. Here is a sample program I made:</p>
<pre><code>import urllib.request

def printout(a, b, c):
    print(str(a) + ", " + str(b) + ", " + str(c))

urllib.request.urlretrieve("http://downloadcenter.mcafee.com/products/tools/foundstone/fport.zip", r"C:\Users\Username\Downloads\fport.zip", reporthook = printout)
</code></pre>
<p>This downloads Fport, a tool I was going to download anyway. Anyway, I got this output:</p>
<pre><code>0, 8192, 57843
1, 8192, 57843
2, 8192, 57843
3, 8192, 57843
4, 8192, 57843
5, 8192, 57843
6, 8192, 57843
7, 8192, 57843
8, 8192, 57843
</code></pre>
<p>Which I thought was exactly what I wanted. I was about to put it in when I noticed a little error. 8192 doesn't go into 57843. Not 8 times. I plugged it into a calculator and discovered that, in fact, it goes in approximately 7 times. Which is a rather large difference, considering. This disconnect affects bigger files less, but it is still there. Is this some kind of metadata or header? If so, it's rather large, isn't it? Is there a way I can account for it (ie, is it always about 16000 bytes)?</p>
</div>
<div class="post-text" itemprop="text">
<p>So, if you look at the <code>Lib/urllib/request.py</code> (CPython around 2.7) code it becomes clear why this is the case:</p>
<pre><code>    with tfp:
        result = filename, headers
        bs = 1024*8  # we read 8KB at a time.
        size = -1
        read = 0
        blocknum = 0
        if "content-length" in headers:
            size = int(headers["Content-Length"])

        if reporthook:
            reporthook(blocknum, bs, size)

        while True:
            block = fp.read(bs)  # here is where we do the read
            if not block:
                break
            read += len(block)
            tfp.write(block)
            blocknum += 1
            if reporthook:
                reporthook(blocknum, bs, size)
</code></pre>
<p>In the last line, the reporthook is told that <code>bs</code> was read, not <code>len(block)</code>, which would probably be more accurate. I'm not sure why this is the case, i.e. if there's a good reason or if it's a minor bug in the library. You could ask on the Python mailers and/or file a bug of course.</p>
<p>Note: I think it's fairly common to read data in fixed-sized blocks, see for example <a href="http://www.cplusplus.com/reference/cstdio/fread/" rel="nofollow"><code>fread</code></a>. There, the return value may not be the same as the number of bytes requested to be read if an EOF (end of file) was encountered, which is similar in the Python <a href="http://docs.python.org/2/library/stdtypes.html?highlight=read#file.read" rel="nofollow"><code>read</code></a> API.</p>
</div>
<div class="post-text" itemprop="text">
<p>The documentation explains that <a href="http://docs.python.org/3/library/urllib.request.html#urllib.request.URLopener.retrieve" rel="nofollow"><code>reporthook</code></a> is called once per "chunk", with a chunk number and total size.</p>
<p><code>urllib.request</code> will not try to make chunk sizes exactly equal; it will try to make chunk sizes a nice power of 2 like 8192, because that's generally fastest and simplest.</p>
<p>So, what you want to do is use the actual bytes for calculating percentage, not the chunk numbers.</p>
<hr/>
<p>The <code>urlretrieve</code> interface doesn't give you an easy way to get the actual bytes. Counting blocks only works if you assume every <code>socket.recv(n)</code> (but the last) actually returns n bytes, which isn't guaranteed. <code>os.stat(filename)</code> only works (on most platforms) if you assume <code>urlretrieve</code> uses unbuffered files or flushes before every call, which again isn't guaranteed.</p>
<p>This is one of the many reasons not to use the "legacy interface".</p>
<p>The high-level interface (just calling <code>urllib.request.urlopen</code> and using the <code>Response</code> as a file object) may look like it's providing less information than <code>urlretrieve</code>, but if you read <a href="http://docs.python.org/3/library/urllib.request.html#urllib-request-restrictions" rel="nofollow"><code>urllib.request</code> Restrictions</a>, it makes it pretty clear that this is an illusion. So, you could just use <code>urlopen</code>, in which case you're just copying from one file object to another instead of using a limited callback interface, so you can use any file-object-copying functions you like, or write your own:</p>
<pre><code>def copy(fin, fout, flen=None):
    sofar = 0
    while True:
        buf = fin.read(8192)
        if not buf:
            break
        sofar += len(buf)
        if flen:
            print('{} / {} bytes'.format(sofar, flen))
        fout.write(buf)
    print('All done')

r = urllib.request.urlopen(url)
with open(path, 'wb') as f:
    copy(r, f, r.headers.get('Content-Length'))
</code></pre>
<p>If you really want something that hooks into the lower-level guts of <code>urllib</code>, then <code>urlretrieve</code> is not that something; it just fakes it. You'll have to create your own opener subclass and the whole mess that goes with it.</p>
<p>If you want an interface that's almost as simple as <code>urlopen</code> but provides as much functionality as a custom opener… well, <code>urllib</code> doesn't have that, which is why third-party modules like <a href="http://python-requests.org" rel="nofollow"><code>requests</code></a> exist.</p>
</div>
<div class="post-text" itemprop="text">
<p><code>urllib.request</code>'s high-level interface really isn't suitable for what you're trying to do. You can use the lower-level interfaces… but really, this is one of those things that the third-party library <a href="http://www.python-requests.org/" rel="nofollow"><code>requests</code></a> makes an order of magnitude simpler. (You don't have to use <code>requests</code>—the various <code>curl</code> wrappers, for example, also make it easier than <code>urllib</code>. But <code>requests</code> is the most <code>urllib</code>-like and the simplest of the third-party alternatives.)</p>
<p><code>requests</code> can work like <code>urllib</code> and pull everything down automatically, but by just adding <a href="http://www.python-requests.org/en/latest/user/advanced/#streaming-requests" rel="nofollow"><code>stream=True</code></a> you can take control of pulling the data. There are a few different interfaces to it (decoded Unicode lines, bytes lines, raw data off the socket, etc.), but <a href="http://www.python-requests.org/en/latest/user/advanced/#body-content-workflow" rel="nofollow"><code>iter_content</code></a> is probably the one you want—it gives you chunks of content on demand, buffering appropriately, transparently mapping chunked transfer mode to flat transfers, dealing with 100 continue, … basically everything HTTP can throw at you. So:</p>
<pre><code>with open(path, 'wb') as f:
    r = requests.get(url, stream=True)
    for chunk in r.iter_content(8192):
        f.write(chunk)
</code></pre>
<p>Adding progress still needs to be done manually. But since you're pulling chunks rather than having them saved to a file behind your back, you know exactly how many bytes you've seen. And, as long as the server has supplied a <code>Content-Length</code> header (which some servers won't do in some cases, but there's nothing you can do about that except deal with it), it's easy:</p>
<pre><code>with open(path, 'wb') as f:
    r = requests.get(url, stream=True)
    total = r.headers.get('content-length')
    sofar = 0
    for chunk in r.iter_content(8192):
        f.write(chunk)
        sofar += len(chunk)
        if total:
            print('{} / {}: {}%'.format(sofar, total, sofar*100.0/total))
        else:
            print('{} / ???: ???%'.format(sofar))
</code></pre>
</div>
<span class="comment-copy">As a side note, why are you using the legacy interface in the first place?</span>
<span class="comment-copy">@abarnert Not sure what you mean by legacy interface... Do you mean <code>urllib</code> instead of <code>urllib2</code>?</span>
<span class="comment-copy">No, I mean <code>urllib.request.urlretrieve</code>, which is only documented as part of the <a href="http://docs.python.org/3/library/urllib.request.html#legacy-interface" rel="nofollow noreferrer">legacy interface</a>, which "might become deprecated at some point in the future".</span>
<span class="comment-copy">Oh. Thanks for telling me, that could have been an unpleasant surprise... whereas now it will be merely unpleasant. I hope they keep reporthook though, it's a real time saver.</span>
<span class="comment-copy">The problem is that <code>reporthook</code> doesn't actually do what you want to—under some circumstances <code>url retrieve</code> downloads everything into memory; it never notifies you about the number of bytes seen; since it deals in pre-decoding instead of post-decoding bytes it wouldn't help anyway; etc. They <i>could</i> fix that, but from what I understand, the plan is to build a new <code>requests</code>-like API for 3.5, then later phase out <code>urlretrieve</code> and the rest of the "legacy" interface, and later still phase out the current "modern" interface. So, if you can just wait 2-4 years to write this script… :)</span>
<span class="comment-copy">Between the two answers to this question I know why this is causing my problem and generally how to fix it, but could you be a bit more specific as to how I might find the "actual bytes" as opposed to the chunk numbers? Thanks</span>
<span class="comment-copy">@user2945577: Give me a sec and I'll write something up.</span>
<span class="comment-copy">Wow, this is great detail. I don't think urlopen is suitable for downloading the "large file" mentioned in my post though, so I guess it'll be another module download once urlretrieve dies... sigh.</span>
