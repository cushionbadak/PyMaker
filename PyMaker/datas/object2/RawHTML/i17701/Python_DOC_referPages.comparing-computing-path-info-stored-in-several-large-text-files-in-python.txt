<div class="post-text" itemprop="text">
<p>I have a bunch of flat files that basically store millions of paths and their corresponding info (name, atime, size, owner, etc)
I would like to compile a full list of all the paths stored collectively on the files.  For duplicate paths only the largest path needs to be kept.  </p>
<p>There are roughly 500 files and approximately a million paths in the text file.  The files are also gzipped.  So far I've been able to do this in python but the solution is not optimized as for each file it basically takes an hour to load and compare against the current list.  </p>
<p>Should I go for a database solution?  sqlite3?  Is there a data structure or better algorithm to go about this in python?  Thanks for any help!</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>So far I've been able to do this in python but the solution is not optimized as for each file it basically takes an hour to load and compare against the current list.</p>
</blockquote>
<p>If "the current list" implies that you're keeping track of all of the paths seen so far in a <code>list</code>, and then doing <code>if newpath in listopaths:</code> for each line, then each one of those searches takes linear time. If you have 500M total paths, of which 100M are unique, you're doing O(500M*100M) comparisons.</p>
<p>Just changing that <code>list</code> to a <code>set</code>, and changing nothing else in your code (well, you need to replace <code>.append</code> with <code>.add</code>, and you can probably remove the <code>in</code> check entirely… but without seeing your code it's hard to be specific) makes each one of those checks take constant time. So you're doing O(500M) comparisons—100M times faster.</p>
<hr/>
<p>Another potential problem is that you may not have enough memory. On a 64-bit machine, you've got enough <em>virtual</em> memory to hold almost anything you want… but if there's not enough <em>physical</em> memory available to back that up, eventually you'll spend more time swapping data back and forth to disk than doing actual work, and your program will slow to a crawl.</p>
<p>There are actually two potential sub-problems here.</p>
<p>First, you might be reading each entire file in at once (or, worse, <em>all</em> of the files at once) when you don't need to (e.g., by decompressing the whole file instead of using <code>gzip.open</code>, or by using <code>f = gzip.open(…)</code> but then doing <code>f.readlines()</code> or <code>f.read()</code>, or whatever). If so… don't do that. Just iterate over the lines in each <code>GzipFile</code>, <code>for line in f:</code>.</p>
<p>Second, maybe even a simple <code>set</code> of however many unique lines you have is too much to fit in memory on your computer. In that case, you probably want to look at a database. But you don't need anything as complicated as sqlite. A <a href="http://docs.python.org/3/library/dbm.html" rel="nofollow"><code>dbm</code></a> acts like a <code>dict</code> (except that its keys and values have to be byte strings), but it's stored on-dict, caching things in memory where appropriate, instead of stored in memory, paging to disk randomly, which means it will go a lot faster in this case. (And it'll be persistent, too.) Of course you want something that acts like a <code>set</code>, not a <code>dict</code>… but that's easy. You can model a set as a <code>dict</code> whose keys are always <code>''</code>. So instead of <code>paths.add(newpath)</code>, it's just <code>paths[newpath] = ''</code>. Yeah, that wastes a few bytes of disk space over building your own custom on-disk key-only hash table, but it's unlikely to make any significant difference.</p>
</div>
<span class="comment-copy">I don't understand the problem. "For duplicate paths only the largest path needs to be kept". If they're duplicates, how could one by larger than the others? Can you give us a very stripped-down concrete example (like "Here are 2 3-line input files, and here's the 4-line output file that I want")?</span>
<span class="comment-copy">Also, how much memory do you have/are you willing to require? Because 500M strings is just pushing the edge of what a typical computer in early 2014 can handle all at once. If you have enough to store everything in a set or dict or even sqlite3 table in memory, that will be a whole lot faster (and possibly simpler) than anything on-disk, but if you don't have the memory, an answer like that would be very useful.</span>
<span class="comment-copy">Finally, please show the code (or, ideally, a stripped-down version that works as a <a href="http://stackoverflow.com/help/mcve">minimal, complete, valid example</a>) for your working-but-too-slow solution.</span>
<span class="comment-copy">Thanks abarnert   <code>#lets say the input files are: a.txt  pathone   100 david 2014-01-01 pathtwo   200 david 2014-01-01  b.txt pathone   314 david 2014-01-02 pathtwo   200 david 2014-01-01 paththree 400 david 2014-01-02  #The output file should be: c.txt pathone  314 david 2014-01-02 (because it chose the largest version of the path) pathtwo   200 david 2014-01-01 paththree 400 david 2014-01-02 </code></span>
<span class="comment-copy">In terms of memory, I don't know, 16gb?  I'm not sure if I have enough memory, but ideally this wouldn't need to be on a fancy computer, so I'll probably need to perform this in chunks or a database.</span>
<span class="comment-copy">Thanks abarnert, I'll take a look and try it out and let you know how things turn out</span>
