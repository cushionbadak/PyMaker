<div class="post-text" itemprop="text">
<p>How does one go about (asides from trail and error) to determine the amount of RAM required to store one's dataset?</p>
<p>I know this is a super general question, so, hopefully this example can narrow down what I am trying to understand:</p>
<p>I have a data file, the data file contains characters[A-Z] and numbers (no special symbols). I want to read the data into RAM (using python), then I store the data in a dictionary. I have a lot of data and computer with only 2 gigs of RAM, so I'd like to know ahead of time whether the data would fit into RAM as this could change the way I load the file with Python and handle the data downstream.  I recognize that all the data may not all fit into RAM - but that's another problem, I just want to know how much RAM the data would take up and what I  need to consider to make this determination.</p>
<p>So knowing the content of my file, it's initial size, and the downstream data structure I want to use, how can I figure out the amount of RAM the data will take-up?</p>
</div>
<div class="post-text" itemprop="text">
<p>The best thing to do here is not to try to guess, or to read the source code and write up a rigorous proof, but to do some tests. There are a lot of complexities that make these things hard to predict. For example, if you have 100K copies of the same string, will Python store 100K copies of the actual string data, or just 1? It depends on your Python interpreter and version, and all kinds of other things.</p>
<p>The documentation for <a href="http://docs.python.org/3/library/sys.html#sys.getsizeof" rel="nofollow"><code>sys.getsizeof</code></a> has a link to a <a href="http://code.activestate.com/recipes/577504" rel="nofollow">recursive sizeof recipe</a>. And that's exactly what you need to measure how much storage your data structure is using.</p>
<p>So load in, say, the first 1% of your data and see how much memory it uses. Then load in 5% and make sure it's about 5x as big. If so, you can guess that your full data will be 20x as big again.</p>
<p>(Obviously this doesn't work for all conceivable data—there are some objects that have more cross-links the farther you get into the file, others—like numbers—that might just get larger, etc. But it will work for a lot of realistic kinds of data. And if you're really worried, you can always test the <em>last</em> 5% against the first 5% and see how they differ, right?)</p>
<p>You can also test at a higher level by using modules like <a href="http://guppy-pe.sourceforge.net/#Heapy" rel="nofollow">Heapy</a>, or completely externally by just watching with Process Manager/Activity Monitor/etc., to double-check the results. One thing to keep in mind is that many of these external measures will show you the <em>peak</em> memory usage of your program, not the <em>current</em> memory usage. And it's not even clear what you'd want to call "current memory usage" anyway. (Python rarely releases memory back to the OS. If it leaves memory unused, it will likely get paged out of physical memory by the OS, but the VM size won't come down. Does that count as in-use to you, or not?)</p>
</div>
<span class="comment-copy">Will it be a single dictionary or multiple dictionaries? I think it shouldn't be far from the file size.</span>
<span class="comment-copy">A single dictionary</span>
<span class="comment-copy">Check out <a href="http://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended" title="which python memory profiler is recommended">stackoverflow.com/questions/110259/…</a> - more specifically, heapy</span>
<span class="comment-copy">@user4673 the data type does matter, and can save you space as you point out (for instance a factor of 4 between 32bit and 8bit integers. You might want to have a look at using a numpy array if part of your dataset is purely numeric (obviously you have some strings, but perhaps not every entry is a string?) numpy will likely be faster when you come to access/analyse the data too. Perhaps you could provide a sample or your dataset in the question?</span>
<span class="comment-copy">@user4673: Python doesn't have 8-bit ints or 32-bit ints; it has arbitary-length ints that take up anywhere from 16 bits to 128Gbits depending on how big they are. More importantly, every Python object, including an integer, is "boxed", with a header that's dozens of bytes long. On the other hand, Python can also collapse immutable built-in types, so if you have 1000000 copies of the number <code>1</code>, you probably only <i>really</i> have 1 copy of the number <code>1</code> (and 1000000 references to it, but a reference is only the same of a pointer).</span>
