<div class="post-text" itemprop="text">
<p><strong>In a Nutshell</strong></p>
<p>I want to change complex python objects concurrently, whereby each object is processed by a single process only. How can I do this (most efficiently)? Would implementing some kind of pickling support help? Would that be efficient?</p>
<p><strong>Full Problem</strong></p>
<p>I have a python data structure <code>ArrayDict</code> that basically consists of a <code>numpy</code> array and a dictionary and maps arbitrary indices to rows in the array. In my case, all keys are integers.</p>
<pre><code>a = ArrayDict()

a[1234] = 12.5
a[10] = 3

print(a[1234])                               #12.5
print(a[10])                                 # 3.0

print(a[1234] == a.array[a.indexDict[1234]]) #true
</code></pre>
<p>Now I have multiple such <code>ArrayDict</code>s and want to fill them in <code>myMethod(arrayDict, params)</code>. Since <code>myMethod</code> is expensive, I want to run it in parallel. Note that <code>myMethod</code> may add many rows to <code>arrayDict</code>. Every process alters its own <code>ArrayDict</code>. I do not need concurrent access to the <code>ArrayDict</code>s.</p>
<p>In <code>myMethod</code>, I change entries in the <code>arrayDict</code> (that is, I change the internal <code>numpy</code> array), I add entries to the <code>arrayDict</code> (that is, I add another index to the dictionary and write a new value in the internal array). Eventually, I would like to be able to exchange <code>arrayDict</code>'s internal <code>numpy</code> array when it becomes too small. This does not happen often and I could perform this action in the non-parallel part of my program, if no better solution exists. My own attempts were not successful even without the array exchange.</p>
<p>I have spent days researching on shared memory and python's <a href="https://docs.python.org/3.6/library/multiprocessing.html" rel="nofollow noreferrer">multiprocessing</a> module. Since I will finally be working on linux, the task seemed to be rather simple: the system call <code>fork()</code> allows to work with copies of the arguments efficiently. My thought was then to change each <code>ArrayDict</code> in its own process, return the changed version of the object, and overwrite the original object. To save memory and save the work for copying, I used in addition <a href="https://pypi.python.org/pypi/sharedmem" rel="nofollow noreferrer">sharedmem</a> arrays to store the data in <code>ArrayDict</code>. I am aware that the dictionary must still be copied.</p>
<pre><code>from sharedmem import sharedmem
import numpy as np

n = ...                   # length of the data array
myData = np.empty(n, dtype=object)
myData[:] = [ArrayDict() for _ in range(n)]
done = False

while not done:
    consideredData = ...  # numpy boolean array of length
                          # n with True at the index of
                          # considered data
    args = ...            # numpy array containing arguments
                          # for myMethod

    with sharedmem.MapReduce() as pool:
        results = pool.map(myMethod, 
                           list(zip(myData[considered], 
                                    args[considered])),
                           star=True)
        myData[considered] = results

    done = ...             # depends on what happens in
                           # myMethod
</code></pre>
<p>What I get is a segmentation fault error. I was able to circumvent this error by creating deepcopies of the <code>ArrayDict</code>s to <code>myMethod</code> and saving them into <code>myData</code>. I do not really understand why this is necessary, and copying my (potentially very large) arrays frequently (the while loop takes long) is not what seems to be efficient to me. However, at least it worked to a certain extent. Nevertheless, my program has some buggy behaviour at the 3rd iteration due to the shared memory. Therefore, I think that my way is not optimal.</p>
<p>I read <a href="https://stackoverflow.com/questions/5549190/is-shared-readonly-data-copied-to-different-processes-for-python-multiprocessing/5550156#5550156">here</a> and <a href="https://stackoverflow.com/questions/15976937/making-my-numpy-array-shared-across-processes">here</a> that it is possible to save aribtrary numpy arrays on the shared memory using <code>multiprocessing.Array</code>. However, I would still need to share the whole <code>ArrayDict</code>, which includes in particular a dictionary, which in turn is not pickable. </p>
<p>How could I achieve my goals in an efficient way? Would it be possible (and efficient) to make my object pickable somehow? </p>
<p>All solutions must run with python 3 and full numpy/scipy support on 64bit Linux.</p>
<p><strong>Edit</strong></p>
<p>I found <a href="https://stackoverflow.com/questions/3671666/sharing-a-complex-object-between-python-processes">here</a> that it is somehow possible to share arbitrary objects using Multiprocessing "Manager" classes and user-defined proxy classes. Will this be efficient? I would like to exploit that I do not need concurrent access to the objects, even though they are not handled in the main process. Would it be an option to create a manager for each object that I want to process? (I might still have some misconceptions about how mangers work.)</p>
</div>
<div class="post-text" itemprop="text">
<p>This seems like a fairly complex class, and I am not able to completely anticipate if this solution will work in your case. A simple compromise for such a complex class is to use <a href="https://docs.python.org/3.2/library/concurrent.futures.html#processpoolexecutor" rel="nofollow noreferrer"><code>ProcessPoolExecutor</code></a>.</p>
<p>If this does not answer your question then it would be good with a minimal, working, example.</p>
<pre><code>from concurrent.futures import ProcessPoolExecutor

import numpy as np

class ArrayDict ():
  keys = None
  vals = None

  def __init__ (self):
    self.keys = dict ()
    self.vals = np.random.rand (1000)

  def __str__ (self):
    return "keys: " + str(self.keys) + ", vals: " + str(self.vals.mean())

def myMethod (ad, args):
  print ("starting:", ad)


if __name__ == '__main__':
  l     = [ArrayDict() for _ in range (5)]
  args  = [2, 3, 4, 1, 3]

  with ProcessPoolExecutor (max_workers = 2) as ex:

    d = ex.map (myMethod, l, args)
</code></pre>
<p>The objects are <a href="https://stackoverflow.com/questions/15857838/modify-object-in-python-multiprocessing">cloned</a> when sent to the child process, you need to return the result (as changes to the object will not propagate back to the main process) and handle how you want to store them.</p>
<blockquote>
<p>Note that changes to class variables will propagate to other objects in the
  same process, e.g. if you have more tasks than processes, changes to class 
  variables will be shared among the instances running in the same process. This is usually undesired behavior.</p>
</blockquote>
<p>This is a high-level interface to parallelization. <code>ProcessPoolExecutor</code> uses the <code>multiprocessing</code> module and can only be used with <a href="https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled" rel="nofollow noreferrer"><em>pickable</em> objects</a>. I suspect that <code>ProcessPoolExecutor</code> has performance similar to <a href="https://docs.python.org/3.2/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow noreferrer">"sharing state between processes"</a>. Under the hood, <code>ProcessPoolExecutor</code> <a href="https://stackoverflow.com/questions/18671528/processpoolexecutor-from-concurrent-futures-way-slower-than-multiprocessing-pool">is using <code>multiprocessing.Process</code></a>, and should exhibit similar performance as <code>Pool</code> (except when using <a href="https://stackoverflow.com/questions/18671528/processpoolexecutor-from-concurrent-futures-way-slower-than-multiprocessing-pool">very long iterables</a> with map). <code>ProcessPoolExecutor</code> does seem to be the intended future API for concurrent tasks in python.</p>
<p>If you can, it is usually faster to use the <a href="https://docs.python.org/3.2/library/concurrent.futures.html#threadpoolexecutor" rel="nofollow noreferrer"><code>ThreadPoolExecutor</code></a> (which can just be swapped for the <code>ProcessPoolExecutor</code>). In this case the object <em>is</em> shared between the processes, and an update to one will propagate back to the main thread.</p>
<p>As mentioned the fastest option is probably to re-structure <code>ArrayDict</code> so that it only uses objects that can be represented by <a href="https://docs.python.org/3.2/library/multiprocessing.html#shared-ctypes-objects" rel="nofollow noreferrer"><code>multiprocessing.Value</code></a> or <code>Array</code>.</p>
<p>If <code>ProcessPoolExecutor</code> does not work, and you cannot optimize <code>ArrayDict</code>, you may be stuck with using a <a href="https://docs.python.org/3.2/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow noreferrer"><code>Manager</code></a>. There are good examples on how to do that <a href="https://stackoverflow.com/questions/3671666/sharing-a-complex-object-between-python-processes?noredirect=1&amp;lq=1">here</a>. </p>
<blockquote>
<p>The greatest performance gain is often likely to be found in <code>myMethod</code>. 
  And, as I mentioned, the overhead of using threads <a href="https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python">is less</a> than that of processes.</p>
</blockquote>
</div>
<span class="comment-copy">How do you modify or use the arrayDict in myMethod? (I assume you mean <code>myMethod</code> not <code>myFunc</code>?)</span>
<span class="comment-copy">@gauteh: Thanks for making me aware of the typo. I corrected it. I also added a description of how I modify arrayDict in myMethod.</span>
<span class="comment-copy">Is it crucial that ArrayDict can take arbitrary key types? Otherwise the class might be restructured to use types that can be shared easily across processes without using a manager. As it is now, using a manager seems to be the best choice since the problem is somewhat complex. The performance loss might not be significant.</span>
<span class="comment-copy">@gauteh: I do only work with integer keys and do not require a solution that works with arbitrary keys. However, I would still be interested on how a solution would look like, if arbitrary keys had to be supported.  <code>ArrayDict</code> does of course have more object variables than I listed: e.g. a set of deleted indices. However, if I have a solution for the dict, I hope to be able to do the rest myself.</span>
<span class="comment-copy">Thanks for the answer! This works well and lets me even exchange the internal array of ArrayDict. Could you add some comments what is going on under the hood? What is the difference to multiprocessing.pool? Are the variables copied? I observed that integer properties of ArrayDict were not changed concurrently, but the entries of the dict were. How can I understand and predict that behaviour? Furthermore, I saw that your solution works in simple cases under Windows and Linux, but with the full ArrayDict only on Linux. Why? Is the overhead created in the "with" statement or in the  "map" method?</span>
<span class="comment-copy">The <code>with</code> statement does not create any extra overhead. What do you mean not changed concurrently? It would be useful with an example here. This might be related to immutable and mutables types. The answer was updated somewhat on the performance.</span>
<span class="comment-copy">The variables are copied (but the memory is shared untill it is changed). You have to return the object from the worker process and replace the original object. Otherwise, use a manager.</span>
<span class="comment-copy">Thanks for the update! With "not changed concurrently" I mean that if the variable is changed in process A, this change does not affect the value of this variable in in process B. That is, what happens in process A does not affect process B. If changes in mutable objects are visiblie in all processes, this allows to implement shared memory very easily. However, in this case the programmer should also ensure that no dirty read and write actions occur. Therefore, it would be great to understand concurrent.futures' behaviour here.</span>
<span class="comment-copy">I do not think this can be happening, unless you explicitly use a system for shared memory. Note that if you have fewer processes than tasks, updates to a class variable will be visible to the tasks that are run on this process. This is probably undesired behavior anyway!  If you use threads, the object is the same between the threads.</span>
