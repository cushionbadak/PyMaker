<div class="post-text" itemprop="text">
<p><strong>Problem:</strong></p>
<p>Replacing multiple string patterns in a large text file is taking a lot of time. (Python)</p>
<p><strong>Scenario:</strong></p>
<p>I have a large text file with no particular structure to it. But, it contains several patterns. For example, email addresses and phone numbers. </p>
<p>The text file has over 100 different such patterns and the file is of size 10mb (size could increase). The text file may or may not contain all the 100 patterns. </p>
<p>At present, I am replacing the matches using <code>re.sub()</code> and the approach for performing replaces looks as shown below.</p>
<pre><code>readfile = gzip.open(path, 'r') # read the zipped file
lines = readfile.readlines() # load the lines 

for line in lines:
    if len(line.strip()) != 0: # strip the empty lines
        linestr += line

for pattern in patterns: # patterns contains all regex and respective replaces
    regex = pattern[0]
    replace = pattern[1]
    compiled_regex = compile_regex(regex)
    linestr = re.sub(compiled_regex, replace, linestr)
</code></pre>
<p>This approach is taking a lot of time for large files. Is there a better way to optimize it?</p>
<p>I am thinking of replacing <code>+=</code> with <code>.join()</code> but not sure how much that would help.</p>
</div>
<div class="post-text" itemprop="text">
<p>you could use <a href="https://github.com/rkern/line_profiler" rel="nofollow noreferrer">lineprofiler</a> to find which lines in your code take the most time</p>
<pre><code>pip install line_profiler    
kernprof -l run.py
</code></pre>
<p>another thing, I think you're building the string too large in memory, maybe you can make use of <a href="https://docs.python.org/3/howto/functional.html#generator-expressions-and-list-comprehensions" rel="nofollow noreferrer">generators</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You may obtain slightly better results doing :</p>
<pre><code>large_list = []

with gzip.open(path, 'r') as fp:
    for line in fp.readlines():
        if line.strip():
            large_list.append(line)

merged_lines = ''.join(large_list)

for regex, replace in patterns:
    compiled_regex = compile_regex(regex)
    merged_lines = re.sub(compiled_regex, replace, merged_lines)
</code></pre>
<p>However, further optimization can be achieved knowing what kind of processing you apply. In fact the last line will be the one that takes up all CPU power (and memory allocation). If regexes can be applied on a per-line basis, you can achieve great results using the multiprocessing package. Threading won't give you anything because of the GIL (<a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow noreferrer">https://wiki.python.org/moin/GlobalInterpreterLock</a>)</p>
</div>
<span class="comment-copy">Do you have regex patterns to look for or simple strings?</span>
<span class="comment-copy">If you have such a big file, you could also sort your data with a primary key once and then simply perform a binary search, which will greatly improve performance. It's a one-time trade-off and seems like a quick win for me. Also, at that size, use of a database should be considered. If you're dealing with a lot of data, applying a structure to it almost always yields a big improvement. Hence the reason that universities often teach data structures as a single course.</span>
<span class="comment-copy">@Krazor: The question author says the file has no structure. So I'm wondering how you're thinking of sorting it?</span>
<span class="comment-copy">Related: <a href="http://stackoverflow.com/questions/15175142/how-can-i-do-multiple-substitutions-using-regex-in-python" title="how can i do multiple substitutions using regex in python">stackoverflow.com/questions/15175142/â€¦</a></span>
<span class="comment-copy">Excuse me then. You should definitely, as mentioned by @salah consider the use of a generator!</span>
<span class="comment-copy">The use of a generator makes sense in this context. I don't understand how lineprofiler will help though?</span>
<span class="comment-copy">@salah Thank you, I am not particularly aware of generators. I will look into that.   So, in your opinion, splitting the text file into chunks and performing regex substitutions might be more optimized?</span>
<span class="comment-copy">I'm not entirely sure whether it'll be faster, it will definitely be more efficient though.</span>
<span class="comment-copy">I echo your thoughts on multiprocessing. And, my scenario involves of both applying regexes line by line and across lines (fixed structure).</span>
<span class="comment-copy">Would it be too expensive to merge all lines, perform regex matches in the merged_lines and split them again later to perform matches per line?   Because there could be multiple blocks of text patterns which could get replaced and it would reduce the length of the file to analyze line by line.</span>
<span class="comment-copy">You may test different variations - the next thing you could do is identify the bottleneck (cpu -&gt; try multiprocessing by splitting your source file or your workflow ; IO -&gt; load everything in memory first)</span>
