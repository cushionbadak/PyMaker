<div class="post-text" itemprop="text">
<p>I'm trying to execute a function on every line of a CSV file as fast as possible. My code works, but I know it could be faster if I make better use of the <code>multiprocessing</code> library.</p>
<pre><code>processes = []

def execute_task(task_details):
    #work is done here, may take 1 second, may take 10
    #send output to another function

with open('twentyThousandLines.csv', 'rb') as file:
    r = csv.reader(file)
    for row in r:
        p = Process(target=execute_task, args=(row,))
        processes.append(p)
        p.start()

for p in processes:
    p.join()
</code></pre>
<p>I'm thinking I should put the tasks into a <code>Queue</code> and process them with a <code>Pool</code> but all the examples make it seem like <code>Queue</code> doesn't work the way I assume, and that I can't map a <code>Pool</code> to an ever expanding <code>Queue</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>I've done something similar using a <code>Pool</code> of workers.  </p>
<pre><code>    from multiprocessing import Pool, cpu_count

    def initializer(arg1, arg2):
        # Do something to initialize (if necessary)

    def process_csv_data(data):
        # Do something with the data

    pool = Pool(cpu_count(), initializer=initializer, initargs=(arg1, arg2))

    with open("csv_data_file.csv", "rb") as f:
        csv_obj = csv.reader(f)
        for row in csv_obj:
            pool.apply_async(process_csv_data, (row,))
</code></pre>
<p>However, as pvg commented under your question, you might want to consider how to batch your data.  Going row by row may not the the right level of granularity.  </p>
<p>You might also want to profile/test to figure out the bottle-neck.  For example, if disk access is limiting you, you might not benefit from parallelizing.  </p>
<p><code>mulprocessing.Queue</code> is a means to <a href="https://docs.python.org/2/library/multiprocessing.html#exchanging-objects-between-processes" rel="nofollow noreferrer">exchanging objects among the processes</a>, so it's not something you'd put a task into.  </p>
</div>
<div class="post-text" itemprop="text">
<p>For me it looks like you are actually trying to speed up</p>
<pre><code>def check(row):
    # do the checking
    return (row,result_of_check)

with open('twentyThousandLines.csv', 'rb') as file:
    r = csv.reader(file)
    for row,result in map(check,r):
        print(row,result)
</code></pre>
<p>which can be done with</p>
<pre><code>#from multiprocessing import Pool # if CPU-bound (but even then not alwys)
from multiprocessing.dummy import Pool # if IO-bound


def check(row):
    # do the checking
    return (row,result_of_check)

if __name__=="__main__": #in case you are using processes on windows
    with open('twentyThousandLines.csv', 'rb') as file:
        r = csv.reader(file)
        with Pool() as p: # before python 3.3 you should do close() and join() explicitly
            for row,result in p.imap_unordered(check,r, chunksize=10): # just a quess - you have to guess/experiement a bit to find the best value
                print(row,result)
</code></pre>
<p>Creating processes takes some time (especially on windows) so in most cases using threads via <a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy" rel="nofollow noreferrer">multiprocessing.dummy</a> is faster (and also multiprocessing is not totally trivial - see <a href="https://docs.python.org/3/library/multiprocessing.html#programming-guidelines" rel="nofollow noreferrer">Guidelines</a>).</p>
</div>
<span class="comment-copy">'a line of csv' is probably pretty lousy granularity for launching piles of processes. You might find it easier to just process sequentially without multiprocessing and just run as many instances of this as you have cores (after splitting up the data)</span>
<span class="comment-copy">The CSV file is a list of DNS nameservers: <a href="http://public-dns.info/nameservers.csv" rel="nofollow noreferrer">public-dns.info/nameservers.csv</a>  I'm checking DNS resolution from each nameserver to make sure my domain is accessible from everywhere, so I basically just create a request, wait up to 10 seconds for a response, then print the response.  I think I'm going about this the correct way, but if there's a better way by all means let me know.</span>
<span class="comment-copy">You're i/o, not compute bound in that case. There's no point launching a zillion processes, you should take a look at asyncio.</span>
<span class="comment-copy">Thanks, I'm looking at it.</span>
<span class="comment-copy">Process creation is not cheap. You should find a way to create a relatively small number of processes and use them over. Also use threads.</span>
<span class="comment-copy">ah that's why Queue was confusing me, I thought it worked like the NodeJS async library's Queue  <code>pool.apply_async()</code> is what I was looking for</span>
<span class="comment-copy">This is actually taking longer now. It's so slow I can read the print statements in the console as the processes complete. I thought limiting the processes to 8 at a time would make it faster due to less scheduling overhead. Code: <a href="http://hastebin.com/gazixegehu.py" rel="nofollow noreferrer">hastebin.com/gazixegehu.py</a></span>
<span class="comment-copy">Spawning processes can add significant time depending on the OS and perhaps other factors.  This worked for me because I was very much compute bound (and I fed large chunks of data to each process).  Hopefully the other suggestions in the comments will help you.  Good luck.</span>
