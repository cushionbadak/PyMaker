<div class="post-text" itemprop="text">
<p>Say I have a list <code>[1,2,3,4,5,6,7]</code>. I want to find the 3 closest numbers to, say, 6.5. Then the returned value would be <code>[5,6,7]</code>.</p>
<p>Finding one closest number is not that tricky in python, which can be done using</p>
<pre><code>min(myList, key=lambda x:abs(x-myNumber))
</code></pre>
<p>But I am trying not to put a loop around this to find k closest numbers. Is there a pythonic way to achieve the above task?</p>
</div>
<div class="post-text" itemprop="text">
<h2>The short answer</h2>
<p>The <a href="https://docs.python.org/2.7/library/heapq.html#heapq.nsmallest" rel="nofollow noreferrer"><em>heapq.nsmallest()</em></a> function will do this neatly and efficiently:</p>
<pre><code>&gt;&gt;&gt; from heapq import nsmallest
&gt;&gt;&gt; s = [1,2,3,4,5,6,7]
&gt;&gt;&gt; nsmallest(3, s, key=lambda x: abs(x-6.5))
[6, 7, 5]
</code></pre>
<p>Essentially this says, "Give me the three input values that have the lowest absolute difference from the number <em>6.5</em>".</p>
<h2>The algorithm and its running time</h2>
<p>The algorithm for <em>nsmallest</em> makes a single pass over the data, keeping no more than the <em>n</em> best values in memory at any time (that means that it works with any input iterator, is cache-efficient, and space-efficient).   </p>
<p>The algorithm only adds new values to the heap when a new "best" value is found.  Accordingly, it minimizes the number of comparisons made.   For example, if you are looking for the 100 best values out of 1,000,000 random inputs, it typically makes fewer than 1,008,000 comparisons (about 0.8% more compares than using <a href="https://docs.python.org/2.7/library/functions.html#min" rel="nofollow noreferrer"><em>min()</em></a> to find the single best value).</p>
<p>The <a href="https://docs.python.org/2.7/glossary.html#term-key-function" rel="nofollow noreferrer"><em>key functions</em></a> for <em>min()</em>, <em>nsmallest()</em>, and <em>sorted()</em> all guarantee that the key function is called exactly once per value in the input iterable.  That means that this technique will be efficient for even more complex and interesting examples of the n-closest value problem (i.e. words that <a href="http://en.wikipedia.org/wiki/Soundex" rel="nofollow noreferrer">sound the most alike</a>, closest <a href="http://en.wikipedia.org/wiki/HSL_and_HSV" rel="nofollow noreferrer">colors</a>, <a href="http://en.wikipedia.org/wiki/Levenshtein_distance" rel="nofollow noreferrer">smallest diffs</a>, fewest genetic mutations, Euclidean distance, etc).</p>
<p>Both <em>nsmallest()</em> and <em>sorted()</em> will return a list rank ordered by nearness (ties are settled by which value was seen first).</p>
<p>For those who are interested, there is a somewhat involved analysis of expected number of comparisons <a href="http://hg.python.org/cpython/file/38a325c84564/Lib/heapq.py#l40" rel="nofollow noreferrer">here</a> and <a href="http://code.activestate.com/recipes/577573-compare-algorithms-for-heapqsmallest" rel="nofollow noreferrer">here</a>.  Quick summary:</p>
<ul>
<li>Average case for random inputs:  <code>n + k * (log(k, 2) * log(n/k) + log(k, 2) + log(n/k))</code></li>
<li>Best case for ascending inputs:  <code>n + k * log(k, 2)</code></li>
<li>Worst case for descending inputs:  <code>n * log(k, 2)</code> </li>
</ul>
<h2>Optimizing for repeated lookups</h2>
<p>In the comments, @Phylliida, asked how to optimize for repeated lookups with differing start points.  The key is to pre-sort the data and then use <a href="https://docs.python.org/3/library/bisect.html#bisect.bisect" rel="nofollow noreferrer">bisect</a> to locate the center of a small search segment:</p>
<pre><code>from bisect import bisect

def k_nearest(k, center, sorted_data):
    'Return *k* members of *sorted_data* nearest to *center*'
    i = bisect(sorted_data, center)
    segment = sorted_data[max(i-k, 0) : i+k]
    return nsmallest(k, segment, key=lambda x: abs(x - center))
</code></pre>
<p>For example:</p>
<pre><code>&gt;&gt;&gt; s.sort()
&gt;&gt;&gt; k_nearest(3, 6.5, s)
[6, 7, 5]
&gt;&gt;&gt; k_nearest(3, 0.5, s)
[1, 2, 3]
&gt;&gt;&gt; k_nearest(3, 4.5, s)    
[4, 5, 3]
&gt;&gt;&gt; k_nearest(3, 5.0, s)
[5, 4, 6]
</code></pre>
<p>Both <em>bisect()</em> and <em>nsmallest()</em> take advantage of the sorted data.  The former runs <em>O(log2 k)</em> time and the latter runs in <em>O(n)</em> time.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could compute distances, and sort:</p>
<pre><code>[n for d, n in sorted((abs(x-myNumber), x) for x in myList)[:k]]
</code></pre>
<p>This does the following:</p>
<ol>
<li>Create a sequence of tuples <code>(d, x)</code> where <code>d</code> is the distance to your target</li>
<li>Select the first <code>k</code> elements of that list</li>
<li>Extract just the number values from the result, discarding the distance</li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p>Both answers were good, and Greg was right, Raymond's answer is more high level and easier to implement, but I built upon Greg's answer because it was easier to manipulate to fit my need. </p>
<p>In case anyone is searching for a way to find the n closest values from a list of dicts.</p>
<p>My dict looks like this, where npi is just an identifier that I need along with the value:</p>
<pre><code>mydict = {u'fnpi': u'1982650024',
 u'snpi': {u'npi': u'1932190360', u'value': 2672},
 u'snpis': [{u'npi': u'1831289255', u'value': 20},
  {u'npi': u'1831139799', u'value': 20},
  {u'npi': u'1386686137', u'value': 37},
  {u'npi': u'1457355257', u'value': 45},
  {u'npi': u'1427043645', u'value': 53},
  {u'npi': u'1477548675', u'value': 53},
  {u'npi': u'1851351514', u'value': 57},
  {u'npi': u'1366446171', u'value': 60},
  {u'npi': u'1568460640', u'value': 75},
  {u'npi': u'1326046673', u'value': 109},
  {u'npi': u'1548281124', u'value': 196},
  {u'npi': u'1912989989', u'value': 232},
  {u'npi': u'1336147685', u'value': 284},
  {u'npi': u'1801894142', u'value': 497},
  {u'npi': u'1538182779', u'value': 995},
  {u'npi': u'1932190360', u'value': 2672},
  {u'npi': u'1114020336', u'value': 3264}]}

value = mydict['snpi']['value'] #value i'm working with below
npi = mydict['snpi']['npi'] #npi (identifier) i'm working with below
snpis = mydict['snpis'] #dict i'm working with below
</code></pre>
<p>To get an <code>[id, value]</code> list (not just a list of values) , I use this:</p>
<pre><code>[[id,val] for diff, val, id in sorted((abs(x['value']-value), x['value'], x['npi']) for x in snpis)[:6]]
</code></pre>
<p>Which produces this:</p>
<pre><code>[[u'1932190360', 2672],
 [u'1114020336', 3264],
 [u'1538182779', 995],
 [u'1801894142', 497],
 [u'1336147685', 284],
 [u'1912989989', 232]]
</code></pre>
<p><em>EDIT</em></p>
<p>I actually found it pretty easy to manipulate Raymond's answer too, if you're dealing with a dict (or list of lists).</p>
<pre><code>from heapq import nsmallest
[[i['npi'], i['value']] for i in nsmallest(6, snpis, key=lambda x: abs(x['value']-value))]
</code></pre>
<p>This will produce the same as the above output.</p>
<p>And this</p>
<p><code>nsmallest(6, snpis, key=lambda x: abs(x['value']-value))</code> will produce a dict instead.</p>
</div>
<span class="comment-copy">Then, why does the documentation state: "The latter two functions perform best for smaller values of n. For larger values, it is more efficient to use the <code>sorted()</code> function"? To me this is equivalent to saying <code>nsmallest</code> is worse than O(nlogn) in asymptotic complexity, but happens to be more efficient for a range of small <code>n</code>s. Either the algorithm isn't as efficient as you state, or the documentation is wrong in some way.</span>
<span class="comment-copy">@Bakuriu: <code>nsmallest()</code> has a time complexity of O(<i>n</i> log <i>k</i>), where <i>n</i> is the size of container and <i>k</i> is the number of smallest values you are looking for.  The complexity of <code>sorted()</code> is O(<i>n</i> log <i>n</i>), but it has a better constant factor.  So if <i>k/n</i> is small compared to 1, <code>nsmallest()</code> will win.  If <i>k</i> becomes bigger compared to <i>n</i>, at some point <code>sorted()</code> will win due to the better constant.  Neither Raymond nor the documentation are wrong, though the documentation could be clearer about that "smaller" actually means smaller compared to the size of the collection.</span>
<span class="comment-copy">@SvenMarnach: Linear time selection is possible as well, which the documentation neglects.</span>
<span class="comment-copy">@NeilG: <code>nsmallest()</code> returns the <i>k</i> smallest elements in sorted order, and the fastest algorithm to do this is O(<i>n</i> log <i>k</i>).  I believe it's still the fastest algorithm if you drop the requirement that the result is sorted.  You can select the kth smallest element in O(<i>n</i>), but I fail to see how you could select the k smallest elements.</span>
<span class="comment-copy">@SvenMarnach: No, the fastest algorithm to do that is O(n).  You can select the kth smallest element in linear time, and you can compare the elements in the original array against it in linear time.  The total algorithm is therefore linear time.</span>
<span class="comment-copy"><code>sorted</code> accepts a key function, so you don't need the decorate-sort-undecorate pattern.</span>
<span class="comment-copy">Oh that's true, and indeed the OP was most of the way there! Anyway, Raymond's answer is better.</span>
