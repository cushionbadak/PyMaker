<div class="post-text" itemprop="text">
<p>I want to parallelize two nested for loops in Python 2.7 but had no success on my own. I don't know how to approach the definition of what to parallelize.</p>
<p>Anyway, here is single processor code:</p>
<pre><code>import time

i = [int(x) for x in range(10000)]
j = [int(x) for x in range(10000, 20000)]

print len(i)
print len(j)

def sum(niz1,niz2):
    suma=[]
    for i in range(len(niz1)):
    suma1=0
        for j in range(len(niz2)):
            suma1=suma1+niz1[i]*niz2[j]
        suma.append(suma1)
    return suma

start_t0=time.time()
suma=sum(i, j)
print len(suma)

print ("Time:  %s seconds " % (time.time() - start_t0))
</code></pre>
<p>I want to have parallelization for the first array. How to perform it since this array is needed also in the second loop?</p>
<p>Simple explanation and code example would be of great help. </p>
</div>
<div class="post-text" itemprop="text">
<p>A couple of thoughts:</p>
<ol>
<li><p>I hope that your real calculation is more complex than the one you posted. If it's not, just compute the sum of <code>niz2</code> once and then multiply each element of <code>niz1</code> by that sum to get your result vector.</p></li>
<li><p>(Assuming that the real use case is more complex.) Python isn't fast for CPU-Bound computations. When crunching a lot of numbers, you should use libraries like <code>numpy</code> instead. Numpy's operations are implemented in C and therefore A LOT faster than regular python implementations.</p></li>
<li><p>(Assuming that numpy isn't an option.) In Python, there's a Global Interpreter Lock (GIL), that ensures that (except from IO tasks) only one thread is active at a time. This means that for computations, using multiple threads won't improve the computation time. The only way to achieve true parallelization is by using multiple processes (<code>import multiprocessing</code>, but then copying computation results between processes can become a bottleneck.</p></li>
</ol>
<p>In summary, try to optimize the single-core execution first. Using numpy can already solve 80% of use cases. Parallelize only if these optimizations aren't enough, and don't expect the improvements from it to be substantial.</p>
<p>Does this help?</p>
</div>
<div class="post-text" itemprop="text">
<p>The first step with calculations on large arrays of values in Python should be to implement it in numpy. This way you can take advantage of the vectorization of numpy. With the script below, I managed to run the same calculation in ~15 microseconds instead of 9 seconds in your version (almost 1000000x faster).</p>
<pre><code>import numpy as np
i = np.arange(10000, dtype=np.int64)
j = np.arange(10000, 20000, dtype=np.int64)
suma = i * j.sum()
</code></pre>
<p>This is probably not your real calculation though, so you should think about how to do this in your use case.</p>
</div>
<span class="comment-copy">Why do you want to parallelize? Is it just to speed up the calculation?</span>
<span class="comment-copy">The only way to truly parallelize cpu-bound computations in Python is to use the <a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing" rel="nofollow noreferrer"><code>multiprocessing</code></a> module. However doing so often entails a lot of overhead especially if data must be shared between the processes—so it may not be worth the trouble and can even make things slower.</span>
<span class="comment-copy">Is Numba an option? This would give with small code changes a speedup of at least 200. Multithreading is also simple and give an additional speedup.</span>
<span class="comment-copy">Thx for suggestions. I am using much complex calculations than stated. It was just for the simplicity of the code. Also, I am already using numpy.</span>
<span class="comment-copy">@MarjanKrstić The exact algorithm is important in deciding how to vectorize or parallelize, so please put it in the question.</span>
<span class="comment-copy">@Rob I will do it but I cannot do it at the moment.</span>
<span class="comment-copy">This is not the real calculation. The real one is really complex. I am already using numpy and it still takes around 18 seconds to do it, and I have at least 100 000 subsequent calls of this function.</span>
