<div class="post-text" itemprop="text">
<p>I am working on an algorithm that checks similarity between English words.</p>
<p>Having defined a function called 'similarity', I go though the whole list of words to check for similar words, if the similarity between two words is very high (=1) the algorithm will change one of the two words to the other one.</p>
<p><strong>Here is the logic:</strong></p>
<pre><code>list_of_word = [word1, word2, word3, word4]
</code></pre>
<p>assume that there is a very high similarity between word1 and word4.</p>
<p>Result:</p>
<pre><code>list_of_word = [word1, word2, word3, word1]
</code></pre>
<p>normally, I just need to loop it, the steps taken will be like:</p>
<ol>
<li>take word1 out, comparing to word1, word2, word3, word4</li>
<li>take word2 out, comparing to word1, word2, word3, word4</li>
<li>take word3 out, comparing to word1, word2, word3, word4</li>
<li>take word4 out, comparing to word1, word2, word3, word4</li>
</ol>
<p>However, there are some useless and repeated actions. For example, I don't have to compare <strong>word1</strong> and <strong>word2</strong> more than once. </p>
<p>The problem is that I have to go through 1 million words and it might take many days to run. </p>
<p>Any advice?</p>
<p>Here is the code I am using at the moment:</p>
<pre><code>from nltk.corpus import wordnet as wn
from itertools import product

def similarity(wordx,wordy):
    sem1, sem2= wn.synsets(wordx), wn.synsets(wordy)
    maxscore = 0
    for i,j in list(product(*[sem1,sem2])):
        score = i.path_similarity(j) # Wu-Palmer Similarity
        maxscore = score if maxscore &lt; score else maxscore
    return maxscore

def group_high_similarity(target_list,tc):
    result = target_list[:]
    num_word = len(result)
    for word in result:
        wordx = word
        i = 0
        while i&lt;len(result):
            wordy = result[i]
            value = similarity(wordx,wordy)
            if  value &gt;= tc:
                result[i] = wordx
                if wordy != wordx :print wordy+"---&gt; "+ wordx 
            i += 1
    return result
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Assume all your word list are not repetitive (means you already put them into set)</p>
<p>IMHO, you can apply set theory math in similarity. </p>
<p>If A similar to B, while X also similar to B, it means A also similar to X.</p>
<p>So you have a set of words
["car", "bus", "cat", "dog", "pen", "duck", "motorcycle"] </p>
<p>As in similarity attribute of "motor vehicle, if "car" similar to "bus",  and "car" similar to "motorcycle". Thus "bus" also similar to "motorcycle". So you can see, you <strong>don't need to compare all similar word that have been found</strong>. So after the "car" similarity compare is done, it already taken away 
["car", "bus", "motorcycle"]. "bus", "motorcycle" don't need to use for comparison again.</p>
<p>you only remains ["cat", "dog", "pen", "duck"] ,etc. </p>
<p>What you need to do next is keeping an index where this similar located. Perhaps a second pass to check the distance score. </p>
<p>(update)
IMPORTANT NOTE : In natural language, same verb and noun can have multiple meaning, e.g. chicken may means coward. E.g. you may miss combined words, proverb,etc. E.g. <strong>chicken out</strong> has nothing to do with chicken going out; <strong>In vitro</strong> is a verb that you can't split them up. 
Above method are pretty aggressive. But you need to start from somewhere, then incrementally add more features to perfect them.</p>
</div>
<div class="post-text" itemprop="text">
<p>Just use a nested loop where the second index starts from the value of the first:</p>
<pre><code>for i in xrange(len(results)):
    for j in xrange(i+1, len(results)):
        # compare element i and j
</code></pre>
<p>Of course, this optimization (that divides your computations by 2) only works because your similarity measure is symmetrical (a similar to b == b similar to a). Also, this does not change the computational complexity, it's still O(n^2) (more exactly: O(n(n-1)/2) ).</p>
<p>An alternative, but more complicated, way to compute similarity measures that is more computationally efficient is to use binomial expansion (I will add more here later).</p>
<p>Also you should avoid while loops, most often they can be replaced by for loops. This is more reliable (no infinite loop) ang more optimizable by the interpreter. </p>
</div>
<div class="post-text" itemprop="text">
<p>Currently you check every word in the list against every other word in the list. Which is exactly n<sup>2</sup>.</p>
<p>You can cut back on this by checking every word to every word after it. Which is 1 + 2 + ... + (n-1) + n = n(n-1)/2. This would deduplicate your checks. Though your checks would need to be symmetric.</p>
<pre><code>from nltk.corpus import wordnet as wn
from itertools import product

def similarity(wordx,wordy):
    sem1, sem2= wn.synsets(wordx), wn.synsets(wordy)
    maxscore = 0
    for i,j in list(product(*[sem1,sem2])):
        score = i.path_similarity(j) # Wu-Palmer Similarity
        maxscore = score if maxscore &lt; score else maxscore
    return maxscore

def group_high_similarity(target_list,tc):
    result = target_list[:]
    for x in xrange(0, len(target_ist)):
        for y in xrange(x + 1, len(target_list)):
            wordx, wordy = target_list[x], target_list[y]
            value = similarity(wordx,wordy)
            if  value &gt;= tc:
                result[x] = wordx
                if wordy != wordx :print wordy+"---&gt; "+ wordx
    return result
</code></pre>
<p>It may still take a long time to run because it's now only about half the size.</p>
</div>
<span class="comment-copy">You could use <a href="https://docs.python.org/3/library/itertools.html#itertools.combinations" rel="nofollow noreferrer"><code>itertools.combinations(words, 2)</code></a> to avoid comparing same words multiple time, but there is still a lot of comparisons...</span>
