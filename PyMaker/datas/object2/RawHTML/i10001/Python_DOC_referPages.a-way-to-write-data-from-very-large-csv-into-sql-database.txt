<div class="post-text" itemprop="text">
<p>I have a multiple csv files with the following content:</p>
<pre><code>Duration (ms),Start date,End date,Start station number,Start station,End station number,End station,Bike number,Member Type
840866,8/31/2016 23:59,9/1/2016 0:13,31117,15th &amp; Euclid St  NW,31228,8th &amp; H St NW,W20409,Registered
</code></pre>
<p>And I have about 10 millions raws of this data. </p>
<p>I need to normalise this data and split it into tables.  I suppose there will be tables: stations, bikes, rides. In terms of OLAP rides are facts, and stations and bikes are dimensions. I'm very new to data analysis so I could use incorrect terms. But I'm trying to use <a href="https://stackoverflow.com/a/1797008/1858864">this</a> approach.</p>
<p>So the question is how to write this data into database as optimal as it possible? The approach I can imagine is following:</p>
<pre><code>1) Read line from csv
2) Create a record for station and get foreign key for it (with direct SQL query).
3) Create a record for a bike and get FK for it.
4) Create a record for a datetime (i'm not sure if it could be useful for further data analysis)
5) Create a record for ride with FK for each of it 'dimensions'
6) Repeat
</code></pre>
<p>But if I have 10 millions of rows, this approach will make ~40 millions of queries to database, which looks terrible and not optimal.</p>
<p>Is there more optimal approaches/algorithms/technologies to do it? I'm going to use python and psql for it, if it's important.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can probably economize on queries by <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache" rel="nofollow">memoizing the function</a> that creates the unique records, for example:</p>
<pre><code>from functools import lru_cache

@lru_cache(maxsize=128)
def save_station(s):
    """Create station record and return primary key."""
    station = create_or_get_station_record(...)
    return station.id
</code></pre>
<p>If the input is sorted by station, then subsequent calls to <code>save_station</code> will not query the database once the record is created. Even if it's not perfectly sorted, this could help.</p>
<p>You can batch the saving of rides. Accumulate records and then call an <code>execute_many</code> function (will depend on the libraries you are using).</p>
<p>You could pre-process the data to create separate CSV files, then load each file.</p>
</div>
<div class="post-text" itemprop="text">
<p>According to the <a href="https://www.postgresql.org/docs/current/static/populate.html" rel="nofollow noreferrer">PostgreSQL documentation</a>, <code>copy</code> command is an optimum approach for populating a table with a large number of rows. At the other hand, for processing csv files <a href="http://pandas.pydata.org/pandas-docs/stable/" rel="nofollow noreferrer">pandas</a> library is one of the best tools.</p>
<p>So the below steps can be an acceptable solution:</p>
<pre><code>Load csv files into data-frames with pandas read_csv()
Process the data-frames into the desired form
Store processed data-frames into temporary csv files
Create desired tables using SQL
Load data from temporary csv files into tables using copy SQL command
</code></pre>
</div>
<span class="comment-copy">1) Load whole data into the temporary table using <code>copy</code> command (<a href="https://www.postgresql.org/docs/current/static/sql-copy.html" rel="nofollow noreferrer">docs for PG</a> and <a href="http://initd.org/psycopg/docs/cursor.html#cursor.copy_from" rel="nofollow noreferrer">docs for psycopg</a>); 2) Split the data using SQL</span>
<span class="comment-copy">Depending on your version of Postgresql, you can use the file_fdw and select directly against the csv files (no pre-loading required).</span>
