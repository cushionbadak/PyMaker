<div class="post-text" itemprop="text">
<p>I have a folder with around 590,035 <code>json</code> files. Each file is a document that has to be indexed. If I index each document using python then it is taking more than 30 hours. How do I index these documents quickly?</p>
<p><strong>Note</strong> - I've seen bulk api but that requires merging all the files into one which takes similar amount of time as above.
Please tell me how to improve the speed. Thank You.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you're sure that I/O is your bottleneck, use threads to read files, i.e. with <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor" rel="nofollow noreferrer"><code>ThreadPoolExecutor</code></a>, and either accumulate for bulk request, or save one by one. ES will have no issues whatsoever, until you're using either unique or internal IDs.</p>
<p>Bulk will work faster, just by saving you time on HTTP overhead, saving 1 by 1 is a little bit easier to code.</p>
</div>
<span class="comment-copy">"which takes similar amount of time as above" =&gt; how do you know that? Have you actually tried it?</span>
<span class="comment-copy">use multiple threads</span>
<span class="comment-copy">@Val Yeah, Used file operations in python to merge the files and the avg time was smiliar</span>
<span class="comment-copy">@Stack Won't there be issues with <code>elasticsearch</code> if two threads try to index two documents at the same time? Will synchronising the <code>index</code> function solve this?</span>
<span class="comment-copy">But did you use the bulk API? i.e. you added the command line between each doc?</span>
<span class="comment-copy">So you're suggesting that I use <code>ThreadPoolExecuter</code> to merge all the files into one and then use the <code>bulk</code> api to index the documentes?</span>
<span class="comment-copy">Merging 590K files together is probably not going to work, way too much data in my opinion, you'll have to do it in chunks, see the link I shared above</span>
<span class="comment-copy">Val if right, you should chunk it by, lets say, 1k files for each request. Threading is only to speed up reading files</span>
<span class="comment-copy">@Val I did see the above link. Thanks for that. How do I make the chunks so that each request has 1k files?</span>
<span class="comment-copy">You have several examples across SO, but here is one: <a href="https://stackoverflow.com/a/54214168/4604579">stackoverflow.com/a/54214168/4604579</a></span>
