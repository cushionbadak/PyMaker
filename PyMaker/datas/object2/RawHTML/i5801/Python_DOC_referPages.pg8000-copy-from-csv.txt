<div class="post-text" itemprop="text">
<p>I'm using <a href="http://pythonhosted.org/pg8000/" rel="nofollow noreferrer">pg8000</a> on an App Engine flask application so that I can be able to process a CSV file and insert it into a PSQL instance (hosted on <code>AZURE</code>).</p>
<p>Why am I using <code>pg8000</code> and not <code>psycopg2</code>? -&gt; Because app engine doesn't support psycopg2. </p>
<p>So far, the docs of <code>pg8000</code> don't state a function that will do this like the one psycopg2 has. I haven't found an example that achieves this on SO or any other place, including the docs. </p>
<p>Anyone knows if this is possible?</p>
</div>
<div class="post-text" itemprop="text">
<p>Looking at <a href="https://github.com/mfenniak/pg8000/" rel="nofollow noreferrer">the source code</a>, there does not seem to be a way to directly import CSVs, nor does the code appear to have any built-in wrapper around <code>INSERT</code> queries, making it possible to </p>
<p>You do have the option of manually using a CSV reader and using <code>executemany</code>:</p>
<pre><code>import csv
import pg8000

conn = pg8000.connect(user="postgres", password="C.P.Snow")
cursor = conn.cursor()

command = 'INSERT INTO book (title) VALUES (%s), (%s) RETURNING id, title'
with open('my-data.csv', 'rb') as fl:
    data = list(csv.reader(fl))
    conn.executemany(command, data)
</code></pre>
<p>As a word of caution, depending on the size of your data, it may be better to use <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow noreferrer"><code>islice</code></a>:</p>
<pre><code>with open('my-data.csv', 'rb') as fl:
    reader = csv.reader(fl)
    slice = itertool.islice(reader, 100)
    while slice:
        conn.executemany(command, slice)
        slice = itertool.islice(reader, 100)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>As suggested in another question <a href="https://stackoverflow.com/questions/14674275/skip-first-linefield-in-loop-using-csv-file">here</a>, you could use the <code>next</code> method before applying the logic on the csv files and before using the csv read method.</p>
<p>Sorry in advance for not inserting as a complement to the previous answer, but I don't have enough points to do so.</p>
<p>I'm having the same issue and I solved the problem using the below. Please notice that for me, the correct way of executing many is on <code>cursor</code> object, not on the <code>conn</code>.</p>
<pre><code>conn = pg8000.connect(user='username', password='password', host='host', port=5432, database='database name')
cursor = conn.cursor()

command = "INSERT INTO public.salesforce_accounts (field1, field2, field3, field4, field5, field6) VALUES (%s, %s, %s, %s, %s, %s)"
with open('test.csv', 'r') as file:
    next(file)
    data = list(csv.reader(file))
    cursor.executemany(command, data)
</code></pre>
</div>
<span class="comment-copy">This works fine, but throws the following error: <code>ProgrammingError: (u'ERROR', u'ERROR', u'42601', u'INSERT has more target columns than expressions', u'79', u'analyze.c', u'884', u'transformInsertRow')</code>, can you assist me with this? One of the columns is autoincrement and not specified in the insert statement.</span>
<span class="comment-copy">That's a SQL error. You should try to get your SQL working first, and then plug it into <code>executemany</code>.</span>
<span class="comment-copy">fixed! however it takes so much time to process. Instead of inserting, can't we use the copy command?</span>
<span class="comment-copy">@codeninja mysql supports load infile - <a href="https://dev.mysql.com/doc/refman/5.7/en/load-data.html" rel="nofollow noreferrer">dev.mysql.com/doc/refman/5.7/en/load-data.html</a> . This should be a lot faster instead of individual inserts. Also be sure to disable and enable keys if you have any on your tables during inserts.</span>
