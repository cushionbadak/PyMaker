<div class="post-text" itemprop="text">
<p>i use iris-dataset to train a simple network with pytorch.</p>
<pre><code>trainset = iris.Iris(train=True)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=150,
                                          shuffle=True, num_workers=2)

dataiter = iter(trainloader)
</code></pre>
<p>the dataset itself has only 150 data points, and pytorch dataloader iterates jus t once over the whole dataset, because of the batch size of 150.</p>
<p>My question is now, is there generally any way to tell dataloader of pytorch to repeat over the dataset if it's once done with iteration? </p>
<p>thnaks</p>
<p><strong>update</strong></p>
<p>got it runnning :)
just created a sub class of dataloader and implemented my own <code>__next__()</code></p>
</div>
<div class="post-text" itemprop="text">
<p>The simplest option is to just use a nested loop:</p>
<pre><code>for i in range(10):
    for batch in trainloader:
        do_something(batch)
</code></pre>
<p>Another option would be to use <a href="https://docs.python.org/3/library/itertools.html#itertools.cycle" rel="nofollow noreferrer">itertools.cycle</a>, perhaps in combination with itertools.take.</p>
<p>Of course, using a DataLoader with batch size equal to the whole dataset is a bit unusual. You don't need to call iter() on the trainloader either.</p>
</div>
<div class="post-text" itemprop="text">
<p>Using <a href="https://docs.python.org/3/library/itertools.html#itertools.cycle" rel="nofollow noreferrer">itertools.cycle</a> has an important drawback, in that it does not shuffle the data after each iteration:</p>
<blockquote>
<p>When the iterable is exhausted, return elements from the saved copy.</p>
</blockquote>
<p>This can negatively affect the performance of your model in some situations. A solution to this can be to write your own cycle generator:</p>
<pre><code>def cycle(iterable):
    while True:
        for x in iterable:
            yield x
</code></pre>
<p>Which you would use as:</p>
<pre><code>dataiter = iter(cycle(trainloader))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>if you use tqdm, the best solution is:</p>
<pre><code>from tqdm import tqdm
pbar = tqdm(itertools.chain(validation_loader,
    validation_loader,
    validation_loader,
    validation_loader)) # 4 times loop through
for batch_index, (x, y) in enumerate(pbar):
    ...
</code></pre>
</div>
<span class="comment-copy">Have you ever read through the PyTorch <a href="http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" rel="nofollow noreferrer">basic tutorial?</a></span>
<span class="comment-copy">Of course i did, what has that got to do with my question?</span>
<span class="comment-copy">The tutorial has exactly what you are asking for here.</span>
