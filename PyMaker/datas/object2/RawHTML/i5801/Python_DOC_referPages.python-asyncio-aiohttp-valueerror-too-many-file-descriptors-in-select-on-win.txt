<div class="post-text" itemprop="text">
<p>
Hello everyone,
I'm having trouble trying to understand asyncio and aiohttp and making both work together properly. Not only I don't properly understand what I'm doing, at this point I've run into a problem that I have no idea how to solve.</p>
<p>I'm using Windows 10 64 bits, latest update.</p>
<p>The following code returns me a list of pages that do not contain html in the Content-Type in the header using asyncio.</p>
<pre><code>import asyncio
import aiohttp

MAXitems = 30

async def getHeaders(url, session, sema):
    async with session:
        async with sema:
            try:
                async with session.head(url) as response:
                    try:
                        if "html" in response.headers["Content-Type"]:
                            return url, True
                        else:
                            return url, False
                    except:
                        return url, False
            except:
                return url, False


def checkUrlsWithoutHtml(listOfUrls):
    headersWithoutHtml = set()
    while(len(listOfUrls) != 0):
        blockurls = []
        print(len(listOfUrls))
        items = 0
        for num in range(0, len(listOfUrls)):
            if num &lt; MAXitems:
                blockurls.append(listOfUrls[num - items])
                listOfUrls.remove(listOfUrls[num - items])
                items +=1
        loop = asyncio.get_event_loop()
        semaphoreHeaders = asyncio.Semaphore(50)
        session = aiohttp.ClientSession()
        data = loop.run_until_complete(asyncio.gather(*(getHeaders(url, session, semaphoreHeaders) for url in blockurls)))
        for header in data:
            if False == header[1]:
                headersWithoutHtml.add(header)
    return headersWithoutHtml


listOfUrls = ['http://www.google.com', 'http://www.reddit.com']
headersWithoutHtml=  checkUrlsWithoutHtml(listOfUrls)

for header in headersWithoutHtml:
    print(header[0])
</code></pre>
<p>
When I run it with, let's say, 2000 urls  (sometimes) it returns something like:</p>
<pre><code>data = loop.run_until_complete(asyncio.gather(*(getHeaders(url, session, semaphoreHeaders) for url in blockurls)))
  File "USER\AppData\Local\Programs\Python\Python36-32\lib\asyncio\base_events.py", line 454, in run_until_complete
    self.run_forever()
  File "USER\AppData\Local\Programs\Python\Python36-32\lib\asyncio\base_events.py", line 421, in run_forever
    self._run_once()
  File "USER\AppData\Local\Programs\Python\Python36-32\lib\asyncio\base_events.py", line 1390, in _run_once
    event_list = self._selector.select(timeout)
  File "USER\AppData\Local\Programs\Python\Python36-32\lib\selectors.py", line 323, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
  File "USER\AppData\Local\Programs\Python\Python36-32\lib\selectors.py", line 314, in _select
    r, w, x = select.select(r, w, w, timeout)
ValueError: too many file descriptors in select()
</code></pre>
<p><strong>Note1</strong>: I edited out my name with USER in the user.</p>
<p><strong>Note2</strong>: For whatever reason reddit.com returns as it doesn't contain HTML, this is a completly separate problem that I will try to solve, however if you notice some other inconsistency in my code that would fix that please point it out.</p>
<p><strong>Note3</strong>: My code is badly constructed because I've tried to change many things to try to debug this problem, but I've got no luck.</p>
<p>I've heard somewhere that this is a restriction of Windows and there is no way to bypass it, the problem is that:</p>
<p>a) I directly don't understand what "too many file descriptors in select()" means.</p>
<p>b) What I'm doing wrong that Windows can't handle? I've seen people push thousands of requests with asyncio and aiohttp but even with my chuncking I can't push 30-50 without getting a Value Error?</p>
<p><strong>Edit</strong>: Turns out with MAXitems = 10 it hasn't crashed me yet, but because I can't follow the pattern I have no idea why or how that tells me anything.</p>
<p><strong>Edit2</strong>: Nevermind, it needed more time to crash, but it did eventually even with MAXitems = 10</p>
</div>
<div class="post-text" itemprop="text">
<p>By default Windows can use only 64 sockets in asyncio loop. This is a limitation of underlying <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/ms740141(v=vs.85).aspx" rel="nofollow noreferrer">select()</a> API call.</p>
<p>To increase the limit please use <code>ProactorEventLoop</code>. Instructions for installation is <a href="https://docs.python.org/3/library/asyncio-eventloops.html#asyncio.ProactorEventLoop" rel="nofollow noreferrer">here</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>I'm having the same problem. Not 100% sure that this is guaranteed to work, but try replacing this:</p>
<pre><code>session = aiohttp.ClientSession()
</code></pre>
<p>with this:</p>
<pre><code>connector = aiohttp.TCPConnector(limit=60)
session = aiohttp.ClientSession(connector=connector)
</code></pre>
<p>By default <code>limit</code> is set to 100 (<a href="http://aiohttp.readthedocs.io/en/stable/client_reference.html#tcpconnector" rel="nofollow noreferrer">docs</a>), meaning that the client can have 100 simultaneous connections open at a time. As Andrew mentioned, Windows can only have 64 sockets open at a time, so we provide a number lower than 64 instead.</p>
</div>
<span class="comment-copy">Thanks! Few questions:  a) How is it possible that I'm reaching the limit with the chunking that I'm doing?  b) I've been trying   from win32file import _setmaxstdio  _setmaxstdio(3072)  since I posted my original question, it seems to work preeetty well, does it do the same as ProactorEventLoop?</span>
<span class="comment-copy">a) yes. aiohttp has limit 100 for concurrent connection number bu default, it's bigger than 64. b) I don't use Windows and cannot check <code>_setmaxstdio</code> but the limit is compile-time macro imho. It cannot be changed in runtime.</span>
<span class="comment-copy">a)I don't see how that actually answers my question. I asked how it is possible to saturate the select if I'm chunking the data and this is the only script that I'm running with aiohttp. Unless aiohttp uses multiple connections per link.  b)Well, I'm going to try your solution later, but _setmaxstdio does make a difference. Without it it bugged out every 1000 links give or take, it has done more than 5000+ with it without problem and the only reason it stopped is because I interrupted it.  Edit: Anyways, I gave you the rick for answering my question, thanks.</span>
<span class="comment-copy">You wrote that you process about 2000 URLs. HTTP connection is returned to internal pool but not released immediately. That's how you can run out of limit for opened sockets.</span>
<span class="comment-copy">I've changed:      loop = asyncio.get_event_loop()  for:       loop = asyncio.ProactorEventLoop()     asyncio.set_event_loop(loop)  and it raises:      raise NotImplementedError in   File "\Python\Python36-32\lib\site-packages\aiodns_<i>init_</i>.py", line 85, in _sock_state_cb     self.loop.add_reader(fd, self._handle_event, fd, READ)   File "\Python\Python36-32\lib\asyncio\events.py", line 453, in add_reader  Edit: I have no idea how to format comments, excuse me</span>
