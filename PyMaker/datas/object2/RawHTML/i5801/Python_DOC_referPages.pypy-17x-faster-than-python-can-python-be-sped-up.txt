<div class="post-text" itemprop="text">
<p>Solving a recent <a href="http://adventofcode.com/2017/day/5" rel="nofollow noreferrer">Advent of Code problem</a>, I found my default Python was ~40x slower than PyPy. I was able to get that down to about 17x with <a href="https://gist.github.com/llimllib/bd05381c431d8d01fb434a918e2a2c56" rel="nofollow noreferrer">this code</a> by limiting calls to <code>len</code> and limiting global lookups by running it in a function.</p>
<p>Right now, <code>e.py</code> runs in 5.162 seconds on python 3.6.3 and .297 seconds on PyPy on my machine.</p>
<p>My question is: is this the irreducible speedup of JIT, or is there some way to speed up the CPython answer? (short of extreme means: I could go to Cython/Numba or something?) How do I convince myself that there's nothing more I can do?</p>
<hr/>
<p>See the gist for the list of numbers input file.</p>
<p>As described in <a href="http://adventofcode.com/2017/day/5" rel="nofollow noreferrer">the problem statement</a>, they represent jump offsets. <code>position += offsets[current]</code>, and increment the current offset by 1.  You're done when a jump takes you outside the list.</p>
<p>Here's the example given (the full input that takes 5 seconds is much longer, and has larger numbers):</p>
<pre><code>(0) 3  0  1  -3  - before we have taken any steps.
(1) 3  0  1  -3  - jump with offset 0 (that is, don't jump at all). Fortunately, the instruction is then incremented to 1.
 2 (3) 0  1  -3  - step forward because of the instruction we just modified. The first instruction is incremented again, now to 2.
 2  4  0  1 (-3) - jump all the way to the end; leave a 4 behind.
 2 (4) 0  1  -2  - go back to where we just were; increment -3 to -2.
 2  5  0  1  -2  - jump 4 steps forward, escaping the maze.
</code></pre>
<p>The code:</p>
<pre><code>def run(cmds):
    location = 0
    counter = 0
    while 1:
        try:
            cmd = cmds[location]
            if cmd &gt;= 3:
                cmds[location] -= 1
            else:
                cmds[location] += 1
            location += cmd
            if location &lt; 0:
                print(counter)
                break
            counter += 1
        except:
            print(counter)
            break

if __name__=="__main__":
    text = open("input.txt").read().strip().split("\n")
    cmds = [int(cmd) for cmd in text]
    run(cmds)
</code></pre>
<p>edit: I compiled and ran the code with Cython, that dropped runtime to 2.53s, but that's still almost an order of magnitude slower than PyPy.</p>
<p>edit: <a href="https://gist.github.com/llimllib/bd05381c431d8d01fb434a918e2a2c56#file-f_numba-py" rel="nofollow noreferrer">Numba gets me to within 2x</a></p>
<p>edit: The best <a href="https://gist.github.com/llimllib/bd05381c431d8d01fb434a918e2a2c56#file-g_cython-pyx" rel="nofollow noreferrer">Cython I could write</a> got down to 1.32s, a little over 4x pypy</p>
<p>edit: Moving the <code>cmd</code> variable into a cdef, as suggested by @viraptor, got the Cython down to .157 seconds! Nearly a full order of magnitude faster, and not <em>that</em> far from regular python. Still, I come away from this extremely impressed with the PyPy JIT, which did all this for free!</p>
</div>
<div class="post-text" itemprop="text">
<p>As a baseline for Python, I wrote this in C (and then decided to use C++ for more convenient array I/O).  It compiles efficiently for x86-64 with clang++.  This runs <strong>82x faster than CPython3.6.2 with the code in the question, on a Skylake x86</strong>, so even your faster Python versions are still a couple factors away from keeping up with near-optimal machine code.  (Yes, I looked at the compiler's asm output to check that it did a good job).</p>
<p><strong>Letting a good JIT or ahead-of-time compiler see the loop logic is key for performance here.</strong>  The problem logic is inherently serial, so there's no scope for getting Python to run already-compiled C to do something over the whole array (like NumPy), because there won't be compiled C for this specific problem unless you use Cython or something.  Having each step of the problem go back to the CPython interpreter is death for performance, when its slowness isn't hidden by memory bottlenecks or anything.</p>
<hr/>
<p>Update: <strong>transforming the array of offsets into an array of pointers speeds it up by another factor of 1.5x</strong> (simple addressing mode + removing an <code>add</code> from the critical path loop-carried dependency chain, bringing it down to just the <a href="http://www.7-cpu.com/cpu/Skylake.html" rel="nofollow noreferrer">4 cycle L1D load-use latency</a> for a simple addressing mode (<a href="https://stackoverflow.com/questions/52351397/is-there-a-penalty-when-baseoffset-is-in-a-different-page-than-the-base">when the pointer comes from another load</a>), not 6c = 5c + 1c for an indexed addressing mode + <code>add</code> latency).</p>
<p>But I think we can be generous to Python and not expect it to keep up with algorithm transformations to suit modern CPUs :P  (Especially because I used 32-bit pointers even in 64-bit mode to make sure the 4585 element array was still only 18kiB so it fits in the 32kiB L1D cache.  Like the Linux x32 ABI does, or the AArch64 ILP32 ABI.)</p>
<hr/>
<p>Also, an alternate expression for the update gets gcc to compile it branchless, like clang does.  (Commented out and original <code>perf stat</code> output left in this answer, because it's interesting the see the effect of branchless vs. branchy with mispredicts.)</p>
<pre class="lang-c prettyprint-override"><code>unsigned jumps(int offset[], unsigned size) {
    unsigned location = 0;
    unsigned counter = 0;

    do {
          //location += offset[location]++;            // simple version
          // &gt;=3 conditional version below

        int off = offset[location];

        offset[location] += (off&gt;=3) ? -1 : 1;       // branchy with gcc
        // offset[location] = (off&gt;=3) ? off-1 : off+1;  // branchless with gcc and clang.  

        location += off;

        counter++;
    } while (location &lt; size);

    return counter;
}

#include &lt;iostream&gt;
#include &lt;iterator&gt;
#include &lt;vector&gt;

int main()
{
    std::ios::sync_with_stdio(false);     // makes cin faster
    std::istream_iterator&lt;int&gt; begin(std::cin), dummy;
    std::vector&lt;int&gt; values(begin, dummy);   // construct a dynamic array from reading stdin

    unsigned count = jumps(values.data(), values.size());
    std::cout &lt;&lt; count &lt;&lt; '\n';
}
</code></pre>
<p>With clang4.0.1 <code>-O3 -march=skylake</code>, the inner loop is branchless; it uses a conditional-move for the <code>&gt;=3</code> condition.  I used <code>? :</code> because that's what I was hoping the compiler would do.  <a href="https://gcc.godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(j:1,source:'unsigned+jumps(int+offset%5B%5D,+unsigned+size)+%7B%0A%09unsigned+location+%3D+0%3B%0A%09unsigned+counter+%3D+0%3B%0A%0A%09do+%7B%0A%09%09//location+%2B%3D+offset%5Blocation%5D%2B%2B%3B++++++++++++//+simple+version%0A%0A%09%09int+off+%3D+offset%5Blocation%5D%3B%0A%09%09offset%5Blocation%5D+%2B%3D+(off%3E%3D3)+%3F+-1+:+1%3B+++++++//+%22conditional%22+version%0A%09%09location+%2B%3D+off%3B%0A%0A%09%09counter%2B%2B%3B%0A%09%7D+while+(location+%3C+size)%3B%0A%0A%09return+counter%3B%0A%7D%0A%0A%23include+%3Ciostream%3E%0A%23include+%3Citerator%3E%0A%23include+%3Cvector%3E%0A%0Aint+main()%0A%7B%0A%09std::ios::sync_with_stdio(false)%3B+++++//+makes+cin+faster%0A%09std::istream_iterator%3Cint%3E+begin(std::cin),+dummy%3B%0A%09std::vector%3Cint%3E+values(begin,+dummy)%3B%0A%0A%09unsigned+count+%3D+jumps(values.data(),+values.size())%3B%0A%09std::cout+%3C%3C+count+%3C%3C+!'%5Cn!'%3B%0A%7D%0A'),l:'5',n:'0',o:'C%2B%2B+source+%231',t:'0')),k:35.30937506743596,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:clang401,filters:(b:'0',binary:'1',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'0',trim:'1'),fontScale:0.8957951999999999,libs:!(),options:'-O3+-fverbose-asm+-march%3Dskylake',source:1),l:'5',n:'0',o:'x86-64+clang+4.0.1+(Editor+%231,+Compiler+%231)',t:'0')),k:33.07976551955818,l:'4',m:100,n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:g72,filters:(b:'0',binary:'1',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'0',trim:'1'),fontScale:0.8957951999999999,libs:!(),options:'-O3+-fverbose-asm+-march%3Dskylake',source:1),l:'5',n:'0',o:'x86-64+gcc+7.2+(Editor+%231,+Compiler+%232)',t:'0')),header:(),k:31.610859413005866,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4" rel="nofollow noreferrer">Source + asm on the Godbolt compiler explorer</a> </p>
<pre><code>.LBB1_4:                                # =&gt;This Inner Loop Header: Depth=1
    mov     ebx, edi               ; silly compiler: extra work inside the loop to save code outside
    mov     esi, dword ptr [rax + 4*rbx]  ; off = offset[location]
    cmp     esi, 2
    mov     ecx, 1
    cmovg   ecx, r8d               ; ecx = (off&gt;=3) ? -1 : 1;  // r8d = -1 (set outside the loop)
    add     ecx, esi               ; off += -1 or 1
    mov     dword ptr [rax + 4*rbx], ecx  ; store back the updated off
    add     edi, esi               ; location += off  (original value)
    add     edx, 1                 ; counter++
    cmp     edi, r9d
    jb      .LBB1_4                ; unsigned compare against array size
</code></pre>
<p>Here's the output of  <code>perf stat ./a.out &lt; input.txt</code> (for the clang version), on my i7-6700k Skylake CPU:</p>
<pre><code>21841249        # correct total, matches Python

 Performance counter stats for './a.out':

         36.843436      task-clock (msec)         #    0.997 CPUs utilized          
                 0      context-switches          #    0.000 K/sec                  
                 0      cpu-migrations            #    0.000 K/sec                  
               119      page-faults               #    0.003 M/sec                  
       143,680,934      cycles                    #    3.900 GHz                    
       245,059,492      instructions              #    1.71  insn per cycle         
        22,654,670      branches                  #  614.890 M/sec                  
            20,171      branch-misses             #    0.09% of all branches        

       0.036953258 seconds time elapsed
</code></pre>
<p>The average instructions-per-clock is much lower than 4 because of the data dependency in the loop.  The load address for the next iteration depends on the load+add for this iteration, and out-of-order execution can't hide that.  It can overlap all the work of updating the value of the current location, though.</p>
<p>Changing from <code>int</code> to <code>short</code> has no performance downside (as expected; <a href="https://stackoverflow.com/questions/13092829/any-way-to-move-2-bytes-in-32-bit-x86-using-mov-without-causing-a-mode-switch-or/47516645#47516645"><code>movsx</code> has the same latency as <code>mov</code> on Skylake</a>), but halves the memory consumption so a larger array could fit in L1D cache if needed.</p>
<p>I tried compiling the array into the program (as <code>int offsets[] = { file contents with commas added };</code> so it doesn't have to be read and parsed.  It also made the size a compile-time constant.  This reduced the run time to ~36.2 +/- 0.1 milliseconds, down from ~36.8, so the version that reads from a file was still spending most of its time on the actual problem, not parsing the input.  (Unlike Python, C++ has negligible startup overhead, and my Skylake CPU ramps up to max clock speed in well under a millisecond thanks to hardware P-state management in Skylake.)</p>
<hr/>
<p>As described earlier, pointer-chasing with a simple addressing mode like <code>[rdi]</code> instead of <code>[rdi + rdx*4]</code> has 1c lower latency, and avoids the <code>add</code> (<code>index += offset</code> becomes <code>current = target</code>).  Intel since IvyBridge has zero-latency integer <code>mov</code> between registers so that doesn't lengthen the critical path.  Here's <strong><a href="https://gcc.godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAKxAEZSBnVAV2OUxAHIBSAJgGY8AO2QAbZlgDU3fgGEGBfEIIA6BDOzcADAEFtOgPQAqI5ICSyzMryohAQ1GT%2BvALQAjQpIAOqYQUzEkgh2yADWwsCSdgiYduiSJiYG%2BgYGkm6YyHbMDJjmkgDudsqSBDFRxMR2AJ6lqOl5tAAc4QBCksIKsfGoAGZOAGxtKWlEkr2ewk68oZIAMrQAIpJZyDEq%2BiOSAPJCorUFJKEMhYQIkn29uQTcAKytd8tTZXmiqAXT7oQnfVHo6MRMAwTgwvCFMFtMCpgCpJABbOyhPKeOySYBvNwOKJCeJoOFePCiPIFM5o5DISQuXpCVAuAl5Fw0ul4CG6VIXQJiYqRKlM%2BmUmn0zZstIAdVJqJ8fgCpWCBEk6FQQKEYE48om8qmflIW08JNEjgyF2YBBcfRcVSEwDyxDwwAQ8rsRWqIBWLGUMkWtGFehFOyE8Nqc2EzAAHpIGNUunDSB15cEvF4rCcxodiLMSWUohHFCAQAA3TJEMh1VBbDJZHJ5F4K6r2OF4CkON5ZAg2AMEVhCH4muoNDod8pvD7OdLfDa6La0FS3cO9OxdQK2SQAZVC1VEiLyEGK8S8xFQmLc%2B0kAAkFwVMAbYwjqkazPnWgBKLYAFSHqFQXhcWUqLPiWBJjiVjILUeAnFQOTylKljED%2BwQMBEpxZqiiH4kSfwAkCiFWvCGDbgALBSm7%2BCI1Qvn6%2BYMLCtwUr0JDYgOWChpg8RvHE0i8O0tB0QxxSYQOsp5KgLyBIWxCIbYE46Mw3Z2kIrGSJB%2BIMAA%2Bl4BDEBAfgXL0VyYDc9yPLGsmIcACnxIhABemBPtIADsDyTloACc7JlOBkhvF%2BUQmrShbIMWeA2ScPgkPssKSK%2Bl6IkhaDMKI8R4PiB6FnGsZuL2irKqq8oIgQ/jENJ2gufRgQQKZ8mKXg0j8MsWi1e0eAyPIwV5DITV8A8XF2dwjnCi5JXuV2kixGsun6Zqyj1KiEDOF88qabJrasXZMGFX2NYEHYxDWvKV6YHCVg3M5g2uZc1x3K0zW3MsnqSBVcnmdVyhPhdBlXTdixGI9ZkWQOBBPtZmB9BA72Ga0WiPL1XEPZVz1Ja9FV%2BBpxCqYDfADODn2PB1A19Ys%2BiSMTkjuSJza1DS8oMHYvTCYEgJ4kdwHoLGZhKVBhQHv4srgQNZNogZQmSKGTQDFEDBwvkRSWPEvQHlLwDkqTKybrhzwIJ5WS5GAYADfD/1GKjJz3b9VXxEYb16Zd/BOb6LkG4pRhWOgXi1csxuce0wN46djsW8grCAiU93G77vr2/7bqyRt91Q7bPolYqDl2yVJVR0Y227ULpv%2B5bcMo5p6NPkYgeVMdHUk6TaQ2QeLiYKGpHxGMkq%2BLB%2BOuRnzBeOgdg86bWd7bVmh1SsQcV1x/C9fwABipQ7XtLi0JIrqDx9XHegnp0laX48h6PZsI0%2ByPKKjxfd73/iV8T7IX33eQ7VUBwSp8HjU8WrL2yVZfB/K91rzcLeX9XLxXblxLq4c072WWAUTWGEIA/2OsPUOmkTa8AGJjMe5d96yFGjiLwL4gFDTSHiAkRIUz1F4BGTAYIqg8zcJaNYQIQD8zSLYascpJA0mIAiA0tR5wGhTAgA8zB7Re2zNtJEAZvLfgYcUNYrDJCAgKLaNsuEaxoBxIQds0dErwhqEaYAeA0o5EYnMbAOhxHIHxAYKg5JFGZXlKeFcooDBrg3FuFY/EETIDrr0KspRDjpBCKEFwRB3AhJWDYqgbhvB2DwBJYqp1ASdmIAGUBhVIHQJ9FHZSXgGDaRKNjIyt0TJPX%2BsDXq/U/blMUi2PuOi46QM7rU3E7pY6j3jqnU6yc%2BrdLOm5Aw9S2zLggaPYprRhntlxuA8Btsq4LJJuyNCXgMLiUkkIROrkSo6UuO7Cal17hTNsLjIh2yWl/UUqgRKqkWz7OOZss5AzUgTOuegW5qBkCPC9qbS4GhPRT1qnPJeK9JCb3aIs9kfBeCaPwCM%2BwohoWSHWdM7e51rYfXuG8j5Xzbr7LBnpf5dVAUyDnpcEFro/kb2vkstIciRAIHIScTM5wlbIHsioXgRNFleU%2BQ00Zczlh/LOcAlyGSAgQLmf0gmhQ4HbgebVXBlTmn2xSSNcVxAsmE2cnoAQwgxASHanIGwChAR2DhP8yceqRDiCkC1QgAQ%2B4kEtbqwQNrDWKoCsWF1%2BgdIImEBACiehqn2wUOgPMJq8yRhEKpFlqkw02AgAI3IhCIW0v0UiE4yApjzkXANMNEbTWxDhKpB1dDnXGo9PwbADQjFCAgAWlAwgnyxnQMwOEcJqjNJco2r1FbZB%2BA0MihwzAgQQAyHW1t7bO2pp9FXW%2BrTo772WHkgp%2BYR1AhUJfOwgbYzrvEJu4GgbZ01MuW0mO%2BzV3qU0hAfdo7qLbt3cOg91Ej1PhPaG3MKAWB/zkC1Jdv7ZD/tVHcWQKoeBEOyboTgLbRBcFuJwUgQguBaEQ6gLgQHZnexYGwdqAhaCIYIChmDLbQggH4AMFQAx%2BC3F4E0AitwtADFuPwJotAmOkDg5wAiiG4R0C0FoJDxHSDoc4IhhgIBBNEc4KhltcBYBIFIYSAI5BKBKaJMQFAatgAES0PQCYohCoSfHcJjw9hiAuk4AR0gTNjq7H2MJrACIrREkcwkosxjmEycQw3TIJoOBWcQ9KLjqHSCaWSlwAjLb6LKBXFkVzWgVBNBcrceyKXaAuUy1lrLnG8BuAk5AFtX54USc4C4bY/BKS9HEm4VAuQXALili4BEbB1B1QYOuTcSIqtMk0pgTALg%2B22hsuJnD7A6Cwfg4h5D3mRNcFFgMFwAwCKq25JIAiKhEvLwgLgQgDE%2BD8HoJIWQqB8TKcCAd2gT5CPEffaQMjzgqMUd4Gl24qW9MuXsgMTjXAeOkD4%2BxwTM3QuifE5JsLt3SDycQN%2Bs7GnVMINO2QgIIBgD2V4KQAzRnKBuFM8IHalnrO2eUPZyzoWnPclc7N/AjM2yFlK6F3zgd/CRaC5YELhHbR8cC9F2wBA4sOA4Il5LqX0vZfF7l/L8AisaXbKV8rlWqQ1bqwNxrlIWsKPa51rco3WDjauz9zgCGhOzdEwtpbK22WSA5VQnbcL9v4djCduHMoDu8GuxD7zd2YhxBR7uh7iX0f0Zoy5CjBECK8FoM4Q3f2AcCZNyDrgYOpO3cm5wXg03hOg897JltKLbAgAIkAA%3D%3D%3D" rel="nofollow noreferrer">the source (with comments) + asm for this hacky optimization</a></strong>.  A typical run (with text parsing into a <code>std::vector</code>): <strong><code>23.26 +- 0.05 ms</code></strong>,  90.725 M cycles (3.900 GHz), <code>288.724 M instructions</code> (3.18 IPC).  Interestingly it's actually more total instructions, but runs much faster due to the lower latency of the loop-carried dependency chain.</p>
<hr/>
<p><strong>gcc uses a branch and it's about 2x slower.  (14% of branches are mispredicted according to <code>perf stat</code> on the whole program.</strong>  It's only branching as part of updating the values, but branch misses stall the pipeline so they affect the critical path too, in a way that data dependencies don't here.  This seems like a poor decision by the optimizer.)</p>
<p>Rewriting the conditional as <code>offset[location] = (off&gt;=3) ? off-1 : off+1;</code> convinces gcc to go branchless with asm that looks good.</p>
<p><strong>gcc7.1.1 -O3 -march=skylake</strong> (for the original source that compiles with a branch for <code>(off &lt;= 3) ? : -1 : +1</code>).</p>
<pre><code>Performance counter stats for './ec-gcc':

     70.032162      task-clock (msec)         #    0.998 CPUs utilized          
             0      context-switches          #    0.000 K/sec                  
             0      cpu-migrations            #    0.000 K/sec                  
           118      page-faults               #    0.002 M/sec                  
   273,115,485      cycles                    #    3.900 GHz                    
   255,088,412      instructions              #    0.93  insn per cycle         
    44,382,466      branches                  #  633.744 M/sec                  
     6,230,137      branch-misses             #   14.04% of all branches        

   0.070181924 seconds time elapsed
</code></pre>
<p><strong>vs. CPython (Python3.6.2 on Arch Linux)</strong>:</p>
<pre><code>perf stat python ./orig-v2.e.py
21841249

 Performance counter stats for 'python ./orig-v2.e.py':

       3046.703831      task-clock (msec)         #    1.000 CPUs utilized          
                10      context-switches          #    0.003 K/sec                  
                 0      cpu-migrations            #    0.000 K/sec                  
               923      page-faults               #    0.303 K/sec                  
    11,880,130,860      cycles                    #    3.899 GHz                    
    38,731,286,195      instructions              #    3.26  insn per cycle         
     8,489,399,768      branches                  # 2786.421 M/sec                  
        18,666,459      branch-misses             #    0.22% of all branches        

       3.046819579 seconds time elapsed
</code></pre>
<p>I don't have PyPy or other Python implementations installed, sorry.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can improve small things, but pypy will (most likely) always be faster in this task.</p>
<p>For both CPython and Cython:</p>
<ul>
<li>Don't read in the whole file at once. You can iterate on lines as you go, which saves you (re)allocation costs. This requires you to pre-allocate the array though.</li>
<li>Maybe switch from a list to an <a href="https://docs.python.org/3/library/array.html" rel="nofollow noreferrer">array</a>.</li>
</ul>
<p>For Cython:</p>
<ul>
<li><a href="https://cython.readthedocs.io/en/latest/src/quickstart/cythonize.html" rel="nofollow noreferrer">Mark the variable types</a>. Especially the counters as <code>int</code>s and the commands as an array of <code>int</code>s to skip most type checks.</li>
</ul>
</div>
<span class="comment-copy">From the problem statement: <i>An urgent interrupt arrives from the CPU: it's trapped in a maze of jump instructions,</i>  Obviously the glib answer is: Don't write IRQ handlers in Python :P</span>
<span class="comment-copy">I'm not very familiar with Python (or the performance of different implementations) but yeah I'd expect that JIT will help a <i>lot</i> for a small CPU-bound loop like this.  Looks like an interesting question to me; upvoted.  Probably you should give a quick summary of what the problem is, instead of just an external link.</span>
<span class="comment-copy">Can you put the <code>try</code> around the loop, so an interpreter doesn't have to enter a new <code>try</code> block every time through the loop?</span>
<span class="comment-copy">At the very least, go through this and make sure you check off <a href="https://wiki.python.org/moin/PythonSpeed" rel="nofollow noreferrer">the well known</a> But this belongs on CodeReview</span>
<span class="comment-copy">@PeterCordes moving the try out of the loop gives a minor bump, runtime down to 4.8s üëç</span>
<span class="comment-copy">* good guess with the array, but it's a net loss</span>
<span class="comment-copy">reading the file is irrelevant to the runtime, it all happens as a constant cost at the beginning. You're right though</span>
<span class="comment-copy">I got the cython down to 1.64s with types, good shout</span>
<span class="comment-copy">@llimllib You can do better: rename <code>cmd</code> in the list comprehension, so it doesn't collide with the one in <code>while</code>. Then add a <code>cdef int cmd = 0</code> before the loop. Otherwise, each <code>cmd</code> is constructed as an object.</span>
<span class="comment-copy">@llimllib cython actually beats pypy with all the types supplied: pyx from the gist: <code>1 loop, best of 3: 1.23 s per loop</code>, after the modifications from the last comment: <code>10 loops, best of 3: 68.3 ms per loop</code> (both compiled with <code>-O3</code> for python3.6)</span>
