<div class="post-text" itemprop="text">
<p>
I'm having a really big CSV file that I need to load into a table in sqlite3. I can't load whole CSV content as a variable into RAM because data is so big, that event with defining types for each column it cannot fit into 64 GB of RAM.</p>
<p>
I've tried to use numpy and pandas to load and convert data, but still jumping way above RAM limit.</p>
<p>
I would like to somehow read CSV 1 row at a time (or in smaller batches) and progressively save them into the database to keep RAM usage low. Would be perfect if it could be done using more than one CPU core.
</p>
</div>
<div class="post-text" itemprop="text">
<p>I've found a solution digging myself and combining answers from other Stack Overflow questions. Code should be like this:</p>
<pre><code>import sqlite3
import pandas as pd

def add_to_db(row, con):
    # Function that make insert to your DB, make your own.

def process_chunk(chunk):
    # Handles one chunk of rows from pandas reader.
    con = sqlite3.connect("favorita.db")
    for row in chunk:
        add_to_db(row, con)
    con.commit()

for chunk in pd.read_csv('data.csv', chunksize=100000):
    # Adjust chunksize to your needs and RAM size.
    process_chunk(chunk.values)
</code></pre>
<p>This could surely be further adjusted to use multi-threading, but I couldn't do it due to deadlocks in database when doing inserts in parallel. But if you have time, this is a solid solution.</p>
</div>
<span class="comment-copy">You're likely to be IO bound. Just read one line at a time at first and see how you get on. If it takes too long you can then do batches, and only then, if it's taking too long, think about multiprocessing. You may find multiprocessing evens slows it down as the locks will serialise access.</span>
<span class="comment-copy">What happened when you tried to use the Python <code>csv</code> module?</span>
<span class="comment-copy">Speed is not a problem, it can run slowly, reading and writing data from my SSD to HDD. It only have to fit in limited amount of RAM. Multiprocessing would be a nice bonus, but it's not required in this case.</span>
<span class="comment-copy">I've tried to use csv module to load CSV file, and it uses all my RAM. Don't know yet how to read in in smaller parts.</span>
<span class="comment-copy"><code>for row in csv.reader(file_object):</code> will read one line at a time. See <a href="https://docs.python.org/3/library/csv.html#csv.reader" rel="nofollow noreferrer"><b><code>csv.reader</code></b></a></span>
