<div class="post-text" itemprop="text">
<p>So, I have a large 30k line program I've been writing for a year. It basically gathers non-normalized and non-standardized data from multiple sources and matches everything up after standardizing the sources.</p>
<p>I've written most everything with ordered dictionaries. This allowed me to keep the columns ordered, named and mutable, which made processing easier as values can be assigned/fixed throughout the entire mess of code.</p>
<p>However, I'm currently running out of RAM from all these dictionaries. I've since learned that switching to namedtuples will fix this, the only problem is that these aren't mutable, so that brings up one issue in doing the conversion. </p>
<p>I believe I could use a class to eliminate the immutability, but will may RAM savings be the same? Another option would be to use namedtuples and reassign them to new namedtouples every time a value needs to change (i.e. NewTup=Tup(oldTup.odj1, oldTup.odj2, "something new"). But I think I'd need an explicit way to destroy the old one afterwords or space could become an issue again.</p>
<p>The bottom line is my input files are about 6GB on disk (lots of data). I'm forced to process this data on a server with 16GB RAM and 4 GB swap. I originally programmed all the rows of these various I/O data sets with dictionaries, which is eating too much RAM... but the mutable nature and named referencing was a huge help in faster development, <strong>how do I cut my addition to dictionaries so that I can utilize the cost savings of other objects without rewriting the entire application do to the immutable nature of tuples.</strong></p>
<p>SAMPLE CODE:</p>
<pre><code>    for tan_programs_row in f_tan_programs:
    #stats not included due to urgent need
    tan_id = tan_programs_row["Computer ID"].strip() #The Tanium ID by which to reference all other tanium files (i.e. primary key)
    if("NO RESULT" not in tan_id.upper()):
        tan_programs_name = tan_programs_row["Name"].strip() #The Program Name
        tan_programs_publisher = tan_programs_row["Publisher"].strip() #The Program Vendor
        tan_programs_version = tan_programs_row["Version"].strip() #The Program Vendor

        try:
            unnorm_tan_dict[tan_id] #test the key, if non-existent go to exception
        except KeyError:
            #form the item since it doesn't exist yet
            unnorm_tan_dict[tan_id] = {
                "Tanium ID": tan_id,
                "Computer Name": "INDETERMINATE",
                "Operating System": "INDETERMINATE",
                "Operating System Build Number": "INDETERMINATE",
                "Service Pack": "INDETERMINATE",
                "Country Code": "INDETERMINATE",
                "Manufacturer": "INDETERMINATE",
                "Model": "INDETERMINATE",
                "Serial": "INDETERMINATE"
            }
        unnorm_tan_prog_list.append(rows.TanRawProg._make([tan_id, tan_programs_name, tan_programs_publisher, tan_programs_version]))

for tan_processes_row in f_tan_processes:
    #stats not included due to urgent need
    tan_id = tan_processes_row["Computer ID"].strip() #The Tanium ID by which to reference all other tanium files (i.e. primary key)
    if("NO RESULT" not in tan_id.upper()):
        tan_process_name = tan_processes_row["Running Processes"].strip() #The Program Name
        try:
            unnorm_tan_dict[tan_id] #test the key, if non-existent go to exception
        except KeyError:
            #form the item since it doesn't exist yet
            unnorm_tan_dict[tan_id] = {
                "Tanium ID": tan_id,
                "Computer Name": "INDETERMINATE",
                "Operating System": "INDETERMINATE",
                "Operating System Build Number": "INDETERMINATE",
                "Service Pack": "INDETERMINATE",
                "Country Code": "INDETERMINATE",
                "Manufacturer": "INDETERMINATE",
                "Model": "INDETERMINATE",
                "Serial": "INDETERMINATE"
            }
        unnorm_tan_proc_list.append(rows.TanRawProc._make([tan_id, tan_process_name]))
</code></pre>
<p>*Later on these values are often changed by bringing in other data sets.</p>
</div>
<div class="post-text" itemprop="text">
<p>Just write your own class, and use <code>__slots__</code> to keep the memory footprint to a minimum:</p>
<pre><code>class UnnormTan(object):
    __slots__ = ('tan_id', 'computer_name', ...)
    def __init__(self, tan_id, computer_name="INDETERMINATE", ...):
        self.tan_id = tan_id
        self.computer_name = computer_name
        # ...
</code></pre>
<p>This <em>can</em> get a little verbose perhaps, and if you need to use these as dictionary keys you'll have more typing to do.</p>
<p>There is a project that makes creating such classes easier: <a href="http://www.attrs.org/en/stable/" rel="nofollow noreferrer"><code>attrs</code></a>:</p>
<pre><code>from attrs import attrs, attrib

@attrs(slots=True)
class UnnormTan(object):
    tan_id = attrib()
    computer_name = attrib(default="INDETERMINATE")
    # ...
</code></pre>
<p>Classes created with the <code>attrs</code> library automatically take care of proper equality testing, representation and hashability.</p>
<p>Such objects are the most memory efficient representation of data Python can offer. If that is not enough (and it could well be that it is not enough), you need to look at offloading your data to disk. The easiest way to do that is to use a SQL database, like with the bundled <a href="https://docs.python.org/3/library/sqlite3.html" rel="nofollow noreferrer"><code>sqlite3</code> SQLite library</a>. Even if you used a <code>:memory:</code> temporary database, the database will manage your memory load by swapping out pages to disk as needed.</p>
</div>
<div class="post-text" itemprop="text">
<p>It seems to me, that your main problem is you trying to create a database entirely in memory. You should use actual database like MySQL or PostgreSQL. You can use nice ORM like peewee or django orm for interaction with databases.</p>
<p>On the other hand, if you can't hadle whole data, you can split your data into parts you can handle.</p>
<p>Module "TinyDB" (<a href="http://tinydb.readthedocs.io/en/latest/" rel="nofollow noreferrer">http://tinydb.readthedocs.io/en/latest/</a>) can help you to keep using dictionaries and don't run of RAM.</p>
</div>
<span class="comment-copy">Reducing the dicts memory use will only get you so far - adds some more data or have some other process eating memory on the same server and you'll have memory issues again.  If you really want to solve the problem you will need a solution that does not requires loading that much data in ram (a proper database - wether relational or not - might be part of the solution).</span>
<span class="comment-copy">SQL is on a separate server, network speed is a big bottle neck unfortunately. Additionally, if I put all the data in the database first and foremost and work on every row there, then I can only really do UPDATES... but these are really slow since they require searching and can only be done one at a time plus I know I will touch every row anyway... INSERTS can be grouped in groups of 500 at time, which is about 100 times faster than 500 UPDATES.</span>
<span class="comment-copy">If you don't wand face any lacks of RAM you actually have no other choice but to use DATAbases for your DATA storage and processing. You can easily setup database on your server and there will be no network involved. You can process your rows anyway you wand, but the key point is storing everything you don't need right now somewhere not in memory. You should take a look at MongoDB.</span>
<span class="comment-copy">Unfortunately the laws of certain compliance regulations prevent this... actually requiring separate networks for DB and App servers... unfortunately, companies don't always design things right and I'm left in these crazy positions.</span>
<span class="comment-copy">If you can't use anything besides raw python for processing your data only choices you have left is to 1) learn to split you data into processable parts and process them one by one 2) use TinyDB (slow, but simple) or CodernityDB (more advanced and fast)</span>
