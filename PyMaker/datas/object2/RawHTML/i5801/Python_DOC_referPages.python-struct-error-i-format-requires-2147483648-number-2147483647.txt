<div class="post-text" itemprop="text">
<h3>Problem</h3>
<p>I'm willing to do a feature engineering using multiprocessing module <code>(multiprocessing.Pool.starmap()</code>.
However, it gives an error message as follows. I guess this error message is about the size of inputs (2147483647 = 2^31 âˆ’ 1?), since the same code worked smoothly for a fraction<code>(frac=0.05)</code> of input dataframes(train_scala, test, ts). I convert types of data frame as smallest as possible, however it does not get better. </p>
<p>The anaconda version is 4.3.30 and the Python version is 3.6 (64 bit).
And the memory size of the system is over 128GB with more than 20 cores. 
Would you like to suggest any pointer or solution to overcome this problem? If this problem is caused by a large data for a multiprocessing module, How much smaller data should I use to utilize the multiprocessing module on Python3?</p>
<p><strong>Code:</strong></p>
<pre><code>from multiprocessing import Pool, cpu_count
from itertools import repeat    
p = Pool(8)
is_train_seq = [True]*len(historyCutoffs)+[False]
config_zip = zip(historyCutoffs, repeat(train_scala), repeat(test), repeat(ts), ul_parts_path, repeat(members), is_train_seq)
p.starmap(multiprocess_FE, config_zip)
</code></pre>
<p><strong>Error Message:</strong></p>
<pre><code>Traceback (most recent call last):
  File "main_1210_FE_scala_multiprocessing.py", line 705, in &lt;module&gt;
    print('----Pool starmap start----')
  File "/home/dmlab/ksedm1/anaconda3/envs/py36/lib/python3.6/multiprocessing/pool.py", line 274, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File "/home/dmlab/ksedm1/anaconda3/envs/py36/lib/python3.6/multiprocessing/pool.py", line 644, in get
    raise self._value
  File "/home/dmlab/ksedm1/anaconda3/envs/py36/lib/python3.6/multiprocessing/pool.py", line 424, in _handle_tasks
    put(task)
  File "/home/dmlab/ksedm1/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/home/dmlab/ksedm1/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py", line 393, in _send_bytes
    header = struct.pack("!i", n)
struct.error: 'i' format requires -2147483648 &lt;= number &lt;= 2147483647
</code></pre>
<h3>Extra infos</h3>
<ul>
<li>historyCutoffs is a list of integers</li>
<li>train_scala is a pandas DataFrame (377MB)</li>
<li>test is a pandas DataFrame (15MB)</li>
<li>ts is a pandas DataFrame (547MB)</li>
<li>ul_parts_path is a list of directories (string)</li>
<li>is_train_seq is a list of booleans</li>
</ul>
<p><strong>Extra Code: Method multiprocess_FE</strong></p>
<pre><code>def multiprocess_FE(historyCutoff, train_scala, test, ts, ul_part_path, members, is_train):
    train_dict = {}
    ts_dict = {}
    msno_dict = {}
    ul_dict = {}
    if is_train == True:
        train_dict[historyCutoff] = train_scala[train_scala.historyCutoff == historyCutoff]
    else:
        train_dict[historyCutoff] = test
    msno_dict[historyCutoff] = set(train_dict[historyCutoff].msno)
    print('length of msno is {:d} in cutoff {:d}'.format(len(msno_dict[historyCutoff]), historyCutoff))
    ts_dict[historyCutoff] = ts[(ts.transaction_date &lt;= historyCutoff) &amp; (ts.msno.isin(msno_dict[historyCutoff]))]
    print('length of transaction is {:d} in cutoff {:d}'.format(len(ts_dict[historyCutoff]), historyCutoff))    
    ul_part = pd.read_csv(gzip.open(ul_part_path, mode="rt"))  ##.sample(frac=0.01, replace=False)
    ul_dict[historyCutoff] = ul_part[ul_part.msno.isin(msno_dict[historyCutoff])]
    train_dict[historyCutoff] = enrich_by_features(historyCutoff, train_dict[historyCutoff], ts_dict[historyCutoff], ul_dict[historyCutoff], members, is_train)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The communication protocol between processes uses <em>pickling</em>, and the pickled data is prefixed with the size of the pickled data. For your method, <em>all arguments together</em> are pickled as one object.</p>
<p>You produced an object that when pickled is larger than fits in a <code>i</code> struct formatter (a four-byte signed integer), which breaks the assumptions the code has made.</p>
<p>You could delegate reading of your dataframes to the child process instead, only sending across the metadata needed to load the dataframe. Their combined size is nearing 1GB, way too much data to share over a pipe between your processes.</p>
<p>Quoting from the <a href="https://docs.python.org/3/library/multiprocessing.html#programming-guidelines" rel="noreferrer"><em>Programming guidelines</em> section</a>:</p>
<blockquote>
<p><em>Better to inherit than pickle/unpickle</em></p>
<p>When using the <code>spawn</code> or <code>forkserver</code> start methods many types from <code>multiprocessing</code> need to be picklable so that child processes can use them. <strong>However, one should generally avoid sending shared objects to other processes using pipes or queues. Instead you should arrange the program so that a process which needs access to a shared resource created elsewhere can inherit it from an ancestor process.</strong></p>
</blockquote>
<p>If you are not running on Windows and use either the <code>spawn</code> or <code>forkserver</code> methods, you could load your dataframes as globals <em>before</em> starting your subprocesses, at which point the child processes will 'inherit' the data via the normal OS copy-on-write memory page sharing mechanisms.</p>
</div>
<div class="post-text" itemprop="text">
<p>this problem was fixed in a recent PR to python
<a href="https://github.com/python/cpython/pull/10305" rel="noreferrer">https://github.com/python/cpython/pull/10305</a></p>
<p>if you want, you can make this change locally to make it work for you right away, without waiting for a python and anaconda release.</p>
</div>
<span class="comment-copy">What exactly is the sys.maxsize in this case?  2147483647 = 2.147GB? Am I able to control the size threshold?</span>
<span class="comment-copy">@SUNDONG: Sorry, it's not <code>sys.maxsize</code>, it's the <code>i</code> struct formatter, so a 4-byte integer, signed. You can't control that size threshold. You are passing around objects that are really way, way too large for such sharing.</span>
<span class="comment-copy">OKAY, I will try loading dataframes in the child method <code>multiprocess_FE</code> instead. However, I could pass smaller dataframes (about the size of row = 1,000-10,000) without any problems.</span>
<span class="comment-copy">@Emmanuel-lin: if your results are that large, write them to some kind of shared storage. A file or a database.</span>
<span class="comment-copy">@MartijnPieters Great answers, thank you! Just a comment, though - is this not massively frustrating? Very old mindset. If passing data to subprocesses over a network, for example, I understand the issue; but to do so between processes with local upwards of 50GB RAM, shared buses etc.. - who cares. Should be scalable. Issue a warning for Pete's sake. Don't hard break on a struct.error.</span>
