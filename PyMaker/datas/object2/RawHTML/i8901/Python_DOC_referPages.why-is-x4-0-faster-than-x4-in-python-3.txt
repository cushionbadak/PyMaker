<div class="post-text" itemprop="text">
<p>Why is <code>x**4.0</code> faster than <code>x**4</code>? I am using CPython 3.5.2.</p>
<pre><code>$ python -m timeit "for x in range(100):" " x**4.0"
  10000 loops, best of 3: 24.2 usec per loop

$ python -m timeit "for x in range(100):" " x**4"
  10000 loops, best of 3: 30.6 usec per loop
</code></pre>
<p>I tried changing the power I raised by to see how it acts, and for example if I raise x to the power of 10 or 16 it's jumping from 30 to 35, but if I'm raising by <strong>10.0</strong> as a float, it's just moving around 24.1~4.</p>
<p>I guess it has something to do with float conversion and powers of 2 maybe, but I don't really know.</p>
<p>I noticed that in both cases powers of 2 are faster, I guess since those calculations are more native/easy for the interpreter/computer. But still, with floats it's almost not moving. <code>2.0 =&gt; 24.1~4 &amp; 128.0 =&gt; 24.1~4</code> <strong>but</strong> <code>2 =&gt; 29 &amp; 128 =&gt; 62</code></p>
<p><hr/> <a href="https://stackoverflow.com/users/2617068/tigerhawkt3">TigerhawkT3</a> pointed out that it doesn't happen outside of the loop. I checked and the situation only occurs (from what I've seen) when the <strong>base</strong> is getting raised. Any idea about that?</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>Why is <code>x**4.0</code> <em>faster</em> than <code>x**4</code> in Python 3<sup>*</sup>?</p>
</blockquote>
<p>Python 3 <code>int</code> objects are a full fledged object designed to support an arbitrary size; due to that fact, they are <a href="https://github.com/python/cpython/blob/master/Objects/longobject.c#L4051" rel="noreferrer">handled as such on the C level</a> (see how all variables are declared as <code>PyLongObject *</code> type in <code>long_pow</code>). This also makes their exponentiation a lot more <em>trickier</em> and <em>tedious</em> since you need to play around with the <code>ob_digit</code> array it uses to represent its value to perform it. (<a href="https://github.com/python/cpython/blob/master/Objects/longobject.c#L4148" rel="noreferrer">Source for the brave.</a> -- See: <a href="https://stackoverflow.com/questions/40344159/understanding-memory-allocation-for-large-integers-in-python">Understanding memory allocation for large integers in Python</a> for more on <code>PyLongObject</code>s.) </p>
<p>Python <code>float</code> objects, on the contrary, <em>can be transformed</em> to a C <code>double</code> type (by using <a href="https://docs.python.org/3/c-api/float.html#c.PyFloat_AsDouble" rel="noreferrer"><code>PyFloat_AsDouble</code></a>) and operations can be performed <a href="https://github.com/python/cpython/blob/master/Objects/floatobject.c#L686" rel="noreferrer">using those native types</a>. <em>This is great</em> because, after checking for relevant edge-cases, it allows Python to <a href="https://github.com/python/cpython/blob/master/Objects/floatobject.c#L784" rel="noreferrer">use the platforms' <code>pow</code></a> (<a href="http://en.cppreference.com/w/c/numeric/math/pow" rel="noreferrer">C's <code>pow</code>, that is</a>) to handle the actual exponentiation:</p>
<pre class="lang-c prettyprint-override"><code>/* Now iv and iw are finite, iw is nonzero, and iv is
 * positive and not equal to 1.0.  We finally allow
 * the platform pow to step in and do the rest.
 */
errno = 0;
PyFPE_START_PROTECT("pow", return NULL)
ix = pow(iv, iw); 
</code></pre>
<p>where <code>iv</code> and <code>iw</code> are our original <code>PyFloatObject</code>s as C <code>double</code>s.</p>
<blockquote>
<p>For what it's worth: Python <code>2.7.13</code> for me is a factor <code>2~3</code> faster, and shows the inverse behaviour.</p>
</blockquote>
<p>The previous fact <em>also explains</em> the discrepancy between Python 2 and 3 so, I thought I'd address this comment too because it is interesting.</p>
<p>In Python 2, you're using the old <code>int</code> object that differs from the <code>int</code> object in Python 3 (all <code>int</code> objects in 3.x are of <code>PyLongObject</code> type). In Python 2, there's a distinction that depends on the value of the object (or, if you use the suffix <code>L/l</code>):</p>
<pre><code># Python 2
type(30)  # &lt;type 'int'&gt;
type(30L) # &lt;type 'long'&gt;
</code></pre>
<p>The <code>&lt;type 'int'&gt;</code> you see here <em>does the same thing <code>float</code>s do</em>, it gets safely converted into a C <code>long</code> <a href="https://github.com/python/cpython/blob/2.7/Objects/intobject.c#L726" rel="noreferrer">when exponentiation is performed on it</a> (The <code>int_pow</code> also hints the compiler to put 'em in a register if it can do so, so that <em>could</em> make a difference):</p>
<pre class="lang-c prettyprint-override"><code>static PyObject *
int_pow(PyIntObject *v, PyIntObject *w, PyIntObject *z)
{
    register long iv, iw, iz=0, ix, temp, prev;
/* Snipped for brevity */    
</code></pre>
<p>this allows for a good speed gain. </p>
<p>To see how sluggish <code>&lt;type 'long'&gt;</code>s are in comparison to <code>&lt;type 'int'&gt;</code>s, if you wrapped the <code>x</code> name in a <code>long</code> call in Python 2 (essentially forcing it to use <code>long_pow</code> as in Python 3), the speed gain disappears:</p>
<pre><code># &lt;type 'int'&gt;
(python2) ➜ python -m timeit "for x in range(1000):" " x**2"       
10000 loops, best of 3: 116 usec per loop
# &lt;type 'long'&gt; 
(python2) ➜ python -m timeit "for x in range(1000):" " long(x)**2"
100 loops, best of 3: 2.12 msec per loop
</code></pre>
<p>Take note that, though the one snippet transforms the <code>int</code> to <code>long</code> while the other does not (as pointed out by @pydsinger), this cast is not the contributing force behind the slowdown. The implementation of <code>long_pow</code> is. (Time the statements solely with <code>long(x)</code> to see).</p>
<blockquote>
<p>[...] it doesn't happen outside of the loop. [...] Any idea about that?</p>
</blockquote>
<p>This is CPython's peephole optimizer folding the constants for you. You get the same exact timings either case since there's no actual computation to find the result of the exponentiation, only loading of values:</p>
<pre><code>dis.dis(compile('4 ** 4', '', 'exec'))
  1           0 LOAD_CONST               2 (256)
              3 POP_TOP
              4 LOAD_CONST               1 (None)
              7 RETURN_VALUE
</code></pre>
<p>Identical byte-code is generated for <code>'4 ** 4.'</code> with the only difference being that the <code>LOAD_CONST</code> loads the float <code>256.0</code> instead of the int <code>256</code>:</p>
<pre><code>dis.dis(compile('4 ** 4.', '', 'exec'))
  1           0 LOAD_CONST               3 (256.0)
              2 POP_TOP
              4 LOAD_CONST               2 (None)
              6 RETURN_VALUE
</code></pre>
<p>So the times are identical.</p>
<hr/>
<p><sup>*All of the above apply solely for CPython, the reference implementation of Python. Other implementations might perform differently.</sup></p>
</div>
<div class="post-text" itemprop="text">
<p>If we look at the bytecode, we can see that the expressions are purely identical. The only difference is a type of a constant that will be an argument of <code>BINARY_POWER</code>. So it's most certainly due to an <code>int</code> being converted to a floating point number down the line.</p>
<pre><code>&gt;&gt;&gt; def func(n):
...    return n**4
... 
&gt;&gt;&gt; def func1(n):
...    return n**4.0
... 
&gt;&gt;&gt; from dis import dis
&gt;&gt;&gt; dis(func)
  2           0 LOAD_FAST                0 (n)
              3 LOAD_CONST               1 (4)
              6 BINARY_POWER
              7 RETURN_VALUE
&gt;&gt;&gt; dis(func1)
  2           0 LOAD_FAST                0 (n)
              3 LOAD_CONST               1 (4.0)
              6 BINARY_POWER
              7 RETURN_VALUE
</code></pre>
<p>Update: let's take a look at <a href="https://github.com/python/cpython/blob/72dccde884d89586b0cafd990675b7e21720a81f/Objects/abstract.c" rel="nofollow noreferrer">Objects/abstract.c</a> in the CPython source code:</p>
<pre class="lang-c prettyprint-override"><code>PyObject *
PyNumber_Power(PyObject *v, PyObject *w, PyObject *z)
{
    return ternary_op(v, w, z, NB_SLOT(nb_power), "** or pow()");
}
</code></pre>
<p><code>PyNumber_Power</code> calls <code>ternary_op</code>, which is too long to paste here, so <a href="https://github.com/python/cpython/blob/72dccde884d89586b0cafd990675b7e21720a81f/Objects/abstract.c#L831" rel="nofollow noreferrer">here's the link</a>.</p>
<p>It calls the <code>nb_power</code> slot of <code>x</code>, passing <code>y</code> as an argument.</p>
<p>Finally, in <code>float_pow()</code> at line 686 of <a href="https://github.com/python/cpython/blob/6f0eb93183519024cb360162bdd81b9faec97ba6/Objects/floatobject.c#L686" rel="nofollow noreferrer">Objects/floatobject.c</a> we see that arguments are converted to a C <code>double</code> right before the actual operation:</p>
<pre class="lang-c prettyprint-override"><code>static PyObject *
float_pow(PyObject *v, PyObject *w, PyObject *z)
{
    double iv, iw, ix;
    int negate_result = 0;

    if ((PyObject *)z != Py_None) {
        PyErr_SetString(PyExc_TypeError, "pow() 3rd argument not "
            "allowed unless all arguments are integers");
        return NULL;
    }

    CONVERT_TO_DOUBLE(v, iv);
    CONVERT_TO_DOUBLE(w, iw);
    ...
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Because one is correct, another is approximation.</p>
<pre><code>&gt;&gt;&gt; 334453647687345435634784453567231654765 ** 4.0
1.2512490121794596e+154
&gt;&gt;&gt; 334453647687345435634784453567231654765 ** 4
125124901217945966595797084130108863452053981325370920366144
719991392270482919860036990488994139314813986665699000071678
41534843695972182197917378267300625
</code></pre>
</div>
<span class="comment-copy">For what it's worth: Python 2.7.13 for me is a factor 2~3 faster, <i>and</i> shows the inverse behaviour: an integer exponent is faster than a floating point exponent.</span>
<span class="comment-copy">@Evert yup, I got 14 usec for <code>x**4.0</code> and 3.9 for <code>x**4</code>.</span>
<span class="comment-copy">Whatever it is, it's related to the loop over a <code>range</code>, as timing only the <code>**</code> operation itself yields no difference between integers and floats.</span>
<span class="comment-copy">The difference only appears when looking up a variable (<code>4**4</code> is just as fast as <code>4**4.0</code>), and this answer doesn't touch on that at all.</span>
<span class="comment-copy">But, constants will get folded @TigerhawkT3 (<code>dis(compile('4 ** 4', '', 'exec'))</code>) so the time should be <i>exactly</i> the same.</span>
<span class="comment-copy">Your last timings seem not to show what you say. <code>long(x)**2.</code> is still faster than <code>long(x)**2</code> by a factor of 4-5. (Not one of the downvoters, though)</span>
<span class="comment-copy">@mbomb007 the elimination of the <code>&lt;type 'long'&gt;</code> type in Python 3 is probably explained by the efforts made to simplify the language. If you can have one type to represent integers it is more manageable than two (and worrying about converting from one to the other when necessary, users getting confused etc). The speed gain is secondary to that. The rationale section of <a href="https://www.python.org/dev/peps/pep-0237/" rel="nofollow noreferrer">PEP 237</a> also offers some more insight.</span>
<span class="comment-copy">@Jean-FrançoisFabre I believe that's due to constant folding.</span>
<span class="comment-copy">I think the implication that there is a conversion and they aren't handled differently down the line "most certainly" is a bit of a stretch without a source.</span>
<span class="comment-copy">@Mitch - Particularly since, in this particular code, there's no difference in the execution time for those two operations. The difference only arises with the OP's loop. This answer is jumping to conclusions.</span>
<span class="comment-copy">Why are you only looking at <code>float_pow</code> when that doesn't even run for the slow case?</span>
<span class="comment-copy">@TigerhawkT3: <code>4**4</code> and <code>4**4.0</code> get constant-folded. That's an entirely separate effect.</span>
<span class="comment-copy">Downvoter, care to explain why?</span>
<span class="comment-copy">I don't know why that downvoter downvoted but I did because this answer doesn't answer the question. Just because something is correct does not in any way imply it is faster or slower. One is slower than the other because one can work with C types while the other has to work with Python Objects.</span>
<span class="comment-copy">Thanks for the explanation. Well, I really thought it was obvious that it's faster to calculate just the approximation of a number to 12 or so digits, than to calculate all of them exactly. After all, the only reason why we use approximations is that they are faster to calculate, right?</span>
