<div class="post-text" itemprop="text">
<p>I have the following simple code causing an error regarding caching: </p>
<pre class="lang-py prettyprint-override"><code>trips_in = sc.textFile("trip_data.csv")
trips = trips_in.map(lambda l: l.split(",")).map(lambda x: parseTrip(x)).cache()

trips.count()
</code></pre>
<p>The function <code>parseTrip()</code> gets a list of strings and creates and returns a class Trip:</p>
<pre class="lang-py prettyprint-override"><code>class Trip:
  def __init__(self, id, duration):
    self.id = id
    self.duration = duration
</code></pre>
<p>I get the error right after the action <code>count()</code>. However, if I remove the <code>cache()</code> at the end of second line everything work fine. 
According to the error the problem is that the class Trip can not be pickled:</p>
<pre><code>PicklingError: Can't pickle __main__.Trip: attribute lookup __main__.Trip failed
</code></pre>
<p>So how can I make it picklable (if it is an actual word)? 
Note that I am using a Databricks notebook so I can not make a separate .py for class definition to make it picklable. </p>
</div>
<div class="post-text" itemprop="text">
<p>Environment does not affect the answer - if you want to use custom classes it has to be importable on every node in the cluster. </p>
<ul>
<li><p>For a single module you can easily use <code>SparkContext.addPyFile</code> with URL to a <a href="https://gist.github.com/" rel="nofollow noreferrer">GitHub Gist</a> (or another supported format: "<a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.addPyFile" rel="nofollow noreferrer"><em>file in HDFS (or other Hadoop-supported filesystems), or an
HTTP, HTTPS or FTP URI</em></a>")</p>
<ul>
<li>Create a gist.</li>
<li>Click on Raw link and copy URL.</li>
<li><p>In your notebook call:</p>
<pre><code>sc.addPyFile(raw_gist_url)
</code></pre></li>
</ul></li>
<li><p>For complex dependencies you distribute egg files.</p>
<ul>
<li><p>Create <a href="https://docs.python.org/3/distutils/setupscript.html" rel="nofollow noreferrer">Python package</a> <a href="https://stackoverflow.com/questions/6344076/differences-between-distribute-distutils-setuptools-and-distutils2#14753678">using <code>setuptools</code></a>.</p>
<p>Directory structure:</p>
<pre><code>.
├── setup.py
└── trip
    └── __init__.py
</code></pre>
<p>Example setup file:</p>
<pre><code>#!/usr/bin/env python

from setuptools import setup

setup(name='trip',
      version='0.0.1',
      description='Trip',
      author='Jane Doe',
      author_email='jane@example.com',
      url='https://example.com',
      packages=['trip'],)
</code></pre></li>
<li><p>Create egg file:</p>
<pre><code>python setup.py bdist_egg
</code></pre>
<p>This will create <code>dist</code> directory with <code>trip-0.0.1-pyX.Y.egg</code> file</p></li>
<li><p>Go to Databricks dashboard -&gt; New -&gt; Libary and upload egg file from dist directory:</p>
<p><a href="https://i.stack.imgur.com/jZvZl.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/jZvZl.png"/></a></p></li>
<li><p>Attach library to the cluster you want to use.</p></li>
</ul></li>
<li><p>Finally if all you want is a record type you can use <code>namedtuple</code> without any additional steps:</p>
<pre><code>from collections import namedtuple

Trip = namedtuple('Trip', ['id', 'duration'])
</code></pre></li>
</ul>
</div>
