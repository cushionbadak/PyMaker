<div class="post-text" itemprop="text">
<p>I have spent a considerable time trying to get the Linux diff and patch tools to work in python with strings. To achieve this I try to use named pipes since they seem the most robust way to go. The problem is that this doesn't work for big files.</p>
<p>Example:</p>
<pre><code>a, b = str1, str2 # ~1MB each string

fname1, fname2 = mkfifos(2)
proc = subprocess.Popen(['diff', fname1, fname2], \
                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)

print('Writing first file.')
with open(fname1, 'w') as f1:
    f1.write(a)
print('Writing second file.')
with open(fname2, 'w') as f2:
    f2.write(b)
</code></pre>
<p>This hangs at the first write. If figured out that if I use <code>a[:6500]</code> it hangs on the second write. So I would assume it has something to do with the buffer. I tried manually flushing after each write, closing, using the lowlevel <code>os.open(f, 'r', 0)</code> with 0 buffer but still the same issue.</p>
<p>I thought of looping through the write in chunks but that feels wrong in a high level language like Python. Any ideas what I am doing wrong?</p>
</div>
<div class="post-text" itemprop="text">
<p>A named pipe is still a pipe. It has a finite buffer on Linux. You can't write an unlimited output unless someone reads from the other end of the pipe at the same time.</p>
<p>If <code>f1.write(a)</code> blocks then it means <code>diff</code> doesn't read all the input files at once (it seems logical: the purpose of the <code>diff</code> program is to compare files <em>line by line</em>--reading from the first file won't be too far ahead of the reading from the second file).</p>
<p>To write different data to different places concurrently, you could use threads/async.io:</p>
<pre><code>#!/usr/bin/env python3
from subprocess import Popen, PIPE
from threading import Thread

def write_input_async(path, text):
    def writelines():
        with open(path, 'w') as file:
            for line in text.splitlines(keepends=True):
                file.write(line)
    Thread(target=writelines, daemon=True).start()

with named_pipes(2) as paths, \
    Popen(['diff'] + paths, stdout=PIPE,stderr=PIPE, universal_newlines=True) as p:
    for path, text in zip(paths, [a, b]):
        write_input_async(path, text)
    output, errors = p.communicate()
</code></pre>
<p>where <a href="https://stackoverflow.com/a/28840955/4279"><code>named_pipes(n)</code> context manager is defined here</a>.</p>
<p>Note: unless you call <code>.communicate()</code>; the <code>diff</code> process may hang as soon as any of its stdout/stderr OS pipe buffers fill up.</p>
<hr/>
<p>You could <a href="https://docs.python.org/3/library/difflib.html" rel="nofollow noreferrer">consider whether <code>difflib.context_diff(a, b)</code> would work in your case</a>.</p>
</div>
<span class="comment-copy">Wouldn't the fifo buffer fill up if you just write to one first - or would diff empty only on of them gradually?</span>
<span class="comment-copy">@J.P.Petersen yes I assume that is what is happening; diff is reading both files gradually so it ends up in a deadlock. It works fine if the first write is done in a thread.</span>
<span class="comment-copy">if the input strings <code>str1</code>, <code>str2</code> are from other processes; take a look at the question from <a href="http://stackoverflow.com/tags/subprocess/info">the subprocess tag description</a> that shows <a href="http://stackoverflow.com/q/28840575/4279">"how to emulate the bash process substitution such as <code>a &lt;(b) &lt;(c)</code>"</a></span>
