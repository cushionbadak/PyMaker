<div class="post-text" itemprop="text">
<p>Is there a well-hidden way to read tokens from a file or file-like object <em>without</em> reading entire lines?  The application I immediately have (someone else's problem, not mine) is transposing a large matrix with a few very long rows, essentially performing an <code>itertools.izip()</code> on iterators that pick out the elements of a single column.  The idea is not not have the entire file in memory during iteration.</p>
<p>The rows are space-delimited ASCII decimal numbers.</p>
<p>The problem would be simple with Java's Scanner class, but I don't see anything in the Python Standard Library that appears to tokenize without having the whole input in a string.</p>
<p>For the record, I know how to write this on my own.  I'm just wondering if there's a standard tool that I missed.  Something FOSS/libre that can be EasyInstalled is good, too, but I don't see anything on PYPI either.</p>
<p>The full problem was to take the sample input:</p>
<pre><code>"123 3 234234 -35434 112312 54 -439 99 0 42\n" +
"13 456 -78 910 333 -44 5555 6 8"
</code></pre>
<p>...and produce the output (as a generator, without reading all of very long rows into memory at once:</p>
<pre><code>[123, 13], [3, 456], [234234, -78], ...etc
</code></pre>
<p>As I said, it's essentially itertools.izip(iterator1, iterator2), pointing iterator1 at the start of the file, and iterator2 just past the newline to read the second row.</p>
</div>
<div class="post-text" itemprop="text">
<p>To read tokens from a file one by one; you could use <code>re</code> module to generate tokens from a <a href="http://docs.python.org/3/library/mmap" rel="nofollow">memory-mapped file</a>:</p>
<pre><code>#!/usr/bin/env python3
import re
import sys
from mmap import ACCESS_READ, mmap    

def generate_tokens(filename, pattern):
    with open(filename) as f, mmap(f.fileno(), 0, access=ACCESS_READ) as mm:
         yield from re.finditer(pattern, mm)

# sum all integers in a file specified at the command-line
print(sum(int(m.group()) for m in generate_tokens(sys.argv[1], br'\d+')))
</code></pre>
<p>It works even if the file doesn't fit in memory.    </p>
</div>
<div class="post-text" itemprop="text">
<p>Here is a generator that processes a file one character at a time and yields tokens when whitespace is encountered.</p>
<pre><code>def generate_tokens(path):
    with open(path, 'r') as fp:
        buf = []
        while True:
            ch = fp.read(1)
            if ch == '':
                break
            elif ch.isspace():
                if buf:
                    yield ''.join(buf)
                    buf = []
            else:
                buf.append(ch)

if __name__ == '__main__':
    for token in generate_tokens('input.txt'):
        print token
</code></pre>
<p>To be more generic, it looks like you might be able to use the <code>re</code> module as described at this link. Just feed the input with a generator from your file to avoid reading the whole file at once.</p>
<p><a href="https://stackoverflow.com/questions/1751949/python-equivalent-of-rubys-stringscanner">Python equivalent of ruby's StringScanner?</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You can read file in chunks with <code>file.read(size)</code>. I would not recomment however to read by 1 byte, as this will drastically affect performance. Following snippet (not much tested, use on your own risk) reads file in chunks an yields numbers. You'll have to read through file first to determine rows starting position though.</p>
<pre><code>def values_chunks(file_object, pos_from=0, chunk_size=32*1024):
    file_object.seek(pos_from)
    eol = False
    tail = ''
    while True:
        raw_data = file_object.read(chunk_size)
        raw_data = tail + raw_data
        raw_data = raw_data.split('\n', 1) # to check for eol, split in tuple
        if len(raw_data) &gt; 1:
            eol = True
        raw_data = raw_data[0]
        raw_values = raw_data.split()
        if not eol and raw_data[-1] != ' ':
            tail = raw_values[-1]
            raw_values = raw_values[:-1]
        else:
            tail = ''
        for value in raw_values: # either case we need only first tuple elem
            yield int(value)
        if not raw_data[0] or eol: # eof/eol
            break

&gt;&gt;&gt; with open('test', 'wb') as test:
...     test.write(' '.join(map(str, range(10**5))))
...     test.write('\n')
...     test.write(' '.join(map(str, range(10**4))))
...
&gt;&gt;&gt; values = list(values_chunks(open('test', 'rb')))
&gt;&gt;&gt; len(values)
100000
&gt;&gt;&gt; sum(values)
4999950000L
</code></pre>
</div>
<span class="comment-copy">I'm pretty sure there's no such built-in available.</span>
<span class="comment-copy">Could you be more specific? Do you want to print a specific column from a file without holding full lines in memory?</span>
<span class="comment-copy">@J.F.Sebastian Supposedly the lines are millions of numbers and I have no idea why they are in a single file as two text lines.</span>
<span class="comment-copy">@stranac Thanks.  I didn't see anything, either, but it wouldn't be the first time I missed something.</span>
<span class="comment-copy">Looks interesting, and semi-portable.  I don't care much for the "from" import hiding the module name, but that's a minor nit.</span>
<span class="comment-copy">@MikeHousky: It should work as is on Unix and Windows. Let me know if it is not so.</span>
<span class="comment-copy">Three useful answers, all upvotes.  I picked this one to accept because it called my attention to the mmap package, which I hadn't seen before.</span>
<span class="comment-copy">Thanks for the code and the link.  I hadn't thought of looking for a parallel to something in Ruby.  A similar search for Java Scanner replacement gave only full-string solutions.</span>
<span class="comment-copy">accumulating input one byte at a time is <a href="http://stackoverflow.com/a/3054831/4279">very slow</a> in Python</span>
<span class="comment-copy">This is closer to what I'd write, except maybe to search specifically for whitespace rather than use <code>split</code> calls.  If the "client" for this favor (not really a client if I'm not really getting paid, right? :^/ ) has a problem with megabytes, then the tokenization is probably worth minimizing too. Thanks for the code, and the different ideas.  They could certainly be useful on a less-weird problem!</span>
<span class="comment-copy">you could look at how <a href="http://hg.python.org/cpython/file/097389413dd3/Lib/_pyio.py#l451" rel="nofollow noreferrer"><code>.readline(limit)</code>, <code>.readlines(hint)</code> are implemented</a>. Just replace <code>'\n'</code> (newline) with <code>' '</code> (space).</span>
