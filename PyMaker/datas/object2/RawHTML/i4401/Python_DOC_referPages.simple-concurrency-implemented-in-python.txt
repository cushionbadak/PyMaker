<div class="post-text" itemprop="text">
<p><strong>Purpose of the question</strong>: learn more about ways to implement concurrency in Python / experimenting.</p>
<p><strong>Context</strong>: I want to count all of the words in all of the files that match a particular pattern. The idea is that I can invoke the function <code>count_words('/foo/bar/*.txt')</code> and all of the words (i.e., strings separated by one or more whitespace characters) will be counted.</p>
<p>In the implementation I am looking for ways to implement <code>count_words</code> using concurrency. So far I managed to use <code>multiprocessing</code> and <code>asyncio</code>. </p>
<p>Do you see alternative approaches to do the same task?</p>
<p>I did not use <code>threading</code> as I noticed the performance improvement was not that impressive due to the limitation of the Python GIL.</p>
<pre><code>import asyncio
import multiprocessing
import time
from pathlib import Path
from pprint import pprint


def count_words(file):
    with open(file) as f:
        return sum(len(line.split()) for line in f)


async def count_words_for_file(file):
    with open(file) as f:
        return sum(len(line.split()) for line in f)


def async_count_words(path, glob_pattern):
    event_loop = asyncio.get_event_loop()
    try:
        print("Entering event loop")
        for file in list(path.glob(glob_pattern)):
            result = event_loop.run_until_complete(count_words_for_file(file))
            print(result)
    finally:
        event_loop.close()


def multiprocess_count_words(path, glob_pattern):
    with multiprocessing.Pool(processes=8) as pool:
        results = pool.map(count_words, list(path.glob(glob_pattern)))
        pprint(results)


def sequential_count_words(path, glob_pattern):
    for file in list(path.glob(glob_pattern)):
        print(count_words(file))


if __name__ == '__main__':
    benchmark = []
    path = Path("../data/gutenberg/")
    # no need for benchmark on sequential_count_words, it is very slow!
    # sequential_count_words(path, "*.txt")

    start = time.time()
    async_count_words(path, "*.txt")
    benchmark.append(("async version", time.time() - start))

    start = time.time()
    multiprocess_count_words(path, "*.txt")
    benchmark.append(("multiprocess version", time.time() - start))

    print(*benchmark)
</code></pre>
<p>For simulating large quantity of files, I downloaded some books from Project Gutenberg (<a href="http://t.dripemail2.com/c/eyJhY2NvdW50X2lkIjoiNjE2ODIxOCIsImRlbGl2ZXJ5X2lkIjoiMjQ0MjQzODQxOCIsInVybCI6Imh0dHA6Ly9ndXRlbmJlcmcub3JnLz9fX3M9ajY3cGpxaXpzY29wZHRueHNwd3ZcdTAwMjZ1dG1fc291cmNlPWRyaXBcdTAwMjZ1dG1fbWVkaXVtPWVtYWlsXHUwMDI2dXRtX2NhbXBhaWduPVdQRStKYW4rMjAxOCtjb2hvcnRcdTAwMjZ1dG1fY29udGVudD0lNUJXZWVrbHkrUHl0aG9uK0V4ZXJjaXNlK0oxOCU1RCtFeGVyY2lzZSslMjMxNSUzQStBbGwrd29yZCtjb3VudCJ9" rel="nofollow noreferrer">http://gutenberg.org/</a>) and used the following command to create several duplicates of the same file.</p>
<pre><code>for i in {000..99}; do cp 56943-0.txt $(openssl rand -base64 12)-$i.txt; done
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><code>async def</code> doesn't make function calls concurrent magically, in asyncio you need to explicitly give up execution to allow other coroutines run concurrently by using <code>await</code> on awaitables. That said, your current <code>count_words_for_file</code> is still executed sequentially.</p>
<p>You may want to introduce <a href="https://github.com/Tinche/aiofiles" rel="nofollow noreferrer">aiofiles</a> to defer the blocking file I/O into threads, allowing concurrent file I/O in different coroutines. Even with that, the piece of CPU-bound code that calculates the number of words still run sequentially in the same main thread. To parallelize that, you still need multiple processes and multiple CPUs (or multiple computers, check <a href="http://www.celeryproject.org/" rel="nofollow noreferrer">Celery</a>).</p>
<p>Besides, there is an issue in your asyncio code - <code>for ... run_until_complete</code> again make the function calls run sequentially. You'll need <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop.create_task" rel="nofollow noreferrer"><code>loop.create_task()</code></a> to start them concurrently, and <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.wait" rel="nofollow noreferrer"><code>aysncio.wait()</code></a> to join the results.</p>
<pre><code>import aiofiles

...

async def count_words_for_file(file):
    async with aiofiles.open(file) as f:
        rv = sum(len(line.split()) async for line in f)
        print(rv)
        return rv


async def async_count_words(path, glob_pattern):
    await asyncio.wait([count_words_for_file(file)
                        for file in list(path.glob(glob_pattern))])
    # asyncio.wait() calls loop.create_task() for you for each coroutine

...

if __name__ == '__main__':

    ...

    loop = asyncio.get_event_loop()
    start = time.time()
    loop.run_until_complete(async_count_words(path, "*.txt"))
    benchmark.append(("async version", time.time() - start))
</code></pre>
</div>
<span class="comment-copy">Thanks for your comment - I was not aware of <code>aiofiles</code>. May I ask you to add some code to your answer related to the changes you are suggesting? Also, I was in the impression that <code>event_loop.run_until_complete</code> was running the function concurrently. When running the code I clearly see a difference compare to the pure sequential method. Thanks</span>
<span class="comment-copy">Updated. I haven't tried, but the difference in your timing is really weird.</span>
