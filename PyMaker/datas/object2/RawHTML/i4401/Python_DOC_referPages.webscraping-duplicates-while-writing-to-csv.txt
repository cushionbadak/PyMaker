<div class="post-text" itemprop="text">
<p>I'm trying to scrape all the urls of the posts on this website: <a href="http://esencjablog.pl/" rel="nofollow noreferrer">http://esencjablog.pl/</a> </p>
<p>I'm new to python and web scraping, mt code works but it produces a lot of duplicates - what I am doing wrong?</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import csv

startURL = 'http://esencjablog.pl/'
f = csv.writer(open('test.csv', 'a+', newline=''))
f.writerow(['adres'])
def parseLinks(url):
    page = requests.get(url).text
    soup = BeautifulSoup(page,'lxml')
    for a in soup.findAll('a',{'class':'qbutton'}):
        href = (a.get('href'))
        print('Saved', href)
        f.writerow([href])

    newlink = soup.find('li', {'class':'next next_last'}).find('a').get('href')
    parseLinks(newlink)
parseLinks(startURL)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Try the below approach. It should no longer produce duplicates. Turn out that your <code>.find_all()</code> method should include <code>post_more</code> class name as well in order for it to work the way you expect.. You can fix this by using <code>.post_more a.qbutton</code> such:</p>
<p>Not recommended:</p>
<pre><code>import requests
from bs4 import BeautifulSoup

startURL = 'http://esencjablog.pl/'

def parseLinks(url):
    page = requests.get(url).text
    soup = BeautifulSoup(page,'lxml')
    links = [a.get('href') for a in soup.select('.post_more a.qbutton')]
    for link in links:
        print(link)

    newlink = soup.select_one('li.next a').get('href')
    parseLinks(newlink)  ##it will continue on and on and never breaks

if __name__ == '__main__':
    parseLinks(startURL)
</code></pre>
<p>However, the better approach would be to use something so that it can watch out as to the newly populated next_page link produces new items or getting stuck in a vortex:</p>
<p>Go for this instead:</p>
<pre><code>import requests
from bs4 import BeautifulSoup

page = 58
URL = 'http://esencjablog.pl/page/{}/'

while True:
    page+=1
    res = requests.get(URL.format(page))
    soup = BeautifulSoup(res.text,'lxml')
    items = soup.select('.post_more a.qbutton')
    if len(items)&lt;=1:break  ##when there are no new links it should break

    for a in items:
        print(a.get("href"))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You are targeting also the <code>a</code> elements on the carousel, they are fixed on each of the pages you visit. You need to narrow the search down. You can either target elements with class <code>qbutton small</code>:</p>
<pre><code>for a in soup.findAll('a', {'class': 'qbutton small'}):
</code></pre>
<p>Or you can use <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors" rel="nofollow noreferrer">CSS selectors</a> just like in <a href="https://stackoverflow.com/a/49734488/4796844">SIM's answer</a> to specify the class of the parent elements.</p>
</div>
<div class="post-text" itemprop="text">
<p>Assuming the requirement is to extract all links presented by the button text "Czytaj dalej", following code works.</p>
<pre><code>import requests
from bs4 import BeautifulSoup
import csv


def writerow(row, filename):
    with open(filename, 'a', encoding='utf-8', newline='\n') as toWrite:
        writer = csv.writer(toWrite)
        writer.writerow([row])


def parseLinks(url):
    page = requests.get(url)
    if page.status_code == 200:     # page is fetched
        soup = BeautifulSoup(page.text, 'html.parser')

        # get total number of pages to scrap
        last_page_link = soup.find('li', class_='last').a['href']
        number_of_pages = int(last_page_link.split("/")[-2])

        # get links from number_of_pages 
        for pageno in range(0, number_of_pages):
            # generate url with page number
            # format: http://esencjablog.pl/page/2/
            page_url = url+"page/"+str(pageno+1)

            # fetch the page, parse links and write to csv
            thepage = requests.get(page_url)
            if thepage.status_code == 200:
                soup = BeautifulSoup(thepage.text, "html.parser")
                for a in soup.find_all('a', class_='qbutton small'):
                    print('Saved {}'.format(a['href']))
                    writerow(a['href'], 'test.csv')


if __name__ == "__main__":    
    startURL = 'http://esencjablog.pl/'
    parseLinks(startURL)
</code></pre>
<p>I believe OP is getting the duplicates because he is scraping links from the top slider as well.</p>
<p>I have used html.parser instead of lxml because I am more comfortable with it.</p>
</div>
<span class="comment-copy">There are some buttons at the top with text <i>Czytaj dalej</i>.  You can navigate those posts by arrows on the left and right. Do you want these posts too? Or, only the one's at the bottom?</span>
<span class="comment-copy">Maybe the site has links that lead you back to already visited pages? You should keep a dictionary with already parsed pages and skip these.</span>
<span class="comment-copy">Why not <code>for pageno in range(1, number_of_pages)</code>? I suggest using <a href="https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin" rel="nofollow noreferrer"><code>urllib.parse.urljoin</code></a> instead of concatenation, so the startURL starting either with or without <code>/</code> can be given to the <code>parseLinks</code> function. You can also use <a href="https://www.python.org/dev/peps/pep-0498/" rel="nofollow noreferrer"><code>f-strings</code></a> on Python 3.6+ instead of concatenation and <code>.format()</code>.</span>
<span class="comment-copy">range(1, number_of_pages) will get number_of_pages - 1. So, if there are 61 pages, it will get 60 pages.   I am using Python 3.5.</span>
<span class="comment-copy">Yeah sure, my bad, I meant <code>for pageno in range(1, number_of_pages + 1)</code> instead of adding + 1 in the next line.</span>
