<div class="post-text" itemprop="text">
<p>I am trying to download a series of Warc files from the CommonCrawl database, each of them about 25mb. This is my script:</p>
<pre><code>import json
import urllib.request
from urllib.error import HTTPError

from src.Util import rooted

with open(rooted('data/alexa.txt'), 'r') as alexa:
    for i, url in enumerate(alexa):
        if i % 1000 == 0:
            try:
                request = 'http://index.commoncrawl.org/CC-MAIN-2018-13-index?url={search}*&amp;output=json' \
                    .format(search=url.rstrip())
                page = urllib.request.urlopen(request)
                for line in page:
                    result = json.loads(line)
                    urllib.request.urlretrieve('https://commoncrawl.s3.amazonaws.com/%s' % result['filename'],
                                               rooted('data/warc/%s' % ''.join(c for c in result['url'] if c.isalnum())))
            except HTTPError:
                pass
</code></pre>
<p>What this is currently doing is requesting the link to download the Warc file via the CommonCrawl REST API and then initiating the download into the 'data/warc' folder.</p>
<p>The problem is that in each <code>urllib.request.urlretrieve()</code> call, the program hangs until the file is completely downloaded before issuing the next download request. Is there any way the <code>urllib.request.urlretrieve()</code> call can be terminated as soon as the download has been issued and then the file downloaded after or some way to spin a new thread for each of these requests and have all the files downloading simultaneously?</p>
<p>Thanks</p>
</div>
<div class="post-text" itemprop="text">
<p>Use threads, <code>futures</code> even :)</p>
<pre><code>jobs = []
with ThreadPoolExecutor(max_workers=100) as executor:
    for line in page:

        future = executor.submit(urllib.request.urlretrieve,
                                'https://commoncrawl.s3.amazonaws.com/%s' % result['filename'],
                                 rooted('data/warc/%s' % ''.join(c for c in result['url'] if c.isalnum()))
        jobs.append(future)
...
for f in jobs:
    print(f.result())
</code></pre>
<p>read more here: <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">https://docs.python.org/3/library/concurrent.futures.html</a></p>
</div>
<span class="comment-copy">Alternatively, you may use <a href="https://boto3.readthedocs.io/en/latest/" rel="nofollow noreferrer">boto3</a> to parallelize the download per WARC file. And do not forget to deduplicate WARC paths before downloading. One WARC file contains about 40,000 records. You could also download a single WARC record directly by a range request (see <a href="https://gist.github.com/sebastian-nagel/18d479bf203b328d2dded46639dd68d8" rel="nofollow noreferrer">example</a>).</span>
