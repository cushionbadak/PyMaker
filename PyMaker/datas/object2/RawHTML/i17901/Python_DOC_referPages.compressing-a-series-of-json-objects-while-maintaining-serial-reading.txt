<div class="post-text" itemprop="text">
<p>I have a bunch of <em>json objects</em> that I need to compress as it's eating too much disk space, approximately <code>20 gigs</code> worth for a few million of them. </p>
<p>Ideally what I'd like to do is compress each individually and then when I need to read them, just iteratively load and decompress each one. I tried doing this by creating a text file with each line being a compressed <em>json object</em> via <em>zlib</em>, but this is failing with a </p>
<p><code>decompress error due to a truncated stream</code>, </p>
<p>which I believe is due to the compressed strings containing new lines.</p>
<p>Anyone know of a good method to do this?</p>
</div>
<div class="post-text" itemprop="text">
<p>Just use a <a href="http://docs.python.org/2/library/gzip.html#gzip.GzipFile" rel="nofollow noreferrer"><code>gzip.GzipFile()</code> object</a> and treat it like a regular file; write JSON objects line by line, and read them line by line.</p>
<p>The object takes care of compression transparently, and will buffer reads, decompressing chucks as needed.</p>
<pre><code>import gzip
import json

# writing
with gzip.GzipFile(jsonfilename, 'w') as outfile:
    for obj in objects:
        outfile.write(json.dumps(obj) + '\n')

# reading
with gzip.GzipFile(jsonfilename, 'r') as infile:
    for line in infile:
        obj = json.loads(line)
        # process obj
</code></pre>
<p>This has the added advantage that the compression algorithm can make use of repetition <em>across</em> objects for compression ratios.</p>
</div>
<div class="post-text" itemprop="text">
<p>You might want to try an incremental json parser, such as <a href="http://pietrobattiston.it/jsaone" rel="nofollow noreferrer">jsaone</a>.</p>
<p>That is, create a <em>single</em> json with all your objects, and parse it like</p>
<pre><code>with gzip.GzipFile(file_path, 'r') as f_in:
    for key, val in jsaone.load(f_in):
        ...
</code></pre>
<p>This is quite similar to Martin's answer, wasting slightly more space but maybe slightly more comfortable.</p>
<p><em>EDIT:</em> oh, by the way, it's probably fair to clarify that I wrote jsaone.</p>
</div>
<span class="comment-copy">Show minimal code reproducing the problem.  You're "doing something wrong", but too hard to <i>guess</i> without seeing what you've done.</span>
<span class="comment-copy">Compressing them individually is going to reduce the potential space savings from compression.</span>
<span class="comment-copy">Do you require random access to the objects, or will you be reading them sequentially?</span>
<span class="comment-copy">I don't require random access, just need sequential.</span>
<span class="comment-copy">Or <code>bz2.BZ2File</code> since 2.3, or <code>lzma.LZMAFile</code> since 3.3.</span>
<span class="comment-copy">Worked great! Exactly what I needed.</span>
<span class="comment-copy">@duhaime: of course! Producing compressed JSON is an exception, not a usual case. When exchanging JSON over HTTP, the HTTP server may still apply content compression transparently, but you'd leave that to your HTTP library to handle (<code>requests</code> does this for you, for example).</span>
<span class="comment-copy">in Python 3 you may have to convert the json string to bytes (like <code>json_str.encode()</code>), as the gzip.GzipFile handler expects to write a bytes-like object, not 'str'.</span>
<span class="comment-copy">@YiboYang or just wrap the <code>outfile</code> object in an <a href="https://docs.python.org/3/library/io.html#io.TextIOWrapper" rel="nofollow noreferrer"><code>io.TextIOWrapper</code> instance</a>.</span>
