<div class="post-text" itemprop="text">
<p>I am pondering over Request/Response methods of Scrapy.
Can somebody explain me what the below statements (the request part) actually do?:</p>
<pre><code>    def get_url():
        url = raw_input('Enter the url of your website (including the http)')
        return url      
    start_urls = str(get_url())
    request= Request(start_urls)
    depth= request.meta['depth']
</code></pre>
<p>thanks</p>
</div>
<div class="post-text" itemprop="text">
<p>When you say "I am pondering over Request/Response methods of Scrapy", I think you're a little confused. <code>Request</code> and <code>Response</code> are classes, not methods.</p>
<p>And, while these classes of course do <em>have</em> methods, you don't ask about any of them, you just ask about one of the data attributes, <code>meta</code>.</p>
<p>Or, if you meant methods in the HTTP sense, well, a <code>Request</code> defaults to <code>GET</code>, but you can specify a different one with the <code>method=</code> argument; what else is there to know?</p>
<hr/>
<p>As the documentation for <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" rel="nofollow"><code>Request</code></a> says:</p>
<blockquote>
<p>A <code>Request</code> object represents an HTTP request, which is usually generated in the <code>Spider</code> and executed by the <code>Downloader</code>, and thus generating a <code>Response</code>.</p>
</blockquote>
<p>In other words, you usually don't want to create one yourself. You give the <code>Spider</code> a list of URLs to start with, and it makes a <code>Request</code> for each URL on the list, and for each additional URL that it discovers while scraping.</p>
<p>You may sometimes need to <em>look at</em> the <code>Request</code> that goes with a <code>Response</code>. And you may occasionally need to <em>customize</em> the creation of <code>Request</code>s inside a complex <code>Spider</code>. But you will rarely need to craft them manually.</p>
<hr/>
<p>Meanwhile, you seem to have confused yourself with your naming. You've got a variable named <code>start_urls</code>, but it's not a list of URLs, it's a single URL. Which, if you actually used it as a <code>start_urls</code> in the normal way, would be treated as a list of single characters. But fortunately, you're not doing that; you're passing <code>start_urls</code> as the <code>url</code> argument to a <code>Request</code> object—and, since it happens to be just a single URL, your two confusions cancel out and you create a valid <code>Request</code> object.</p>
<p>You <em>could</em> then feed this <code>Request</code> object to a <code>Downloader</code> manually to get back a <code>Response</code> object. But there's no good reason to do that. If you want to download files manually instead of spidering them, just use the stdlib module <a href="http://docs.python.org/2/library/urllib2.html" rel="nofollow"><code>urllib2</code></a>/<a href="http://docs.python.org/3/library/urllib.request.html" rel="nofollow"><code>urllib.request</code></a>, or a third-party library specifically designed for making manual downloading easy, like <code>requests</code>, not a spidering library.</p>
<hr/>
<p>Meanwhile, <code>depth= request.meta['depth']</code> will just return a <code>KeyError</code>. As the <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta" rel="nofollow"><code>meta</code></a> docs explain, it's just a dictionary, which is empty for new <code>Request</code>s (unless you pass a <code>meta=</code> argument to the constructor, which you didn't), and:</p>
<blockquote>
<p>… is usually populated by different Scrapy components (extensions, middlewares, etc). So the data contained in this dict depends on the extensions you have enabled.</p>
</blockquote>
<p>Since you haven't actually done anything with the <code>Request</code> after creation, it doesn't matter what extensions you have enabled; it's going to be empty.</p>
</div>
<span class="comment-copy">what exactly part don't you understand?</span>
<span class="comment-copy">Have you seen <a href="http://doc.scrapy.org/en/latest/topics/spiders.html" rel="nofollow noreferrer">doc.scrapy.org/en/latest/topics/spiders.html</a> describe how start_urls and requests works together?</span>
<span class="comment-copy">I am just wondering how I can use request() in combination of response(). I already checked the documentation, however, it's not clear to me how they work together</span>
<span class="comment-copy">I don't know why you're calling a single URL <code>start_urls</code> in your code. Fortunately, the <code>Request</code> constructor doesn't want a <code>start_urls</code> list, it wants a single <code>url</code>, so your confusion cancels itself out.</span>
<span class="comment-copy">Anyway, you usually don't want to generate a <code>Request</code> manually; they get generated by the <code>Spider</code>, and executed by the <code>Downloader</code>, giving you the <code>Response</code> objects directly. You sometimes have to <i>look</i> at the <code>Request</code> object that goes with the <code>Response</code>, but you rarely have to create them. (And if you don't want to use a <code>Spider</code>, you probably don't want <code>scrapy</code> in the first place, just use <code>urllib2</code>/<code>urllib.request</code> or <code>requests</code>.)</span>
<span class="comment-copy">Gr8 explanation</span>
