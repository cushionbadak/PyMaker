<div class="post-text" itemprop="text">
<p>I'm trying to read in a somewhat large dataset using <code>panda</code>s <code>read_csv</code> or <code>read_stata</code> functions, but I keep running into <code>Memory Error</code>s. What is the maximum size of a dataframe? My understanding is that dataframes should be okay as long as the data fits into memory, which shouldn't be a problem for me. What else could cause the memory error?</p>
<p>For context, I'm trying to read in the <a href="http://www.federalreserve.gov/econresdata/scf/scf_2007survey.htm#ASCIIDAT" rel="noreferrer">Survey of Consumer Finances 2007</a>, both in ASCII format (using <code>read_csv</code>) and in Stata format (using <code>read_stata</code>). The file is around 200MB as dta and around 1.2GB as ASCII, and opening it in Stata tells me that there are 5,800 variables/columns for 22,000 observations/rows. </p>
</div>
<div class="post-text" itemprop="text">
<p>I'm going to post this answer as was discussed in comments. I've seen it come up numerous times without an accepted answer.</p>
<p>The Memory Error is intuitive - out of memory. But sometimes the solution or the debugging of this error is frustrating as you have enough memory, but the error remains.</p>
<p><strong>1) Check for code errors</strong></p>
<p>This may be a "dumb step" but that's why it's first. Make sure there are no infinite loops or things that will knowingly take a long time (like using something the <code>os</code> module that will search your entire computer and put the output in an excel file)</p>
<p><strong>2) Make your code more efficient</strong></p>
<p>Goes along the lines of Step 1. But if something simple is taking a long time, there's usually a module or a better way of doing something that is faster and more memory efficent. That's the beauty of Python and/or open source Languages! </p>
<p><strong>3) Check The Total Memory of the object</strong> </p>
<p>The first step is to check the memory of an object. There are a ton of threads on Stack about this, so you can search them. Popular answers are <a href="https://stackoverflow.com/questions/563840/how-can-i-check-the-memory-usage-of-objects-in-ipython">here</a> and <a href="https://stackoverflow.com/questions/33978/find-out-how-much-memory-is-being-used-by-an-object-in-python">here</a></p>
<p>to find the size of an object in bites you can always use <a href="https://docs.python.org/3/library/sys.html#sys.getsizeof" rel="noreferrer"><code>sys.getsizeof()</code></a>:</p>
<pre><code>import sys
print(sys.getsizeof(OBEJCT_NAME_HERE))
</code></pre>
<p>Now the error might happen before anything is created, but if you read the csv in chunks you can see how much memory is being used per chunk.</p>
<p><strong>4) Check the memory while running</strong></p>
<p>Sometimes you have enough memory but the function you are running consumes a lot of memory at runtime. This causes memory to spike beyond the actual size of the finished object causing the code/process to error. Checking memory in real time is lengthy, but can be done. Ipython is good with that. Check <a href="http://pynash.org/2013/03/06/timing-and-profiling/" rel="noreferrer">Their Document</a>.</p>
<p>use the code below to see the documentation straight in Jupyter Notebook:</p>
<pre><code>%mprun?
%memit?
</code></pre>
<p>Sample use:</p>
<pre><code>%load_ext memory_profiler
def lol(x):
    return x
%memit lol(500)
#output --- peak memory: 48.31 MiB, increment: 0.00 MiB
</code></pre>
<p>If you need help on magic functions <a href="https://stackoverflow.com/questions/19942653/interactive-python-cannot-get-lprun-to-work-although-line-profiler-is-impor">This is a great post</a></p>
<p><strong>5) This one may be first.... but Check for simple things like bit version</strong></p>
<p>As in your case, a simple switching of the version of python you were running solved the issue.</p>
<p>Usually the above steps solve my issues.</p>
</div>
<span class="comment-copy">Possible duplicate of <a href="http://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas" title="large persistent dataframe in pandas">stackoverflow.com/questions/11622652/â€¦</a>  Try that for a reference</span>
<span class="comment-copy">are you running 32-bit python?</span>
<span class="comment-copy">I read in both a <code>csv</code> version and the <code>dta</code> version. They both worked fine with 64 bit python/pandas 0.13.1. Peak memory usage for the <code>csv</code> file was 3.33G, and for the <code>dta</code> it was 3.29G. That's right in the region where a 32-bit version is likely to choke. So @Jeff's question is very good one.</span>
<span class="comment-copy">Thanks Jeff and Karl, I was indeed unaware of the fact that I was running a 32 bit Python, switched to 64 and it works like a charm!</span>
<span class="comment-copy">@Jeff or KarlD. You should probably post this as an answer so that other people who find it in the future, know that it has an answer and not have to look into the comment section.</span>
