<div class="post-text" itemprop="text">
<p>Suppose I am given a set of structured data. The data is known to be problematic, and I need to somehow "score" them on consistency. For example, I have the data as shown below:</p>
<pre><code>fieldA | fieldB | fieldC
-------+--------+-------
foo    | bar    | baz
fooo   | bar    | baz
foo    | bar    | lorem
..     | ..     | ..
lorem  | ipsum  | dolor
lorem  | upsum  | dolor
lorem  | ipsum  | baz
</code></pre>
<p>So assume the first row is considered the correct entry because there are relatively more data in that combination compared to the records in second and third row. In the second row, the value for <code>fieldA</code> should be <code>foo</code> (inconsistent due to misspelling). Then in the third row, the value of <code>fieldC</code> should be <code>baz</code> as other entries in the dataset with similar values for <code>fieldA</code> (<code>foo</code>) and <code>fieldB</code> (<code>bar</code>) suggest.</p>
<p>Also, in other part of the dataset, there's another combination that is relatively more common (<code>lorem</code>, <code>ipsum</code>, <code>dolor</code>). So the problem in the following records are the same as the one mentioned before, just that the value combination is different.</p>
<p>I initially dumped everything to a SQL database, and use statements with <code>GROUP BY</code> to check consistency of fields values. So there will be 1 query for each field I want to check for consistency, and for each record.</p>
<pre><code>SELECT    fieldA, count(fieldA)
FROM      cache
WHERE     fieldB = 'bar' and fieldC = 'baz'
GROUP BY  fieldA
</code></pre>
<p>Then I could check if the value of <code>fieldA</code> of a record is consistent with the rest by referring the record to the object below (processed result of the previous SQL query).</p>
<pre><code>{'foo': {'consistency': 0.99, 'count': 99, 'total': 100}
 'fooo': {'consistency': 0.01, 'count': 1, 'total': 100}}
</code></pre>
<p>However it was very slow (dataset has about 2.2million records, and I am checking 4 fields, so making about 9mil queries), and would take half a day to complete. Then I replaced SQL storage to elasticsearch, and the processing time shrunk to about 5 hours, can it be made somehow faster?</p>
<p>Also just out of curiosity, am I re-inventing a wheel here? Is there an existing tool for this? Currently it is implemented in Python3, with elasticsearch.</p>
</div>
<div class="post-text" itemprop="text">
<p>I just read your question and found it quite interesting. I did something similar using <a href="http://www.nltk.org/" rel="nofollow noreferrer">ntlk</a> (python Natural Language Toolkit). 
Anyway, in this case I think you dont need the sophisticated <a href="https://stackoverflow.com/questions/1471153/string-similarity-metrics-in-python">string comparison algorithms</a>.</p>
<p>So I tried an approach using the python <a href="https://docs.python.org/3/library/difflib.html" rel="nofollow noreferrer">difflib</a>. The Title sounds promising: difflib — <strong>Helpers for computing deltas</strong>¶</p>
<p>The <strong>difflib.SequenceMatcher</strong> class says:</p>
<p><em>This is a flexible class for comparing pairs of sequences of any type, so long as the sequence elements are hashable.</em></p>
<p>By the way I think that if you want to save time you could hold and process 2.000.000  3-tuples of (relatively short) strings easily in Memory. (see testruns and Mem Usage below)</p>
<p>So I wrote a <a href="https://gist.github.com/pythononwheels/37f5570affe643b626358ba45799b764" rel="nofollow noreferrer">demo App</a> that produces 2.000.000 (you can vary that) 3-tuples of randomly slightly shuffled strings. The shuffled strings are based and compared with a default pattern like yours: ['foofoo', 'bar', 'lorem']. It then compares them using difflib.SequenceMatcher. All in Memory.</p>
<p><strong>Here is the compare code:</strong> </p>
<pre><code>def compare(intuple, pattern_list):
    """
    compare two strings with difflib
    intuple: in this case a n-tuple of strings
    pattern_list: a given pattern list.
    n-tuple and list must be of the same lenght.

    return a dict (Ordered) with the tuple and the score
    """
    d = collections.OrderedDict()
    d["tuple"] = intuple
    #d["pattern"] = pattern_list
    scorelist = []
    for counter in range(0,len(pattern_list)):
        score = difflib.SequenceMatcher(None,intuple[counter].lower(),pattern_list[counter].lower()).ratio()
        scorelist.append(score)
    d["score"] = scorelist
    return d
</code></pre>
<p><strong>Here are the runtime and Memory usage results:</strong></p>
<p>2000 3-tuples: 
 - compare time: 417 ms = 0,417 sec
 - Mem Usage: 594 KiB</p>
<p>200.000 3-tuples:
 - compare time: 5360 ms = 5,3 sec
 - Mem Usage: 58 MiB</p>
<p>2.000.000 3-tuples: 
 - compare time: 462241 ms = 462 sec
 - Mem Usage: 580 MiB</p>
<p>So it scales linear in time and Mem usage. And it (only) needs 462 seconds for 2.000.000 3-tuple strings tom compare.</p>
<p>The result looks like this:(example for 200.000 rows)</p>
<pre><code>[ TIMIMG ]
build                function took 53304.028034 ms
[ TIMIMG ]
compare_all          function took 462241.254807 ms
[ INFO ]

num rows: 2000000
pattern: ['foofoo', 'bar', 'lorem']
[ SHOWING 10 random results ]
0: {"tuple": ["foofoo", "bar", "ewrem"], "score": [1.0, 1.0, 0.6]}
1: {"tuple": ["hoofoo", "kar", "lorem"], "score": [0.8333333333333334, 0.6666666666666666, 1.0]}
2: {"tuple": ["imofoo", "bar", "lorem"], "score": [0.6666666666666666, 1.0, 1.0]}
3: {"tuple": ["foofoo", "bar", "lorem"], "score": [1.0, 1.0, 1.0]}
....
</code></pre>
<p>As you can see you get an score based on the similarity of the string compared to the pattern. 1.0 means equal and everything below gets worse the lower the score is.</p>
<p>difflib is known as not to be the fastest algorithm for that but I think 7 minutes is quite an improvement to half a day or 5 hours.</p>
<p>I hope this helps you (and is not complete missunderstanding) but it was a lot of fun to program this yesterday. And I learned a lot. ;)
For example to track memory usage using <a href="https://docs.python.org/3/library/tracemalloc.html" rel="nofollow noreferrer">tracemalloc</a>. Never did that before.</p>
<p>I dropped the code to <a href="https://gist.github.com/pythononwheels/37f5570affe643b626358ba45799b764" rel="nofollow noreferrer">github (as a one file gist)</a>.</p>
</div>
<span class="comment-copy">i havent got time to look at the solution, can i use it to "score" entries with multiple terms? e.g. "foo bar" vs. "fooz bar"</span>
<span class="comment-copy">should work as well. difflib uses hashes to compare. so anything hashable will work.</span>
<span class="comment-copy">lol, doesn't look like the tool I need. Because I don't have all the possible (relatively) correct canonical values and combinations for each field before hand.</span>
<span class="comment-copy">You can do the comparison exactly at that time when you would do the compare run in the DB. Just like you stated in your question. As I understand you have 2.2 Mio Datasets and a compare pattern then. Thats fine. Just read anything into Mem and run the compare function. There is no need to have anything beforehand.</span>
<span class="comment-copy">not sure if it is relevant, but i have updated the question with another example</span>
