<div class="post-text" itemprop="text">
<p>I was trying out the one of the programming pearls:</p>
<blockquote>
<p>Given a file containing at most ten million 7-digit integers with no duplicates. What is an efficient way to print these numbers in ascending order using just 1.5Mb RAM and reading the data just once? What are the consequences of only having 1Mb of RAM and no other storage? How would your answer change if duplicates were permitted?</p>
</blockquote>
<p>In order to create a test case I, generated 8999999 numbers and wrote them to a file.
Then for each line, i started inserting the same to a tree, finally creating a trie structure.</p>
<p>Sample Code:</p>
<pre><code>from sys import getsizeof

tree = dict()
xtree = dict()
f = open("data2.txt", "r")
cnt = 0
for number in f:
    cnt += 1
    currTree = tree
    xtree[number] = dict()
    for n in number.strip():
        if n not in currTree:
            currTree[n] = dict()
        currTree = currTree[n]
f.close()

print(cnt)
print(getsizeof(tree))
print(getsizeof(xtree))
print(tree)
</code></pre>
<p>sample file data2.txt has 20 records</p>
<p>The tree generated is </p>
<p><a href="https://i.stack.imgur.com/xqetR.png" rel="nofollow noreferrer"><img alt="Generated Tree" src="https://i.stack.imgur.com/xqetR.png"/></a> </p>
<p>Now the question is that when i do a memory sizing of the tree that is built, at 20 lines it shows a memory foot print of 240 bytes</p>
<p>At 100 line, size of tree becomes 368 bytes</p>
<p><strong>and at 8999999 lines also it gives 368 bytes</strong></p>
<p>I built an auxiliary map named <code>xtree</code> which just feeds in the data</p>
<p>The sizes for xtree and tree are in bytes.</p>
<p><a href="https://i.stack.imgur.com/7U8ei.png" rel="nofollow noreferrer"><img alt="Data analysis" src="https://i.stack.imgur.com/7U8ei.png"/></a></p>
<p>can anyone please explain how this is so..??</p>
</div>
<div class="post-text" itemprop="text">
<p>Your <code>tree</code> is just a dict with up to 10 key-value pairs. In bigger tree, there aren't any more key-value pairs. There are more values inside the values inside the … inside the key-value pairs, but there's still only 10 key-value pairs in the dict. And a dict with around 10 key-value pairs taking 368 bytes seems like about what you should expect.<sup>1</sup></p>
<p>As the docs for <a href="https://docs.python.org/3/library/sys.html#sys.getsizeof" rel="nofollow noreferrer"><code>getsizeof</code></a> say:</p>
<blockquote>
<p>Only the memory consumption directly attributed to the object is accounted for, not the memory consumption of objects it refers to.</p>
</blockquote>
<p>…</p>
<blockquote>
<p>See <a href="https://code.activestate.com/recipes/577504" rel="nofollow noreferrer">recursive sizeof recipe</a> for an example of using <code>getsizeof()</code> recursively to find the size of containers and all their contents.</p>
</blockquote>
<p>Since you don't actually have a completely arbitrary data structure, but just a dict of dicts of etc. And, while you <em>do</em> have some shared references (e.g., if you read the number <code>1234567</code> while you already have an int with the same value in memory, Python will just reuse the same object), if you're trying to verify that you can fit into 1.5MB, you really want a worst-case measurement, so you probably want to skip the check for already-seen values.</p>
<p>So, you can write something simpler instead of using that recipe if you want. But the idea will be the same:</p>
<pre><code>def total_dict_size(d):
    size = sys.getsizeof(d)
    if isinstance(d, dict):
        for key, value in d.items():
            size += sys.getsizeof(key) + total_dict_size(value)
    return size
</code></pre>
<hr/>
<p>Your <code>xtree</code>, on the other hand, is a dict with 8999999 key-value pairs. Doing the same back-of-the-envelope calculation, I'd expect that to be a bit under 300MB. Instead, it's a bit over 300MB. Close enough.</p>
<p>And you're also storing the 8999999 7-digit integers on the heap. To take some nice round numbers, let's say there are 5M distinct integers that don't fall into the handful of small values pre-created and cached by CPython. Each of those integers is small enough to fit into one 30-bit digit, so they take 28 bytes apiece on 64-bit CPython. So, that's another 140MB not accounted for in <code>sys.getsizeof(xtree)</code> (but they are accounted for—in fact, over-accounted, with the worst-case-measuring implementation given) if you call the recursive function above on either <code>tree</code> or <code>xtree</code>.</p>
<p>So, your total memory use between <code>tree</code>, <code>xtree</code>, and the actual integers is probably somewhere on the order of 750MB, which doesn't quite fit the <code>&lt; 1.5MB</code> requirement.</p>
<hr/>
<p><sub>1. Every Python object has some fixed header overhead, for things like the refcount, the pointer to the type, etc., plus type-specific things, like the length for most container types. Call that 64 bytes. A dict then has a hash table. It needs to be a bit bigger than 10 slots, to keep the load well below 1.0; call it 13 slots. Each slot needs a hash value, a reference to the key, and a reference to the value, so that's 3 pointers, or 24 bytes. 64 + 13 * 24 = 376. So that back-of-the-envelope calculation is off by only 8 bytes…</sub></p>
</div>
<span class="comment-copy">A tree implemented from dicts uses far too much memory. What you want is a single integer in which the <code>i</code>th bit is set if <code>i</code> is in the list. 10,000,000 bits  is just a bit under 1.2Mb.</span>
<span class="comment-copy">@chepner Please don't spoil the puzzle for the asker (and anyone who happens to read your comment). They didn't ask for a solution to the puzzle.</span>
<span class="comment-copy">@jbch This isn't puzzling.stackexchange.com.</span>
<span class="comment-copy">Thanks buddy...well explained...</span>
