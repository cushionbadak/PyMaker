<div class="post-text" itemprop="text">
<p>I have just written a script that is intended to be run 24/7 to update some files. However, if it takes 3 minutes to update one file, then it would take 300 minutes to update 100 files. </p>
<p>Is it possible to run n instances of the script to manage n separate files to speed up the turnaround time? </p>
</div>
<div class="post-text" itemprop="text">
<p>Yes it is possible. Use the <a href="http://docs.python.org/3/library/multiprocessing.html" rel="nofollow">multiprocessing</a> module to start several concurrent processes. This has the advantage that you do not run into problems because of the Global Interpreter Lock and threads as is explained in the manual page. The manual page includes all the examples you will need to make your script execute in parallel. Of course this works best if the processes do not have to interact, which your example suggests.</p>
</div>
<div class="post-text" itemprop="text">
<p>I suggest you first find out if there is any way to reduce the 3 minutes in a single thread.
The method I use to discover speedup opportunities is <a href="https://stackoverflow.com/a/4299378/23771"><em>demonstrated here</em></a>.</p>
<p>That will also tell you if you are purely I/O bound.
If you are completely I/O bound, and all files are on a single disk, parallelism won't help.
In that case, possibly storing the files on a solid-state drive would help.</p>
<p>On the other hand, if you are CPU bound, parallelism will help, as @hochl said.
Regardless, find the speedup opportunities and fix them.
I've never seen any good-size program that didn't have one or several of them.
That will give you one speedup factor, and parallelism will give you another, and the total speedup will be the product of those two factors.</p>
</div>
<span class="comment-copy">Threads.....if the files and updates are truly separate.</span>
<span class="comment-copy">Is the program I/O bound? If so, there is no point in adding more processes. If it's CPU bound then maybe. Do you know what's limiting your program's speed?</span>
<span class="comment-copy">I think the reason why my program is running so slowly is because i am using python + imacros to update my website 24/7. Since i'm using a lot of for loop. It's to loop through each path separately before it completes the cycle. I am thinking if the number of pages increase to 100, it's going to take forever to update. My main concern now is that simultaneous update of the website may cause some conflict in terms of data transmission (i.e. data get mixed up).</span>
<span class="comment-copy">Why are you constantly updating files on a website? Have you considered generating them when they're requested, and caching?</span>
<span class="comment-copy">Do you have any access to the server that's hosting the website? If so you should just update the pages there. If not, you should download a copy of the website, process it locally, and then send it back up in one go. What you should <i>not</i> do is edit a remote website as if it were local.</span>
<span class="comment-copy">hochl, do i have to rewrite the whole script for multi-processing to work? The script is logging into a private website to retrieve the data and updating it as well. So, would multiple instances of the script clash? If anyone has similar experience, do share.</span>
<span class="comment-copy">Of course it is possible that several processes would access the logging website concurrently. If logging can be done concurrently there is no problem, but that all depends on your Implementation which we do not know. But it is possible to synchronize the processes as is explained in <code>17.2.1.3. Synchronization between processes</code> so for those critical sections you could synchronize your code. Citation: "For instance one can use a lock to ensure that only one process prints to standard output at a time".</span>
<span class="comment-copy">Thanks Mike. Will read it now!</span>
<span class="comment-copy">+1 right you are :)</span>
