<div class="post-text" itemprop="text">
<p>I have 5,00,000 urls. and want to get response of each asynchronously.</p>
<pre><code>import aiohttp
import asyncio    

@asyncio.coroutine
def worker(url):
    response = yield from aiohttp.request('GET', url, connector=aiohttp.TCPConnector(share_cookies=True, verify_ssl=False))
    body = yield from response.read_and_close()

    print(url)

def main():
    url_list = [] # lacs of urls, extracting from a file

    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait([worker(u) for u in url_list]))

main()
</code></pre>
<p>I want 200 connections at a time(concurrent 200), not more than this because </p>
<p>when I run this program for 50 urls it works fine, i.e <code>url_list[:50]</code>
but if I pass whole list, i get this error</p>
<pre><code>aiohttp.errors.ClientOSError: Cannot connect to host www.example.com:443 ssl:True Future/Task exception was never retrieved future: Task()
</code></pre>
<p>may be frequency is too much and server is denying to respond after a limit?</p>
</div>
<div class="post-text" itemprop="text">
<p>Yes, one can expect a server to stop responding after causing too much traffic (whatever the definition of "too much traffic") to it.</p>
<p>One way to limit number of concurrent requests (throttle them) in such cases is to use <a href="https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore" rel="noreferrer"><code>asyncio.Semaphore</code></a>, similar in use to these used in multithreading: just like there, you create a semaphore and make sure the operation you want to throttle is aquiring that semaphore prior to doing actual work and releasing it afterwards.</p>
<p>For your convenience, <a href="https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore" rel="noreferrer"><code>asyncio.Semaphore</code></a> implements context manager to make it even easier.</p>
<p>Most basic approach:</p>
<pre class="lang-py prettyprint-override"><code>CONCURRENT_REQUESTS = 200


@asyncio.coroutine
def worker(url, semaphore):
    # Aquiring/releasing semaphore using context manager.
    with (yield from semaphore):
        response = yield from aiohttp.request(
            'GET',
            url,
            connector=aiohttp.TCPConnector(share_cookies=True,
                                           verify_ssl=False))
        body = yield from response.read_and_close()

        print(url)


def main():
    url_list = [] # lacs of urls, extracting from a file

    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait([worker(u, semaphore) for u in url_list]))    
</code></pre>
</div>
