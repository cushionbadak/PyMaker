<div class="post-text" itemprop="text">
<p>I maintain a benchmark library with gz-compressed files that contain descriptive metadata in the first few lines. By hand, I can decompress a 246MB gz-compressed file (using gunzip), change it, and compress it back (using gzip) in under 2 minutes using the linux terminal. On the same file, the following script takes almost 5 minutes to complete (using Python 2.7.5) and 12+ minutes (using Python 3.4.1) before I killed it. </p>
<pre><code>import os, gzip, shutil

def renew_file(file, tempfile):
  f = gzip.open(file,'r')

  try:
    # Read and modify first line(s)
    buf = f.readline()
    buf = 'changing first line\n'

    # Write change to temporary file
    f2 = gzip.open(tempfile,'w')
    try:
      f2.write(buf)
      shutil.copyfileobj(f,f2)
    finally:
      f2.close()

  finally:
    f.close()

  # Overwrite file
  os.rename(tempfile, file)
</code></pre>
<p>Any suggestions on how to achieve higher performance?</p>
</div>
<div class="post-text" itemprop="text">
<p>Gzip on the command line <a href="http://linux.die.net/man/1/gzip" rel="nofollow">defaults to a compression level of 6</a>.  Python, however, <a href="https://docs.python.org/3/library/gzip.html#gzip.open" rel="nofollow">defaults to a compression level of 9</a>, which is slower but produces smaller files.  You can either pass <code>compresslevel=6</code> to <code>gzip.open()</code> if you want larger files sooner, or pass <code>-9</code> to <code>gzip</code> if you want smaller files later.</p>
</div>
<span class="comment-copy">Probably there are better solutions but you can try running the actual gzip &amp;&amp; gunzip binaries in a subprocess.</span>
<span class="comment-copy">don't store the metadata compressed... maybe a separate file? or an extra header?</span>
<span class="comment-copy">What do you mean by "an extra header"?</span>
<span class="comment-copy">Since the underlying zlib releases the GIL, there is benefit to doing this multithreaded - ditch copyfileobj and create a background thread to do the writes and use a queue to pass the data. In my experiment, the operation went from 77 seconds to 50.</span>
<span class="comment-copy">@tdelaney It sounds like you have a working example. I would love to see this posted as an answer.</span>
<span class="comment-copy">Made me facepalm at first, but passing gzip -9 to the terminal actually only adds 20s to the command line solution..</span>
<span class="comment-copy">Hm... Well, I can tell you why Python 3.x is slower: It's Unicode decoding/encoding the entire file, while 2.x is just copying the bytes verbatim.</span>
<span class="comment-copy">Adding compresslevel=6 to gzip.open() changed the execution time to just 1 minute..! So I guess your answer is correct, with the twist of a version-difference of gzip as command and embedded in Python.. Any trick for making Python 3.x as fast as Python 2.x?</span>
<span class="comment-copy">@kevin - my python3 experiments were about as fast as python2... and gzip passed back <code>bytes</code>, not <code>str</code>, so it wasn't unicode encoding (which would be bad, considering that unicode encode/decode isn't guaranteed to be loss-less).</span>
