<div class="post-text" itemprop="text">
<p>In a python script, I have a large dataset that I would like to apply multiple functions to. The functions are responsible for creating certain outputs that get saved to the hard drive. </p>
<p>A few things of note:</p>
<ol>
<li>the functions are independent</li>
<li>none of the functions return anything</li>
<li>the functions will take variable amounts of time</li>
<li>some of the functions may fail, and that is fine</li>
</ol>
<p>Can I multiprocess this in any way that each function and the dataset are sent separately to a core and run there? This way I do not need the first function to finish before the second one can kick off? There is no need for them to be sequentially dependent.
Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>Since your functions are independent and only read data, as long as it is not an issue if your data is modified during the execution of a function, then they are also thread safe.</p>
<p>Use a <a href="https://www.metachris.com/2016/04/python-threadpool/" rel="nofollow noreferrer">thread pool (click)</a> . You would have to create a task per function you want to run.</p>
<p>Note: In order for it to run on more than one core you must use <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">Python Multiprocessing</a>. Else all the threads will run on a single core. This happens because Python has a Global Interpreter Lock (GIL). For more information <a href="https://stackoverflow.com/questions/4496680/python-threads-all-executing-on-a-single-core?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa">Python threads all executing on a single core</a></p>
<p>Alternatively, you could use <a href="http://dask.pydata.org/en/latest/" rel="nofollow noreferrer">DASK</a> , which augments the data in order to run some multi threading. While adding some overhead, it might be quicker for your needs.</p>
</div>
<div class="post-text" itemprop="text">
<p>I was in a similar situation as yours, and used <a href="https://docs.python.org/3/library/multiprocessing.html?highlight=process#module-multiprocessing" rel="nofollow noreferrer">Process</a>es with the following function:</p>
<pre><code>import multiprocessing as mp

def launch_proc(nproc, lst_functions, lst_args, lst_kwargs):
    n = len(lst_functions)
    r = 1 if n % nproc &gt; 0 else 0
    for b in range(n//nproc + r):
        bucket = []
        for p in range(nproc):
             i = b*nproc + p
             if i == n:
                 break
             proc = mp.Process(target=lst_functions[i], args=lst_args[i], kwargs=lst_kwargs[i])
             bucket.append(proc)
        for proc in bucket:
            proc.start()
        for proc in bucket:
            proc.join()
</code></pre>
<p>This has a major drawback: all Processes in a bucket have to finish before a new bucket can start. I tried to use a <a href="https://docs.python.org/2/library/multiprocessing.html#multiprocessing.JoinableQueue" rel="nofollow noreferrer">JoinableQueue</a> to avoid this, but could not make it work.</p>
<p>Example:</p>
<pre><code>def f(i):
    print(i)

nproc = 2
n     = 11
lst_f      = [f] * n
lst_args   = [[i] for i in range(n)]
lst_kwargs = [{}] * n
launch_proc(nproc, lst_f, lst_args, lst_kwargs)
</code></pre>
<p>Hope it can help.</p>
</div>
<span class="comment-copy">I will have a separate problem later where the JoinableQueue has been utilized in the past. So it's very useful to see the application here as you explain. Thanks for this!</span>
