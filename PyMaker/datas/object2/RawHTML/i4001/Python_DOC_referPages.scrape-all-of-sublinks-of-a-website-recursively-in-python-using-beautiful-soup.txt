<div class="post-text" itemprop="text">
<p>NEWEST UPDATE:
I'm reducing my question to how to get all links from a site, including sublinks of each page etc, recursively.</p>
<p>I think I know how to get all sublinks of one page:</p>
<pre><code>from bs4 import BeautifulSoup
import requests
import re

def get_links(site, filename):
    f=open(filename, 'w')
    url = requests.get(site)
    data = url.text
    soup = BeautifulSoup(data, 'lxml')
    for links in soup.find_all('a'):
        f.write(str(links.get('href'))+"\n")
    f.close()

r="https://en.wikipedia.org/wiki/Main_Page"
filename="wiki"
get_links(r,filename)
</code></pre>
<p>How do I recursively ensure all links on the site are also harvested and written onto the same file? </p>
<p>So I tried this, and it's not even compiling.</p>
<pre><code>def is_url(link):
    #checks using regex if 'link' is a valid url
    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+#]|[!*/\\,() ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', link)
    return (" ".join(url)==link)

def get_links(site, filename):
    f=open(filename, 'a')
    url = requests.get(site)
    data = url.text
    soup = BeautifulSoup(data, 'lxml')
    for links in soup.find_all('a'):
        if is_url(links):
            f.write(str(links.get('href'))+"\n")
            get_links(links, filename)
    f.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Answering your question, this is how I would fetch all links of a page with beautilfulsoup and save them to a file:</p>
<pre><code>from bs4 import BeautifulSoup
import requests


def get_links(url):
    response = requests.get(url)
    data = response.text
    soup = BeautifulSoup(data, 'lxml')

    links = []
    for link in soup.find_all('a'):
        link_url = link.get('href')

        if link_url is not None and link_url.startswith('http'):
            links.append(link_url + '\n')

    write_to_file(links)
    return links


def write_to_file(links):
    with open('data.txt', 'a') as f:
        f.writelines(links)


def get_all_links(url):
    for link in get_links(url):
        get_all_links(link)


r = 'https://en.wikipedia.org/wiki/Main_Page'
write_to_file([r])
get_all_links(r)
</code></pre>
<p>This will, however, not prevent cicles (which would result in infinite recursion). In order to do so you may use a <a href="https://docs.python.org/3/tutorial/datastructures.html#sets" rel="nofollow noreferrer"><code>set</code></a> to store already visited links and not visit them again.</p>
<p>You should really consider using something like <a href="https://scrapy.org/" rel="nofollow noreferrer">Scrapy</a> for this kind of task. I think a <a href="https://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.CrawlSpider" rel="nofollow noreferrer"><code>CrawlSpider</code></a> is what you should look into.</p>
<p>For the purpose of extracting the urls from the <code>wikipedia.org</code> domain you may do something like this:</p>
<pre><code>from scrapy.spiders import CrawlSpider
from scrapy.spiders import Rule
from scrapy.linkextractors import LinkExtractor

from scrapy import Item
from scrapy import Field


class UrlItem(Item):
    url = Field()


class WikiSpider(CrawlSpider):
    name = 'wiki'
    allowed_domains = ['wikipedia.org']
    start_urls = ['https://en.wikipedia.org/wiki/Main_Page/']

    rules = (
        Rule(LinkExtractor(), callback='parse_url'),
    )

    def parse_url(self, response):
        item = UrlItem()
        item['url'] = response.url

        return item
</code></pre>
<p>And run it with</p>
<pre><code>scrapy crawl wiki -o wiki.csv -t csv
</code></pre>
<p>and you get the urls in a <code>csv</code> format on the <code>wiki.csv</code> file.</p>
</div>
<span class="comment-copy">Can you provide the part where the functions are being called? Also note that <code>get_links</code> and <code>read_text</code> leave file handlers open. That can be a problem.</span>
<span class="comment-copy">@bla I will update this right away. Thanks for the tip also!</span>
<span class="comment-copy">@polkadot have you considered using scrapy?</span>
<span class="comment-copy">@JonClements I actually started with scrapy, but i'm quite a novice and it proved too complex, if only because I could find more help online for beautiful soup.</span>
<span class="comment-copy">@polkadot okay... Without knowing what difficulties you had - it's hard to help there - but it'd literally be changing 2/3 lines from a default spider template which is IMHO a lot easier than your current approach. You might also want to consider if something like <a href="https://www.gnu.org/software/wget/" rel="nofollow noreferrer">gnu.org/software/wget</a> would work for you.</span>
