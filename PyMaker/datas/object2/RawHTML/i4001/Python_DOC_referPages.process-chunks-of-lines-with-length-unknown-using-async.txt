<div class="post-text" itemprop="text">
<p>I have a file with a url on each line. I am running an async requests call using aiohttp on these urls in batches.</p>
<p>The file is huge and my memory is tiny. I don't know how many lines are in the file and reading the whole thing line by line with a counter will take an age.</p>
<p>How can I:</p>
<ul>
<li>grab 100,000 lines into a list</li>
<li>process those</li>
<li>pause file read</li>
<li>grab next 100,000 lines
= repeat until I'm grabbing the remainder?</li>
</ul>
<p>I was thinking along the lines of async but I may have massively misunderstood this library.</p>
<pre><code>counter = 0
inputs=[]
async with open("test.txt") as f:
    for line in f:
        counter=counter+1
        if counter%100000 != 0:
              inputs.append(line.strip())
        else:
              await get_req_fn(inputs)
              inputs=[]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The "pause" you are looking for is precisely what <code>await</code> does. It blocks the current coroutine (allowing others to make progress) until the one it awaits finishes. Your code is basically correct, except for the use of <code>async with</code> around <code>open</code>. Here is a more complete version (untested):</p>
<pre><code>import asyncio, aiohttp
from itertools import islice

BATCH_SIZE = 1000  # start with something reasonable

async def main():
    async with aiohttp.ClientSession() as session:
        with open("test.txt") as f:
            while True:
                # take the next BATCH_SIZE lines
                batch = [line.strip() for line in islice(f, BATCH_SIZE)]
                if not batch:
                    # no more lines - we're done
                    break
                await get_req_fn(batch, session)
</code></pre>
<p>When implementing <code>get_req_fn</code>, you need to take care to enable parallel execution, but also to allow awaiting completion of the whole batch. The key ingredient for that are coroutine combinators, functions that aggregate multiple coroutines into a single awaitable object. One that is powerful and very simple to use is <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.gather" rel="nofollow noreferrer"><code>gather</code></a>:</p>
<pre><code>async def get_req_fn(urls, session):
    coros = []
    for url in urls:
        coros.append(single_req(url, session))
    await asyncio.gather(*coros)
</code></pre>
<p><code>gather</code> starts the given coroutines and, when <code>await</code>ed, blocks the current one until all of them complete. This allows <code>await get_req_fn(batch, session)</code> to pause reading until the whole batch has been downloaded.</p>
<p>Finally, <code>single_req</code> could look like this:</p>
<pre><code>async def single_req(url, session):
    try:
        async with session.get(url) as resp:
            text = await resp.text()
            # process text, or save it to a file, etc
    except IOError as e:
        print(f'error fetching {url}: {e}')
</code></pre>
<p>All functions accept a <a href="https://aiohttp.readthedocs.io/en/stable/client_reference.html#aiohttp.ClientSession" rel="nofollow noreferrer">session object</a> created in <code>main()</code> because creating a new session for each request is <a href="https://aiohttp.readthedocs.io/en/stable/client_quickstart.html#make-a-request" rel="nofollow noreferrer">strongly discouraged</a> in the documentation.</p>
<p>Finally, to run the whole thing, use something like:</p>
<pre><code>loop = asyncio.get_event_loop()
loop.run_until_complete(main())
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Reading and processing the read data asynchronously doesn't make it faster. Instead, you might use for async when you have a long task like data processing and want to render something whilst doing so. </p>
<p>I would just read the file into memory, then process the data. Of course, if you can't do such thing because it doesn't fit into memory I would use <code>await</code> which @user4815162342 mentioned in their answer. </p>
</div>
<span class="comment-copy">I don't see why you need <code>asyncio</code> or something else. Just read 10.000 lines, to the http-stuff and when finnished go on with reading. Am I wrong?</span>
<span class="comment-copy">you are almost certainly right. i basically have no idea what im doing. how do you pause reading? tell() and seek() ??</span>
<span class="comment-copy">Are you requesting 10.000 urls at the <b>same</b> time with aiohttp? That is why the <code>for</code> loop doesn't wait for the 10.000 answers before it read the next 10.000? Am I right? In that case you should do the async request in a separate thread and the main thread (with the for loop) has to wait until the Thread ends.</span>
<span class="comment-copy">@buhtz Maybe the OP is using something like <code>create_task</code> or <code>ensure_future</code> to have the batch run in parallel, but missed something like <code>gather</code> to wait for the downloads to actually complete. I've now written an answer that shows a use of <code>gather</code> for that purpose.</span>
<span class="comment-copy">Ok, @user4815162342. Thank you, I will modify my answer accordingly.</span>
