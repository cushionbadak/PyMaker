<div class="post-text" itemprop="text">
<p>I am running some Python program and I've noticed that the bottleneck is in the line doing the following </p>
<pre class="lang-py prettyprint-override"><code>all(foo(s) for s in l)
</code></pre>
<p>what I am wondering is - what would be the best way to make this into a parallel computation? foo(s) is a thread safe method inspecting s and returning True/False with respect to some criteria. No data structure is changed by foo.</p>
<p>So the question is</p>
<blockquote>
<p>How to test in parallel if all elements of a list l have property foo
  , exiting as soon as one element of l does not satisfy foo?</p>
</blockquote>
<p>Edit. Adding more context. I do not know what kind of context you are looking for but in my scenario s is a graph and foo(s) computes some graph theoretical invariant (for example average distance or perhaps something similar)</p>
</div>
<div class="post-text" itemprop="text">
<p>This will sort of depend on what <code>foo(s)</code> is doing.  If it is I/O bound, waiting on blocking calls, than just using threads will help.  The easiest way is to create a pool of threads and use <code>pool.map</code>:</p>
<pre><code>from multiprocessing.pool import ThreadPool
pool = ThreadPool(10)
all(pool.map(foo, l))
</code></pre>
<p>If, however, the function is cpu bound, using a lot of processor power, this will not help you.  Instead you need to use the multiprocessing pool: </p>
<pre><code>from multiprocessing import Pool
pool = Pool(4)
all(pool.map(foo, l))
</code></pre>
<p>This will use separate processes instead of threads, allowing multiple cpu cores to be used.  If your function <code>foo</code> is very quick, though, the overhead will eliminate any advantage of parallel processing, so you need to test to make sure you get the results you expect</p>
<p>see: <a href="https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers" rel="nofollow">https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers</a></p>
<p><strong>EDIT:</strong>
I've assumed you're using Python 2.7.x.  If you're using Python3 you have more advanced concurrency features in <a href="https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures" rel="nofollow">concurrent.futures</a>.  Including <code>ThreadPoolExecutor</code> and <code>ProcessPoolExecutor</code>.  </p>
<p>I would recommend using those for parallel processing and the <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow">asyncio</a> lib for I/O bound problems.</p>
</div>
<div class="post-text" itemprop="text">
<p>Python comes with the  <code>multiprocessing</code> module; there's an <a href="http://pymotw.com/2/multiprocessing/mapreduce.html" rel="nofollow">example</a> implementing a classical <code>reduce</code> algorithm (which could be used to implement <code>all</code>). Generally, you might want to look at <a href="https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers" rel="nofollow"><code>Pool</code></a> functionality:</p>
<blockquote>
<p>The Pool class represents a pool of worker processes. It has methods which allows tasks to be offloaded to the worker processes in a few different ways.</p>
</blockquote>
</div>
<span class="comment-copy">there is not enough context for this imho ...</span>
<span class="comment-copy">Without more context, the best suggestion I could give would be to use the multiprocessing module with a pool of workers. However, if foo() isn't a "fat" enough function then the speed up will be minimal (if at all).</span>
