<div class="post-text" itemprop="text">
<p>I'm looking for a more efficient and Pythonic way of using itertools' <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow noreferrer"><code>roundrobin</code></a> recipe on the groups formed by <code>itertools.groupby()</code>.</p>
<p>Specifically, I have a list of URLs (not sorted), and want to re-order them so that the ordering of their result places the maximum "distance" (or diversification, maybe) between each unique netloc (host), as defined by the attribute from <code>urllib.parse</code>.  Reproducible example below.</p>
<p>I'm currently using <code>itertools.groupby()</code> plus its roundrobin recipe, but because of the nature of <code>groupby()</code>,</p>
<blockquote>
<p>The returned group is itself an iterator that shares the underlying iterable with <code>groupby()</code>. Because the source is shared, when the <code>groupby()</code> object is advanced, the previous group is no longer visible. So, if that data is needed later, it should be stored as a list.</p>
</blockquote>
<p>...this seems to necessitate forming an intermediate list out of each group.</p>
<p>Sample data:</p>
<pre><code>import itertools as it
import urllib.parse

bases = ('https://www.google.com', 'https://www.youtube.com',
         'https://docs.scipy.org', 'https://www.group.me')
urls = []
counts = (1, 5, 10, 15)
for c, b in zip(counts, bases):
    for i in range(c):
        urls.append(f'{b}/{i}')

pprint(urls)
# ['https://www.google.com/0',
#  'https://www.youtube.com/0',
#  'https://www.youtube.com/1',
#  'https://www.youtube.com/2',
#  'https://www.youtube.com/3',
#  'https://www.youtube.com/4',
#  'https://docs.scipy.org/0',
#  'https://docs.scipy.org/1',
#  'https://docs.scipy.org/2',
#  'https://docs.scipy.org/3',
#  'https://docs.scipy.org/4',
#  'https://docs.scipy.org/5',
#  'https://docs.scipy.org/6',
#  'https://docs.scipy.org/7',
#  'https://docs.scipy.org/8',
#  'https://docs.scipy.org/9',
#  'https://www.group.me/0',
#  'https://www.group.me/1',
#  'https://www.group.me/2',
#  'https://www.group.me/3',
#  'https://www.group.me/4',
#  'https://www.group.me/5',
#  'https://www.group.me/6',
#  'https://www.group.me/7',
#  'https://www.group.me/8',
#  'https://www.group.me/9',
#  'https://www.group.me/10',
#  'https://www.group.me/11',
#  'https://www.group.me/12',
#  'https://www.group.me/13',
#  'https://www.group.me/14']
</code></pre>
<p>Current solution (take 1 from each group, or skip the group if it is empty, until all groups raise <code>StopIteration</code>):</p>
<pre><code>grp = it.groupby(sorted(urls), key=lambda u: urllib.parse.urlsplit(u).netloc)
shuffled = list(roundrobin(*(list(g) for _, g in grp)))
#                            ^^ Each group is otherwise lost because
#                               groupby() itself is an iterator
</code></pre>
<p>The expected output for the sample is as follows:</p>
<pre><code>['https://docs.scipy.org/0',
 'https://www.google.com/0',
 'https://www.group.me/0',
 'https://www.youtube.com/0',
 'https://docs.scipy.org/1',
 'https://www.group.me/1',
 'https://www.youtube.com/1',
 'https://docs.scipy.org/2',
 'https://www.group.me/10',
 'https://www.youtube.com/2',
 'https://docs.scipy.org/3',
 'https://www.group.me/11',
 'https://www.youtube.com/3',
 'https://docs.scipy.org/4',
 'https://www.group.me/12',
 'https://www.youtube.com/4',
 'https://docs.scipy.org/5',
 'https://www.group.me/13',
 'https://docs.scipy.org/6',
 'https://www.group.me/14',
 'https://docs.scipy.org/7',
 'https://www.group.me/2',
 'https://docs.scipy.org/8',
 'https://www.group.me/3',
 'https://docs.scipy.org/9',
 'https://www.group.me/4',
 'https://www.group.me/5',
 'https://www.group.me/6',
 'https://www.group.me/7',
 'https://www.group.me/8',
 'https://www.group.me/9']
</code></pre>
<p>What is a more efficient way of going about this?</p>
</div>
<div class="post-text" itemprop="text">
<p>Not a huge improvement, but you could use <code>itertools.zip_longest</code> to achieve the same effect with a little tweak:</p>
<pre><code>shuffled = list(x for i in it.zip_longest(*(list(g) for _, g in grp)) for x in i if x)
# flattening the sublists and only returning the non-None values
</code></pre>
<p>The benefit is you don't have to define the <code>roundrobin</code> recipe.  The time saving is negligible however (timed for <code>n=10000</code>):</p>
<pre><code># 3.7466756048055094 # zip_longest
# 4.077965201903506  # roundrobin
</code></pre>
<p>I feel like there's another solution that could use <code>collections.Counter</code> or use <code>sort(key=...)</code> on the <code>sorted(list)</code>, but I haven't cracked that case yet, feels like the time complexity might be more severe than your implementation since it might rely on more python code than compiled modules.  This is an interesting problem though, will probably revisit this later.</p>
</div>
<span class="comment-copy">Perhaps a flexible alternative to <code>groupby</code> may help, e.g. a<code>defaultdict</code>.</span>
<span class="comment-copy">It’s a tough one from time complexity perspective, seems difficult to do in less than N^2. Likewise I wonder if there’s some magic formula given the counts</span>
