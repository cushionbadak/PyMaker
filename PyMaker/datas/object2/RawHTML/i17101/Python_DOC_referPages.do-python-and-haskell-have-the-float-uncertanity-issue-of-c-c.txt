<div class="post-text" itemprop="text">
<p>First of all, I was not studying math in English language, so I may use wrong words in my text.</p>
<p><strong>Float numbers can be finite(42.36) and infinite (42.363636...)</strong></p>
<p><strong>In C/C++ numbers are stored at base 2</strong>. <strong>Our minds operate floats at base 10</strong>. </p>
<p>The problem is -</p>
<p><code>many (a lot, actually) of float numbers with base 10, that are finite, have no exact finite representation in base 2, and vice-versa.</code></p>
<p>This doesn't mean anything most of the time. The <strong>last digit of double may be off by 1 bit</strong> - not a problem.</p>
<p>A problem arises when we compute two floats that are actually integers. <code>99.0/3.0</code> on C++ can result in <code>33.0</code> as well as <code>32.9999...99</code>. And if you convert it to integer then - you are in for a surprise. I always add a special value <strong>(2*smallest value for given type and architecture)</strong> before rounding up in C for this reason. Should I do it in <strong>Python or not</strong>?</p>
<p>I have run some tests in Python and it <em>seems</em> float division always results as expected. But <em>some tests</em> are not enough because the problem is architecture-dependent. Do somebody know for sure if it is taken care of, and on what level - in float type itself or only in rounding up and shortening functions?</p>
<p>P.S. And if somebody can clarify the same thing for <strong>Haskell</strong>, which I am only starting with - it would be great.</p>
<p><strong>UPDATE</strong> 
Folks pointed out to an official document stating there <em>is</em> uncertainty in floating point arithmetic. The remaining question is - do <code>math</code> functions like <code>ceil</code> take care of them or should I do it on my own? This <strong>must</strong> be pointed out to beginner users every time we speak of these functions, because otherwise they will all stumble on that problem. </p>
</div>
<div class="post-text" itemprop="text">
<p>The format C and C++ use for representing float and double is standardized (IEEE 754), and the problems you describe are inherent in that representation. Since Python is implemented in C, its floating point types are prone to the same rounding problems. </p>
<p>Haskell's Float and Double are a somewhat higher level abstraction, but since most (all?) modern CPUs use IEEE754 for floating point calculations, you most probably will have that kind of rounding errors there as well. </p>
<p>In other words: Only languages/libraries which choose to <em>not</em> base their floating point types on the underlying architecture <em>might</em> be able to circumvent the IEEE754 rounding problems to a certain degree, but since the underlying hardware does not support other representations directly, there has to be a performance penalty. Therefore, probably most languages will stick to the standard, not least because its limitations are well known.</p>
</div>
<div class="post-text" itemprop="text">
<p>Real numbers themselves, including floats, are never "infinite" in any mathematical sense. They may have infinite decimal representations, but that's only a technical problem of the way we write them (or store them in computers). In fact though, IEEE754 also specifies +∞ and -∞ values, those are actual infinities... but they don't represent real numbers and are mathematically quite horrible in many a way.</p>
<p>Also... "And if you convert it to integer then" you should never "convert" floats to integers anyway, it's not really possible: you can only <em>round</em> them to integers. and if you do that with e.g. Haskell's <code>round</code>, it's pretty safe indeed, certainly</p>
<blockquote>
<p>Prelude&gt; round $ 99/3<br/>
  33</p>
</blockquote>
<p>Though ghci calculates the division with floating-point.</p>
<p>The only things that are always unsafe:</p>
<ul>
<li><p>Of course, implicit conversion from float to int is completely crazy, and positively a mistake in the C-languages. Haskell and Python are both properly <em>strongly typed</em>, so such stuff won't happen by accident.</p></li>
<li><p>Floating-points should generally not be expected to be <em>exactly equal</em> to anything particular. It's not really useful to expect so anyway, because for actual real numbers any single one is a <a href="http://en.wikipedia.org/wiki/Null_set" rel="nofollow">null set</a>, which roughly means the only way two real number can be equal is if there's so deep mathematical reason for it. But for any distribution e.g. from a physical process, the probability for equalness is <em>exactly zero</em>, so why would you check?<br/>Only <em>comparing</em> numbers OTOH, with <code>&lt;</code>, is perfectly safe (unless you're dealing with very small differences between huge numbers, or you use it to "simulate" equality by also checking <code>&gt;</code>).</p></li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>Yes, this is a problem in Python.</p>
<p>See <a href="https://docs.python.org/2/tutorial/floatingpoint.html" rel="nofollow">https://docs.python.org/2/tutorial/floatingpoint.html</a></p>
</div>
<div class="post-text" itemprop="text">
<p>Python internally represents numbers as C doubles, so you will have all the problems inherent to floating point arithmetics. But it also includes some algorithms to "fix" the obvious cases. The example you give, 32.99999... is recognised as being 33.0. From Python 2.7 and 3.1 onwards they do this using <a href="http://bugs.python.org/issue7117" rel="nofollow">Gay's algorithm</a>; that is, the shortest string that rounds back to the original value. You can see a description in <a href="https://docs.python.org/3/whatsnew/3.1.html" rel="nofollow">Python 3.1 release notes.</a>  In earlier versions, it just rounds to the first 17 decimal places.</p>
<p>As they themselves warn, it doesn't mean that it is going to work as decimal numbers.</p>
<pre><code>&gt;&gt;&gt; 1.1 + 2.2
3.3000000000000003
&gt;&gt;&gt; 1.1 + 2.2 == 3.3
False
</code></pre>
<p>(But that should already be ringing your bells, as comparing floating point numbers for equality is never a good thing)</p>
<p>If you want to assure precision to a number of decimal places (for example, if you are working with finances), you can use the module <a href="https://docs.python.org/2/library/decimal.html" rel="nofollow">decimal</a> from the standard library. If you want to represent fractional numbers, you could use <a href="https://docs.python.org/2/library/fractions.html" rel="nofollow">fractions</a>, but they are both slower than plain numbers.</p>
<pre><code>&gt;&gt;&gt; import decimal
&gt;&gt;&gt; decimal.Decimal(1.1) + decimal.Decimal(2.2) 
Decimal('3.300000000000000266453525910')
# Decimal is getting the full floating point representation, no what I type!

&gt;&gt;&gt; decimal.Decimal('1.1') + decimal.Decimal('2.2')
Decimal('3.3')
# Now it is fine.
&gt;&gt;&gt; decimal.Decimal('1.1') + decimal.Decimal('2.2') == 3.3
False
&gt;&gt;&gt; decimal.Decimal('1.1') + decimal.Decimal('2.2') == decimal.Decimal(3.3)
False
&gt;&gt;&gt; decimal.Decimal('1.1') + decimal.Decimal('2.2') == decimal.Decimal('3.3')
True
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In addition to the other fantastic answers here, saying roughly that IEEE754 has exactly the same issues no matter which language you interface to them with, I'd like to point out that many languages have libraries for other kinds of numbers. Some standard approaches are to use fixed-point arithmetic (many, but not all, of IEEE754's nuances come from being floating-point) or rationals. Haskell also libraries for the computable reals and cyclotomic numbers.</p>
<p>In addition, using these alternative kinds of numbers is especially convenient in Haskell due to its typeclass mechanism, which means that doing arithmetic with these other types of numbers looks and feels exactly the same and doing arithmetic with your usual IEEE754 <code>Float</code>s and <code>Double</code>s; but you get the better (and worse!) properties of the alternate type. For example, with appropriate imports, you can see:</p>
<pre><code>&gt; 99/3 :: Double
33.0
&gt; 99/3 :: Fixed E12
33.000000000000
&gt; 99/3 :: Rational
33 % 1
&gt; 99/3 :: CReal
33.0
&gt; 99/3 :: Cyclotomic
33
&gt; 98/3 :: Rational
98 % 3
&gt; sqrt 2 :: CReal
1.4142135623730950488016887242096980785697
&gt; sqrtInteger (-5) :: Cyclotomic
e(20) + e(20)^9 - e(20)^13 - e(20)^17
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><a href="http://www.haskell.org/onlinereport/haskell2010/haskellch6.html#x13-1350006.4" rel="nofollow noreferrer">Haskell</a> doesn't require Float and Double to be IEEE single- and double-precision floating-point numbers, but it strongly recommends it.  GHC follows the recommendation.  IEEE floating-point numbers have the same issues across all languages.  Some of this is handled by the LIA standard, but Haskell only implements that in "a library".  (No, I'm not sure what libraryor if it even exists.)</p>
<p><a href="https://stackoverflow.com/a/22818675/2008899">This great answer</a> shows the various other numeric representations that are either part of Haskell (like <a href="http://www.haskell.org/onlinereport/haskell2010/haskellch22.html#x30-25800022" rel="nofollow noreferrer">Rational</a>) or available from <a href="http://hackage.haskell.org/" rel="nofollow noreferrer">hackage</a> like (<a href="http://hackage.haskell.org/package/base-4.6.0.1/docs/Data-Fixed.html" rel="nofollow noreferrer">Fixed</a>, <a href="http://hackage.haskell.org/package/numbers-3000.2.0.1/docs/Data-Number-CReal.html" rel="nofollow noreferrer">CReal</a>, and <a href="http://hackage.haskell.org/package/cyclotomic" rel="nofollow noreferrer">Cyclotomic</a>).</p>
<p>Rational, Fixed, and Cyclotomic might have similar Python libraries; Fixed is somewhat similar to the .Net Decimal type.  CReal also might, but I think it might take advantage of Haskell's call-by-need and could be difficult to directly port to Python; it's also pretty slow.</p>
</div>
<span class="comment-copy">Short answer to question title: Yes.</span>
<span class="comment-copy">Yes, there is a problem, or yes, it is taken care of?</span>
<span class="comment-copy">Please see <a href="https://docs.python.org/2/tutorial/floatingpoint.html" rel="nofollow noreferrer">Python docs - Floating Point Arithmetic: Issues and Limitations</a>. Also, this is not specific to any particular language: <a href="http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html" rel="nofollow noreferrer">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a>.</span>
<span class="comment-copy">Regarding the UPDATE: <code>math.ceil((0.1 + 0.2)*10)</code> is <code>4.0</code>. So no, <code>ceil</code> does not "take care" of floating point issues.</span>
<span class="comment-copy">It is an error to think that 'float numbers can be finite and infinite'. Floating-point numbers in computing are nothing more than an <i>approximate representation</i> of real numbers with which mathematic applications deal. The representation is always finite and, unless you positively can prove the opposite, must be thought of as inexact. There are coutable subclasses of real numbers which can be represented exactly. The most useful one is that of rational numbers. Unlike most languages, Haskell has out-of-the-box support for exact representation rational numbers, but that's a different story.</span>
<span class="comment-copy">The format C and C++ use for representing float and double is <i>not</i> standardized, and in fact varies from one implementation to another.  (And not all modern CPUs use IEEE754.  The most obvious exception is the IBM mainframes, but in fact, most mainframes have proprietary formats.  Rarely base 2, for that matter.)</span>
<span class="comment-copy">@JamesKanze modern IBM mainframes have IEEE754 units.  They just also have non IEEE754 floating point units.</span>
<span class="comment-copy">@PhilipJF The last time I looked, however, the IEEE754 units were significantly slower than the native floating point, and no one actually used them.</span>
<span class="comment-copy">@JamesKanze: Do you have any links to back that up?  I remember hearing that Linux on System z10 tended to use the IEEE 754 unit (but of course now I can't figure out where I think I heard that).  I'd be very interested to see evidence of users of the hex floating-point unit.</span>
<span class="comment-copy">(I should add that I have an ulterior motive; I'm trying to convince myself that it would be okay for future versions of Python to insist on IEEE 754.  I'd like to understand the potential for breakage that that would introduce on various systems.)</span>
<span class="comment-copy">I wonder in the math is different between the coutries or what? 5/3 = 1.(6) - infinite floating point number. Of cause it is exactly 5/3, but you will never be able to express 5/3 as a floating point in base 10, only indefinitely approach. However, you can provide exact finite value as floating point in other base.</span>
<span class="comment-copy">@BarafuAlbino: "1.(6) - infinite floating point number". No. Repeating decimal (in this case periodic). The number itself is perfectly finite. However, <b><i>it's representation</i></b> in the decimal system would consist of infinite many digits. The number itself is perfectly finite. The number will always be finite, however, the representation might not, depending on the system.</span>
<span class="comment-copy">Wow. I have to admit I may have skipped the intro part.</span>
<span class="comment-copy">I would call it a feature, not a problem. Python does have ways to circunvent them, if necessary.</span>
