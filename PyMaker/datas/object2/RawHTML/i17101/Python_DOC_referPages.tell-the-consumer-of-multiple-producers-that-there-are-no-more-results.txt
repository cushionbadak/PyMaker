<div class="post-text" itemprop="text">
<p>I have some code that adds work to an one queue, where it is processed by multiple workers, and then the results are put into another queue where they are processed by a final worker. When I have multiple producers adding material to this results queue, how can I reliably signal the <code>collector</code> process that there is nothing more to process?</p>
<pre><code>import multiprocessing
import time

J = multiprocessing.Queue()
R = multiprocessing.Queue()

def intermediate_worker(jobs, results):
    while True:
        task = jobs.get()
        if task is None:
            jobs.put(None)
            break
        print 'working', task
        results.put(task)

def collector(result_queue) :
    total = 0
    while True :
        result = result_queue.get()
        total += result
        print 'collection', total


[multiprocessing.Process(target=intermediate_worker, args=(J,R)).start()
 for i in xrange(2)]

multiprocessing.Process(target=collector, args=(R,)).start()

for chunk_dummy in xrange(10) :
    J.put(chunk)
J.put(None)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>There's a few ways.  One is to look for <em>n</em> poison pills, where <em>n</em> is the number of producers you have.  You already are familiar with this approach since you're using it to shut down your producers.  This works okay, but is a bit clunky - you need extra logic in your consumer to keep track of how many poison pills you've seen.</p>
<p>My personal favorite is to use a <a href="http://docs.python.org/3/library/multiprocessing.html#multiprocessing.Semaphore" rel="nofollow"><code>Semaphore</code></a> to do the counting for me.  If we know how many producers are alive, that is sufficient information to figure out when to shut down the consumer - we shut it down when <code>(no producers are alive)</code> and <code>(the queue is empty)</code>.</p>
<pre><code>J = multiprocessing.Queue()
R = multiprocessing.Queue()
S = multiprocessing.Semaphore(NUMWORKERS)

def intermediate_worker(jobs, results, sem):
    with sem: #context manager handles incrementing/decrementing the semaphore 
        while True:
            task = jobs.get()
            if task is None:
                jobs.put(None)
                break
            print 'working', task
            results.put(task)

def collector(result_queue, sem) :
    total = 0
    while sem.get_value() &lt; NUMWORKERS or not result_queue.empty():
        result = result_queue.get()
        total += result
        print 'collection', total

[multiprocessing.Process(target=intermediate_worker, args=(J,R,S)).start()
 for i in xrange(NUMWORKERS)]

multiprocessing.Process(target=collector, args=(R,S)).start()
</code></pre>
<p>Rough code, entirely untested, but should get the point across.</p>
</div>
