<div class="post-text" itemprop="text">
<p>The code i'm writing calculates the sum of divisors of all numbers up to a specified limit. I tried writing it in three ways, two using numpy and one using the array module.</p>
<pre><code>N = 10**5
def func2(N,a):
for f in range(2, N//2+1):
    for i in range(f*2, N+1, f):
        a[i] += f
def func3(N,sod):
    for div in range(2,N//2 +1):
        sod[2*div::div]+=div
</code></pre>
<p>the timing results (using timeit) are:</p>
<pre><code>%timeit func2(N,sod)
%timeit func3(N,a)

1 loop, best of 3: 801 ms per loop
1 loop, best of 3: 703 ms per loop
</code></pre>
<p>And the results really go against my intuition, I would expect that the version with only one for loop would be faster then the version with two for loops and wouldn't expect there to be such a big difference between numpy and the array module. So, what am I missing?</p>
</div>
<div class="post-text" itemprop="text">
<p>The first problem is that you're using <code>cProfile</code> for benchmarking. As the big box at the top of <a href="https://docs.python.org/3/library/profile.html" rel="nofollow noreferrer">the docs</a> says:</p>
<blockquote>
<p>The profiler modules are designed to provide an execution profile for a given program, not for benchmarking purposes (for that, there is timeit for reasonably accurate results). This particularly applies to benchmarking Python code against C code…</p>
</blockquote>
<hr/>
<p>The second problem is that you seem to have read your results out of order:</p>
<pre><code>1    0.359    0.359    0.359    0.359 toiprof.py:13(func3)
1    1.568    1.568    1.569    1.569 toiprof.py:4(func1)
1    0.679    0.679    0.679    0.679 toiprof.py:9(func2)
</code></pre>
<p>Yes, the second output is much slower than the first and third—but the second output is <code>func1</code>, not <code>func2</code>. So numpy slicing (<code>func2</code>) is actually much faster than looping over numpy (<code>func1</code>), not slower.</p>
<hr/>
<p>The third problem is that you're creating the arrays inside the loop that you're timing, and this is actually a non-trivial cost. Numpy is slower at creating large arrays than <code>array</code>, but that's rarely an issue in real life for the simple reason that you're usually doing a whole lot more work per array than just creating it. I'm not sure if that's relevant in this case or not, but it could be, and it's a lot easier to just not do that than to try to measure and prove that doing that didn't really hurt anything.</p>
<hr/>
<p>When I change the arrays to be parameters, use <code>%timeit</code> in iPython, and interpret the results in the right order, I see <code>func2</code> as a little faster than <code>func3</code> and much faster than <code>func1</code>. Since that agrees with the results that kazemakase posted in a comment, it's pretty likely that at least one of three problems above is responsible for your different numbers.</p>
<p>In other words, numpy slicing seems to be slower because you measured it wrong, not because of anything to do with numpy or array or loops.</p>
</div>
<div class="post-text" itemprop="text">
<p>The reason two for loops are faster than slicing is that when slicing returns only a small subset of the array iterating is faster.</p>
<pre><code>def npslice(arr,num):
    arr[num::num]+=num
def arriter(arr,num,l):
    for a in range(num,l,num):
        arr[a]+=num
</code></pre>
<p>and the timing*:</p>
<pre><code>%timeit -n1 -r1 npslice(sod2,50000)
%timeit -n1 -r1 arriter(sod2,50000,N+1)

1 loop, best of 1: 82 µs per loop
1 loop, best of 1: 65.1 µs per loop
</code></pre>
<p>So for most of the way (inside the outer for loop) the (inner) for loop is a bit faster than slicing using numpy.</p>
<p>the "-n1 -r1" options are set because the python interpeter optimizes the slicing operation when it is preformed more than once so the timeit function throws this warning when it is not set - </p>
<pre><code>The slowest run took 5.07 times longer than the fastest. This could 
mean that an intermediate result is being cached.
</code></pre>
</div>
<span class="comment-copy">Get the arrays out of the func definitions and measure again.</span>
<span class="comment-copy">Something wrong with your measurement (or the interpretation of the results). I just ran the functions with <code>%timeit</code> in IPython and got 500ms, 150ms, and 300ms. So <code>func2</code> is clearly fastest.</span>
<span class="comment-copy">Why are you using <code>cProfile</code> instead of <code>timeit</code>? As <a href="https://docs.python.org/3/library/profile.html" rel="nofollow noreferrer">the big box right at the top of the <code>cProfile</code> docs</a> says, "The profiler modules are designed to provide an execution profile for a given program, not for benchmarking purposes (for that, there is timeit for reasonably accurate results). This particularly applies to benchmarking Python code against C code:" Granted, the error is <i>usually</i> going to go in the opposite direction from what you're seeing, but the more important point is that these aren't useful benchmarks in the first place.</span>
<span class="comment-copy">Also… you've apparently presented the results out of order, and then misinterpreted as a result. The slow one in the middle isn't <code>func2</code>, the <code>numpy</code> slicing version, it's <code>func1</code>, the double-loop-over-<code>numpy</code> version.</span>
<span class="comment-copy">For slices that pick out very few values, like <code>a[3000::1500]</code> when <code>a</code> has 10000 elements, I could easily believe that iterating only 5 times is faster than operating on an extended slice. I don't think that's relevant here, that most of your slices will be pretty big, but reading this on a phone screen while holding it out of the rain… maybe someone else can check that.</span>
<span class="comment-copy">Now I am doing exactly what you did  - preallocating the arrays and still the version with the array module is faster for me by about 100 ms.</span>
