<div class="post-text" itemprop="text">
<p>I am on windows and want to run my multi-threaded python app that saves data to .csv in an async way. As reported <a href="https://stackoverflow.com/questions/6774724/why-python-has-limit-for-count-of-file-handles">here</a>, <a href="https://stackoverflow.com/questions/36474264/python-2-7-on-windows-too-many-open-files">here</a> and <a href="https://stackoverflow.com/questions/18280612/ioerror-errno-24-too-many-open-files">here</a>, I am getting the following error at some point:</p>
<pre><code>&lt;type 'exceptions.IOError'&gt; 
Traceback (most recent call last):
  File "results_path", line 422, in function
    df_results.to_csv(results_file)
IOError: [Errno 24] Too many open files
</code></pre>
<p><a href="https://stackoverflow.com/a/6776345/6060083">This</a> proposes a fix that includes with-statements for every file IO operation:</p>
<pre><code>with open(results_path, 'a') as results_file:
         df_results.to_csv(results_file)
</code></pre>
<p>However, I am still getting <code>IOError</code> as described above (<em>In a nutshell, none of the SO questions solved my issue</em>). Therefore, the <code>with</code>-statement apparently does not properly close the .csv file after the append operation.</p>
<p>First, I now increases the number of open files. This admittedly just delays the crash:</p>
<pre><code>import win32file
max_open_files = 2048     # Windows-specific threshold for max. open file count
win32file._setmaxstdio(max_open_files)
</code></pre>
<p>Second, my temporary approach is <strong>(A)</strong> to check for open .csv-files consequtively, and <strong>(B)</strong> forcefully restart the whole script if the open file count gets anywhere near the threshold allowed for windows:</p>
<pre><code>from psutil import Process 
import os, sys
proc = Process() 
open_file_count = 0                                         # Set up count of open files
for open_file in proc.open_files():                         # Iterate open files list
        if ".csv" in str(open_file):                        # Is file of .csv type?
                open_file_count += 1                        # Count one up
            else:
                continue
    else:
        if open_file_count &gt; (max_open_files / 2):              # Threshold, see above
            os.execl(sys.executable, sys.executable, *sys.argv) # Force restart
        else:
            pass
</code></pre>
<p>This approach is ugly and inefficient in many ways (loop through all open files in every iteration/thread). At the very least, this needs to work without forcefully restarting the whole code.</p>
<p><strong>Q1: How to properly close .csv files using python on windows?</strong></p>
<p><strong>Q2: If closing fails after IO operation, how to forcefully close open all .csv files at once?</strong></p>
</div>
<div class="post-text" itemprop="text">
<p>Those answers are correct.  The <code>with</code> statement is the <em>correct</em> and Pythonic way to open and automatically close files.  It works and is <strong>well</strong> tested.  I suspect, however, that it's the multiprocessing or threading that's throwing a spanner in the works.</p>
<p>In particular, how many of your threads or processes are writing to your CSV?  If more than one, then I'm confident that's the issue.  Instead, have a single writer, and pass what needs to be written to that writing thread or process via a multiprocessing.Queue or regular (thread-safe) queue.  In effect, a funnel, in which all processes that want to add data to the CSV would instead put the data into the queue, and the writing process will take each queue item out and write it the file.</p>
<p>Given a lack of working example in the question, I'll simply leave a pointer to Python's documentation on <a href="https://docs.python.org/3.6/library/multiprocessing.html#exchanging-objects-between-processes" rel="nofollow noreferrer">multiprocess communication</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>Use ThreadPoolExecutor from <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">https://docs.python.org/3/library/concurrent.futures.html</a> so you can keep a maximum number of threads running at one time to be less than the maximum number of file descriptors.</p>
<p>The <code>with</code> statement is the best way to handle the closing of files even when exceptions happen so you don't forget.</p>
</div>
<div class="post-text" itemprop="text">
<p>Just close normal, not "with"???</p>
<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame()

In [3]: fw = open("test2.txt","a")

In [4]: df.to_csv(fw)

In [5]: fw.close()

In [6]: !ls
test2.txt
</code></pre>
</div>
<span class="comment-copy">It would be good to see a minimal script that actually reproduces your error.</span>
<span class="comment-copy">You claim the with-statement doesn't seem to close the files properly. Is there a chance you actually simply have too many files open at once? Without an example script, it's hard to tell what's going on. But with async threads, and if each thread takes a while, could you have several hundreds of threads going at once, and thus have several hundreds of files open at once?</span>
<span class="comment-copy">Yes, but not anywhere near 2048</span>
<span class="comment-copy">We don't know; we just have to take your word for it. Two of the answers below suggest a similar reasoning: there are just too many threads keeping files open.</span>
<span class="comment-copy">Possibly something happens in <code>to_csv</code> that duplicates and leaks a file descriptor. But this is pointless speculation without a minimal, complete, and verifiable example.</span>
<span class="comment-copy">that's how i started, and no, it also causes the error</span>
