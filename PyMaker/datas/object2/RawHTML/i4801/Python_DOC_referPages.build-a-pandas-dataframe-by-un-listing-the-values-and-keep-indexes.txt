<div class="post-text" itemprop="text">
<p>From a json like this:</p>
<pre><code>{'data': [{
   'values': [0, 0.4, 7 ... to 99 elements]}
   'values': [0, 3, 2 ... to 99 elements]}
   ... 1 mil rows
    }
</code></pre>
<p>I would like to have a df like this (indexes from 0 to 99 and then the values as a column):</p>
<pre><code>        0
    0   0.0
    1   0.4
    2   0.0
    3   4.5
    ...
    98  0
    99  0
    1   3.5
    2   0
    ...
    99
    ...
</code></pre>
<p>To replicate it: </p>
<pre><code>np.random.seed(0)
data = np.random.rand(4800,100)
</code></pre>
<p>which gives:</p>
<pre><code>array([[ 0.5488135 ,  0.71518937,  0.60276338, ...,  0.02010755,
         0.82894003,  0.00469548],
</code></pre>
<p>But then the only way I managed to do it is by creating a df from each item, which is really slow:</p>
<pre><code>def sort(data):
    list_dfs = []
    for index, item in enumerate(data):
        list_dfs.append(pd.DataFrame(item))
    sorted_df = pd.concat(list_dfs)

    return sorted_df

%time sort(data) # 633ms
</code></pre>
<p>This is 633ms for 4800 rows. So I wonder if this can be improved to work for millions of rows. Somehow to not append pd.DataFrame(item), but the item and keep the indexes...</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://docs.python.org/3/library/itertools.html#itertools.chain" rel="nofollow noreferrer"><code>itertools.chain</code></a> provides one solution, which seems to yield a ~3.5x improvement:</p>
<pre><code>from itertools import chain
import numpy as np

np.random.seed(0)
data = np.random.rand(4800,100)

def sort(data):
    list_dfs = []
    for index, item in enumerate(data):
        list_dfs.append(pd.DataFrame(item))
    sorted_df = pd.concat(list_dfs)

    return sorted_df

def sort2(data):
    df = pd.DataFrame(list(chain(*data)))
    df.index = list(chain(*(range(len(i)) for i in data)))
    return df

%timeit sort(data)   # 1 loop, best of 3: 1.29 s per loop
%timeit sort2(data)  # 10 loops, best of 3: 390 ms per loop
</code></pre>
<p>If each set of data has the same number of elements, you can further optimize index calculation:</p>
<pre><code>def sort3(data):
    n = data.shape[1]
    df = pd.DataFrame(list(chain(*data)))
    df.index = df.index % n
    return df

%timeit sort3(data)  # 10 loops, best of 3: 111 ms per loop
</code></pre>
</div>
<span class="comment-copy">thanks, .chain fares so much better with a larger number of elements</span>
