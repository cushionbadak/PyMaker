<div class="post-text" itemprop="text">
<p>I have a scrapy code here that outputs a json file. I then imported the json file into a dataframe using jupyter notebook to clean the data. However, what I really want to do is be able to run the scrapy spider and output the data as a CSV already cleaned, instead of me saving the scraped data as a json file and going into jupyter notebook to clean it. Is there anyway to connect the two process? Below is my spider code:</p>
<pre><code>import scrapy
import pandas as pd
import json
import datetime
from seatgeek.items import SeatgeekItem

class seatgeekSpider(scrapy.Spider):
    name = "seatgeek_spider"
    #showname = input("Enter Show name (lower case please): ")
    #showname = showname.replace(' ', '-')
    start_urls = ["https://seatgeek.com/come-from-away-tickets?page={}".format(i) for i in range (1,35)]

    custom_settings = {
        'DOWNLOAD_DELAY': 3,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 3,
        'HTTPCACHE_ENABLED': True,
        'FEED_FORMAT': 'json',
        'FEED_URI': 'test.json'
    }

    def parse(self, response):
        for href in response.xpath('//a[@class="event-listing-title"]/@href').extract():
            item = SeatgeekItem()
            item['performance'] = href.split('/')[-3]
            item["eventId"] = href.split('/')[-1]

            yield scrapy.Request(
                url = 'https://seatgeek.com/listings?id=' + item['eventId'] + '&amp;aid=11955&amp;client_id=MTY2MnwxMzgzMzIwMTU4',
                callback=self.parse_ticketinv,
                meta={'item': item})

    def parse_ticketinv(self, response):
        jsonresponse = json.loads(response.body_as_unicode())
        item = response.meta['item']
        for i in jsonresponse["listings"]:
            item["sectionName"] = i["s"]
            item["zoneName"] = i["s"].split(' ')[0]
            item["currentPrice"] = i["p"]
            item["listingPrice"] = i["p"]
            item["row"] = i["r"]
            item["seatNumbers"] = ""
            item['listingId'] = i["id"]
            item['quantity'] = i["q"]
            item['vendor'] = "SeatGeek"

            yield item
</code></pre>
<p>And this is my code for cleaning the data:</p>
<pre><code>import json
import pandas as pd
import datetime

path = r'C:\Users\...\Desktop\Code\seatgeek\test.json'

with open(path) as f:
    data = json.load(f)

df = pd.DataFrame(data)

df = df[df.performance.str.contains('gerald')]
df['performance'] = df['performance'].map(lambda x: x.lstrip('gerald-schoenfeld-theatre-').rstrip('-pm'))
df['performance'] = df['performance'].apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d-%H'))

df['datePulled'] = pd.to_datetime('today')

filename = input('Enter filename: ')
df.to_csv(filename + '.csv', mode='a', index=False)
</code></pre>
<p><strong>EDIT:</strong></p>
<p>Per @stranac's help, I've updated my pipeline with the the code below. However for some reason, I seem to be missing/some rows I do not want are being deleted. It seems like if I comment out <code>item['performance'] = datetime.datetime.strptime(item['performance'][26:],'%Y-%m-%d-%H-%p')</code>, it would works fine.</p>
<pre><code>def process_item(self, item, spider):
   if 'gerald' not in item['performance']:
      raise DropItem
   item['performance'] = datetime.datetime.strptime(item['performance'][26:],'%Y-%m-%d-%H-%p')
   item['datePulled'] = datetime.datetime.now()
   return item
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The simplest way to do this would be to write your own <a href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html" rel="nofollow noreferrer">item pipeline</a>, and do the cleanup per item in <code>process_item()</code>, instead of doing it all at once using pandas.</p>
<pre><code>class YourPipeline(object):
    def process_item(self, item, spider):
        if 'gerald' not in item['performance']:
            raise DropItem
        # do your cleanup
        return item
</code></pre>
<p>Sidenote:<br/><a href="https://docs.python.org/3/library/stdtypes.html#str.lstrip" rel="nofollow noreferrer">lstrip</a> and <a href="https://docs.python.org/3/library/stdtypes.html#str.rstrip" rel="nofollow noreferrer">rstrip</a> don't do what you think, they just happen to produce the correct result in this case.</p>
<p>EDIT:</p>
<p>The error you get is caused by the same item being yielded multiple times from <code>parse_ticketinv</code>.
This causes the already-modified item (<code>item['performance']</code> being replaced by a <code>datetime</code> object) to pass through the pipeline again, causing a <code>TypeError</code>.</p>
<p>The simplest way to solve the problem is creating a new item each loop, or yielding item copies:</p>
<pre><code>def parse_ticketinv(self, response):
    # for whatever:
        # do stuff
        yield item.copy()
</code></pre>
</div>
<span class="comment-copy">Thanks for your reply. I seem to get a name error <code>NameError: name 'DropItem' is not defined</code> how can I fix this? <i>(updated code in my original post)</i> Also for some reason, I seem to be missing/some rows I do not want are being deleted. It seems like if I comment out <code>item['performance'] = datetime.datetime.strptime(item['performance'][26:],'%Y-%m-%d-%H-%p')</code>, it would works fine, so the problem is with this line, but I can't figure out what.</span>
<span class="comment-copy"><code>DropItem</code> needs to be imported (shown in the docs link). As for your missing rows, check the log for errors, but my guess is that the value of <code>item['performance']</code> might be different than what you expect.</span>
<span class="comment-copy">I fixed the <code>DropItem</code> problem - thank you. Playing around with the pipeline, it seems like if I comment out all of the data cleaning, I have all of the rows I need. However even if I add a line like: <code>item['performance'] = item['performance'][26:]</code>, some rows are being deleted and I get no errors in my log.</span>
<span class="comment-copy">I've edited the answer, updating your post with the error (preferably a full traceback) is probably a good idea.</span>
<span class="comment-copy">Ah the updated answer worked! Thank you so much for your help.</span>
