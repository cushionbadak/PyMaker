<div class="post-text" itemprop="text">
<p>I am using PyCharm 2016.3.2 with Python 3.6 as the interpreter to convert PDF files to .TXT
The code I have (see below) works fine, but it converts files sequentially and slowly. I wonder if I can take advantage of my 8 core cpu to make this a bit faster. Here is the code:</p>
<pre><code>from tika import parser
from os import listdir
for filename in listdir("C:\\Dropbox\\Data"):
    text = parser.from_file('C:\\Dropbox\\Data'+filename)
    with open('C:\\Dropbox\\Data\\textoutput\\'+filename+'.txt', 'w+') as outfile : 
        outfile.write(text["content"])
</code></pre>
<p>I am new to Python coding so any help in parallelizing this block of code will be much appreciated, since I'm dealing with &gt;100,000 files (65 GB+)</p>
<p>Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>Since you have lots of documents that you want to parse, this is relatively easy. (If you had been asking how to parse a single document on multiple cores, it would have been much harder).</p>
<p>The technique is to create a pool of workers (usually one for each core, because you rarely get much benefit beyond that), and then assign tasks to the workers.</p>
<p>As the basis for your pool, you can use either <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer">multiprocessing.Pool</a> (uses multiple processes), <a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy" rel="nofollow noreferrer">multiprocessing.dummy.Pool</a> (replicates the same API but uses threading), or something more recent like <a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor" rel="nofollow noreferrer">ThreadPoolExecutor</a>.</p>
<p>I will use the first of these for this example:</p>
<pre><code>import os
from multiprocessing import Pool

from tika import parser

dirname = "C:\\Dropbox\\Data"
# I changed your output dir, otherwise `listdir` will include `textoutput`
output_dirname = "C:\\Dropbox\\textoutput"

def process_file(filename):
    text = parser.from_file(os.path.join(dirname, filename))
    with open(os.path.join(output_dirname, filename + '.txt'), 'w+') as outfile:
        outfile.write(text["content"])

pool = Pool(processes=8)
pool.map(process_file, os.listdir(dirname))
</code></pre>
<p>This is untested in the details, but the basic outline I have tested.</p>
<p>Note that threading often runs into limitations with the GIL in Python, so you end up not using all the CPU you could, while multiple processes gets around that, at the cost of startup and communication overhead for each task.</p>
</div>
