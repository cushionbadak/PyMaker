<div class="post-text" itemprop="text">
<p>I want to create a multi-index <code>DataFrame</code> by reading a textfile. Is it faster to create the multi-index and then allocate data to it from the text file using <code>df.loc[[],[]]</code>, or concatenate rows to the <code>DataFrame</code> and set the index of the <code>DataFrame</code> at the end? Or, is it faster to use a list or <code>dict</code> to store the data as it's read from the file, and then create a <code>DataFrame</code> from them? Is there a more pythonic or faster option?</p>
<p>Example text file:</p>
<pre><code>A = 1
 B = 1
  C data
  0 1
  1 2
A = 1
 B = 2
  C data
  1 3
  2 4
A = 2
 B = 1
  C data
  0 5
  2 6
</code></pre>
<p>Output DataFrame:</p>
<pre><code>A B C data
1 1 0 1
    1 2
1 2 1 3
    2 4
2 1 0 5
    2 6
</code></pre>
<p><strong>Update Jan 18:</strong> This is linked to <a href="https://stackoverflow.com/questions/47982949/how-to-parse-complex-text-files-using-python">How to parse complex text files using Python?</a> I also wrote a <a href="http://www.vipinajayakumar.com/parsing-text-with-python/" rel="nofollow noreferrer">blog article explaining how to parse complex files to beginners</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>Element by element lookups in pandas is an expensive operation, so is aligning by index. I would read everything into arrays, create a DataFrame of values, and then set the hierarchical index directly. Usually much faster if you can avoid append or lookups.</p>
<p>Here is a sample result assuming you have a dataset 2-D array with everything lumped in:</p>
<pre><code>In [106]: dataset
Out[106]: 
array([[1, 1, 0, 1],
       [1, 1, 1, 2],
       [1, 2, 1, 3],
       [1, 2, 2, 4],
       [2, 1, 0, 5],
       [2, 1, 2, 6]])

In [107]: pd.DataFrame(dataset,columns=['A','B','C', 'data']).set_index(['A', 'B', 'C'])
     ...: 
Out[107]: 
       data
A B C      
1 1 0     1
    1     2
  2 1     3
    2     4
2 1 0     5
    2     6

In [108]: data_values = dataset[:, 3] 
     ...: data_index = pd.MultiIndex.from_arrays( dataset[:,:3].T, names=list('ABC'))
     ...: pd.DataFrame(data_values, columns=['data'], index=data_index)
     ...: 
Out[108]: 
       data
A B C      
1 1 0     1
    1     2
  2 1     3
    2     4
2 1 0     5
    2     6

In [109]: %timeit pd.DataFrame(dataset,columns=['A','B','C', 'data']).set_index(['A', 'B', 'C'])
%%timeit
1000 loops, best of 3: 1.75 ms per loop

In [110]: %%timeit
     ...: data_values = dataset[:, 3] 
     ...: data_index = pd.MultiIndex.from_arrays( dataset[:,:3].T, names=list('ABC'))
     ...: pd.DataFrame(data_values, columns=['data'], index=data_index)
     ...: 
1000 loops, best of 3: 642 Âµs per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Parsing the text file will be the bulk of your processing overhead.</p>
<p>If speed is the main concern I'd suggest using pickle or shelve to store the DataFrame object in a binary file ready for use.</p>
<p>If you need to use the text file for any reason, a separate module could be written for translating between formats.</p>
</div>
<span class="comment-copy">Maybe you could use the <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow noreferrer"><code>timeit</code></a> module to test it out.</span>
<span class="comment-copy">How big is the text file?</span>
<span class="comment-copy">@MartinEvans order of 100 MB</span>
