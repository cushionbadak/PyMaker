<div class="post-text" itemprop="text">
<p>I have a large list. I want to process each item. I'd like to segment the list and process each segment on a different CPU. I'm using the pathos multiprocessing library. I've created the following function:</p>
<pre><code>def map_list_in_segments (l, f):
  cpus = max(1, int(cpu_count() / 2) - 1)
  seg_length = int(len(l) / cpus)
  segments = [l[x:x+seg_length] for x in range(0,len(l),seg_length)]
  pool = Pool(nodes=cpus)
  mapped_segments = pool.map(lambda seg: f(seg), segments)
  return (sg for seg in mapped_segments for sg in seg)
</code></pre>
<p>It returns the correct result and uses all (or almost all) the CPUs. However, iterating over the returned list results in very large amounts of memory being consumed unexpectedly.</p>
<p>At first I was returning a list comprehension. I switched that to a generator, hoping for less memory consumption, but that didn't improve anything.</p>
<p><em>Update based on comments:</em></p>
<p>I was unaware of <code>imap</code> and <code>uimap</code> and that they automatically chunk the input list. I gave <code>uimap</code> a try but saw very low CPU utilization and very long running times. One of the processes had very high CPU utilization though. What I think is happening is that there is a lot of pickling going on. The <code>f</code> that I'm passing in has a large object in a closure. When using the ProcessingPool methods (<code>map</code>, <code>imap</code>, <code>uimap</code>) this object needs to be pickled for each element in the list. I suspect that this is what the one process that is very busy is doing. The other processes are throttled by this pickling.</p>
<p>If so, this explains why my manual segmenting is causing significant gains in CPU utilization: the large object only needs to be pickled once per segment instead of for every item.</p>
<p>I then tried using <code>uimap</code> in my <code>map_list_in_segments</code>, hoping for a drop in memory consumption but this did not occur. Here's how the code looks that calls the method and iterates the results:</p>
<pre><code>segments = multiprocessing.map_list_in_segments(l, lambda seg: process_segment(seg, large_object_needed_for_processing))
for seg in segments:
  for item in seg:
    # do something with item
</code></pre>
<p>My (limited) understanding of generators is that the first <code>for</code> loop that is looping through the segments should release each one from memory as it iterates. If so it would seem that the large memory usage is the pickling of the return values of the <code>process_segment</code> method. I'm not returning large amounts of data (about 1K bytes for each item) and the size of <code>l</code> I'm working with is 6000 items. Not sure why 5GB of memory gets consumed.</p>
</div>
<div class="post-text" itemprop="text">
<p>The problem with <code>multiprocessing</code> is that communication between processes is expensive.  If your result is equivalent in size to your input, you're probably going to spend most of your time pickling and unpickling data rather than doing anything useful.  This depends on how expensive <code>f</code> is, but you might be better off not using <code>multiprocessing</code> here.  </p>
</div>
<div class="post-text" itemprop="text">
<p>Some further testing reveals that the pickling isn't the issue. The processing I was doing in the <code>for item in seg</code> was constructing additional objects that were consuming a large amount of memory.</p>
<p>The insights derived from this exercise and the intelligent commenters:</p>
<ol>
<li>ProcessPool methods (<code>map</code>, <code>imap</code>, <code>uimap</code>) automatically chunk the list.</li>
<li>If you are passing in a large object to <code>f</code> (via a closure) you might find that manually chunking the list (as above) saves on a lot of pickling and increases CPU utilization.</li>
<li>Using <code>imap</code> and <code>uimap</code> can significantly reduce memory usage.</li>
</ol>
</div>
<span class="comment-copy"><code>mapped_segments</code> is already materialized into a list because you used <code>Pool.map</code>, consider using <code>Pool.imap</code> or <code>Pool.imap_unordered</code></span>
<span class="comment-copy">Also, are you sure this is working correctly? <code>map</code> is already chunking your data, why are you chunking it by hand?</span>
<span class="comment-copy"><a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map" rel="nofollow noreferrer"><code>Pool.map()</code></a> has an optional <code>chunksize</code> parameter which will chop up the iterable for you. Try using that instead of doing it yourself (and adjust its value to change the amount of memory used at one time).</span>
<span class="comment-copy">@martineau chunksize does not have to be specified, AFAIK, it will guess for you</span>
<span class="comment-copy">@juanpa: Yes, I am aware of thatâ€”but I suggested explicitly specify it In this case in order gain control over memory use.</span>
<span class="comment-copy"><code>f</code> is fairly expensize. By using <code>multiprocessing</code> I am reducing run times quite significantly. I've updated the post with additional explanations/insights. I believe the problem is in the way things get pickled/unpickled. Not sure if there is a way to have the unpickling of the result done in batches so that I don't consume all of the available memory on the server.</span>
