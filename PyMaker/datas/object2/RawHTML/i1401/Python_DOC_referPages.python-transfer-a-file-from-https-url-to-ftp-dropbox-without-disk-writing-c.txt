<div class="post-text" itemprop="text">
<p>I have a large file (500 Mb-1Gb) stored on a HTTP(S) location<br/>
(say <code>https://example.com/largefile.zip</code>).</p>
<p>I have read/write access to an FTP server</p>
<p>I have normal user permissions (no sudo).</p>
<p>Within these constraints I want to read the file from the HTTP URL via requests and send it to the FTP server without writing to disk first.</p>
<p>So normally, I would do.</p>
<pre><code>response=requests.get('https://example.com/largefile.zip', stream=True)
with open("largefile_local.zip", "wb") as handle:                                                                                                     
 for data in response.iter_content(chunk_size=4096):
  handle.write(data)     
</code></pre>
<p>and then upload the local file to FTP. But I want to avoid the disk I/O. I cannot mount the FTP as a fuse filesystem because I don't have super user rights.</p>
<p>Ideally I would do something like <code>ftp_file.write()</code> instead of <code>handle.write()</code>. Is that possible? The ftplib documentation seems to assume only local files will be uploaded, not <code>response.content</code>. So ideally I would like to do</p>
<pre><code>response=requests.get('https://example.com/largefile.zip', stream=True)
for data in response.iter_content(chunk_size=4096):
 ftp_send_chunk(data)   
</code></pre>
<p>I am not sure how to write <code>ftp_send_chunk()</code>.</p>
<p>There is a similar question here (<a href="https://stackoverflow.com/questions/51632215/python-upload-a-in-memory-file-generated-by-api-calls-in-ftp-by-chunks">Python - Upload a in-memory file (generated by API calls) in FTP by chunks</a>). My use case requires retrieving  a chunk from the HTTP URL and writing it to FTP.</p>
<p>P.S.: The solution provided in the answer (wrapper around urllib.urlopen) will work with dropbox uploads as well. I had problems working with my ftp provider ,so finally used dropbox, which is working reliably.</p>
<p>Note that Dropbox has a "add web upload" feature in the api which does the same thing (remote upload). That only works with "direct" links. In my use case the http_url came from a streaming service that was i.p. restricted. So this workaround became necessary.
Here's the code</p>
<pre><code>import dropbox;
d = dropbox.Dropbox(&lt;ACTION-TOKEN&gt;);
f=FileWithProgress(filehandle);
filesize=filehandle.length;
targetfile='/'+fname;
CHUNK_SIZE=4*1024*1024
upload_session_start_result = d.files_upload_session_start(f.read(CHUNK_SIZE));
num_chunks=1
cursor = dropbox.files.UploadSessionCursor(session_id=upload_session_start_result.session_id,
                                           offset=CHUNK_SIZE*num_chunks)
commit = dropbox.files.CommitInfo(path=targetfile)
while CHUNK_SIZE*num_chunks &lt; filesize:
 if ((filesize - (CHUNK_SIZE*num_chunks)) &lt;= CHUNK_SIZE):
  print d.files_upload_session_finish(f.read(CHUNK_SIZE),cursor,commit)
 else:
  d.files_upload_session_append(f.read(CHUNK_SIZE),cursor.session_id,cursor.offset)
 num_chunks+=1
cursor.offset = CHUNK_SIZE*num_chunks
link = d.sharing_create_shared_link(targetfile)  
url = link.url
dl_url = re.sub(r"\?dl\=0", "?dl=1", url)
dl_url = dl_url.strip()
print 'dropbox_url: ',dl_url;
</code></pre>
<p>I think it should even be possible to do this with google-drive via their python api , but using credentials with their python wrapper is too hard for me. Check this<a href="https://developers.google.com/api-client-library/python/guide/media_upload" rel="nofollow noreferrer">1</a> and this<a href="https://google.github.io/google-api-python-client/docs/epy/googleapiclient.http.MediaInMemoryUpload-class.html" rel="nofollow noreferrer">2</a></p>
</div>
<div class="post-text" itemprop="text">
<p>It should be easy with <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.urlopen" rel="nofollow noreferrer"><code>urllib.request.urlopen</code></a>, as it returns a file-like object, which you can use directly with <a href="https://docs.python.org/3/library/ftplib.html#ftplib.FTP.storbinary" rel="nofollow noreferrer"><code>FTP.storbinary</code></a>.</p>
<pre><code>ftp = FTP(host, user, passwd)

filehandle = urllib.request.urlopen(http_url)

ftp.storbinary("STOR /ftp/path/file.dat", filehandle)
</code></pre>
<hr/>
<p>If you want to monitor progress, implement a wrapper file-like object that will delegate calls to <code>filehandle</code> object, but will also display the progress:</p>
<pre><code>class FileWithProgress:

    def __init__(self, filehandle):
        self.filehandle = filehandle
        self.p = 0

    def read(self, blocksize):
        r = self.filehandle.read(blocksize)
        self.p += len(r)
        print(str(self.p) + " of " + str(self.p + self.filehandle.length)) 
        return r

filehandle = urllib.request.urlopen(http_url)

ftp.storbinary("STOR /ftp/path/file.dat", FileWithProgress(filehandle))
</code></pre>
<hr/>
<p>For Python 2 use:</p>
<ul>
<li><code>urllib.urlopen</code>, instead of <code>urllib.request.urlopen</code>.</li>
<li><code>filehandle.info().getheader('Content-Length')</code> instead of <code>str(self.p + filehandle.length)</code></li>
</ul>
</div>
<span class="comment-copy">Do you have shell access to <code>https://example.com/largefile.zip</code>? If so, why don't you upload <code>largefile.zip</code>  directly to the ftp server using <code>lftp</code> or similiar app?</span>
<span class="comment-copy">Nope. I can only read the data from the url (its a cloud streaming service)</span>
<span class="comment-copy"><code>urllib.request.urlopen</code> in Python 3, <code>urllib.urlopen</code> in Python 2, as my answer says.</span>
<span class="comment-copy">Thanks. Great answer . I  tested this and this worked perfectly when the url was "direct". In case of redirection this gives a dummy file (1 Kb) which I think is the first level of redirection . In that case one can do urllib.urlopen(requests.open(http_url,stream=True).url).</span>
<span class="comment-copy">Btw, I do not have any problem with redirects. <code>urlopen</code> redirects automatically for me.</span>
