<div class="post-text" itemprop="text">
<p>I am working on a CPU intensive ML problem which is centered around an additive model. Since addition is the main operation I can divide the input data into pieces and spawn multiple models which are then merged by the overriden <code>__add__</code> method.</p>
<p>The code relating to the multiprocessing looks like this:</p>
<pre><code>def pool_worker(filename, doshuffle):
    print(f"Processing file: {filename}")
    with open(filename, 'r') as f:
        partial = FragmentModel(order=args.order, indata=f, shuffle=doshuffle)
        return partial

def generateModel(is_mock=False, save=True):
    model = None
    with ThreadPool(args.nthreads) as pool:
        from functools import partial
        partial_models = pool.imap_unordered(partial(pool_worker, doshuffle=is_mock), args.input)
        i = 0
        for m in partial_models:
            logger.info(f'Starting to merge model {i}')
            if model is None:
                import copy
                model = copy.deepcopy(m)
            else:
                model += m
            logger.info(f'Done merging...')
            i += 1

    return model
</code></pre>
<p>The issue is that the memory consumption scales exponentially as the model order increases, so at order 4 each instance of the model is about 4-5 GB, which causes the threadpool to crash as the intermediate model objects are then not pickleable. </p>
<p>I read about this a bit and it appears as even if the pickling is not an issue, it's still extremely inefficient to pass data like this, as commented to <a href="https://stackoverflow.com/a/14677441/328725">this answer</a>.</p>
<p>There is very little guidance as to how one can use shared memory for this purpose, however. Is it possible to avoid this problem without having to change the internals of the model object? </p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Use files!</strong></p>
<p>No, really, use files -- they are are efficient (OS will cache the content), and allow you to work on much larger problems (data set doesn't have to fit into RAM).</p>
<p>Use any of <a href="https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.io.html" rel="noreferrer">https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.io.html</a> to dump/load numpy arrays to/from files and only pass file names between the processes.</p>
<p>P.S. benchmark serialisation methods, depending on the intermediate array size, the fastest could be "raw" (no conversion overhead) or "compressed" (if file ends up being written to disk) or something else. IIRC loading "raw" files may require knowing data format (dimensions, sizes) in advance.</p>
</div>
<div class="post-text" itemprop="text">
<p>Check out the <a href="https://ray.readthedocs.io/en/latest/" rel="nofollow noreferrer">ray</a> project which is a distributed execution framework that makes use of <a href="https://arrow.apache.org/" rel="nofollow noreferrer">apache arrow</a> for serialization. It's especially great if you're working with numpy arrays and hence is a great tool for ML workflows.</p>
<p>Here's a snippet from the docs on <a href="https://ray.readthedocs.io/en/latest/serialization.html" rel="nofollow noreferrer">object serialization</a></p>
<blockquote>
<p>In Ray, we optimize for numpy arrays by using the Apache Arrow data
  format. When we deserialize a list of numpy arrays from the object
  store, we still create a Python list of numpy array objects. However,
  rather than copy each numpy array, each numpy array object holds a
  pointer to the relevant array held in shared memory. There are some
  advantages to this form of serialization.</p>
<ul>
<li>Deserialization can be very fast. </li>
<li>Memory is shared between processes
  so worker processes can all read the same data without having to copy
  it.</li>
</ul>
</blockquote>
<p>In my opinion it's even easier to use than the multiprocessing library for parallel execution especially when looking to use shared memory, intro to usage in the <a href="https://ray.readthedocs.io/en/latest/tutorial.html" rel="nofollow noreferrer">tutorial</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>You should use Manager proxy object for shared editable objects: <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing-managers" rel="nofollow noreferrer">https://docs.python.org/3/library/multiprocessing.html#multiprocessing-managers</a>
The access lock would be handled by that Manager proxy object.</p>
<p>In <a href="https://docs.python.org/3/library/multiprocessing.html#customized-managers" rel="nofollow noreferrer">Customized managers</a> section there is an example, that should suit you:</p>
<pre><code>from multiprocessing.managers import BaseManager

class MathsClass:
    def add(self, x, y):
        return x + y
    def mul(self, x, y):
        return x * y

class MyManager(BaseManager):
    pass

MyManager.register('Maths', MathsClass)

if __name__ == '__main__':
    with MyManager() as manager:
        maths = manager.Maths()
        print(maths.add(4, 3))         # prints 7
        print(maths.mul(7, 8))         # prints 56
</code></pre>
<p>After that you have to connect from different processes (as shown in <a href="https://docs.python.org/3/library/multiprocessing.html#using-a-remote-manager" rel="nofollow noreferrer">using a remote manager</a>) to that manager and edit it as you wish.</p>
</div>
<span class="comment-copy">This answers how to share data between processes using shared memory and turn off pickling:  <a href="https://stackoverflow.com/a/14135569/9521723">stackoverflow.com/a/14135569/9521723</a></span>
<span class="comment-copy">@SimonF there is a crucial difference between the questions, the one you linked refers to child processes referencing (i.e. reading but not writing) large objects. In my case, I want to <b>return</b> large objects, my child processes get their input data independently of each other.</span>
<span class="comment-copy">Use the multiprocessing module. Read its documentation to know how to do it.</span>
<span class="comment-copy">Using file as @Dima_Tisnek suggested is the right option. Cloud services commonly store large data in file format, especially when your single chunk hits GB. Merge can be done after all chunks dumped to filesystem.</span>
<span class="comment-copy">@knh190 The issue is that the large numpy arrays are all variables in custom objects</span>
<span class="comment-copy">How would that work if the numpy arrays in question are variables in a custom object?</span>
<span class="comment-copy">Instead of files, you could use memory maps. Python has the <a href="https://docs.python.org/3.0/library/mmap.html" rel="nofollow noreferrer"><code>mmap</code> module</a> and numpy has the memmap module (<a href="https://stackoverflow.com/questions/16149803/working-with-big-data-in-python-and-numpy-not-enough-ram-how-to-save-partial-r">example</a>).</span>
<span class="comment-copy">@posdef Start by writing the numpy arrays from the object to a file or mmap. Then, provide each subprocess or thread with the section (the offset) of the file/mmap that it is responsible for. With something like <code>mmap</code>/<code>numpy.memmap</code>, only the indices accessed are loaded into memory.</span>
<span class="comment-copy">@posdef you'll have to <b>save</b> and <b>load</b> these objects; given that you've overridden the <code>__add__</code> method, I assume the implementation is under your control. If there's only one <code>ndarray</code> per fragment, that's straightforward. If there's more, consider <code>pandas</code> for convenience or hack something custom for speed.</span>
<span class="comment-copy">@knh190 I'm unfamiliar with these libraries but here's the documentation, containing a numpy.memmap example using an array:  <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html#numpy.memmap" rel="nofollow noreferrer">docs.scipy.org/doc/numpy/reference/generated/â€¦</a></span>
