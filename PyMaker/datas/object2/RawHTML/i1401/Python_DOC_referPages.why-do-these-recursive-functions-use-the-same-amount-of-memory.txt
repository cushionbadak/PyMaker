<div class="post-text" itemprop="text">
<p>I was trying to replicate the memory usage test <a href="http://deeplearning.net/software/theano/tutorial/python-memory-management.html" rel="nofollow noreferrer">here</a>.</p>
<p>Essentially, the post claims that given the following code snippet:</p>
<pre><code>import copy
import memory_profiler

@profile
def function():
    x = list(range(1000000))  # allocate a big list
    y = copy.deepcopy(x)
    del x
    return y

if __name__ == "__main__":
    function()
</code></pre>
<p>Invoking </p>
<pre><code>python -m memory_profiler memory-profile-me.py
</code></pre>
<p>prints, on a 64-bit computer</p>
<pre><code>Filename: memory-profile-me.py

Line #    Mem usage    Increment   Line Contents
================================================
 4                             @profile
 5      9.11 MB      0.00 MB   def function():
 6     40.05 MB     30.94 MB       x = list(range(1000000)) # allocate a big list
 7     89.73 MB     49.68 MB       y = copy.deepcopy(x)
 8     82.10 MB     -7.63 MB       del x
 9     82.10 MB      0.00 MB       return y
</code></pre>
<p>I copied and pasted the same code but my profiler yields</p>
<pre><code>Line #    Mem usage    Increment   Line Contents
================================================
 3   44.711 MiB   44.711 MiB   @profile
 4                             def function():
 5   83.309 MiB   38.598 MiB       x = list(range(1000000))  # allocate a big list
 6   90.793 MiB    7.484 MiB       y = copy.deepcopy(x)
 7   90.793 MiB    0.000 MiB       del x
 8   90.793 MiB    0.000 MiB       return y
</code></pre>
<p>This post could be outdated --- either the profiler package or python could have changed. In any case, my questions are, in Python 3.6.x</p>
<p>(1) Should <code>copy.deepcopy(x)</code> (as defined in the code above) consume a nontrivial amount of memory?</p>
<p>(2) Why couldn't I replicate?</p>
<p>(3) If I repeat <code>x = list(range(1000000))</code> after <code>del x</code>, would the memory increase by the same amount as I first assigned <code>x = list(range(1000000))</code> (as in line 5 of my code)?</p>
</div>
<div class="post-text" itemprop="text">
<p><code>copy.deepcopy()</code> recursively copies <em>mutable object only</em>, immutable objects such as integers or strings are not copied. The list being copied consists of immutable integers, so the <code>y</code> copy ends up sharing references to the same integer values:</p>
<pre><code>&gt;&gt;&gt; import copy
&gt;&gt;&gt; x = list(range(1000000))
&gt;&gt;&gt; y = copy.deepcopy(x)
&gt;&gt;&gt; x[-1] is y[-1]
True
&gt;&gt;&gt; all(xv is yv for xv, yv in zip(x, y))
True
</code></pre>
<p>So the copy only needs to create a new list object with 1 million references, an object that takes a little over 8MB of memory on my Python 3.6 build on Mac OS X 10.13 (a 64-bit OS):</p>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.getsizeof(y)
8697464
&gt;&gt;&gt; sys.getsizeof(y) / 2 ** 20   # Mb
8.294548034667969
</code></pre>
<p>An empty <code>list</code> object takes 64 bytes, each reference takes 8 bytes:</p>
<pre><code>&gt;&gt;&gt; sys.getsizeof([])
64
&gt;&gt;&gt; sys.getsizeof([None])
72
</code></pre>
<p>Python list objects overallocate space to grow, converting a <code>range()</code> object to a list causes it to make a little more space for additional growth than when using <code>deepcopy</code>, so <code>x</code> is slightly larger still, having room for an additional 125k objects before having to resize again:</p>
<pre><code>&gt;&gt;&gt; sys.getsizeof(x)
9000112
&gt;&gt;&gt; sys.getsizeof(x) / 2 ** 20
8.583175659179688
&gt;&gt;&gt; ((sys.getsizeof(x) - 64) // 8) - 10**6
125006
</code></pre>
<p>while the copy only has additional space for left for about 87k:</p>
<pre><code>&gt;&gt;&gt; ((sys.getsizeof(y) - 64) // 8) - 10**6
87175
</code></pre>
<p>On Python 3.6 I can't replicate the article claims either, in part because Python has seen a lot of memory management improvements, and in part because the article is wrong on several points.</p>
<p>The behaviour of <code>copy.deepcopy()</code> regarding lists and integers has <em>never</em> changed in the long history of the <code>copy.deepcopy()</code> (see the <a href="https://github.com/python/cpython/commit/409780f8f27c6c80481d7c6f62eb8bcbd132c47e" rel="nofollow noreferrer">first revision of the module, added in 1995</a>), and the interpretation of the memory figures is wrong, even on Python 2.7.</p>
<p>Specifically, I <em>can</em> reproduce the results using Python 2.7 This is what I see on my machine:</p>
<pre><code>$ python -V
Python 2.7.15
$ python -m memory_profiler memtest.py
Filename: memtest.py

Line #    Mem usage    Increment   Line Contents
================================================
     4   28.406 MiB   28.406 MiB   @profile
     5                             def function():
     6   67.121 MiB   38.715 MiB       x = list(range(1000000))  # allocate a big list
     7  159.918 MiB   92.797 MiB       y = copy.deepcopy(x)
     8  159.918 MiB    0.000 MiB       del x
     9  159.918 MiB    0.000 MiB       return y
</code></pre>
<p>What is happening is that Python's memory management system is allocating a new chunk of memory for additional expansion. It's not that the new <code>y</code> list object takes nearly 93MiB of memory, that's just the additional memory the OS has allocated to the Python process when that process requested some more memory for the object heap. The list object itself is a <em>lot</em> smaller.</p>
<p>The <a href="https://docs.python.org/3/library/tracemalloc.html" rel="nofollow noreferrer">Python 3 <code>tracemalloc</code> module</a> is a lot more accurate about what actually happens:</p>
<pre><code>python3 -m memory_profiler --backend tracemalloc memtest.py
Filename: memtest.py

Line #    Mem usage    Increment   Line Contents
================================================
     4    0.001 MiB    0.001 MiB   @profile
     5                             def function():
     6   35.280 MiB   35.279 MiB       x = list(range(1000000))  # allocate a big list
     7   35.281 MiB    0.001 MiB       y = copy.deepcopy(x)
     8   26.698 MiB   -8.583 MiB       del x
     9   26.698 MiB    0.000 MiB       return y
</code></pre>
<p>The Python 3.x memory manager and list implementation is smarter than those one in 2.7; evidently the new list object was able to fit into existing already-available memory, pre-allocated when creating <code>x</code>.</p>
<p>We can test Python 2.7's behaviour with a <a href="http://pytracemalloc.readthedocs.io/install.html#manual-installation" rel="nofollow noreferrer">manually built Python 2.7.12 tracemalloc binary</a> and a <a href="https://github.com/pythonprofilers/memory_profiler/pull/201" rel="nofollow noreferrer">small patch to <code>memory_profile.py</code></a>. Now we get more reassuring results on Python 2.7 as well:</p>
<pre><code>Filename: memtest.py

Line #    Mem usage    Increment   Line Contents
================================================
     4    0.099 MiB    0.099 MiB   @profile
     5                             def function():
     6   31.734 MiB   31.635 MiB       x = list(range(1000000))  # allocate a big list
     7   31.726 MiB   -0.008 MiB       y = copy.deepcopy(x)
     8   23.143 MiB   -8.583 MiB       del x
     9   23.141 MiB   -0.002 MiB       return y
</code></pre>
<p>I note that the author was confused as well:</p>
<blockquote>
<p><code>copy.deepcopy</code> copies both lists, which allocates again ~50 MB (<strong>I am not sure where the additional overhead of 50 MB - 31 MB = 19 MB comes from</strong>)</p>
</blockquote>
<p>(Bold emphasis mine).</p>
<p>The error here is to assume that all memory changes in the Python process size can directly be attributed to specific objects, but the reality is far more complex, as the memory manager can add (<em>and remove!</em>) memory 'arenas', blocks of memory reserved for the heap, as needed and will do so in larger blocks if that makes sense. The process here is complex, as it depends on <a href="https://stackoverflow.com/a/15492488">interactions between Python's manager and the OS <code>malloc</code> implementation details</a>. The author has found an older article on Python's model that they have misunderstood to be current, the <a href="https://hbfs.wordpress.com/2013/01/08/python-memory-management-part-ii/#comment-3492" rel="nofollow noreferrer">author of that article themselves has already tried to point this out</a>; as of Python 2.5 the claim that Python doesn't free memory is no longer true.</p>
<p>What's troubling, is that the same misunderstandings then lead the author to recommend against using <code>pickle</code>, but in reality the module, even on Python 2, never adds more than a little bookkeeping memory to track recursive structures. See <a href="https://gist.github.com/mjpieters/1d9ce2c84b858ef7cb7192311e49bb49" rel="nofollow noreferrer">this gist for my testing methodology</a>; using <code>cPickle</code> on Python 2.7 adds a one-time 46MiB increase (doubling the <code>create_file()</code> call results in no further memory increase). In Python 3, the memory changes have gone altogether.</p>
<p>I'll open a dialog with the Theano team about the post, the article is wrong, confusing, and Python 2.7 is soon to be made entirely obsolete anyway so they really should focus on Python 3's memory model. <sup>(*)</sup></p>
<p>When you create a <em>new list</em> from <code>range()</code>, not a copy, you'll see a similar increase in memory as for creating <code>x</code> the first time, because you'd create a new set of integer objects in addition to the new list object. Aside from <a href="https://stackoverflow.com/questions/306313/is-operator-behaves-unexpectedly-with-integers">a specific set of small integers</a>, Python doesn't cache and re-use integer values for <code>range()</code> operations.</p>
<hr/>
<p><sup>(*)</sup> <em>addendum</em>: I opened <a href="https://github.com/Theano/Theano/issues/6619" rel="nofollow noreferrer">issue #6619</a> with the Thano project. The project agreed with my assessment and <a href="https://github.com/Theano/Theano/pull/6621" rel="nofollow noreferrer">removed the page from their documentation</a>, although they haven't yet updated the published version.</p>
</div>
<span class="comment-copy">The third question could simply be solved by trying it out yourself and viewing the results</span>
<span class="comment-copy">@user3483203 I'm not sure if the package is giving me the right answer.</span>
<span class="comment-copy">The docs clearly state "Note that you might get different results on a different platform or with a different python version."  I don't think it's realistic to expect identical results.</span>
<span class="comment-copy">Great answer! Follow up question: when you use python 3, why do you reduce memory usage by only 8mb when you did <code>del x</code>? Does that mean the allocated memory for <code>x</code> is still not free?</span>
<span class="comment-copy">@tryingtosolve: because Python uses reference counting to track what memory can be freed. Deleting <code>x</code> removes the last reference to one of the two list objects, which each consist of ~8MB of 8-byte references. The two lists each reference the same 1 million integer objects, and after the list referenced by <code>x</code> has been removed from memory, the list referenced by <code>y</code> is still keeping those integers alive. You'll need to delete <code>y</code> too before the memory for those integers can be freed.</span>
