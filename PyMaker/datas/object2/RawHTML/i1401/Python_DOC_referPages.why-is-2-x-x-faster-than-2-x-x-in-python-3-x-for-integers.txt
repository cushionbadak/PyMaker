<div class="post-text" itemprop="text">
<p>The following Python 3.x integer multiplication takes on average between 1.66s and 1.77s:</p>
<pre><code>import time
start_time = time.time()
num = 0
for x in range(0, 10000000):
    # num += 2 * (x * x)
    num += 2 * x * x
print("--- %s seconds ---" % (time.time() - start_time))
</code></pre>
<p>if I replace <code>2 * x * x</code> with <code>2 *(x * x)</code>, it takes between <code>2.04</code> and <code>2.25</code>. How come?</p>
<p>On the other hand it is the opposite in Java: <code>2 * (x * x)</code> is faster in Java. Java test link: <a href="https://stackoverflow.com/questions/53452713/why-is-2-i-i-faster-than-2-i-i-in-java">Why is 2 * (i * i) faster than 2 * i * i in Java?</a></p>
<p>I ran each version of the program 10 times, here are the results.</p>
<pre><code>   2 * x * x        |   2 * (x * x)
---------------------------------------
1.7717654705047607  | 2.0789272785186768
1.735931396484375   | 2.1166207790374756
1.7093875408172607  | 2.024367570877075
1.7004504203796387  | 2.047525405883789
1.6676218509674072  | 2.254328966140747
1.699510097503662   | 2.0949244499206543
1.6889283657073975  | 2.0841963291168213
1.7243537902832031  | 2.1290600299835205
1.712965488433838   | 2.1942825317382812
1.7622807025909424  | 2.1200053691864014
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>First of all, note that we don't see the same thing in Python 2.x:</p>
<pre><code>&gt;&gt;&gt; timeit("for i in range(1000): 2*i*i")
51.00784397125244
&gt;&gt;&gt; timeit("for i in range(1000): 2*(i*i)")
50.48330092430115
</code></pre>
<p>So this leads us to believe that this is due to how integers changed in Python 3: specifically, Python 3 uses <code>long</code> (arbitrarily large integers) everywhere.</p>
<p>For small enough integers (including the ones we're considering here), CPython actually just uses the O(MN) <a href="https://en.wikipedia.org/wiki/Multiplication_algorithm#Long_multiplication" rel="noreferrer">grade-school digit by digit multiplication algorithm</a> (for larger integers it switches to the <a href="https://en.wikipedia.org/wiki/Karatsuba_algorithm" rel="noreferrer">Karatsuba algorithm</a>). You can see this yourself in the <a href="https://github.com/python/cpython/blob/b509d52083e156f97d6bd36f2f894a052e960f03/Objects/longobject.c#L3245" rel="noreferrer">source</a>.</p>
<p>The number of digits in <code>x*x</code> is roughly twice that of <code>2*x</code> or <code>x</code> (since log(x<sup>2</sup>) = 2 log(x)). Note that a "digit" in this context is not a base-10 digit, but a 30-bit value (which are treated as single digits in CPython's implementation). Hence, <code>2</code> is a single-digit value, and <code>x</code> and <code>2*x</code> are single-digit values for all iterations of the loop, but <code>x*x</code> is two-digit for <code>x &gt;= 2**15</code>. Hence, for <code>x &gt;= 2**15</code>, <code>2*x*x</code> only requires single-by-single-digit multiplications whereas <code>2*(x*x)</code> requires a single-by-single and a single-by-double-digit multiplication (since <code>x*x</code> has 2 30-bit digits).</p>
<p>Here's a direct way to see this (Python 3):</p>
<pre><code>&gt;&gt;&gt; timeit("a*b", "a,b = 2, 123456**2", number=100000000)
5.796971936999967
&gt;&gt;&gt; timeit("a*b", "a,b = 2*123456, 123456", number=100000000)
4.3559221399999615
</code></pre>
<p>Again, compare this to Python 2, which doesn't use arbitrary-length integers everywhere:</p>
<pre><code>&gt;&gt;&gt; timeit("a*b", "a,b = 2, 123456**2", number=100000000)
3.0912468433380127
&gt;&gt;&gt; timeit("a*b", "a,b = 2*123456, 123456", number=100000000)
3.1120400428771973
</code></pre>
<p>(One interesting note: If you look at the source, you'll see that the algorithm actually has a special case for squaring numbers (which we're doing here), but even still this is not enough to overcome the fact that <code>2*(x*x)</code> just requires processing more digits.)</p>
</div>
<div class="post-text" itemprop="text">
<p>Python intern representation of integers is special, it uses slots of 30 bits :</p>
<pre><code>In [6]: sys.getsizeof(2**30-1)
Out[6]: 28 # one slot + heading

In [7]: sys.getsizeof(2**30)
Out[7]: 32 # two slots 
</code></pre>
<p>So everything happens as if Python counts in base <code>B = 2**30 = 1 073 741 824 ~1 billion</code>.</p>
<p>For a human who want to calculate 2*4*4, two ways :</p>
<ul>
<li>(2*4)*4 = 8*4 =32 = 30 + 2 is immediate if you knows your add tables.</li>
<li>2*(4*4) = 2*16 = 2*10 + 2*6 = (2*10+10) + 2 = 30 + 2 since we have to put the operation down.  </li>
</ul>
<p>Python have the same problem. If <code>x</code> is a number such than <code>2x &lt; B &lt; x²</code> , let <code>x² = aB+b</code> , with <code>a,b &lt;B</code>. <code>x²</code> is stored in 2 slots, which I note <code>(a|b)</code>.  Computations leads to (without managing carries here):</p>
<pre><code>   (x*x)*2 =&gt;  (a|b)*2 =&gt; (2*a|2*b)
   (2*x)*x =&gt;  (2x)*x =&gt;(2a|2b)
</code></pre>
<p>in the first case  the <code>2*</code> operation is done two times, against only one in the  first case. That  explains the difference.</p>
</div>
<div class="post-text" itemprop="text">
<p>If your benchmark is right (didn't check), it may come from the fact that Python  integers may be two different things : native integers when they are small (with a quick computation), and big integers when they increase in size (slower computation). The first syntax keeps the size smaller after the first operation while the second syntax may lead to two operations involving big integers.</p>
</div>
<div class="post-text" itemprop="text">
<p>From what I can tell, it comes down to a little bit more memory access in the version using <code>2 * (x * x)</code>. I printed the disassembled bytecode and it seems to prove that:</p>
<p>Relevant part of <code>2 * x * x</code>:</p>
<pre><code>7          28 LOAD_FAST                1 (num)
           30 LOAD_CONST               3 (2)
           32 LOAD_FAST                2 (x)
           34 BINARY_MULTIPLY
           36 LOAD_FAST                2 (x)
           38 BINARY_MULTIPLY
           40 INPLACE_ADD
           42 STORE_FAST               1 (num)
           44 JUMP_ABSOLUTE           24
</code></pre>
<p>Relevant part of <code>2 * (x * x)</code>:</p>
<pre><code>  7          28 LOAD_FAST                1 (num)
             30 LOAD_CONST               3 (2)
             32 LOAD_FAST                2 (x)
             34 LOAD_FAST                2 (x)
             36 BINARY_MULTIPLY                 &lt;=== 1st multiply x*x in a temp value
             38 BINARY_MULTIPLY                 &lt;=== then multiply result with 2
             40 INPLACE_ADD
             42 STORE_FAST               1 (num)
             44 JUMP_ABSOLUTE           24
</code></pre>
</div>
<span class="comment-copy">Little hint: Use the <code>timeit</code> module for better statistics</span>
<span class="comment-copy">For good measure you might as well throw in <code>2 * pow(x,2)</code> and <code>2 * x**2</code> as well. Also, please redo your timings using <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow noreferrer"><code>timeit</code></a>, it's much more accurate than <code>time.time()</code> for short processes.</span>
<span class="comment-copy">Related near-duplicate, although not very clearly-stated: <a href="https://stackoverflow.com/questions/37053379/times-two-faster-than-bit-shift-for-python-3-x-integers">Times-two faster than bit-shift, for Python 3.x integers?</a></span>
<span class="comment-copy">Is Karatsuba algorithm slower than digit multiplication algorithm?</span>
<span class="comment-copy">@BanghuaZhao It has a better runtime complexity (wrt to number of digits), but the number of digits has to be large enough for it to actually be worth it.</span>
<span class="comment-copy">source : #define KARATSUBA_CUTOFF 70 . So the Karatsuba algorithm is only used if ints have about 600 decimal digits. it is not the problem here.</span>
<span class="comment-copy">In Python, a "digit" is a 30 or 60-bit chunk, right?  So base <code>2^30</code>, not a <i>decimal</i> digit.  Also critically important: Java <code>int</code> wraps at 2^32, while Python does arbitrary precision.  Plus Java JIT-compiles to (inefficient) native code using 32-bit registers, while CPython interprets all the way.  So those are massive differences.</span>
<span class="comment-copy">The numbers involved in the question are small enough that Karatsuba is not involved, and beyond that, they are small enough that asymptotic considerations are completely irrelevant. All operands and results involved fit in two 30-bit internal "digits".</span>
<span class="comment-copy">Where do <code>a</code>, <code>b</code> and <code>X</code> come from, what do they represent and how do they relate to the value of <code>x</code>?</span>
<span class="comment-copy">@Bergi: <code>a</code> and <code>b</code> are the high and low 30-bit chunks of <code>x*x</code>. (Capital <code>X</code> no longer appears in the current revision of the answer.)</span>
<span class="comment-copy">Seems more like a guess (albeit a good guess) than an answer. Using <code>timeit</code> with different size <code>x</code> might give the needed evidence to push it from a guess to an answer.</span>
<span class="comment-copy">python has "cached" integers for -5 to 256 <a href="https://docs.python.org/3.6/c-api/long.html#c.PyLong_FromLong" rel="nofollow noreferrer">Dok</a> - this would be easily veryfied if both formulas are closer to the same times if only small integers are in play? The max value goes to 2e+14 which is small enough to fit a 64-bit signed int so processor int calculation limitations is probably out of the game - it is too big for 32-bit unsigned int, so on 32-bit this might factor in?</span>
<span class="comment-copy">I still <a href="https://gist.github.com/vxgmichel/6fd2d13590eb5c9ef74ac764fea4444b" rel="nofollow noreferrer">measure a 5% difference</a> using 2 as a value for x.</span>
<span class="comment-copy">The answer might seem more like a guess, though in my eyes it is helpful because I just learned something more about Python.</span>
<span class="comment-copy">If this was the case we'd see the same effect in Python 2, but we don't.</span>
<span class="comment-copy">it's just moved a <code>LOAD_FAST</code> one instruction later. how is this "more memory access"?</span>
<span class="comment-copy">@arshajii You are right, I checked the disassembled code for Python 2 and it shows the same thing. Although I still believe it might have an impact but is negligible compared to the one you noted.</span>
<span class="comment-copy">@SamMason In the second case, it needs to write back the result of <code>x*x</code> in a temp value because it is needed for the next multiply whereas in the first case, it always writes in the same register. So the second case is a read after write. It is however a really small penalty but in a hot loop, it can have an impact.</span>
<span class="comment-copy">CPython is a stack machine, it always leaves/writes results of computations to the top of the stack…  there might be some cache related impact, but it's going to be incredibly small for code this close</span>
