<div class="post-text" itemprop="text">
<p>The prevalent amount of NLTK documentation and examples is devoted to lemmatization and stemming but is very sparse on such matters of normalization as:</p>
<ul>
<li>converting all letters to lower or upper case</li>
<li>removing punctuation</li>
<li><strong>converting numbers into words</strong></li>
<li>removing accent marks and other diacritics</li>
<li><strong>expanding abbreviations</strong></li>
<li>removing stopwords or "too common" words</li>
<li><strong>text canonicalization</strong> (tumor = tumour, it's = it is)</li>
</ul>
<p>Please point me where in NLTK to dig. Any NLTK equivalents (JAVA or any other) for aforementioned purposes are welcome. Thanks.</p>
<p><strong>UPD</strong>. I have written a python library of text normalization for the text-to-speech purposes <a href="https://github.com/soshial/text-normalization" rel="noreferrer">https://github.com/soshial/text-normalization</a>. It might suit you as well.</p>
</div>
<div class="post-text" itemprop="text">
<p>Also in NLTK spec a lot of (sub-)tasks are solved using purely python <a href="http://docs.python.org/release/2.5.2/lib/string-methods.html">methods</a>.</p>
<p><strong>a) converting all letters to lower or upper case</strong></p>
<pre><code>text='aiUOd'
print text.lower()
&gt;&gt; 'aiuod'
print text.upper()
&gt;&gt; 'AIUOD'
</code></pre>
<p><strong>b) removing punctuation</strong></p>
<pre><code>text='She? Hm, why not!'
puncts='.?!'
for sym in puncts:
    text= text.replace(sym,' ')
print text
&gt;&gt; 'She  Hm  why not '
</code></pre>
<p><strong>c) converting numbers into words</strong></p>
<p>Here, it would be not that wasy to write a fewliner, but there are a lot of already existing solutions, if you google it. <a href="http://www.daniweb.com/software-development/python/code/216839">Code snippets</a>, <a href="http://code.google.com/p/numword/">libraries</a> etc</p>
<p><strong>d) removing accent marks and other diacritics</strong></p>
<p>look up point <em>b)</em>, just create the list with diacritics as <em>puncts</em></p>
<p><strong>e) expanding abbreviations</strong></p>
<p>Create a dictionary with abbreviations:</p>
<pre><code>text='USA and GB are ...'
abbrevs={'USA':'United States','GB':'Great Britain'}
for abbrev in abbrevs:
    text= text.replace(abbrev,abbrevs[abbrev])
print text
&gt;&gt; 'United States and Great Britain are ...'
</code></pre>
<p><strong>f) removing stopwords or "too common" words</strong></p>
<p>Create a list with stopwords:</p>
<pre><code>text='Mary had a little lamb'
temp_corpus=text.split(' ')
stops=['a','the','had']
corpus=[token for token in temp_corpus if token not in stops]
print corpus
&gt;&gt; ['Mary', 'little', 'lamb']
</code></pre>
<p><strong>g) text canonicalization (tumor = tumour, it's = it is)</strong></p>
<p>for tumor-&gt; tumour use <a href="http://docs.python.org/library/re.html">regex</a>.</p>
<p>Last, but not least, please note that all of the examples above usually need calibration on the real textes, I wrote them as the direction to go.</p>
</div>
<div class="post-text" itemprop="text">
<p>I suggest using stopwords.words() for stopword removal. Supports following languages: Danish, Dutch, English, French, German, Italian, Norwegian, Portuguese, Russian, Spanish, Swedish.</p>
</div>
<div class="post-text" itemprop="text">
<p>I might be a little bit late, but this may be helpful. Here are the stop words for some languages (English, French, German, Finish, Hungarian, Turkish, Russian, Czech, Greek, Arabic, Chinese, Japanese, Korean, Catalan, Polish, Hebrew, Norwegian, Swedish, Italian, Portuguese and Spanish):
<a href="https://pypi.python.org/pypi/many-stop-words" rel="nofollow">https://pypi.python.org/pypi/many-stop-words</a></p>
</div>
<span class="comment-copy">As I deem an NLP toolkit, it should be able to do all processing operations that might involve some linguistic data. It means that I thought and I still think that nltk already has the dictionaries of equivalent words, abbreviation dictionary, canonicalization dict, converting into text numbers, <b>dates</b>, temperature, <b>currencies</b> and so on... Maybe we just do not know it well?</span>
<span class="comment-copy">I'm sure you can't solve casemapping in the general case with just <code>.lower()</code> and <code>.upper()</code>. Consider Turkish <code>I</code>=<code>ı</code>, <code>İ</code>=<code>i</code>; German <code>ß</code>=<code>SS</code>; Greek <code>Σ</code>=both <code>ς</code> and <code>σ</code>.</span>
<span class="comment-copy">Resolving abbreviations is risky. How do you know 'US' stands for 'United States'? 'You and me: US!'  --&gt; 'You and me: United States!'</span>
<span class="comment-copy">@hippietrail, <a href="https://docs.python.org/3/library/stdtypes.html#str.casefold" rel="nofollow noreferrer">docs.python.org/3/library/stdtypes.html#str.casefold</a></span>
<span class="comment-copy">Of course this task can be done without NLTK, but you would have to create your own data (e.g. lists of stopwords, abbreviations). IMHO the OP means to ask for ready to use methods, e.g. <a href="http://www.nltk.org/api/nltk.stem.html" rel="nofollow noreferrer">nltk.org/api/nltk.stem.html</a>.</span>
