<div class="post-text" itemprop="text">
<p>In my application, I am sending off several <code>request.post()</code> requests in threads. Depending on the amount of data I have to post, the number of threads created can be in their hundreds. </p>
<p>The actual creation of the <code>request</code> object is made using <code>requests-oauthlib</code>, which inserts authentication data into the <code>request</code> object when it is used.</p>
<p>My issue is that when there is a large amount of data being sent in parallel, that the log is flooded with the following messages, and eventually no more input is sent to the log:</p>
<p><code>Connection pool is full. Discarding connection.</code></p>
<p>My question is, with the use of <code>requests-oauthlib</code>, is there a way to specity, perhaps within the <code>post</code> method itself, the size of the connection pool, or whether it should block so that other requests can complete before creating more? I ask for this because with the use of <code>requests-oauthlib</code>, it would be tricky to construct a custom <code>request</code> object, and ask <code>requests-oauthlib</code> to use it.</p>
<p>One thing I have tried is as follows, but it had no effect - I continued to get the warnings:</p>
<pre><code>import requests
s = requests.Session()
a = requests.adapters.HTTPAdapter(pool_block=True)
s.mount('http://', a)
s.mount('https://', a)
</code></pre>
<p><strong>Update - The threads are now being created in a controlled manner.</strong></p>
<pre><code>with futures.ThreadPoolExecutor(max_workers=10) as executor:
    executor.submit(function, args)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The easiest way to block the requests so only N of them are trying to use the connection pool at once is to only create N at a time.</p>
<p>The easiest way to do that is to use a pool of N threads servicing queue of M requests, instead of a separate thread for every request. If you're using Python 3.2+, this is very easy with the <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow"><code>concurrent.futures</code></a> library—in fact, it's nearly identical to the first <code>ThreadPoolExecutor</code> example, except that you're using <code>requests</code> instead of <code>urllib</code>. If you're not using 3.2+, there's a backport of the stdlib module named <a href="https://pypi.python.org/pypi/futures" rel="nofollow"><code>futures</code></a> that provides the same functionality back to… I think 2.6, but don't quote me on that (PyPI is down at the moment).</p>
<p>There may be an even easier solution: there's a third-party library named <a href="https://pypi.python.org/pypi/requests-futures" rel="nofollow"><code>requests-futures</code></a> that, I'm guessing from the name (again, PyPI down…), wraps that up for you in some way.</p>
<p>You may also want to consider using something like <a href="https://pypi.python.org/pypi/grequests" rel="nofollow"><code>grequests</code></a> to do it all in one thread with <code>gevent</code> greenlets, but that won't be significantly different, as far as your code is concerned, from using a thread pool.</p>
</div>
<span class="comment-copy">Thanks for your answer. I've implemented what you said using ThreadPoolExecutor (I am on Python 2.7) and added it to my original question. Could you just confirm this is what you had in mind? I no longer get the warning of exceeding the connection pool and am very happy with how controlled the requests are being sent off. Thanks for your help!</span>
<span class="comment-copy">@Cristian: Exactly; glad it worked for you.</span>
