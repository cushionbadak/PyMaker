<div class="post-text" itemprop="text">
<p>I have a dictionary with 500 DateFrames in it. Each data frame has columns <em>'date'</em> , <em>'num_patients'</em>. I apply the model to all the data frames in the dictionary, but Python kernel crash due to large data in the dictionary.  </p>
<pre><code>prediction_all = {}
for key, value in dict.items():
    model = Prophet(holidays = holidays).fit(value)
    future = model.make_future_dataframe(periods = 365)
    forecast = model.predict(future)
    prediction_all[key] = forecast.tail()
</code></pre>
<p>So, then I've subsetted the dictionary and applied the model to each subset.  </p>
<pre><code>dict1 = {k: dict[k] for k in sorted(dict.keys())[:50]}
prediction_dict1 = {}
for key, value in dict1.items():
    model = Prophet(holidays = holidays).fit(value)
    future = model.make_future_dataframe(periods = 365)
    forecast = model.predict(future)
    prediction_dict1[key] = forecast.tail()

dict2 = {k: dict[k] for k in sorted(dict.keys())[50:100]}
prediction_dict2 = {}
for key, value in dict2.items():
    model = Prophet(holidays = holidays).fit(value)
    future = model.make_future_dataframe(periods = 365)
    forecast = model.predict(future)
    prediction_dict2[key] = forecast.tail()
</code></pre>
<p>But I will need to run the code above for 10 times since I have 500 DataFrames (10 subsets). Is there a more efficient way to do this?  </p>
</div>
<div class="post-text" itemprop="text">
<p>One immediate improvement is to drop the <em>sorted()</em> and slicing step and replace it with <a href="https://docs.python.org/3/library/heapq.html#heapq.nsmallest" rel="nofollow noreferrer"><em>heapq.nsmallest()</em></a> which will do many fewer comparisons.  Also, the <code>.keys()</code> is not necessary since dicts automatically iterate over their keys by default.</p>
<p>Replace:</p>
<pre><code> dict1 = {k: dict[k] for k in sorted(dict.keys())[:50]}
 dict2 = {k: dict[k] for k in sorted(dict.keys())[50:100]}
</code></pre>
<p>With:</p>
<pre><code> lowest_keys = heapq.nsmallest(100, dict)
 dict1 = {k : dict[k] for k in lowest_keys[:50]}
 dict2 = {k : dict[k] for k in lowest_keys[50:100]}
</code></pre>
<p>The big for-loop in your code looks to only need <code>.values()</code> instead of <code>.items()</code> since <em>key</em> doesn't seem to be used.</p>
</div>
<span class="comment-copy">Why not just put this in a loop? And what does this mean 'I first subset the dictionary to avoid Python kernel crash due to large data'? Is there some reason you are sorting the keys repeatedly?</span>
<span class="comment-copy">Right now you're fitting a separate model for each of the elements in the dictionary. Is this what you want?</span>
<span class="comment-copy">Won't 50*10 items take the same amount of memory as 500? You have to process and dump the intermediate dicts.</span>
<span class="comment-copy">@pvg Sorry I didn't express my question clearly. I've edited my post to make it clearer. The kernel died when I applied the model to all the 500 data frames at once. That's why I subset the dictionary and then apply the model to 50 data frames each time.</span>
<span class="comment-copy">@Kewl Thank you for asking. I am fitting the same model for each of the elements/values in the dictionary.</span>
<span class="comment-copy">Hi Raymond, Thank you for answering. I've edited my post. I applied the model to each DataFrame. For example, I will have 50 prediction results from the first subset (50 DataFrames). That's why I need <i>key</i> in the big for-loop. Would you suggest I apply the model to each subset(see the big for-loop on my post) or write a for-loop to apply the model subset by subset.</span>
<span class="comment-copy">@Peggy There doesn't seem to be any obvious reason that one approach would win over the other.  So, you can either opt for the one the seems to be clearest.  Alternatively, try both and time the results to see if one has some subtle advantage over the other (possibly for hard to fathom reasons like cache effects or stride-size).</span>
