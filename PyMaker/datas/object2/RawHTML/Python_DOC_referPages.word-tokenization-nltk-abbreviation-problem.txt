<div class="post-text" itemprop="text">
<p>I want to know how to word tokenize the following sentence (string):</p>
<pre><code>"I am good. I e.g. wash the dishes."
</code></pre>
<p>In to the following words:</p>
<pre><code>["I", "am", "good", ".", "I", "e.g.", "wash", "the", "dishes"]
</code></pre>
<p>Now, the problem is when it comes to abbreviations like <code>"e.g."</code> it is tokenized by NLTK word_tokenizer as follows <code>["e.g", "."]</code></p>
<p>I tried using using punkt trained with <code>"e.g."</code> to sentence tokenize it first but I realised that after I word tokenize it I would get the same result.</p>
<p>Any thoughts on how I would achieve my goal.</p>
<p>Note: I am rstricted to using NLTK.</p>
</div>
<div class="post-text" itemprop="text">
<p>The <a href="http://www.nltk.org/api/nltk.tokenize.html?highlight=regexp#module-nltk.tokenize.regexp" rel="nofollow noreferrer">NLTK regexp_tokenize</a> module splits a string into substrings using a regular expression. A regex <code>pattern</code> can be defined which will build a tokenizer that matches the groups in this pattern. We can write a pattern for your particular use-case which looks for words, abbreviations(both upper and lower case) and symbols like <code>'.'</code>, <code>';'</code> etc.</p>
<pre><code>import nltk
sent = "I am good. I e.g. wash the dishes."
pattern = r'''(?x)          # set flag to allow verbose regexps
        (?:[A-Za-z]\.)+        # abbreviations(both upper and lower case, like "e.g.", "U.S.A.")
        | \w+(?:-\w+)*        # words with optional internal hyphens 
        | [][.,;"'?():_`-]    # these are separate tokens; includes ], [
    '''
nltk.regexp_tokenize(sent, pattern)
#Output:
['I', 'am', 'good', '.', 'I', 'e.g.', 'wash', 'the', 'dishes', '.']
</code></pre>
<p>The Regex pattern for abbreviations is <code>(?:[A-Za-z]\.)+</code>. The <code>\.</code> matches the <code>"."</code> in a forward lookup containing characters in A-Z or a-z. </p>
<p>On the other hand, the full stop is matched as an independent symbol in the following pattern which is not bound to a positive or negative lookahead or containment in a set of alphabets:</p>
<pre><code>'[][.,;"'?():_`-]'
</code></pre>
</div>
<span class="comment-copy">Possible duplicate of <a href="https://stackoverflow.com/questions/34805790/how-to-avoid-nltks-sentence-tokenizer-splitting-on-abbreviations" title="how to avoid nltks sentence tokenizer splitting on abbreviations">stackoverflow.com/questions/34805790/â€¦</a></span>
<span class="comment-copy">Not a duplicate, since the question you are referring deals with sentence tokens. My question is based on word tokens. I did look at the question you are referring to previously and tried to use some principles from there in my program but it became redundant since word tokenization step brings me back to my problem mentioned in the question.</span>
<span class="comment-copy">If its not too much trouble, could you please explain how your regex differentiates between an end of a sentence full-stop and an end of an abbreviation full_stop. Thank you.</span>
<span class="comment-copy">I've just added an explanation</span>
