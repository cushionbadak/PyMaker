<div class="post-text" itemprop="text">
<p>I've been playing around with text classification in PyTorch and I've encountered a problem with 1 dimensional convolutions. </p>
<p>I've set an embedding layer of dimesions (x, y, z) where:
x - denotes the batch size
y - denotes the length of a sentence (fixed with padding, so 40 words)
z - the dimensionality of pre-trained word embedding (for now 100)</p>
<p>For simplicity sake, let's assume I put in a matrix of (1,40, 100)</p>
<p>However, when to my knowledge once I perform torch.nn.conv1d(*args), 
The resulting matrix becomes (batch size = 1, word size = 40, feature map size = 98) with kernel size of 3.</p>
<p>Basically, as I understand it convolves around y axis instead of x axis and it turn does not capture the spacial relationship between word embeddings.</p>
<p>Is there any way to change the convolutional layer so it calculates feature maps around different axis?</p>
<p>TL, DR:</p>
<p>Torch conv1d layer behaves this way on embedding layer:
<a href="https://i.stack.imgur.com/45tUd.png" rel="nofollow noreferrer">enter image description here</a></p>
<p>But I want it to behave like this</p>
<p><a href="https://i.stack.imgur.com/UjL3Z.png" rel="nofollow noreferrer">enter image description here</a></p>
<p>Any help would be much appreciated.</p>
</div>
<div class="post-text" itemprop="text">
<p>conv1d expects the input's size to be (batch_size, num_channels, length) and there is no way to change that, so you have two possible ways ahead of you, you can either <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute" rel="nofollow noreferrer"><code>permute</code></a> the output of embedding or you can use a conv1d instead of you embedding layer(in_channels = num_words, out_channels=word_embedding_size, and kernel_size=1) which is slower than embedding and not a good idea!</p>
<pre><code>input = torch.randint(0, 10, (batch_size, sentence_length))
embeddings = word_embedding(input) #(batch_size, sentence_length, embedding_size)
embeddings_permuted = embeddings.permute(0, 2, 1) #(batch_size, embedding_size, sentence_length)
conv_out = convolution(embeddings_permuted) #(batch_size, conv_out_channels, changed_sentence_length)
#now you can either use the output as it is or permute it back (based on your upper layers)
#also note that I wrote changed_sentence_length because it is a fucntion of your padding and stride 
</code></pre>
</div>
<span class="comment-copy">Thank you very much, I guess I'll try to perform permutation on embedding matrix and see how it goes, as I already got seeded trained model in keras. I would up-vote you but since this is my first post I'm unable to do so due to point system or something :)</span>
