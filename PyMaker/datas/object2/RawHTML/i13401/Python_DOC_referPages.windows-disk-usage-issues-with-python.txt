<div class="post-text" itemprop="text">
<p>I am executing the python code that follows.</p>
<p>I am running it on a folder ("articles") which has a couple hundred subfolders and 240,226 files in all.</p>
<p>I am timing the execution. At first the times were pretty stable but went non-linear after 100,000 files. Now the times (I am timing at 10,000 file intervals) can go non_linear after 30,000 or so (or not). </p>
<p>I have the Task Manager open and correlate the slow-downs to 99% Disk usage by python.exe. I have done gc-collect(). dels etc., turned off Windows indexing. I have re-started Windows, emptied the trash (I have a few hundred GBs free). Nothing helps, the disk usage seems to be getting more erratic if anything.</p>
<p>Sorry for the long post - Thanks for the help</p>
<pre><code>def get_filenames():
    for (dirpath, dirnames, filenames) in os.walk("articles/"):
        dirs.extend(dirnames)

    for dir in dirs:
        path = "articles" + "\\" + dir        
        nxml_files.extend(glob.glob(path + "/*.nxml"))

    return nxml_files

def extract_text_from_files(nxml_files):  
    for nxml_file in nxml_files:       
        fast_parse(nxml_file)

def fast_parse(infile):
    file = open(infile,"r")
    filetext = file.read()
    tag_breaks = filetext.split('&gt;&lt;')
    paragraphs = [tag_break.strip('p&gt;').strip('&lt;/') for tag_break in tag_breaks if tag_break.startswith('p&gt;')]

def run_files(): 
    nxml_files = get_filenames()
    extract_text_from_files(nxml_files)

if __name__ == "__main__":    
    run_files()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>There are some things that could be optimized. </p>
<p>At first, is you open files, close them as well. A <code>with open(...) as name:</code> block will do that easily. BTW in Python 2 <code>file</code> is a bad choice for a variable name, it is built-in function's name.</p>
<p>You can remove one disc read by doing string comparisons instead of the glob. </p>
<p>And last but not least: <code>os.walk</code> spits out the results cleverly, so don't buffer them into a list, process everything inside one loop. This will save a lot of memory.</p>
<p>That is what I can advise from the code. For more details on what is causing the I/O you should use profiling. See <a href="https://docs.python.org/2/library/profile.html" rel="nofollow">https://docs.python.org/2/library/profile.html</a> for details.</p>
</div>
<span class="comment-copy">If you can, upgrade to Python 3.5; your code doesn't seem to use specific version features, and <code>os.walk</code> got reimplemented using the new <a href="https://docs.python.org/3/library/os.html#os.scandir" rel="nofollow noreferrer"><code>os.scandir</code></a> function. Though <code>os.walk</code> doesn't provide the free <code>stat</code> information to you through its interface, it performs far less disk I/O; on Python &lt;=3.4, it had to read in the complete list of files (a small handful of I/O ops with limited seeking), then <code>stat</code> them all to separate files and directories (in your case ~100,000 random reads). Using <code>os.scandir</code> eliminates the ~100,000 <code>stats</code>.</span>
<span class="comment-copy">According to the <a href="https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-471" rel="nofollow noreferrer">Python 3.5 release notes</a>, <code>os.scandir</code>'s use in <code>os.walk</code> seamlessly improves the speed of <code>os.walk</code> on Windows systems by 7-20x. That's huge, and if you're scanning over a hundred thousand files, you'd almost certainly benefit from the speed up.</span>
<span class="comment-copy">Another opportunity to reduce disk I/O: <code>os.walk</code> gives a list of <code>filenames</code> which are discarded, and <code>glob.glob</code> is used to get a filtered set of the files, when <a href="https://docs.python.org/3/library/fnmatch.html" rel="nofollow noreferrer"><code>fnmatch</code></a> could be used to filter <code>filenames</code> directly and avoid the redundant disk I/O involved in <code>glob.glob</code>. Edit: Oops, I see you mentioned that. Leaving this here for the link to <code>fnmatch</code> (which is what <code>glob.glob</code> actually uses for the filtering, so it can simulate <code>glob</code> precisely on a known list of file names).</span>
<span class="comment-copy">Yes, that would be an alternative to what I wrote. I leave it to the OP to test which is faster and more suitable.</span>
<span class="comment-copy">thanks for the suggestions, did them all, the same behavior on disk (notice in code getting the filenames is a one time hit) I think the major question is why is the extract_text_from_files code (even with a file.close()) erratically sucking up Disk??</span>
