<div class="post-text" itemprop="text">
<p>I have created a class with a number of methods. One of the methods is very time consuming, <code>my_process</code>, and I'd like to do that method in parallel. I came across <a href="https://stackoverflow.com/questions/15790816/python-multiprocessing-apply-class-method-to-a-list-of-objects">Python Multiprocessing - apply class method to a list of objects</a> but I'm not sure how to apply it to my problem, and what effect it will have on the other methods of my class.</p>
<pre><code>class MyClass():
    def __init__(self, input):
        self.input = input
        self.result = int

    def my_process(self, multiply_by, add_to):
        self.result = self.input * multiply_by
        self._my_sub_process(add_to)
        return self.result

    def _my_sub_process(self, add_to):
        self.result += add_to

list_of_numbers = range(0, 5)
list_of_objects = [MyClass(i) for i in list_of_numbers]
list_of_results = [obj.my_process(100, 1) for obj in list_of_objects] # multi-process this for-loop

print list_of_numbers
print list_of_results

[0, 1, 2, 3, 4]
[1, 101, 201, 301, 401]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I'm going to go against the grain here, and suggest sticking to the simplest thing that could possibly work ;-)  That is, <code>Pool.map()</code>-like functions are ideal for this, but are restricted to passing a single argument.  Rather than make heroic efforts to worm around that, simply write a helper function that only needs a single argument:  a tuple.  Then it's all easy and clear.</p>
<p>Here's a complete program taking that approach, which prints what you want under Python 2, and regardless of OS:</p>
<pre><code>class MyClass():
    def __init__(self, input):
        self.input = input
        self.result = int

    def my_process(self, multiply_by, add_to):
        self.result = self.input * multiply_by
        self._my_sub_process(add_to)
        return self.result

    def _my_sub_process(self, add_to):
        self.result += add_to

import multiprocessing as mp
NUM_CORE = 4  # set to the number of cores you want to use

def worker(arg):
    obj, m, a = arg
    return obj.my_process(m, a)

if __name__ == "__main__":
    list_of_numbers = range(0, 5)
    list_of_objects = [MyClass(i) for i in list_of_numbers]

    pool = mp.Pool(NUM_CORE)
    list_of_results = pool.map(worker, ((obj, 100, 1) for obj in list_of_objects))
    pool.close()
    pool.join()

    print list_of_numbers
    print list_of_results
</code></pre>
<h2>A big of magic</h2>
<p>I should note there are many advantages to taking the very simple approach I suggest.  Beyond that it "just works" on Pythons 2 and 3, requires no changes to your classes, and is easy to understand, it also plays nice with all of the <code>Pool</code> methods.</p>
<p>However, if you have multiple methods you want to run in parallel, it can get a bit annoying to write a tiny worker function for each.  So here's a tiny bit of "magic" to worm around that.  Change <code>worker()</code> like so:</p>
<pre><code>def worker(arg):
    obj, methname = arg[:2]
    return getattr(obj, methname)(*arg[2:])
</code></pre>
<p>Now a single worker function suffices for any number of methods, with any number of arguments.  In your specific case, just change one line to match:</p>
<pre><code>list_of_results = pool.map(worker, ((obj, "my_process", 100, 1) for obj in list_of_objects))
</code></pre>
<p>More-or-less obvious generalizations can also cater to methods with keyword arguments.  But, in real life, I <em>usually</em> stick to the original suggestion.  At some point catering to generalizations does more harm than good.  Then again, I like obvious things ;-)</p>
</div>
<div class="post-text" itemprop="text">
<p>Generally the easiest way to run the same calculation in parallel is the <code>map</code> method of a <code>multiprocessing.Pool</code> (or the <code>as_completed</code> function from <code>concurrent.futures</code> in Python 3).</p>
<p>However, the <code>map</code> method applies a function <em>that only takes one argument</em> to an iterable of data using multiple processes.</p>
<p>So this function cannot be a normal method, because that requires at least <em>two</em> arguments; it must also include <code>self</code>! It could be a staticmethod, however. See also <a href="https://stackoverflow.com/a/7017546/1219295">this answer</a> for a more in-depth explanation.</p>
</div>
<div class="post-text" itemprop="text">
<p>If your class is not "huge", I think process oriented is better.
Pool in multiprocessing is suggested.<br/>
This is the tutorial -&gt; <a href="https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers" rel="nofollow noreferrer">https://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers</a></p>
<p>Then seperate the <code>add_to</code> from <code>my_process</code> since they are quick and you can wait util the end of the last process.</p>
<pre><code>def my_process(input, multiby):
    return xxxx
def add_to(result,a_list):
    xxx
p = Pool(5)
res = []
for i in range(10):
    res.append(p.apply_async(my_process, (i,5)))
p.join()  # wait for the end of the last process
for i in range(10):
    print res[i].get()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Based on the answer of <a href="https://stackoverflow.com/questions/15790816/python-multiprocessing-apply-class-method-to-a-list-of-objects">Python Multiprocessing - apply class method to a list of objects</a> and your code: </p>
<ol>
<li><p>add <code>MyClass object</code> into <code>simulation object</code></p>
<pre><code>class simulation(multiprocessing.Process):
    def __init__(self, id, worker, *args, **kwargs):
        # must call this before anything else
        multiprocessing.Process.__init__(self)
        self.id = id
        self.worker = worker
        self.args = args
        self.kwargs = kwargs
        sys.stdout.write('[%d] created\n' % (self.id))
</code></pre></li>
<li><p>run what you want in <code>run</code> function</p>
<pre><code>    def run(self):
        sys.stdout.write('[%d] running ...  process id: %s\n' % (self.id, os.getpid()))
        self.worker.my_process(*self.args, **self.kwargs)
        sys.stdout.write('[%d] completed\n' % (self.id))
</code></pre></li>
</ol>
<p>Try this:</p>
<pre><code>list_of_numbers = range(0, 5)
list_of_objects = [MyClass(i) for i in list_of_numbers]
list_of_sim = [simulation(id=k, worker=obj, multiply_by=100*k, add_to=10*k) \
    for k, obj in enumerate(list_of_objects)]  

for sim in list_of_sim:
    sim.start()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you don't absolutely need to stick with Multiprocessing module then,
it can easily achieved using <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">concurrents.futures</a> library</p>
<p>here's the example code:</p>
<pre><code>from concurrent.futures.thread import ThreadPoolExecutor, wait

MAX_WORKERS = 20

class MyClass():
    def __init__(self, input):
        self.input = input
        self.result = int

    def my_process(self, multiply_by, add_to):
        self.result = self.input * multiply_by
        self._my_sub_process(add_to)
        return self.result

    def _my_sub_process(self, add_to):
        self.result += add_to

list_of_numbers = range(0, 5)
list_of_objects = [MyClass(i) for i in list_of_numbers]

With ThreadPoolExecutor(MAX_WORKERS) as executor:
    for obj in list_of_objects:
        executor.submit(obj.my_process, 100, 1).add_done_callback(on_finish)

def on_finish(future):
    result = future.result() # do stuff with your result
</code></pre>
<p>here executor returns future for every task it submits. keep in mind that if you use <code>add_done_callback()</code> finished task from thread returns to the main thread (which would block your <strong>main thread</strong>) if you really want true parallelism then you should wait for future objects separately. here's the code snippet for that.</p>
<pre><code>futures = []
with ThreadPoolExecutor(MAX_WORKERS) as executor:
    for objin list_of_objects:
        futures.append(executor.submit(obj.my_process, 100, 1))
wait(futures)

for succeded, failed in futures:
    # work with your result here
    if succeded:
       print (succeeeded.result())
    if failed:
        print (failed.result())
</code></pre>
<p>hope this helps.</p>
</div>
<span class="comment-copy">Wouldn't it use however many cores are available if NUM_CORE isn't set?</span>
<span class="comment-copy">Sure.  That's up to you.  However, for a CPU-bound task, it's typical to ask for <i>fewer</i> cores than actually exist, so the OS gets some cycles to run other stuff too.  But, again, that's up to you.  <code>mp.cpu_count()</code> returns the number of cores that exist.</span>
<span class="comment-copy">Keep in mind that on the standard implementation of Python only one thread at a time can be executing Python bytecode. On this implementation threads will not improve CPU-bound performance.</span>
<span class="comment-copy">For CPU-bound tasks you just need to swap the <code>ThreadPoolExecutor</code> for a <code>ProcessPoolExecutor</code>.  You'll take a small hit as the processes start up, but after that the workers can execute at the same time.  Note that you the data that is returned from the sub-processes needs to be pickle-able.</span>
