<div class="post-text" itemprop="text">
<p>I have an embarrassingly parallel problem but have been wondering how to "design" function so that it achieves the end result</p>
<p>So, here is the sequential version</p>
<pre><code>def train_weights(Xtr, ztr, Xte, zte):
    regr = some_model()
    regr.fit(Xtr, ztr)
    error = np.mean((regr.predict(Xte) - zte) ** 2)
    return regr.coef_, error

rnge = range(z_train.shape[0])
weights = []
errors = []
for i in rnge:
    z_dim_tr = z_train[:,i]
    z_dim_te = z_test[:, i]
    weight, error = train_weights(X_train, z_dim_tr, X_test, z_dim_te)
    weights.append(wgts)
    errors.append(error)
</code></pre>
<p>So, I am just slicing a column from a matrix (train and test matrices)
and then passing it to the function..
Note that, order of output matters.. which is the index of weight in weights list corresponds to a particular "i" and same for error.</p>
<p>How do i parallelize this?</p>
</div>
<div class="post-text" itemprop="text">
<p>it can easily achieved using <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">concurrents.futures</a> library</p>
<p>here's the example code:</p>
<pre><code>from concurrent.futures.thread import ThreadPoolExecutor

MAX_WORKERS = 20

def train_weights(Xtr, ztr, Xte, zte):
    regr = some_model()
    regr.fit(Xtr, ztr)
    error = np.mean((regr.predict(Xte) - zte) ** 2)
    return regr.coef_, error

def work_done(future):
    weights.append(future.result())

rnge = range(z_train.shape[0])
weights = []
errors = []
for i in rnge:
    z_dim_tr = z_train[:, i]
    z_dim_te = z_test[:, i]
    with ThreadPoolExecutor(MAX_WORKERS) as executor:
        executor.submit(train_weights, X_train, X_test, Xte, z_dim_te).add_done_callback(work_done)
</code></pre>
<p>here executor returns future for every task it submits. keep in mind that if you use <code>add_done_callback()</code> finished task from thread returns to the main thread (which would block your <strong>main thread</strong>) if you really want true parallelism then you should wait for future objects separately. here's the code snippet for that.</p>
<pre><code>futures = []
for i in rnge:
    z_dim_tr = z_train[:, i]
    z_dim_te = z_test[:, i]
    with ThreadPoolExecutor(MAX_WORKERS) as executor:
        futures.append(executor.submit(train_weights, X_train, X_test, Xte, z_dim_te))

wait(futures)

for succeded, failed in futures:
    # work with your result here
    if succeded:
        weights.append(succeded.result())
    if failed:
        errors.append(failed.result())
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Since this is just a general parallel processing problem, you can use <code>Pool</code> from <code>multiprocessing.dummy</code>.</p>
<p>Since I don't have your data set, let's instead consider this following example.</p>
<pre><code>import multiprocessing
from multiprocessing.dummy import Pool

def test(args):
    a, b = args
    return a

data = [
    (1, 2),
    (2, 3),
    (3, 4),
]

pool = Pool(multiprocessing.cpu_count())

results = pool.map(test, data)

pool.close()
pool.join()

for result in results:
    print(result)
</code></pre>
<p>Pool creates an amount of worker processes (in this case <code>multiprocessing.cpu_count()</code>). Each worker then continuously execute a job until all jobs have been executed. In other words <code>map()</code> first returns when all jobs have been executed.</p>
<p>All in all, the above example, when calling <code>map()</code> it returns a list of results which are in the same order as they're given. So in the end the above code prints <code>1</code>, <code>2</code> then <code>3</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>check out joblib</p>
<p><a href="https://pythonhosted.org/joblib/parallel.html" rel="nofollow noreferrer">https://pythonhosted.org/joblib/parallel.html</a></p>
<blockquote>
<p>Joblib provides a simple helper class to write parallel for loops
  using multiprocessing. The core idea is to write the code to be
  executed as a generator expression, and convert it to parallel
  computing:</p>
</blockquote>
<pre><code>&gt;&gt;&gt; from math import sqrt
&gt;&gt;&gt; [sqrt(i ** 2) for i in range(10)]
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
</code></pre>
<blockquote>
<p>can be spread over 2 CPUs using the following:</p>
</blockquote>
<pre><code>&gt;&gt;&gt; from math import sqrt
&gt;&gt;&gt; from joblib import Parallel, delayed
&gt;&gt;&gt; Parallel(n_jobs=2)(delayed(sqrt)(i ** 2) for i in range(10))
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's one way to parallelize the code using <a href="https://github.com/ray-project/ray" rel="nofollow noreferrer">Ray</a>. Some advantages of using Ray</p>
<ul>
<li>Large data will be stored in shared memory and can be accessed by multiple workers (in a read only fashion) so that workers don't need to create their own copy of the data.</li>
<li>The same code will run on one machine or on multiple machines.</li>
</ul>
<p>Ray is a library for writing parallel and distributed Python.</p>
<pre><code>import numpy as np
import ray

ray.init()

z_train = np.random.normal(size=(100, 30))
z_test = np.random.normal(size=(50, 30))


@ray.remote(num_return_vals=2)
def train_weights(ztr, zte):
    # Fit model.
    predictions = np.random.normal(size=zte.shape[0])
    error = np.mean((predictions - zte) ** 2)
    coef = np.random.normal()
    return coef, error


weight_ids = []
error_ids = []
for i in range(z_train.shape[1]):
    z_dim_tr = z_train[:, i]
    z_dim_te = z_test[:, i]
    weight_id, error_id = train_weights.remote(z_dim_tr, z_dim_te)
    weight_ids.append(weight_id)
    error_ids.append(error_id)

weights = ray.get(weight_ids)
errors = ray.get(error_ids)
</code></pre>
<p>You can read more in the <a href="https://ray.readthedocs.io/en/latest/?badge=latest" rel="nofollow noreferrer">Ray documentation</a>. Note I'm one of the Ray developers.</p>
</div>
<span class="comment-copy">Is it the part of calling <code>train_weights()</code> you wish to be done in parallel? While still keeping the order of the results appended to the lists?</span>
<span class="comment-copy">@Vallentin: yepp.. that is correct</span>
