<div class="post-text" itemprop="text">
<p>I am working at a company that deals with phish and fake Facebook accounts.  I want to show my dedication to the "mission".  We are unable to passively monitor facebook pages for when they are removed.  I am thinking a web crawler but I am curious on how to design one that constant checks a specific link to see if the Facebook page is still active or not?  I hope this made sense?  </p>
</div>
<div class="post-text" itemprop="text">
<p>Yes! You can use crawling. However, if you want it to be as fast as possible, crawling may not be the best way to do it. If you're interested this is how I'd do it using HTTPConnection. also, unfortunately, the link has to be <em>completely</em> broken.</p>
<p>If you need more information then you will most likely have to use an API or web crawler to check if the link is broken(Thus meaning it has to link to nowhere),</p>
<pre><code>from http.client import HTTPConnection # Importing HTTPConnection from http.client.

conn = HTTPConnection('www.google.com') # Connecting to 'google.com'

conn.request('HEAD', '/index.html') # Request data.
res = conn.getresponse() # Now we get the data sent back.

print(res.status, res.reason) # Finally print it.
</code></pre>
<p>If it returns '302 Found' then it should be an active web page,
I hope this helps! Please tell me if this isn't what you wanted. :)</p>
<p>Thanks,</p>
<p>~Coolq</p>
</div>
<div class="post-text" itemprop="text">
<p>You can send a http request to tell the account is active or not by it's response status, Python has some standard library, you may have a look at <a href="https://docs.python.org/3/library/internet.html" rel="nofollow">Internet Protocols and Support</a>.
Personally, I will recommend use requests:</p>
<pre><code>import requests
response = requests.get("http://facebook.com/account")
if response.status_code == 302: # or 404
    # the page is missing
</code></pre>
<p>If you really care about speed or performance, you should use multiprocessing or asynchronous i/o (like gevent) in Python.</p>
<p>If you are focus on crawl,you may have a look at <a href="http://doc.scrapy.org/en/latest/intro/overview.html" rel="nofollow">Scrapy</a></p>
<blockquote>
<p>Here you notice one of the main advantages about Scrapy: requests are
  scheduled and processed asynchronously. This means that Scrapy doesnâ€™t
  need to wait for a request to be finished and processed, it can send
  another request or do other things in the meantime. This also means
  that other requests can keep going even if some request fails or an
  error happens while handling it.</p>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch/answer/Raghavendran-Balu" rel="nofollow">https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch/answer/Raghavendran-Balu</a>
One of the best articles I have read about Crawlers.</p>
<p>A web crawler might sound like a simple fetch-parse-append system, but watch out! you may over look the complexity. I might deviate from the question intent by focussing more on architecture than implementation specifics.I believe it is necessary because, to build a web scale crawler, the architecture of the crawler is more important than the choice of language/ framework.</p>
<p><strong>Architecture:</strong></p>
<p>A bare minimum crawler needs at least these components:</p>
<ul>
<li><p>HTTP Fetcher : To retrieve web page from the server.</p></li>
<li><p>Extractor: Minimal support to extract URL from page like anchor links.</p></li>
<li><p>Duplicate Eliminator: To make sure same content is not extracted twice unintentionally. Consider it as a set based data structure.</p></li>
<li><p>URL Frontier: To prioritize URL that has to fetched and parsed. Consider it as a priority queue</p></li>
<li><p>Datastore: To store retrieve pages and URL and other meta data.</p></li>
</ul>
<p><strong>A good starting point to learn about architecture is:</strong></p>
<ol>
<li><a href="http://dl.acm.org/citation.cfm?id=1734789" rel="nofollow">Web Crawling</a> </li>
<li><a href="http://link.springer.com/chapter/10.1007/978-3-662-10874-1_7" rel="nofollow">Crawling the Web</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=598684.598733" rel="nofollow">Mercator: A scalable, extensible Web crawler</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=1045969" rel="nofollow">UbiCrawler: a scalable fully distributed web crawler</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=1541823" rel="nofollow">IRLbot: Scaling to 6 billion pages and beyond</a></li>
<li>(single-sever crawler) and <a href="http://dl.acm.org/citation.cfm?id=2127045.2127065" rel="nofollow">MultiCrawler: a pipelined architecture</a></li>
</ol>
<p><strong>Programming Language</strong>: Any high level language with good network library that you are comfortable with is fine. I personally prefer Python/Java. As your crawler project might grow in terms of code size it will be hard to manage if you develop in a design-restricted programming language. While it is possible to build a crawler using just unix commands and shell script, you might not want to do so for obvious reasons.</p>
<p><strong>Framework/Libraries</strong>: Many frameworks are already suggested in other answers. I shall summarise here:</p>
<ol>
<li>Apache Nutch and Heritrix (Java): Mature, Large scale, configurable</li>
<li><p>Scrapy (Python): Technically a scraper but can be used to build a crawler.</p>
<p>You can also visit <a href="https://github.com/scrapinghub/distributed-frontera" rel="nofollow">https://github.com/scrapinghub/distributed-frontera</a> - URL frontier and data storage for Scrapy, allowing you to run large scale crawls.</p></li>
<li><a href="https://www.node.io/" rel="nofollow">node.io</a> (Javascript): Scraper.  Nascent, but worth considering, if you are ready to live with javascript.</li>
</ol>
<p>For Python: Refer <a href="http://www-rohan.sdsu.edu/~gawron/python_for_ss/course_core/book_draft/web/web_intro.html" rel="nofollow">Introduction to web-crawling in Python</a></p>
<p>Code in Python: <a href="https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch/answer/Rishi-Giri-1" rel="nofollow">https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch/answer/Rishi-Giri-1</a></p>
<p><strong>Suggestions for scalable distributed crawling</strong>:</p>
<ol>
<li>It is better to go for a asynchronous model, given the nature of the problem.</li>
<li>Choose a distributed data base for data storage ex. Hbase.</li>
<li>A distributed data structure like redis is also worth considering for URL frontier and duplicate detector.</li>
</ol>
<p>For more Information visit: <a href="https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch" rel="nofollow">https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch</a> </p>
<p><strong>References</strong>:</p>
<ol>
<li><p>Olston, C., &amp; Najork, M. (2010). Web crawling. Foundations and Trends in Information Retrieval, 4(3), 175-246.</p></li>
<li><p>Pant, G., Srinivasan, P., &amp; Menczer, F. (2004). Crawling the web. In Web Dynamics (pp. 153-177). Springer Berlin Heidelberg.</p></li>
<li><p>Heydon, A., &amp; Najork, M. (1999). Mercator: A scalable, extensible web crawler.World Wide Web, 2(4), 219-229.</p></li>
<li><p>Boldi, P., Codenotti, B., Santini, M., &amp; Vigna, S. (2004). Ubicrawler: A scalable fully distributed web crawler. Software: Practice and Experience, 34(8), 711-726.</p></li>
<li><p>Lee, H. T., Leonard, D., Wang, X., &amp; Loguinov, D. (2009). IRLbot: scaling to 6 billion pages and beyond. ACM Transactions on the Web (TWEB), 3(3), 8.</p></li>
<li><p>Harth, A., Umbrich, J., &amp; Decker, S. (2006). Multicrawler: A pipelined architecture for crawling and indexing semantic web data. In The Semantic Web-ISWC 2006 (pp. 258-271). Springer Berlin Heidelberg.</p></li>
</ol>
</div>
