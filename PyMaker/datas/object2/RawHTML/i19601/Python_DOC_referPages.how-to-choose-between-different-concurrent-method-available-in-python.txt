<div class="post-text" itemprop="text">
<p>There's different ways of doing concurrent in Python, below is a simple list:</p>
<ul>
<li>process-based: <code>process.Popen</code>, <code>multiprocessing.Process</code>, old fashioned <code>os.system</code>, <code>os.popen</code>, <code>os.exe*</code></li>
<li>thread-based: <code>threading.Thread</code></li>
<li>microthread-based: <code>greenlet</code></li>
</ul>
<p>I know the difference between thread-based concurrency and process-based concurrency, and I know some (but not too much) about <code>GIL</code>'s impact in <code>CPython</code>'s thread support.</p>
<p>For a beginner who want to implement some level of concurrency, how to choose between them? Or, what's the general difference between them? Are there any more ways to do concurrent in Python?</p>
<p>I'm not sure if I'm asking the right question, please feel free to improve this question.</p>
</div>
<div class="post-text" itemprop="text">
<p>The reason all three of these mechanisms exist is that they have different strengths and weaknesses.</p>
<p>First, if you have huge numbers of small, independent tasks, and there's no sensible way to batch them up (typically, this means you're writing a <a href="http://en.wikipedia.org/wiki/C10k_problem">C10k</a> server, but that's not the only possible case), microthreads win hands down. You can only run a few hundred OS threads or processes before everything either bogs down or just fails. So, either you use microthreads, or you give up on automatic concurrency and start writing explicit callbacks or coroutines. This is really the <em>only</em> time microthreads win; otherwise, they're just like OS threads except a few things don't work right.</p>
<p>Next, if your code is <a href="http://en.wikipedia.org/wiki/CPU_bound">CPU-bound</a>, you need processes. Microthreads are an inherently single-core solution; Threads in Python generally can't parallelize well because of the GIL; processes get as much parallelism as the OS can handle. So, processes will let your 4-core system run your code 4x as fast; nothing else will. (In fact, you might want to go farther and distribute across separate computers, but you didn't ask about that.) But if your code is <a href="http://en.wikipedia.org/wiki/I/O_bound">I/O-bound</a>, core-parallelism doesn't help, so threads are just as good as processes.</p>
<p>If you have lots of shared, mutable data, things are going to be tough. Processes require explicitly putting everything into sharable structures, like using <a href="http://docs.python.org/3/library/multiprocessing.html#multiprocessing.Array"><code>multiprocessing.Array</code></a> in place of <code>list</code>, which gets nightmarishly complicated. Threads share everything automatically—which means there are race conditions everywhere. Which means you need to think through your flow very carefully and use locks effectively. With processes, an experienced developers can build a system that works on all of the test data but has to be reorganized every time you give it a new set of inputs. With threads, an experienced developer can write code that runs for weeks before accidentally and silently scrambling everyone's credit card numbers.</p>
<p>Whichever of those two scares you more—do that one, because you understand the problem better. Or, if it's at all possible, step back and try to redesign your code to make most of the shared data independent or immutable. This may <em>not</em> be possible (without making things either too slow or too hard to understand), but think about it hard before deciding that.</p>
<p>If you have lots of independent data or shared immutable data, threads clearly win. Processes need either explicit sharing (like <code>multiprocessing.Array</code> again) or marshaling. <code>multiprocessing</code> and its third-party alternatives make marshaling pretty easy for the simple cases where everything is picklable, but it's still not as simple as just passing values around directly, and it's also a lot slower.</p>
<p>Unfortunately, most cases where you have lots of immutable data to pass around are the exact same cases where you need CPU parallelism, which means you have a tradeoff. And the best answer to this tradeoff may be OS threads on your current 4-core system, but processes on the 16-core system you have in 2 years. (If you organize things around, e.g., <a href="http://docs.python.org/3/library/multiprocessing.html#using-a-pool-of-workers"><code>multiprocessing.ThreadPool</code></a> or <a href="http://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor"><code>concurrent.futures.ThreadPoolExecutor</code></a>, and trivially switch to <code>Pool</code> or <code>ProcessPoolExecutor</code> later—or even with a runtime configuration switch—that pretty much solves the problem. But this isn't always possible.)</p>
<p>Finally, if your application inherently requires an event loop (e.g., a GUI app or a network server), pick the framework you like first. Coding with, say, <code>PySide</code> vs. <code>wx</code>, or <code>twisted</code> vs. <code>gevent</code>, is a bigger difference than coding with microthreads vs. OS threads. And, once you've picked the framework, see how much you can take advantage of its event loop where you thought you needed real concurrency. For example, if you need some code to run every 30 seconds, don't start a thread (micro- or OS) for that, ask the framework to schedule it however it wants.</p>
</div>
<span class="comment-copy">It sounds like you know the difference between the different types of concurrency, in which case you already know the answer to your question.  Also, a more answerable question would identify the specific situation, in order to choose between them.  There are plenty of websites discussing the general difference between concurrency methods</span>
<span class="comment-copy">As a side note: You should not be using the <code>os.*</code> methods for process-based concurrency. If <code>multiprocessing</code> (or a similar third-party module) isn't what you want, and you just want to exec a child process, use the <code>subprocess</code> module.</span>
<span class="comment-copy">+1 to @lxop. The key things we'd need to know before we could even begin to answer this are whether you're expecting to be IO-bound or CPU-bound, what kind of data sharing you need between your threads of execution (using the term loosely to mean all three), and whether you already inherently need a main loop for some reason (e.g., a GUI or a network server—in which case the answer may be "none of the above, do it in a single-threaded event loop").</span>
<span class="comment-copy">@abarnert I think your comment would be a good answer to this question. There's a huge gap between knowing the difference (like my current knowledge) and choosing the right method when dealing with a real problem.</span>
<span class="comment-copy">@yegle: Well, I wrote it as a comment because I suck at brevity and conciseness unless it's forced on me. As you can see from the version I've now posted as an answer. :)</span>
<span class="comment-copy">One thing I should have mentioned: C extension modules can release the GIL. And some important ones, like <code>numpy</code>, do so in many cases where it's useful to. Which means, in those cases, you can use threads instead of processes, and still get core parallelism.</span>
