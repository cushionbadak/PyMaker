<div class="post-text" itemprop="text">
<p>I have a job that uses the <code>multiprocessing</code> package and calls a function via </p>
<p><code>resultList = pool.map(myFunction, myListOfInputParameters)</code>. </p>
<p>Each entry of the list of input parameters is independent from others.</p>
<p>This job will run a couple of hours. For safety reasons, I would like to store the results that are made in between in regular time intervals, like e.g. once an hour.</p>
<p>How can I do this and be able to continue with the processing when the job was aborted and I want to restart it based on the last available backup?</p>
</div>
<div class="post-text" itemprop="text">
<p>There are at least two possible options.</p>
<ol>
<li>Have each call of <code>myFunction</code> save its output into a uniquely named file. The file name should be based on or linked to the input data. Use the parent program to gather the results. In this case <code>myFunction</code> should return an identifier of the item that is finished.</li>
<li>Use <code>imap_unordered</code> instead of <code>map</code>. This will start yielding results as soon as they are available, instead of returing when all processing is finished. Have the parent program save the returned data and a indication which items are finished.</li>
</ol>
<p>In both cases, the program would have to examine the data saved from previous runs to adjust <code>myListOfInputParameters</code> when it is being re-started.</p>
<p>Which option is best depends to a large degree on the amount of data returned by <code>myFunction</code>. If this is a large amount, there is a significant overhead associated with transferring it back to the parent. In that case option 1 is probably best.</p>
<p>Since writing to disk is relatively slow, calculations wil probably go faster with option 2. And it is easier for the parent program to track progress.</p>
<p>Note that you can also use <code>imap_unordered</code> with option 1.</p>
</div>
<div class="post-text" itemprop="text">
<p>Perhaps use pickle. Read more here:</p>
<p><a href="https://docs.python.org/3/library/pickle.html" rel="nofollow noreferrer">https://docs.python.org/3/library/pickle.html</a></p>
<p>Based on aws_apprentice's comment I created a full multiprocessing example in case you weren't sure how to use intermediate results.  The first time this is run it will print "None" as there are no intermediate results.  Run it again to simulate restarting.</p>
<pre><code>from multiprocessing import Process
import pickle

def proc(name):
  data = None

  # Load intermediate results if they exist
  try:
    f = open(name+'.pkl', 'rb')
    data = pickle.load(f)
    f.close()
  except:
    pass

  # Do something
  print(data)
  data = "intermediate result for " + name

  # Periodically save your intermediate results
  f = open(name+'.pkl', 'wb')
  pickle.dump(data, f, -1)
  f.close()

processes = []
for x in range(5):
  p = Process(target=proc, args=("proc"+str(x),))
  p.daemon = True
  p.start()
  processes.append(p)

for process in processes:
  process.join()

for process in processes:
  process.terminate()
</code></pre>
<p>You can also use json if that makes sense to output intermediate results in human readable format.  Or sqlite as a database if you need to push data into rows. </p>
</div>
<span class="comment-copy">does  your task itself have steps that you can logically break into smaller chunks and write as you go? Also, depending on number of tasks, all tasks are not running at the same time, that depends on your number of workers/cpu cores. So, a more realistic scenario is that 3 or 4 input parameters completed all the way and then a crash happened. In such setups, can you just check which outputs were completed for inputs?</span>
<span class="comment-copy">The steps can be broken into smaller chunks, yes. But there are several thousands of steps. Therefore a manual approach is not so nice.</span>
<span class="comment-copy">Thanks for the info on imap_unordered. In between I looked for DMTCP ( <a href="http://dmtcp.sourceforge.net/" rel="nofollow noreferrer">dmtcp.sourceforge.net</a> ), but realised that it is not maintained very well - and seems to be restricted to Python2.x presently.</span>
<span class="comment-copy">sure pickle is a way to store <i>something</i> but this doesn't answer OPs question</span>
<span class="comment-copy">I don't understand.  What makes you think it isn't possible to continue processing the data he pickled?</span>
<span class="comment-copy">In general, I would advise to use human-readable data formats (like e.g. JSON) when saving data to disk whenever possible. It makes debugging and inspecting results <i>much</i> easier.</span>
<span class="comment-copy">Yep completely agree though it depends on his data.  The link to pickle I provided also discusses this a bit.</span>
<span class="comment-copy">I never said it wasnâ€™t possible. Your original answer simply included a link to pickle not an answer to OPs question but seeing as you edited I have removed my downvote</span>
