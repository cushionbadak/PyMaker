<div class="post-text" itemprop="text">
<p>I used the multiprocessing lib to create multi-thread to process a list of files(20+ files). </p>
<p>When I run the py file, I set the pool number as 4. But in cmd, it showed there are over 10 processes. And most of them have been running for a long time. Because it's large file and takes long time to process so I'm not sure if the process is hanging or still executing.</p>
<p>So my question is:</p>
<p>if it's executing, how to set the process number as exactly 4?</p>
<p>if it's hanging, it means child process will not shut down after finished. Can I set it automatically shutting down after finished?</p>
<pre><code>from multiprocessing import Pool
poolNum = int(sys.argv[1])
pool = Pool(poolNum)
pool.map(processFunc, fileList)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>It won't, not until the <code>Pool</code> is <code>close</code>-ed or <code>terminate</code>-ed (IIRC <code>Pool</code>s at least at present have a reference cycle involved, so even when the last live reference to the <code>Pool</code> goes away, the <code>Pool</code> is not deterministically collected, even on CPython, which uses reference counting and normally has deterministic behavior).</p>
<p>Since you're using <code>map</code>, your work is definitely done when <code>map</code> returns, so the simplest solution is just to use a <code>with</code> statement for guaranteed termination:</p>
<pre><code>from multiprocessing import Pool

def main():
    poolNum = int(sys.argv[1])

    with Pool(poolNum) as pool:  # Pool created
        pool.map(processFunc, fileList)
    # terminate has been called, all workers will be killed

# Adding main guard so this code is valid on Windows and anywhere else which
# doesn't use forking for whatever reason
if __name__ == '__main__':
    main()
</code></pre>
<p>As I commented, I used a <code>main</code> function with the standard guard against being invoked on <code>import</code>, as Windows <em>simulates</em> forking by reimporting the main module (but not naming it <code>__main__</code>); without the guard, you can end up with the child process creating new processes automatically, which is problematic.</p>
<p>Side-note: If you are dispatching a bunch of tasks but not waiting on them immediately (so you don't want to terminate the pool anywhere near when you create it, but want to ensure the workers are cleaned up promptly), you can still use context management to help out. Just <a href="https://docs.python.org/3/library/contextlib.html#contextlib.closing" rel="nofollow noreferrer">use <code>contextlib.closing</code></a> to <code>close</code> the pool once all the tasks are dispatched; you must dispatch all the tasks before the end of the <code>with</code> block, but you can retrieve the results later, and when all results are computed, the child processes will close. For example:</p>
<pre><code>from contextlib import closing
from multiprocessing import Pool

def main():
    poolNum = int(sys.argv[1])

    with closing(Pool(poolNum)) as pool:  # Pool created
        results = pool.imap_unordered(processFunc, fileList)
    # close has been called, so no new work can be submitted,
    # and when all outstanding tasks complete, the workers will exit
    # immediately/cleanly

    for res in results:
        # Can still retrieve results even after pool is closed

# Adding main guard so this code is valid on Windows and anywhere else which
# doesn't use forking for whatever reason
if __name__ == '__main__':
    main()
</code></pre>
</div>
<span class="comment-copy">I think you've created a multiprocessing bomb</span>
<span class="comment-copy">@juanpa.arrivillaga do you have any solutions? should I still use multiprocessing lib or change to another one which supports what I want?</span>
<span class="comment-copy">Thanks for your detailed answer! I'm a little confused about some points, just to ensure:1.if I just use 'with', all workers will be killed after ALL workers are finished(instead of killed separately right after a single one is finished) 2.my processFunc is actually writing processed result for each line into another file, does it still work for such situation? Thanks!</span>
<span class="comment-copy">@IHaveAProblem: 1) If you use plain <code>with</code>, they're all killed forcefully (via <code>terminate</code>) as soon as execution leaves the block (but since you used <code>map</code>, which doesn't return until all the work is done, that's fine). 2) It should work, though if they're writing to the same file, the output may not be ordered, and may interleave if you're not careful. Typically, if you're writing to a single file, it's best to have the function <i>return</i> the data to write, and have your main process perform the I/O from a single thread, to avoid those risks.</span>
<span class="comment-copy">"map, which doesn't return until all the work is done" does it mean that map is not a good choice for my situation? Should I use another function? Plus, I still didn't get the point of 'with'. killed separately right after finished or killed as a batch after all other processes finished? I ran the first snippet, got <code>AttributeError: __exit__</code>. for the second one, I wrote a simple processFunc to write things to txt, but there is no such file after py finished.</span>
<span class="comment-copy">@IHaveAProblem: Sigh. The first snippet assumed you were on Python 3. If you're not, you just have to simulate it, and it's possible <code>close</code> with <code>imap_unordered</code> behaved differently on Python 2. I'd just using the <code>closing</code> helper (to ensure processes are closed when they finish), but stick to <code>map</code>. I overcomplicated things for you, sorry. As for the difference between <code>close</code> and <code>terminate</code>, please read the docs. The <code>with</code> block without <code>closing</code> justs calls <code>terminate</code>, not when work is finished, but when the block is exited (you just won't exit the block until the work is finished).</span>
