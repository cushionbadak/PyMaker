<div class="post-text" itemprop="text">
<p>I have a large python dictionary of values (around 50 GB), and I've stored it as a JSON file. I am having efficiency issues when it comes to opening the file and writing to the file. I know you can use ijson to read the file efficiently, but how can I write to it efficiently? </p>
<p>Should I even be using a Python dictionary to store my data? Is there a limit to how large a python dictionary can be? (the dictionary will get larger). </p>
<p>The data basically stores the path length between nodes in a large graph. I can't store the data as a graph because searching for a connection between two nodes takes too long. </p>
<p>Any help would be much appreciated. Thank you!</p>
</div>
<div class="post-text" itemprop="text">
<p>Although it will truly depend on what operations you want to perform on your network dataset you might want to considering storing this as a pandas Dataframe and then write it to disk using Parquet or Arrow.</p>
<p>That data could then be loaded to networkx or even to Spark (GraphX) for any network related operations.</p>
<p>Parquet is compressed and columnar and makes reading and writing to files much faster especially for large datasets.</p>
<p>From the Pandas Doc:</p>
<blockquote>
<p>Apache Parquet provides a partitioned binary columnar serialization
  for data frames. It is designed to make reading and writing data
  frames efficient, and to make sharing data across data analysis
  languages easy. Parquet can use a variety of compression techniques to
  shrink the file size as much as possible while still maintaining good
  read performance.</p>
<p>Parquet is designed to faithfully serialize and de-serialize DataFrame
  s, supporting all of the pandas dtypes, including extension dtypes
  such as datetime with tz.</p>
</blockquote>
<p>Read further here: <a href="https://pandas.pydata.org/pandas-docs/stable/io.html#io-parquet" rel="nofollow noreferrer">Pandas Parquet</a></p>
</div>
<div class="post-text" itemprop="text">
<p>try to use it with pandas: <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html" rel="nofollow noreferrer">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html</a></p>
<pre><code>pandas.read_json(path_or_buf=None, orient=None, typ='frame', dtype=True, convert_axes=True, convert_dates=True, keep_default_dates=True, numpy=False, precise_float=False, date_unit=None, encoding=None, lines=False, chunksize=None, compression='infer')
Convert a JSON string to pandas object
</code></pre>
<p>it very lightweight and useful library to work with large data</p>
</div>
<span class="comment-copy">To check the max size of lists, strings, dicts, and many other containers can have you can use: <code>&gt;&gt;&gt; import sys &gt;&gt;&gt; print sys.maxsize</code> - <a href="https://docs.python.org/3/library/sys.html#sys.maxsize" rel="nofollow noreferrer">docs.python.org/3/library/sys.html#sys.maxsize</a>. Regarding the most efficient way to write, not a complete solution, but I'd split the big file into smaller ones, you can try using the first two key letters as filename, this way will be able reduce the IO load.</span>
<span class="comment-copy">How about using generators, they are good with handling large(infinite they say) volumes of data.</span>
<span class="comment-copy">Use a database, create your own database engine or store the top level keys as separate files. Reading and writing to a 50GB file is no where near ideal.</span>
<span class="comment-copy">what @Torxed said + I would use elasticsearch to index everything and optimize searching.</span>
<span class="comment-copy">SQLite could be the right tool for the job. With a table like <code>CREATE TABLE distances (i INTEGER, j INTEGER, distance NUMBER);</code> and indexes on <code>i</code> and <code>j</code>, and an unique index over <code>i, j</code>, you can get pretty fast random access without having to load everything to memory. (Just be sure to only store and access <code>i, j</code> in sorted order.)</span>
<span class="comment-copy">The issue with a pandas dataframe is that I would need to load all of the data onto my RAM. Whereas, if I use an SQL database, I can just access the data when I need it</span>
<span class="comment-copy">True. If memory is going to be an issue the only choice is some sort of database or a distributed computing framework like Hadoop. Either way Parquet \ Arrow will help you with serialization speed to and from disk.</span>
