<div class="post-text" itemprop="text">
<p>Tried using beautifulsoup to scrape a shopify site, using <code>findAll('url')</code> returns an empty list. How do I retrieve the desired content? </p>
<pre><code>import requests
from bs4 import BeautifulSoup as soupify
import lxml

webSite = requests.get('https://launch.toytokyo.com/sitemap_pages_1.xml')
pageSource = webSite.text
webSite.close()

pageSource = soupify(pageSource, "xml")
print(pageSource.findAll('url'))
</code></pre>
<p>The page that I'm trying to scrape: <a href="https://launch.toytokyo.com/sitemap_pages_1.xml" rel="nofollow noreferrer">https://launch.toytokyo.com/sitemap_pages_1.xml</a></p>
<p><strong>What I'm getting:</strong> an empty list</p>
<p><strong>What I should be getting:</strong> not an empty list</p>
<p>Thanks everyone for helping, figured out the problem in my code, I was using an older version of findAll instead of find_all</p>
</div>
<div class="post-text" itemprop="text">
<p>You can do:</p>
<pre><code>import requests
from bs4 import BeautifulSoup as bs

url = 'https://launch.toytokyo.com/sitemap_pages_1.xml'

soup = bs(requests.get(url).content,'html.parser')


urls = [i.text for i in soup.find_all('loc')]
</code></pre>
<p>So basically I get the contents and locate loc tag that contains the urls, then I grab the content ;)</p>
<p>Updated: Required url tag and generate a dictionary </p>
<pre><code>urls = [i for i in soup.find_all('url')]

s = [[{k.name:k.text} for k in urls[i] if not isinstance(k,str)] for i,_ in enumerate(urls)]
</code></pre>
<p>Use from pprint import pprint as print to get a beautiful print of s:</p>
<pre><code>print(s)
</code></pre>
<p>Notes: you can use lxml parser as it is faster than html.parser</p>
</div>
<div class="post-text" itemprop="text">
<p>Another way using xpath</p>
<pre><code>import requests
from lxml import html
url = 'https://launch.toytokyo.com/sitemap_pages_1.xml'
tree = html.fromstring( requests.get(url).content)
links = [link.text for link in tree.xpath('//url/loc')]
print(links)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>As an alternative to <code>BeautifulSoup</code>, you can always use <a href="https://docs.python.org/3/library/xml.etree.elementtree.html" rel="nofollow noreferrer"><code>xml.etree.ElementTree</code></a> to parse your XML urls located at the <code>loc</code> tag:</p>
<pre><code>from requests import get
from xml.etree.ElementTree import fromstring, ElementTree
from pprint import pprint

url = 'https://launch.toytokyo.com/sitemap_pages_1.xml'

req = get(url)
tree = ElementTree(fromstring(req.text))

urls = []
for outer in tree.getroot():
    for inner in outer:
        namespace, tag = inner.tag.split("}")
        if tag == 'loc':
            urls.append(inner.text)

pprint(urls)
</code></pre>
<p>Which will give the following URLs in a list:</p>
<pre><code>['https://launch.toytokyo.com/pages/about',
 'https://launch.toytokyo.com/pages/help',
 'https://launch.toytokyo.com/pages/terms',
 'https://launch.toytokyo.com/pages/visit-us']
</code></pre>
<p>From this, you can group your info into a <a href="https://docs.python.org/3/library/collections.html#collections.defaultdict" rel="nofollow noreferrer"><code>collections.defaultdict</code></a>:</p>
<pre><code>from requests import get
from xml.etree.ElementTree import fromstring, ElementTree
from collections import defaultdict
from pprint import pprint

url = 'https://launch.toytokyo.com/sitemap_pages_1.xml'

req = get(url)
tree = ElementTree(fromstring(req.text))

data = defaultdict(dict)
for i, outer in enumerate(tree.getroot()):
    for inner in outer:
        namespace, tag = inner.tag.split("}")
        data[i][tag] = inner.text

pprint(data)
</code></pre>
<p>Which gives the following defaultdict of dictionaries with indices as keys:</p>
<pre><code>defaultdict(&lt;class 'dict'&gt;,
            {0: {'changefreq': 'weekly',
                 'lastmod': '2018-07-26T14:37:12-07:00',
                 'loc': 'https://launch.toytokyo.com/pages/about'},
             1: {'changefreq': 'weekly',
                 'lastmod': '2018-11-26T07:58:43-08:00',
                 'loc': 'https://launch.toytokyo.com/pages/help'},
             2: {'changefreq': 'weekly',
                 'lastmod': '2018-08-02T08:57:58-07:00',
                 'loc': 'https://launch.toytokyo.com/pages/terms'},
             3: {'changefreq': 'weekly',
                 'lastmod': '2018-05-21T15:02:36-07:00',
                 'loc': 'https://launch.toytokyo.com/pages/visit-us'}})
</code></pre>
<p>If you wish to instead group by categories, then you can use a defaultdict of lists instead:</p>
<pre><code>data = defaultdict(list)
for outer in tree.getroot():
    for inner in outer:
        namespace, tag = inner.tag.split("}")
        data[tag].append(inner.text)

pprint(data)
</code></pre>
<p>Which gives this different structure:</p>
<pre><code>defaultdict(&lt;class 'list'&gt;,
            {'changefreq': ['weekly', 'weekly', 'weekly', 'weekly'],
             'lastmod': ['2018-07-26T14:37:12-07:00',
                         '2018-11-26T07:58:43-08:00',
                         '2018-08-02T08:57:58-07:00',
                         '2018-05-21T15:02:36-07:00'],
             'loc': ['https://launch.toytokyo.com/pages/about',
                     'https://launch.toytokyo.com/pages/help',
                     'https://launch.toytokyo.com/pages/terms',
                     'https://launch.toytokyo.com/pages/visit-us']})
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I've tried to show exactly the way you have already tried. The only thing you  need to rectify is <code>webSite.text</code>. You could get valid response if you used <code>webSite.content</code> instead.</p>
<p>This is the corrected version of your existing attempt:</p>
<pre><code>import requests
from bs4 import BeautifulSoup

webSite = requests.get('https://launch.toytokyo.com/sitemap_pages_1.xml')
pageSource = BeautifulSoup(webSite.content, "xml")
for k in pageSource.find_all('url'):
    link = k.loc.text
    date = k.lastmod.text
    frequency = k.changefreq.text
    print(f'{link}\n{date}\n{frequency}\n')
</code></pre>
</div>
<span class="comment-copy">Can you post more code so wee can look where is the problem and give you a good answer? However, I can say maybe you don't have a tag url at root level, but it's a try not a sure answer</span>
<span class="comment-copy">Try changing “xml”, to “html.parser” and see what that does. I’m not near a computer to try it at the moment, but that’s first thing I’d do to see what’s returning.</span>
<span class="comment-copy">The second thing I’d look at is see if the page loads dynamically. If it is, look at using selenium or html-requests library.</span>
<span class="comment-copy">edited the post to include the whole code @lulian</span>
<span class="comment-copy">If you are using lxml for parsing you need to pass "lxml" as argument not "xml" and so you're bs object will use lxml. Let us know if this is enough to make everything work.</span>
<span class="comment-copy">Yeah I figured that finding 'loc' works, but I'm working on a monitor that posts to a discord, and some products have image links included, plus product name, so grabbing the whole 'url' content helps.</span>
<span class="comment-copy">Yes,you can do that too! Would you like me to update my code? You can change loc to url ;)</span>
<span class="comment-copy">Yes please. Previously I had to find the loc tag, the image link tag, and image title tag (which I used as the product name), which I combined as a dictionary in the format {prodName:(loc, imagelink)}. However I ran into problems when some products were sold out and were missing image links and image title, and I thought retrieving the whole thing would help make things easier as I can replace the missing items with random stuff.</span>
<span class="comment-copy">Ah, we can create a dictionary of each found tag. Updating code ...</span>
<span class="comment-copy">Updated as you desired</span>
<span class="comment-copy">Yeah this is nice, forgot about <code>xpath</code>.</span>
<span class="comment-copy">@PraysonW.Daniel Cheers. I thought it was a good idea to recommend the other option.</span>
<span class="comment-copy">I love your use of defaultdict;)</span>
<span class="comment-copy">@PraysonW.Daniel Thanks. I included two ways the OP could group his data with <code>defaultdict</code> if he decided to go down this path.</span>
