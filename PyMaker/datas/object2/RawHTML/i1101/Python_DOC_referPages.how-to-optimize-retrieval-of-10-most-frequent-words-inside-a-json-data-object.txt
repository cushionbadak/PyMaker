<div class="post-text" itemprop="text">
<p>I'm looking for ways to make the code more efficient (runtime and memory complexity)
Should I use something like a Max-Heap? 
Is the bad performance due to the string concatenation or sorting the dictionary not in-place or something else?
<strong>Edit: I replaced the dictionary/map object to applying a Counter method on a list of all retrieved names (with duplicates)</strong> </p>
<p><strong>minimal request:</strong>  script should take less then 30 seconds
 <strong>current runtime:</strong> it takes 54 seconds</p>
<pre><code>   # Try to implement the program efficiently (running the script should take less then 30 seconds)
import requests

# Requests is an elegant and simple HTTP library for Python, built for human beings.
# Requests is the only Non-GMO HTTP library for Python, safe for human consumption.
# Requests is not a built in module (does not come with the default python installation), so you will have to install it:
# http://docs.python-requests.org/en/v2.9.1/
# installing it for pyCharm is not so easy and takes a lot of troubleshooting (problems with pip's main version)
# use conda/pip install requests instead

import json

# dict subclass for counting hashable objects
from collections import Counter

#import heapq

import datetime

url = 'https://api.namefake.com'
# a "global" list object. TODO: try to make it "static" (local to the file)
words = []

#####################################################################################
# Calls the site http://www.namefake.com  100 times and retrieves random names
# Examples for the format of the names from this site:
# Dr. Willis Lang IV
# Lily Purdy Jr.
# Dameon Bogisich
# Ms. Zora Padberg V
# Luther Krajcik Sr.
# Prof. Helmer Schaden            etc....
#####################################################################################

requests.packages.urllib3.disable_warnings()

t = datetime.datetime.now()

for x in range(100):
    # for each name, break it to first and last name
    # no need for authentication
    # http://docs.python-requests.org/en/v2.3.0/user/quickstart/#make-a-request
    responseObj = requests.get(url, verify=False)

    # Decoding JSON data from returned response object text
    # Deserialize ``s`` (a ``str``, ``bytes`` or ``bytearray`` instance
    #    containing a JSON document) to a Python object.
    jsonData = json.loads(responseObj.text)
    x = jsonData['name']

    newName = ""
    for full_name in x:
        # make a string from the decoded python object concatenation
        newName += str(full_name)

    # split by whitespaces
    y = newName.split()

    # parse the first name (check first if header exists (Prof. , Dr. , Mr. , Miss)
    if "." in y[0] or "Miss" in y[0]:
        words.append(y[2])
    else:
        words.append(y[0])

    words.append(y[1])

# Return the top 10 words that appear most frequently, together with the number of times, each word appeared.
# Output example: ['Weber', 'Kris', 'Wyman', 'Rice', 'Quigley', 'Goodwin', 'Lebsack', 'Feeney', 'West', 'Marlen']
# (We don't care whether the word was a first or a last name)

# list of tuples
top_ten =Counter(words).most_common(10)

top_names_list = [name[0] for name in top_ten ]

print((datetime.datetime.now()-t).total_seconds())

print(top_names_list)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You are calling an endpoint of an API that generates dummy information <em>one</em> person at a time - that takes considerable amount of time. </p>
<p>The rest of the code is taking almost no time. </p>
<p>Change the endpoint you are using (there is no bulk-name-gathering on the one you use) or use built-in dummy data provided by python modules.</p>
<hr/>
<p>You can clearly see that "counting and processing names" is not the bottleneck here:</p>
<pre><code>from faker import Faker          # python module that generates dummy data
from collections import Counter
import datetime
fake = Faker()
c = Counter()

# get 10.000 names, split them and add 1st part
t = datetime.datetime.now() 
c.update( (fake.name().split()[0] for _ in range(10000)) )

print(c.most_common(10))
print((datetime.datetime.now()-t).total_seconds())
</code></pre>
<p>Output for 10000 names:</p>
<pre><code>[('Michael', 222), ('David', 160), ('James', 140), ('Jennifer', 134), 
 ('Christopher', 125), ('Robert', 124), ('John', 120), ('William', 111), 
 ('Matthew', 111), ('Lisa', 101)]
</code></pre>
<p>in</p>
<pre><code>1.886564 # seconds
</code></pre>
<hr/>
<p>General advise for code optimization: <em>measure first</em> then optimize the <em>bottlenecks</em>. </p>
<p>If you need a <em>codereview</em> you can check <a href="https://codereview.stackexchange.com/help/on-topic">https://codereview.stackexchange.com/help/on-topic</a> and see if your code fits with the requirements for the codereview stackexchange site. As with SO some effort should be put into the question first - i.e. analyzing <em>where</em> the majority of your time is being spent. </p>
<hr/>
<p>Edit - with performance measurements:</p>
<pre><code>import requests
import json
from collections import defaultdict
import datetime


# defaultdict is (in this case) better then Counter because you add 1 name at a time
# Counter is superiour if you update whole iterables of names at a time 
d = defaultdict(int)

def insertToDict(n):
    d[n] += 1

url = 'https://api.namefake.com'
api_times = []
process_times = []
requests.packages.urllib3.disable_warnings()
for x in range(10):
    # for each name, break it to first and last name
    try:
        t = datetime.datetime.now()      # start time for API call
        # no need for authentication
        responseObj = requests.get(url, verify=False)
        jsonData = json.loads(responseObj.text)

        # end time for API call
        api_times.append( (datetime.datetime.now()-t).total_seconds() )
        x = jsonData['name']

        t = datetime.datetime.now()      # start time for name processing
        newName = ""
        for name_char in x:
            # make a string from the decoded python object concatenation
            newName = newName + str(name_char)

        # split by whitespaces
        y = newName.split()

        # parse the first name (check first if header exists (Prof. , Dr. , Mr. , Miss)
        if "." in y[0] or "Miss" in y[0]:
            insertToDict(y[2])
        else:
            insertToDict(y[0])
        insertToDict(y[1])

        # end time for name processing
        process_times.append( (datetime.datetime.now()-t).total_seconds() )
    except:
        continue

newA = sorted(d, key=d.get, reverse=True)[:10]
print(newA)
print(sum(api_times))
print(sum( process_times )) 
</code></pre>
<p>Output:</p>
<pre><code>['Ruecker', 'Clare', 'Darryl', 'Edgardo', 'Konopelski', 'Nettie', 'Price',
 'Isobel', 'Bashirian', 'Ben']
6.533625
0.000206
</code></pre>
<p>You can make the parsing part better .. I did not, because it does not matter. </p>
<hr/>
<p>It is better to use <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow noreferrer">timeit</a> for performance testing (it calls code multiple times and averages, smoothing artifacts due to caching/lag/...) (thx @<a href="https://stackoverflow.com/users/41316/bruno-desthuilliers">bruno desthuilliers</a> ) - in this case I did not use timeit because I do not want to call API 100000 times to average results </p>
</div>
<span class="comment-copy">use collections.Counter?</span>
<span class="comment-copy">SO is for non-working code. Workiung code reviews <i>may</i> be on topic on codereview.stackexchange.com - read and heed their FAQ before posting.</span>
<span class="comment-copy">I tried by it didn't work / still ran slow</span>
<span class="comment-copy">@PatrickArtner , I didn't know. thanks for informing me</span>
<span class="comment-copy">For performance optimization use profiling - see where your code is slow - if calling an API 100 times takes 70s and the rest of your code is ok, look for other API-endpoint</span>
<span class="comment-copy">well,I was requested to use the names from this website. and you say thats the problem?</span>
<span class="comment-copy">Because it doesn't support bulk queries?</span>
<span class="comment-copy">@generationX No. What I tell you is: you have to analyze where the bottlenecks are. See edit. When calling the API for 10 names it takes <code>6.5</code> seconds ...the (inefficient -to be optimized-) analysis and counting code for the API-Result takes <code>0.0002</code> seconds ...where do you think you can optimize?</span>
<span class="comment-copy">@PatrickArtner for benchmarks, better to use the <code>timeit</code> module.</span>
<span class="comment-copy">@PatrickArtner not necessarily indeed - but I thought it was worth mentionning nonetheless for readers that don't know about this module.</span>
