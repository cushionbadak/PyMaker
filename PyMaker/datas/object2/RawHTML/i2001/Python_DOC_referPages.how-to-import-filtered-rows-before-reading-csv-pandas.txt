<div class="post-text" itemprop="text">
<p>Hi I have to upload large number of csv files in pandas dataframe. Can I filter out data from these csv files before loading it so as I dont get any memory error.</p>
<p>I the existing set up it gives me memory error</p>
<p>I have a column Location which has 32 values but I only want 3-4 locations to be filtered before importing.</p>
<p>Is this possible?</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use the <a href="https://docs.python.org/3/library/csv.html" rel="nofollow noreferrer">csv library</a> to read line by line and keep only the records you need:</p>
<pre><code>import csv
with open('names.csv', newline='') as csvfile:
     reader = csv.DictReader(csvfile)
     for row in reader:
         print(row['first_name'], row['last_name'])
</code></pre>
<p>After that you can save your filtered rows to csv files using <a href="https://docs.python.org/3/library/csv.html#csv.writer" rel="nofollow noreferrer">writerow</a></p>
</div>
<span class="comment-copy">Check <code>usecols</code> , you can define which columns to import. check <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" rel="nofollow noreferrer">this</a> . Also if you want a large file to be imported, there is something called <code>chunks</code> which reads the values in chunks. All examples already exist in this platform :)</span>
<span class="comment-copy">I am already using usecols for using limited columns but I need to filter out rows. I have around 50M rows out of which I need to filter to work upon</span>
<span class="comment-copy">For that you can process in chunks. something like <code>chunksize = 5000 for chunk in pd.read_csv(filename, chunksize=chunksize):     process(chunk)</code> and then do your manipulation</span>
<span class="comment-copy">How often do you need to do this? Every day? Just once? If the answer is more than once maybe you should consider exporting your data to a database and use SQL-language to load the frames.</span>
