<div class="post-text" itemprop="text">
<p>I have a <strong>speed/efficiency</strong> related question about <em>Python</em>:</p>
<p>I need to extract multiple fields from a nested JSON File (after writing to the <code>.txt</code> files, they have ~<strong>64k</strong> lines and the current snippet does it in ~ <strong>9 mins</strong>),
where each line can contain floats and strings.</p>
<p>Normally, I would just put all my data in <code>numpy</code> and use <code>np.savetxt()</code> to save it.. </p>
<p>I have resorted to simply assembling the lines as strings, but this is a <strong>tad slow</strong>. So far I'm doing:</p>
<ul>
<li>Assemble each line as a string(extract the desired field from JSON)</li>
<li>Write string to the concerned file</li>
</ul>
<p>I have several problems with this: 
- it's leading to more separate <code>file.write()</code> commands, which are very slow as well..(around 64k * 8 calls (for 8 files))</p>
<p>So my question is: </p>
<ul>
<li>What is a good routine for this kind of problem? One that balances out <code>speed vs memory-consumption</code> for most efficient writing to disk.</li>
<li>Should i increase my <code>DEFAULT_BUFFER_SIZE</code>? (its currently 8192)</li>
</ul>
<p>I have checked this <a href="https://stackoverflow.com/questions/3538156/file-i-o-in-every-programming-language">File I/O in Every Programming Language</a> and this <a href="https://docs.python.org/3/library/io.html" rel="nofollow noreferrer">python org: IO</a> but didn't help much except(in my understanding after going through it, file io should already be buffered in python 3.6.x) and I found that my default <code>DEFAULT_BUFFER_SIZE</code> is <code>8192</code>.</p>
<p>Thanks in advance for the help!!</p>
<p>Here's the part of My Snippet - </p>
<pre><code>def read_json_line(line=None):
    result = None
    try:        
        result = json.loads(line)
    except Exception as e:      
        # Find the offending character index:
        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      
        # Remove the offending character:
        new_line = list(line)
        new_line[idx_to_replace] = ' '
        new_line = ''.join(new_line)     
        return read_json_line(line=new_line)
    return result

def extract_features_and_write(path_to_data, inp_filename, is_train=True):
    # It's currently having 8 lines of file.write(), which is probably making it slow as writing to disk is  involving a lot of overheads as well
    features = ['meta_tags__twitter-data1', 'url', 'meta_tags__article-author', 'domain', 'title', 'published__$date',\
                'content', 'meta_tags__twitter-description']

    prefix = 'train' if is_train else 'test'

    feature_files = [open(os.path.join(path_to_data,'{}_{}.txt'.format(prefix, feat)),'w', encoding='utf-8')
                    for feat in features]

    with open(os.path.join(PATH_TO_RAW_DATA, inp_filename), 
              encoding='utf-8') as inp_json_file:
​
        for line in tqdm_notebook(inp_json_file):
            for idx, features in enumerate(features):
                json_data = read_json_line(line)  
​
                content = json_data['meta_tags']["twitter:data1"].replace('\n', ' ').replace('\r', ' ').split()[0]
                feature_files[0].write(content + '\n')
​
                content = json_data['url'].split('/')[-1].lower()
                feature_files[1].write(content + '\n')
​
                content = json_data['meta_tags']['article:author'].split('/')[-1].replace('@','').lower()
                feature_files[2].write(content + '\n')
​
                content = json_data['domain']
                feature_files[3].write(content + '\n')
​
                content = json_data['title'].replace('\n', ' ').replace('\r', ' ').lower()
                feature_files[4].write(content + '\n')
​
                content = json_data['published']['$date']
                feature_files[5].write(content + '\n')
​
                content = json_data['content'].replace('\n', ' ').replace('\r', ' ')
                content = strip_tags(content).lower()
                content = re.sub(r"[^a-zA-Z0-9]", " ", content)
                feature_files[6].write(content + '\n')
​
                content = json_data['meta_tags']["twitter:description"].replace('\n', ' ').replace('\r', ' ').lower()
                feature_files[7].write(content + '\n')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>From comment: </p>
<p>why do you think that 8 writes result in 8 physical writes to your harddisk? The file object itself buffers what to write, if it decides to write to your OS, your OS might as well wait a little until it physically writes - and even then your harrdrives got buffers that might keep the files content for a while until it starts to really write. See <a href="https://stackoverflow.com/questions/3167494/how-often-does-python-flush-to-a-file">How often does python flush to a file?</a></p>
<hr/>
<p>You should not use exceptions as control flow, nor recurse where it is not needed. Each recursion prepares new call stacks for the function call - that takes ressources and time - and all of it has to be reverted as well.</p>
<p>The best thing to do would be to clean up your data before feeding it into the json.load() ... the next best thing to do would be to avoid recursing ... try something along the lines of:</p>
<pre><code>def read_json_line(line=None):
    result = None

    while result is None and line: # empty line is falsy, avoid endless loop
        try:        
            result = json.loads(line)
        except Exception as e:
            result = None      
            # Find the offending character index:
            idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      
            # slice away the offending character:
            line = line[:idx_to_replace]+line[idx_to_replace+1:]

     return result
</code></pre>
</div>
<span class="comment-copy">why do you think that 8 writes result in 8 physical writes to your harddisk? The file object itself buffers what to write, if it decides to write to your OS, your OS might as well wait a little until it physically writes - and even then your harrdrives got buffers that might keep the files content for a while until it starts to really write ...</span>
<span class="comment-copy">See <a href="https://stackoverflow.com/questions/3167494/how-often-does-python-flush-to-a-file">how often does python flush a file</a></span>
<span class="comment-copy">Thanks Patrick, I will check them surely, any other way i can improve the speed? I agree with your comment but still 9 mins for writing 64k lines which don't have more than 30 words in them(except one) is still slow</span>
