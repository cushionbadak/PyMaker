<div class="post-text" itemprop="text">
<p>We've built a large python repo that uses lots of libraries (numpy, scipy, tensor flow, ...) And have managed these dependencies through a conda environment. Basically we have lots of developers contributing and anytime someone needs a new library for something they are working on they 'conda install' it. </p>
<p>Fast forward to today and now we need to deploy some applications that use our repo. We are deploying using docker, but are finding that these images are really large and causing some issues, e.g. 10+ GB. However each individual application only uses a subset of all the dependencies in the environment.yml. </p>
<p>Is there some easy strategy for dealing with this problem? In a sense I need to know the dependencies for each application, but I'm not sure how to do this in an automated way. </p>
<p>Any help here would be great. I'm new to this whole AWS, Docker, and python deployment thing... We're really a bunch of engineers and scientists who need to scale up our software. We have something that works, it just seems like there has to be a better way üòÅ.</p>
</div>
<div class="post-text" itemprop="text">
<p>First see if there are easy wins to shrink the image, like using Alpine Linux and being very careful about what gets installed with the OS package manager, and ensuring you only allow installing dependencies or recommended items when truly required, and that you clean up and delete artifacts like package lists, big things you may not need like Java, etc.</p>
<p>The base Anaconda/Ubuntu image is ~ 3.5GB in size, so it's not crazy that with a lot of extra installations of heavy third-party packages, you could get up to 10GB. In production image processing applications, I routinely worked with Docker images in the range of 3GB to 6GB, and those sizes were <em>after</em> we had heavily optimized the container.</p>
<p>To your question about splitting dependencies, you should provide each different application with its own package definition, basically a setup.py script and some other details, including dependencies listed in some mix of requirements.txt for pip and/or environment.yaml for conda.</p>
<p>If you have Project A in some folder / repo and Project B in another, you want people to easily be able to do something like <code>pip install &lt;GitHub URL to a version tag of Project A&gt;</code> or <code>conda env create -f ProjectB_environment.yml</code> or something, and voila, that application is installed.</p>
<p>Then when you deploy a specific application, have some CI tool like Jenkins build the container for that application using a <code>FROM</code> line to start from your thin Alpine / whatever container, and <em>only</em> perform conda install or pip install for the dependency file <em>for that project</em>, and not all the others.</p>
<p>This also has the benefit that multiple different projects can declare different version dependencies even among the same set of libraries. Maybe Project A is ready to upgrade to the latest and greatest pandas version, but Project B needs some refactoring before the team wants to test that upgrade. This way, when CI builds the container for Project B, it will have a Python dependency file with one set of versions, while in Project A's folder or repo of source code, it might have something different.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are many ways to tackle this problem:</p>
<ol>
<li><p>Lean docker images - start with a very simple base image; and layer your images. See <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/" rel="nofollow noreferrer">best practices for building images</a>.</p></li>
<li><p>Specify individual app requirements using <code>requirements.txt</code> files (make sure you <a href="https://nvie.com/posts/pin-your-packages/" rel="nofollow noreferrer">pin your versions</a>) and see <a href="https://conda.io/docs/user-guide/tasks/manage-pkgs.html#preventing-packages-from-updating-pinning" rel="nofollow noreferrer">specific instructions for conda</a>. </p></li>
<li><p>Build and install "on demand"; when you do docker build, only install those requirements for the specific applications and not one giant image for every possible eventuality.</p></li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p>I‚Äôd recommend:</p>
<ol>
<li>Make use of standard Python application packaging; your individual applications should be well-behaved Python packages that declare their own dependencies.</li>
<li>Each application becomes its own Docker image and manages its own dependencies.</li>
<li>Don‚Äôt try to maintain a giant all-the-dependencies base image.</li>
<li>Developers don‚Äôt have to use Docker for local development if they don‚Äôt want to.</li>
</ol>
<p>Python has good built-in support for applications declaring their own dependencies, and for running an application in an environment with the specific library dependencies it needs.  Your developers should <a href="https://docs.python.org/3/library/venv.html" rel="nofollow noreferrer">set up Python virtual environments</a> for each project they‚Äôre working on (or if they prefer one shared one for their local development).  There are <a href="https://packaging.python.org/discussions/install-requires-vs-requirements/#install-requires-vs-requirements-files" rel="nofollow noreferrer">two paths for a Python application to declare its dependencies</a>, and each project should declare its dependencies in its project-specific <code>setup.py</code> and/or <code>requirements.txt</code> files.</p>
<p>All of this is totally independent of Docker.  Many people <em>do</em> use Docker as part of their core development flow, but if you maintain good practices around packages declaring their dependencies correctly, this both helps building containers and helps a purely local development process.</p>
<p>When you get to deploying a Docker image, each application should have its own image.  The <code>Dockerfile</code> should be checked into source control, typically in the root directory of the project alongside the <code>setup.py</code> and/or <code>requirements.txt</code> files.  (It can be pretty boilerplate; the example in the <a href="https://hub.docker.com/_/python/" rel="nofollow noreferrer">Docker Hub python image documentation</a> will actually work fine for most applications.)</p>
<p>One giant base image with every conceivable dependency will cause problems.  Practically, I‚Äôve run into network errors with images and layers in the gigabyte+ range; longer-term, you‚Äôll run into trouble when two applications need incompatible versions of the same library.  I‚Äôd abandon this approach.</p>
<p>You might need to use some of the common tricks like multi-stage builds if you‚Äôre doing things like compiling numpy from source.  Your runtime images don‚Äôt need full C and FORTRAN toolchains and you don‚Äôt need to carry around the intermediate build artifacts.  Build the package in a base layer (maybe into a wheel, maybe installing it) and copy it into your final layer.</p>
</div>
<span class="comment-copy">what's your base image?May be you can try alpine image</span>
<span class="comment-copy">Ubuntu is the base image. It's not an easy change to use Alpine because I have to build a lot of massive libraries that are not available in our configuration through a package manager. That being said, it's still possible to use, I'll just have to spend the time to rebuild  all the dependencies.</span>
<span class="comment-copy">I haven't worked with Deep Learning libraries in containers yet, however i was able to tone down a container to run all the major ML libraries within 450MB. It probably included Scipy, Numpy, Pandas, Python3, Pip3, Sklearn, Flask and their associated dependencies.</span>
<span class="comment-copy"><code>docker images</code> tells me the base <code>ubuntu:18.04</code> image is only 84 MB.  IME a 1 GB image is unusually large; starting from an Ubuntu base certainly doesn‚Äôt automatically push you into the multi-gigabyte image space.</span>
<span class="comment-copy">@DavidMaze I would consider a 1GB image very, very small. It just depends on different use cases. In my world, common image sizes are 4GB+, even after spending months of time doing layer optimization, multi-stage builds, pushing dependencies out into volumes to be mounted rather than built-in, etc. The point is not to try to generalize because it varies hugely in different settings and use cases. For example, just look up the continuumio/anaconda3 image, which shows 3.6GB for me, freshly pulled.</span>
<span class="comment-copy">E.g. see the anaconda3 Dockerfile here: <a href="https://hub.docker.com/r/continuumio/anaconda3/~/dockerfile/" rel="nofollow noreferrer">hub.docker.com/r/continuumio/anaconda3/~/dockerfile</a> (this results in a 3.6GB image for me building it directly or pulling).</span>
<span class="comment-copy">Is there an automated way to extract dependencies for a given application? An application might import foo, and then foo imports bar, and it goes on and on. I've really only had success tracking down dependencies manually.</span>
<span class="comment-copy">When you are talking about <code>foo</code> or <code>bar</code>, do you mean your own applications or third-party packages? You don't need to worry about that for third party packages because when you e.g. <code>pip install foo</code> then part of installation will be to check <code>foo</code> dependencies, realize that you also need a particular version of <code>bar</code> for whatever version of <code>foo</code>, and then install that recursively if not already installed. Package managers like <code>pip</code> and <code>conda</code> automatically do this for you, and it is a primary benefit of declaring your internal applications with a requirements.txt or environment.yml file.</span>
<span class="comment-copy">For your own packages, the best way is: don't try to read code and track down dependencies. Instead, start converting to a package that declares dependencies on other internal and external packages, and then set up some basic unit tests that hit all the different source files. Do <code>pip install</code> or <code>conda install</code> for that application, run the tests, and observe if anything breaks due to 'module not found' errors, then add those to the dependency file, and if necessary (because they are other in-house programs) go and package them so they can be consumed as installable dependencies as well.</span>
<span class="comment-copy">We kind of got ourselves in a mess by having one massive requirements.yml and a single conda environment for all our modules and applications. This ended up working fine for our windows development machines. Now that we've started deploying, it's been a little troublesome. I ended up doing exactly what you proposed here (start with a blank environment and then just add back dependencies with conda install; it worked great). Now the images for applications are much more decomposed and manageable. Awesome stuff.</span>
<span class="comment-copy">We're also imposing a process change (as suggested) such that each developer while writing an application is responsible for the Dockerfile (and requirement.yml) moving forward</span>
