<div class="post-text" itemprop="text">
<p>I have to work on large 3D cubes of data. I want to store them in HDF5 files (using h5py or maybe pytables). I often want to perform analysis on just a section of these cubes. This section is too large to hold in memory. I would like to have a numpy style view to my slice of interest, without copying the data to memory (similar to what you could do with a numpy memmap). Is this possible? As far as I know, performing a slice using h5py, you get a numpy array in memory.</p>
<p>It has been asked why I would want to do this, since the data has to enter memory at some point anyway. My code, out of necessity, already run piecemeal over data from these cubes, pulling small bits into memory at a time. These functions are simplest if they simply iterate over the entirety of the datasets passed to them. If I could have a view to the data on disk, I simply could pass this view to these functions unchanged. If I cannot have a view, I need to write all my functions to only iterate over the slice of interest. This will add complexity to the code, and make it more likely for human error during analysis. </p>
<p>Is there any way to get a view to the data on disk, without copying to memory?</p>
</div>
<div class="post-text" itemprop="text">
<p>One possibility is to create a <a href="https://wiki.python.org/moin/Generators" rel="nofollow noreferrer">generator</a> that yields the elements of the slice one by one. Once you have such a generator, you can pass it to your existing code and iterate through the generator as normal. As an example, you can use a for loop on a generator, just as you might use it on a slice. Generators do not store all of their values at once, they 'generate' them as needed.</p>
<p>You might be able create a slice of just the locations of the cube you want, but not the data itself, or you could generate the next location of your slice programmatically if you have too many locations to store in memory as well. A generator could use those locations to yield the data they contain one by one.</p>
<p>Assuming your slices are the (possibly higher-dimensional) equivalent of cuboids, you might generate coordinates using nested <code>for</code>-<code>range()</code> loops, or by applying <code>product()</code> from the <a href="https://docs.python.org/3/library/itertools.html#itertools.product" rel="nofollow noreferrer"><code>itertools</code></a> module to range objects.</p>
</div>
<span class="comment-copy">Have you heard of <a href="http://pandas.pydata.org/" rel="nofollow noreferrer">pandas</a>. It can be very useful in <a href="http://pandas.pydata.org/pandas-docs/stable/io.html?highlight=hdf5#hdf5-pytables" rel="nofollow noreferrer">reading/writing an HDF5 store</a>?</span>
<span class="comment-copy">This is a follow up to my earlier question: <a href="http://stackoverflow.com/q/27710245/1361752">stackoverflow.com/q/27710245/1361752</a></span>
<span class="comment-copy">Yes, I'm quite familiar with pandas DataFrames (although, not so much their 3D functionality). However, that mostly works in-memory, correct? I know you can use pytables to copy the tables to hdf5 files. Is there a way to use this for the functionality I need?  Also, pandas usually provides high-level datatypes for tabular data I think. Isn't it overkill for simple arrays? That said, if it does what I need, I'd happily use it.</span>
<span class="comment-copy">If you need the entire subset for your processing to function and that subset doesn't fit in memory then I don't see how you can tackle it without updating your processing to work on a subset of the subset. Apart from that h5py supports numpy-like slicing which should function through hyperslab selections but I don't know enough about your data to say whether that's sufficient.</span>
<span class="comment-copy">For me this sounds like <code>dask</code> arrays would be what you're searching for (although I've never worked with it for serious applications I should say). It's designed among others for exactly that what you describe: data is too large to fit into memory and integrates with wellknown tools like <code>pandas</code> and <code>numpy</code>. see main website:<a href="http://dask.pydata.org/en/latest/docs.html" rel="nofollow noreferrer">dask.pydata.org/en/latest/docs.html</a> and how to create dask arrays, also from hfd5: <a href="http://dask.pydata.org/en/latest/array-creation.html" rel="nofollow noreferrer">dask.pydata.org/en/latest/array-creation.html</a>. At least in their docs they write "<i>changes the space limitation from “fits in memory” to “fits on disk”.</i>"</span>
