<div class="post-text" itemprop="text">
<p>Consider the following:</p>
<pre><code>tmp1 = ['a', 'b', 'c', 'd', 'e']
tmp2 = ['f', 'g', 'h', 'b', 'd']
tmp3 = ['b', 'i', 'j', 'k', 'l']
matr = np.array([tmp1, tmp2, tmp3])

matr
</code></pre>
<p>Yields a matrix:</p>
<pre><code>array([['a', 'b', 'c', 'd', 'e'],
   ['f', 'g', 'h', 'b', 'd'],
   ['b', 'i', 'j', 'k', 'l']], 
  dtype='|S1')
</code></pre>
<p>Now, I want to know the sum of values in each row that intersects a vector. Say, </p>
<pre><code>vec = ['a', 'c', 'f', 'b']
[sum([y in vec for y in row]) for row in matr]
</code></pre>
<p>Returns,</p>
<pre><code>[3, 2, 1]
</code></pre>
<p>This is the desired output. The problem with it is that my 'matr' is actually ≈ 1000000 x 2200, and I have 6700 vectors to compare against. The solution I have here is far too slow to attempt. </p>
<p>How can I improve what I'm doing? </p>
<p>It's worth noting that the values inside of the matr come from a set of ~30000 values, and I have the full set. I've considered solutions where I make a dict of these 30000 values against each vector, and use the dict to convert to True/False throughout the matrix before just summing by row. I'm not sure if this will help. </p>
</div>
<div class="post-text" itemprop="text">
<p>For <code>matr</code> and <code>vec</code> as arrays, here's one with <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html" rel="nofollow noreferrer"><code>np.searchsorted</code></a> -</p>
<pre><code>def count_in_rowwise(matr,vec):
    sidx = vec.argsort()
    idx = np.searchsorted(vec,matr,sorter=sidx)
    idx[idx==len(vec)] = 0
    return (vec[sidx[idx]] == matr).sum(1)
</code></pre>
<p>With a comparatively smaller <code>vec</code>, we can pre-sort it and use, to give us an alternative one to compute the row-counts, like so -</p>
<pre><code>def count_in_rowwise_v2(matr,vec,assume_sorted=False):
    if assume_sorted==1:
        sorted_vec = vec
    else:
        sorted_vec = np.sort(vec)
    idx = np.searchsorted(sorted_vec,matr)
    idx[idx==len(sorted_vec)] = 0
    return (sorted_vec[idx] == matr).sum(1)
</code></pre>
<p>The above solution(s) works on generic inputs(numbers or strings alike). To solve our specific case of strings, we could optimize it further by converting the strings to numbers by using <code>np.unique</code> and then re-using <code>count_in_rowwise/count_in_rowwise_v2</code> and that will give us our second approach, like so -</p>
<pre><code>u,ids = np.unique(matr, return_inverse=True)
out = count_in_rowwise(ids.reshape(matr.shape),ids[np.searchsorted(u,vec)])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You could use set intersection to speed things up a bit. Here's a comparison:</p>
<p>Your present solution with list comprehensions:</p>
<pre><code>%%timeit
print([sum([y in vec for y in row]) for row in matr])
#Output
[3,2,1]
20 µs ± 1.9 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
<p>Proposed solution with set intersection in list comprehension:</p>
<pre><code>%%timeit
print([len(set(row).intersection(vec)) for row in matr])
#Output:
[3,2,1]
17.8 µs ± 1.46 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
<p>And if the <code>vec</code> is also a set, we get even better efficiency:</p>
<pre><code>%%timeit
vec = {'a', 'c', 'f', 'b'}
print([len(set(row).intersection(vec)) for row in matr])
#Output:
[3, 2, 1]
16.6 µs ± 1.99 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's a simple readable solution with <code>np.isin()</code> (<a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.isin.html" rel="nofollow noreferrer">docs</a>):</p>
<pre class="lang-py prettyprint-override"><code>np.sum(np.isin(matr, vec), axis=1)
</code></pre>
<p>As a bonus, you can just use <code>np.isin()</code> without the summing if you want to get which elements of the matrix are in the vectors:</p>
<pre class="lang-py prettyprint-override"><code>&gt;&gt;&gt; np.isin(matr, vec)
array([[ True,  True,  True, False, False],
       [ True, False, False,  True, False],
       [ True, False, False, False, False]])
</code></pre>
<p>which shows why summing along the rows produces the desired output.</p>
</div>
<div class="post-text" itemprop="text">
<p>Let's look at the speed of your current algorithm. According to the python wiki, checking if an item is in an array like <code>y in vec</code> is O(n), meaning worst case, it has to go through every element in <code>vec</code>. Since you're doing that check for every element of your matrix, your total number of operations is <code>numRows * numCols * vecLen</code>, which is <code>O(n^3)</code>.</p>
<p>A faster way to do it is to construct a dictionary for <code>vec</code> to optimize lookups because dictionaries are <code>O(1)</code> instead of <code>O(n)</code>, meaning they can do your check in 1 operation, no matter how long vec is:</p>
<pre><code>vecDict = dict([(x, 1) for x in vec])
</code></pre>
<p>So, your new time complexity is <code>(numRows * numCols) + vecLen</code>, which is <code>O(n^2)</code>, which I think is as fast as you can get for your data.</p>
<pre><code>[sum([y in vecDict for y in row]) for row in matr]
</code></pre>
</div>
<span class="comment-copy">Would all elements be single characters in the actual use case?</span>
<span class="comment-copy">My fault in portraying it that way. Definitely not- closer to 12-13 characters.</span>
<span class="comment-copy">Are those <code>6700 vectors</code> of the same length? What's the typical length of those vectors?</span>
<span class="comment-copy">Unfortunately they're not. Some are as small as ~50, but they're all between 10 and 1000.</span>
<span class="comment-copy">While this seems interesting, I'm skeptical that it'll be faster... I'll benchmark tomorrow and get back to you for sure.</span>
<span class="comment-copy">@JohnRouhana Any particular reason to be skeptical? And faster than the original loopy one?</span>
<span class="comment-copy">It seemed like more steps, so I'm surprised that it's faster. I'm a bioinformatics guy, not pure computer science, so I don't really follow "why" it's so much faster than the other suggested methods. But it is. I ran benchmarks on a 1000x2200 matrix against 50 vectors. Going from my method to yours, the average time was brought down from ~4.9s/vector to ~0.088s/vector. That's approximately a 55x speedup- awesome.</span>
<span class="comment-copy">@JohnRouhana Good to get the feedback! Well I was banking on <code>searchsorted</code> because it's pretty efficient, just needs a bit more work for its sorted requirement. Also, if you are operating on the same <code>matr</code> against those 6700 vectors, I would suggest going with the suggestion at the end : <code>np.unique(</code>.. to pre-compute <code>u,ids</code> once and re-use it on those 6700 <code>vec</code>'s, to only use <code>count_in_rowwise(ids.reshape(matr.shape),ids[np.searchsorted(u,vec)])</code> for each new <code>vec</code>.</span>
<span class="comment-copy">Yes- this is precisely what I ended up doing. It still seems inconvenient to run against a 1,000,000x2200 matrix; by my projections, it would still take around 7 days to do this for all the vectors. That being said your solution has brought 100,000x2200 down to about 18 hours, which is totally acceptable. In most cases I won't need to run the full million.</span>
<span class="comment-copy">This solution becomes significantly more efficient than others if the matrix and vectors consist of sets in the first place.</span>
<span class="comment-copy">I can adjust the matrix and vectors to be sets instead of lists; all the values per row are unique, and the vector has unique values. This is promising. I'll benchmark with a larger matrix and vector tomorrow to see if this holds.</span>
<span class="comment-copy">I ran my benchmarks using this method against my original method using a larger matrix of 1000x2200 and 50 vectors. In practice, I didn't see a substantial difference between having a list vec and a set vec; I typically saw a difference of .0001 seconds per vector, with the set vec being consistently faster.  This method is a substantial improvement over what I had; I observed a 20-25 fold speedup at the matrix size above. Some of the other methods are benchmarking faster, though.</span>
<span class="comment-copy">You may also consider adding threading or multiprocessing into the mix and perform benchmarking on all the methods. That should give you the best possible results.</span>
<span class="comment-copy">This was going to be something I tried, but it turns out the system admin is running Numpy 1.11. This came out in 1.13. I'm going to bug him tomorrow to get us an update, and then I'll benchmark with the other solutions to see how we do.</span>
<span class="comment-copy">@JohnRouhana ah, boo! If you are indeed restricted to solutions no greater than a specific numpy version, then I would suggest to edit OP and add a tag with that constraint (for future readers).</span>
<span class="comment-copy">I definitely hear you. I'm glad this potential solution is here though, and I'm working on getting an update for numpy so I can see how this method pars up with the others. I'll update the OP if I'm not able to get a benchmark on this.</span>
<span class="comment-copy">I got this to work; by my benchmarks, this is the second best method suggested.</span>
<span class="comment-copy">Definitely valid point, and this is a solution I was considering before I saw the other solutions. While this is a substantial improvement over what I had originally (I observed a 10-15x speedup compared to my method when working with a matrix of 1000x2200), Divakar's and amanb's methods seem to be notably faster.</span>
<span class="comment-copy">Hey, if it's faster, go for it. I'm not as up on python's numpy library. It could be that even though they're doing more operations, they're doing it natively, so it might be more optimized.</span>
