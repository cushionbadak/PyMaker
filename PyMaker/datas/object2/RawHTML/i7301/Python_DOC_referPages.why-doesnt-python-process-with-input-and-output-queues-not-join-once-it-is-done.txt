<div class="post-text" itemprop="text">
<p>This simple Python3 program using <code>multiprocessing</code> does not seem to work as expected.</p>
<p>All the input processes share an input queue from which they consume data. They all share an output queue where they write a result once they are fully done. I find that this program hangs at the process <code>join()</code>. Why is that?</p>
<pre><code>#!/usr/bin/env python3

import multiprocessing

def worker_func(in_q, out_q):
    print("A worker has started")    
    w_results = {}
    while not in_q.empty():
        v = in_q.get()
        w_results[v] = v
    out_q.put(w_results)
    print("A worker has finished")

def main():

    # Input queue to share among processes
    fpaths = [str(i) for i in range(10000)]
    in_q = multiprocessing.Queue()
    for fpath in fpaths:
        in_q.put(fpath)

    # Create processes and start them
    N_PROC = 2
    out_q = multiprocessing.Queue()
    workers = []
    for _ in range(N_PROC):
        w = multiprocessing.Process(target=worker_func, args=(in_q, out_q,))
        w.start()
        workers.append(w)
    print("Done adding workers")

    # Wait for processes to finish
    for w in workers:
        w.join()
    print("Done join of workers")

    # Collate worker results
    out_results = {}
    while not out_q.empty():
        out_results.update(out_q.get())

if __name__ == "__main__":
    main()
</code></pre>
<p>I get this result from this program when <code>N_PROC = 2</code>:</p>
<pre><code>$ python3 test.py
Done adding workers
A worker has started
A worker has started
A worker has finished
&lt;---- I do not get "A worker has finished" from second worker
&lt;---- I do not get "Done join of workers"
</code></pre>
<p>It does not work even with a single child process <code>N_PROC = 1</code>:</p>
<pre><code>$ python3 test.py
Done adding workers
A worker has started
A worker has finished
&lt;---- I do not get "Done join of workers"
</code></pre>
<p>If I try a smaller input queue with say 1000 items, everything works fine.</p>
<p>I am aware of some old StackOverflow questions that say that the Queue has a limit. Why is this not documented in the Python3 docs?</p>
<p>What is an alternative solution I can use? I want to use multi-processing (not threading), to split the input among N processes. Once their shared input queue is empty, I want each process to collect its results (can be a big/complex data structure like dict) and return it back to the parent process. How to do this?</p>
</div>
<div class="post-text" itemprop="text">
<p>This is a classical bug caused by your design. When the worker are terminating, they stall because they have not been able to put all the data in the <code>out_q</code>, thus deadlocking your program. This has to do with size of the pipe buffer underlying your queue.</p>
<p>When you are using a <code>multiprocessing.Queue</code>, you should empty it before trying to join the feeder process, to make sure that the <code>Process</code> does not stall waiting for all the object to be put in the <code>Queue</code>. So putting your <code>out_q.get</code> call before the joinning the processes should solve your problem:. You can use a sentinel pattern to detect the end of the computations.</p>
<pre><code>#!/usr/bin/env python3

import multiprocessing
from multiprocessing.queues import Empty

def worker_func(in_q, out_q):
    print("A worker has started")    
    w_results = {}
    while not in_q.empty():
        try:
            v = in_q.get(timeout=1)
            w_results[v] = v
        except Empty:
            pass
    out_q.put(w_results)
    out_q.put(None)
    print("A worker has finished")

def main():

    # Input queue to share among processes
    fpaths = [str(i) for i in range(10000)]
    in_q = multiprocessing.Queue()
    for fpath in fpaths:
        in_q.put(fpath)

    # Create processes and start them
    N_PROC = 2
    out_q = multiprocessing.Queue()
    workers = []
    for _ in range(N_PROC):
        w = multiprocessing.Process(target=worker_func, args=(in_q, out_q,))
        w.start()
        workers.append(w)
    print("Done adding workers")

    # Collate worker results
    out_results = {}
    n_proc_end = 0
    while not n_proc_end == N_PROC:
        res = out_q.get()
        if res is None:
            n_proc_end += 1
        else:
            out_results.update(res)

    # Wait for processes to finish
    for w in workers:
        w.join()
    print("Done join of workers")

if __name__ == "__main__":
    main()
</code></pre>
<p>Also, note that your code has a race condition in it. The queue <code>in_q</code> can be emptied between the moment you check <code>not in_q.empty()</code> and the <code>get</code>. You should use a non blocking get to make sure you don't deadlock, waiting on an empty queue. </p>
<p>Finally, you are trying to implement something that look like a <code>multiprocessing.Pool</code>, which handle this kind of communication in a more robust way. you can also look at the <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer"><code>concurrent.futures</code></a> API, which is even more robust and in some sense, better designed.</p>
</div>
