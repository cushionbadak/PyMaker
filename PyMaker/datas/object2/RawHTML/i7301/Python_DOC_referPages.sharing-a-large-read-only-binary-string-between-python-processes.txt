<div class="post-text" itemprop="text">
<div class="question-status question-originals-of-duplicate">
<p>This question already has an answer here:</p>
<ul>
<li>
<a dir="ltr" href="/questions/659865/multiprocessing-sharing-a-large-read-only-object-between-processes">multiprocessing: sharing a large read-only object between processes?</a>
<span class="question-originals-answer-count">
                    8 answers
                </span>
</li>
</ul>
</div>
<p>I have a large, read-only <code>bytes</code> object that I need to operate against across several different Python (3) processes, with each one "returning" (adding to a result queue) a list of results based on their work.</p>
<p>Since this object is very large and read-only, I'd like to avoid copying it into the address space of each worker process. The research I've done suggests that shared memory is the right way to go about this, but I couldn't find a good resource/example of how exactly to do this with the <code>multiprocessing</code> module.</p>
<p>Thanks in advance.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use a <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Array" rel="nofollow noreferrer"><code>multiprocessing.Array</code></a>, which is like <a href="https://docs.python.org/3/library/ctypes.html#ctypes.Array" rel="nofollow noreferrer"><code>ctypes.Array</code></a> but for shared memory, when given a <code>ctypes</code> type.</p>
<pre><code># No lock needed, as no write will be done.
array = multiprocessing.Array(ctypes.c_char, long_byte_string, lock=False)
</code></pre>
<p>For example:</p>
<pre><code>&gt;&gt;&gt; import multiprocessing
&gt;&gt;&gt; import ctypes
&gt;&gt;&gt; array = multiprocessing.Array(ctypes.c_char, b'\x01\x02\xff\xfe', lock=False)
&gt;&gt;&gt; array[0]
b'\x01'
&gt;&gt;&gt; array[2:]
b'\xff\xfe'
&gt;&gt;&gt; array[:]
b'\x01\x02\xff\xfe'
&gt;&gt;&gt; b'\xff' in array
True
</code></pre>
</div>
<span class="comment-copy">What OS are you using?</span>
<span class="comment-copy">Linux (Ubuntu LTS). An ideal solution would work across Windows as well, but that can be sacrificed if necessary.</span>
<span class="comment-copy">Then just load your data and access it from the global namespace of your main process - on POSIX/fork-enabled systems <code>multiprocessing</code> just forks the current process so you can take the copy-on-write benefits. Just make sure you don't do anything to modify that data because at that point it will be copied to your sub-process stack.</span>
<span class="comment-copy">Thanks for the tip. I saw from some other SO questions that I can take advantage of CoW, <i>until</i> the Python runtime itself updates any metadata associated with the object (i.e., even if I don't modify the object itself). Is that a practical concern?</span>
<span class="comment-copy">That depends on the data... While there are a few scenarios that I know of, chances are that standard CPython won't be inclined to mess with statically accessed string/bytes structure initialized early on - I'd just avoid hard-slicing if you need a large chunks of the data later and use ranged iterators instead.</span>
<span class="comment-copy">Thanks for the response. Do you happen to know if <code>array[:]</code> accesses an internal (shared) representation, or instantiates the returned <code>bytes</code> within the child process?</span>
<span class="comment-copy">@woodruffw The internal representation is as a <code>multiprocessing.sharedctypes.c_char_Array_4</code>. Even a slicing on a regular bytes object creates a new object (i.e. <code>bytestring[1:] is not bytestring</code>). I assume it has to create a new bytes object to access the whole bytestring, but if you just take parts, it should not use as much memory. But this avoids pipes and things, as it is from shared memory.</span>
<span class="comment-copy">Thanks again. Unfortunately I need the entire object all at once, so I can't do much with the shared <code>Array</code> instance unless the internal representation is something I can use without creating new copies.</span>
<span class="comment-copy">@woodruffw I don't think it's possible to share Python objects accross processes, as processes have independent memory space. You can have (different) small objects that just refer to the same memory (Like this, think of it as a proxy), but you can't have to objects with the same memory location in different processes.</span>
<span class="comment-copy">Yeah, that's what I'm realizing. Thanks anyways.</span>
