<div class="post-text" itemprop="text">
<p>I have a gz file, and i want to extract the unique values from each column from the file, field separator is |, i tried using python as below. </p>
<pre><code>import sys,os,csv,gzip
from sets import Set
ig = 0
max_d = 1
with gzip.open("fundamentals.20170724.gz","rb") as f:
    reader = csv.reader(f,delimiter="|")
    for i in range(0,400):
        unique = Set()
        print "Unique_value for column "+str(i+1)
        flag = 0
        for line in reader:
            try:
                unique.add(line[i])
                max_d +=1
                if len(unique) &gt;= 10:
                    print unique
                    flag = 1
                    break
            except:
                continue
        if flag == 0: print unique
</code></pre>
<p>I don't find it efficient for large files, although it is working somehow, but seeking this problems from bash point of view.</p>
<p>any shell script solution? </p>
<p>for example i have the data in my file as </p>
<pre><code>5C4423,COMP,ISIN,CA2372051094,2016-04-19,
41C528,COMP,ISIN,US2333774071,2000-01-01,
B62545,COMP,ISIN,NL0000344265,2000-01-01,2007-05-11
9E7F41,COMP,ISIN,CA39260W1023,2013-02-13,2013-08-09
129DC8,COMP,ISIN,US37253A1034,2012-09-07,
4DE8CD,COMP,ISIN,QA000A0NCQB1,2008-03-06,
</code></pre>
<p>and in want all unique values from each column.</p>
</div>
<div class="post-text" itemprop="text">
<p>With the gunzipped file, you could do:</p>
<pre><code>awk -F, 'END { for (i=1;i&lt;=NF;i++) { print  "cut -d\",\" -f "i" filename | uniq" } }' filename | sh
</code></pre>
<p>Set the field separator to , and then for each field in the file, construct a cut command piping through uniq and finally pipe the whole awk response through sh. The use of cut, uniq and sh will slow things down and there is probably a more efficient way but it's worth a go.</p>
</div>
<div class="post-text" itemprop="text">
<p>A shell built pipeline could indeed do this job faster, though likely less memory efficient. The primary reasons are two: parallellism and native code. </p>
<p>First, since we have little description of the task, I'll have to read the Python code and figure out what it does. </p>
<p><code>from sets import Set</code> is an odd line; <a href="https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset" rel="nofollow noreferrer">sets</a> are part of the standard library, and I don't know what your <code>sets</code> module contains. I'll have to guess it's at best another name for the standard set type, or at least a less efficient variant of the same concept. </p>
<p><code>gzip.open</code> lets the script read a gzipped file. We can replace this with a <code>zcat</code> process. </p>
<p><code>csv.reader</code>reads character separated values, in this case splitting on <code>'|'</code>. Deeper inside the code we find only one column (<code>line[i]</code>) is read, so we can replace it with <code>cut</code> or <code>awk</code> ... until <code>i</code> changes. <code>awk</code> can handle that case too, but it's a little trickier. </p>
<p>The trickiest part is the end logic. Every time 10 unique values are found in a column, the program outputs those values and switches to the next column. By the way, <a href="https://docs.python.org/3/tutorial/controlflow.html#break-and-continue-statements-and-else-clauses-on-loops" rel="nofollow noreferrer">Python's <code>for</code> has an <code>else</code> clause specifically for this case</a>, so you don't need a <code>flag</code> variable. </p>
<p>One of the odder parts of the code is how you catch <em>all</em> exceptions from the inner data processing block. Why is this? There are basically only two sources of exceptions in there: Firstly, the indexing could fail if there aren't that many columns. Secondly, the unknown <code>Set</code> type could be throwing exceptions; the standard <code>set</code> type would not. </p>
<p>So, the analysis of your function is: in a diagonal manner (since the file is never rewound, and columns are not processed in parallel), collect unique values from each column until ten are found, and print them. This means, for instance, that if the first column had less than ten unique items nothing is ever printed for any other columns. I'm not sure this is the logic you intended. </p>
<p>With such complicated logic, Python's set functionality actually is a good choice; if we could partition the data more easily then <code>uniq</code> might have been better. What throws us off is how the program moves from column to column and only wants a specific number of values. </p>
<p>Thus, the two big time wasters in the Python program are decompressing in the same thread as we do all the parsing, and splitting into all columns when we only need one. The former can be addressed using a <a href="https://docs.python.org/3/library/threading.html" rel="nofollow noreferrer">thread</a>, and the latter is probably best done using a <a href="https://docs.python.org/3/library/re.html" rel="nofollow noreferrer">regular expression</a> such as <code>r'^(?:[^|]*\|){3}([^|]*)'</code>. That expression would skip three columns and the fourth can be read as group 1. It gets more complicated if the CSV has quoting to contain the separator within some column. We could do the line parsing itself in a separate thread, but that wouldn't solve the issue of the many unneeded string allocations. </p>
<p>Note that the problem actually becomes considerably different if what you really want is to process all columns from the start of the file. I also don't know why you specifically process 400 columns regardless of the amount that exist. If we remove those two constraints, the logic would be more like:</p>
<pre><code>firstline=next(reader)
sets = [{column} for column in firstline]
for line in reader:
    for column,columnset in zip(line,sets):
        columnset.add(column)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>this is a pure python version based on your idea:</p>
<pre><code>from io import StringIO
from csv import reader

txt = '''5C4423,COMP,ISIN,CA2372051094,2016-04-19,
41C528,COMP,ISIN,US2333774071,2000-01-01,
B62545,COMP,ISIN,NL0000344265,2000-01-01,2007-05-11
9E7F41,COMP,ISIN,CA39260W1023,2013-02-13,2013-08-09
129DC8,COMP,ISIN,US37253A1034,2012-09-07,
4DE8CD,COMP,ISIN,QA000A0NCQB1,2008-03-06,'''


with StringIO(txt) as file:
    rows = reader(file)
    first_row = next(rows)
    unique = [{item} for item in first_row]
    for row in rows:
        for item, s in zip(row, unique):
            s.add(item)
</code></pre>
<p>which yields for your input:</p>
<pre><code>[{'129DC8', '41C528', '4DE8CD', '5C4423', '9E7F41', 'B62545'},
 {'COMP'},
 {'ISIN'},
 {'CA2372051094',
  'CA39260W1023',
  'NL0000344265',
  'QA000A0NCQB1',
  'US2333774071',
  'US37253A1034'},
 {'2000-01-01', '2008-03-06', '2012-09-07', '2013-02-13', '2016-04-19'},
 {'', '2007-05-11', '2013-08-09'}]
</code></pre>
<p>oops, now that i have posted my answer i see, that this is exactly what <a href="https://stackoverflow.com/users/379311/yann-vernier">Yann Vernier</a> proposes at the end of <a href="https://stackoverflow.com/a/45325275/4954037">his answer</a>. please upvote this answer which was here way earlier than mine...</p>
<hr/>
<p>if you want to limit the number of unique values, you could use a <a href="https://docs.python.org/3/library/collections.html#collections.deque" rel="nofollow noreferrer"><code>deque</code></a> as data structure:</p>
<pre><code>from io import StringIO
from csv import reader

MAX_LEN = 3

with StringIO(txt) as file:
    rows = reader(file)
    first_row = next(rows)
    unique = [{item} for item in first_row]
    for row in rows:
        for item, s in zip(row, unique):
            if len(s) &lt; MAX_LEN:
                s.add(item)

print(unique)
</code></pre>
<p>with the result:</p>
<pre><code>[{'41C528', '5C4423', 'B62545'},
 {'COMP'},
 {'ISIN'},
 {'CA2372051094', 'NL0000344265', 'US2333774071'},
 {'2000-01-01', '2013-02-13', '2016-04-19'},
 {'', '2007-05-11', '2013-08-09'}]
</code></pre>
<p>this way you would save some memory if one of your columns holds only unique values.</p>
</div>
<span class="comment-copy">You parse all the values of each column in a lists ,one for each column and the with the command set(your_list)  you ll get the unique values for each column</span>
<span class="comment-copy">without an example input and desired corresponding output it is hard to guess what you want... please make the job of people trying to help easier by providing a <a href="https://stackoverflow.com/help/mcve">minimal, complete and verifiable example</a>.</span>
<span class="comment-copy">Why are you doing <code>from sets import Set</code>? That module is ancient, Python has had a built-in set type for ages, since Python 2.4. What Python version are you using? Also, please don't use bare <code>except</code>, use a named exception instead, otherwise you can catch things that you don't intend to catch. OTOH, I'm curious to know what you expect to catch in that <code>try.. except</code> block.</span>
<span class="comment-copy">@PM2Ring in try catch i want catch the error index out of range, let me check with the set type.</span>
<span class="comment-copy">@hiroprotagonist, thanks for the suggestion, will edit the question</span>
<span class="comment-copy">this is going to print all, not the unique one, btw thanks for the try</span>
<span class="comment-copy">As the values are placed in an array as indices, the printing of the indices will only print the unique entries.</span>
<span class="comment-copy">tried with the data as in example in question, giving all, not unique</span>
<span class="comment-copy">Well you said there was a | delimiter before you changed it to commas!!! I have amended.</span>
<span class="comment-copy">Boo, hiss for calling <code>system()</code> from <code>awk</code>. While here you aren't parameterizing any non-integer values in this particular example, in almost every real-world usage I've ever seen this tends to be prone to shell injection vulnerabilities (ie. passing filenames in such a way that a command embedded in the form of a command substitution in that filename will be run by the invoking shell).</span>
<span class="comment-copy">Apparently the <code>sets</code> module is in the library; it is a historical artifact, however, and less efficient than using the default set type. It was introduced in 2.3 and deprecated in 2.6, and I was reading more recent documentation.</span>
<span class="comment-copy">will it work for large data files?</span>
<span class="comment-copy">the iteration over the csv file is lazy; there is 1 row in memory at any given time; what will consume memory is the list of unique values. but that is an issue for all implementations (that iterate over the file once). so my guess is: yes, that should work nicely. why not give it a try on a large file?</span>
<span class="comment-copy">Well, it lacks an end condition, so any columns which have consistently unique values will be loaded in their entirety. The original code wouldn't hold more than 10. The exercise would be to make a limited size set; perhaps just wrapping the add call in an <code>if len(s)&lt;10</code>.</span>
<span class="comment-copy">@YannVernier added an update for that case.</span>
<span class="comment-copy">The <code>deque</code> isn't hashed, so it'll do linear searches for the <code>in</code> check. A set size check is just one int comparison. So this deque variant will be slower, particularly if MAX_LEN is large.</span>
