<div class="post-text" itemprop="text">
<p>A quick question about parallel processing in Python.  Lets say I have a big shared data structure and want to apply many functions on it in parallel.  These functions are read only on the data structure but perform mutation in a result object:</p>
<pre><code>def compute_heavy_task(self):
    big_shared_object = self.big_shared_object
    result_refs = self.result_refs
    for ref in result_refs:
         some_expensive_task(ref, big_shared_object)
</code></pre>
<p>How do I do these in parallel, say 5 at a time, or 10 at a time.  How how about number of processors at a time?</p>
</div>
<div class="post-text" itemprop="text">
<p>You cannot usefully do this with threads in Python (at least not the CPython implementation you're probably using). The Global Interpreter Lock means that, instead of the near-800% efficiency you'd like out of 8 cores, you only get 90%.</p>
<p>But you can do this with separate processes. There are two options for this built into the standard library: <a href="http://docs.python.org/3/library/concurrent.futures.html" rel="nofollow"><code>concurrent.futures</code></a> and <a href="http://docs.python.org/3/library/multiprocessing.html" rel="nofollow"><code>multiprocessing</code></a>. In general, <code>futures</code> is simpler in simple cases and often easier to compose; <code>multiprocessing</code> is more flexible and powerful in general. <code>futures</code> also only comes with Python 3.2 or later, but there's <a href="https://pypi.python.org/pypi/futures" rel="nofollow">a backport for 2.5-3.1 at PyPI</a>.</p>
<p>One of the cases where you want the flexibility of <code>multiprocessing</code> is when you have a big shared data structure. See <a href="http://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow">Sharing state between processes</a> and the sections directly above, below, and linked from it for details.</p>
<p>If your data structure is really simple, like a giant array of ints, this is pretty simple:</p>
<pre><code>class MyClass(object):
    def __init__(self, giant_iterator_of_ints):
        self.big_shared_object = multiprocessing.Array('i', giant_iterator_of_ints)
    def compute_heavy_task(self):
        lock = multiprocessing.Lock()
        def subtask(my_range):
            return some_expensive_task(self.big_shared_object, lock, my_range)
        pool = multiprocessing.pool.Pool(5)
        my_ranges = split_into_chunks_appropriately(len(self.big_shared_object)
        results = pool.map_async(subtask, my_ranges)
        pool.close()
        pool.join()
</code></pre>
<p>Note that the <code>some_expensive_task</code> function now takes a lock object—it has to make sure to acquire the lock around every access to the shared object (or, more often, every "transaction" made up of one or more accesses). Lock discipline can be tricky, but there's really no way around it if you want to use direct data sharing.</p>
<p>Also note that it takes a <code>my_range</code>. If you just call the same function 5 times on the same object, it'll do the same thing 5 times, which probably isn't very useful. One common way to parallelize things is to give each task a sub-range of the overall data set. (Besides being usually simple to describe, if you're careful with this, with the right kinds of algorithms, you can even avoid a lot of locking this way.)</p>
<p>If you instead want to map a bunch of different <em>functions</em> to the same <em>dataset</em>, you obviously need some collection of functions to work on, rather than just using <code>some_expensive_task</code> repeatedly. You can then, e.g., iterate over these functions calling <code>apply_async</code> on each one. But you can also just turn it around: write a single applier function, as a closure around the data, that takes takes a function and applies it to the data. Then, just <code>map</code> that function over the collection of functions.</p>
<p>I've also assumed that your data structure is something you can define with <code>multiprocessing.Array</code>. If not, you're going to have to design the data structure in C style, implement it as a <code>ctypes</code> <code>Array</code> of <code>Structure</code>s or vice-versa, and then use the <code>multiprocessing.sharedctypes</code> stuff.</p>
<p>I've also moved the result object into results that just get passed back. If they're also huge and need to be shared, use the same trick to make them sharable.</p>
<hr/>
<p>Before going further with this, you should ask yourself whether you really do need to share the data. Doing things this way, you're going to spend 80% of your debugging, performance-tuning, etc. time adding and removing locks, making them more or less granular, etc. If you can get away with passing immutable data structures around, or work on files, or a database, or almost any other alternative, that 80% can go toward the rest of your code.</p>
</div>
<span class="comment-copy">If you can give a concrete, simple, runnable example of the kind of code you want to parallelize, and explain what kind of parallelism you're looking for (just running the same function on the same data 5 times in a row doesn't seem very useful… unless there are side-effects you haven't described, which will make things more complicated), it will be a lot easier to give you a concrete, runnable example of a parallelized version.</span>
<span class="comment-copy">One more thing: Are you sure you need parallel processing? Is your code actually CPU-bound? Have you looked into alternatives (better algorithms, <code>numpy</code>, <code>pypy</code>, <code>cython</code>, …) that can give a much better performance gain for the same effort?</span>
<span class="comment-copy">theres also several pycloud type solutions ... but then you have internet latency ... but for huge datasets it might be worth it</span>
<span class="comment-copy">@abarnert do you have an example of hwo to do this?  Im reading the multiprocessing docs but none of it seems quite clear.</span>
<span class="comment-copy">@DavidWilliams: Since your question was pretty vague, the answer I gave is also pretty vague. You need to determine how to parallelize your dataset, and what kind of locking you need, and so on, before you can write the code. But I tried.</span>
<span class="comment-copy">@DavidWilliams: OK, I've changed it from "vague" to "rambling", trying to cover as many possibilities as possible. I'm not sure if that makes it more useful or less…</span>
<span class="comment-copy">@abarnert, nothing wrong with your answer. more like the python doc on multiprocessing.  Data set does not need to be parallelized.  thanks for the update.</span>
