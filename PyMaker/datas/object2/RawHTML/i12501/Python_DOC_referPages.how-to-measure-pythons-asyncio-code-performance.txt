<div class="post-text" itemprop="text">
<p>I can't use normal tools and technics to measure the performance of a coroutine because the time it takes at <code>await</code> should not be taken in consideration (or it should just consider the overhead of reading from the awaitable but not the IO latency).</p>
<p>So how do measure the time a coroutine takes ? How do I compare 2 implementations and find the more efficent ? What tools do I use ?</p>
</div>
<div class="post-text" itemprop="text">
<p><em>This answer originally contained two different solutions: the first one was based on monkey-patching and the second one does not work for python 3.7 and onward. This new version hopefully presents a better, more robust approach.</em></p>
<p>First off, standard timing tools such as <a href="https://en.wikipedia.org/wiki/Time_(Unix)" rel="nofollow noreferrer">time</a> can be used to determine the CPU time of a program, which is usually what we're interested in when testing the performance of an asynchronous application. Those measurements can also be performed in python using the <a href="https://docs.python.org/3/library/time.html#time.process_time" rel="nofollow noreferrer">time.process_time()</a> function: </p>
<pre><code>import time

real_time = time.time()
cpu_time = time.process_time()

time.sleep(1.)
sum(range(10**6))

real_time = time.time() - real_time
cpu_time = time.process_time() - cpu_time

print(f"CPU time: {cpu_time:.2f} s, Real time: {real_time:.2f} s")
</code></pre>
<p>See below the similar output produced by both methods:</p>
<pre><code>$ /usr/bin/time -f "CPU time: %U s, Real time: %e s" python demo.py
CPU time: 0.02 s, Real time: 1.02 s  # python output
CPU time: 0.03 s, Real time: 1.04 s  # `time` output
</code></pre>
<p>In an asyncio application, it might happen that some synchronous part of the program ends up performing a blocking call, effectively preventing the event loop from running other tasks. So we might want to record separately the time the event loop spends waiting from the time taken by other IO tasks. </p>
<p>This can be achieved by subclassing the <a href="https://docs.python.org/3/library/selectors.html#selectors.DefaultSelector" rel="nofollow noreferrer">default selector</a> to perform some timing operation and using a <a href="https://docs.python.org/3/library/asyncio-policy.html#asyncio-policies" rel="nofollow noreferrer">custom event loop policy</a> to set everything up. <a href="https://gist.github.com/vxgmichel/620eb3a02d97d3da9dacdc508a5d5321" rel="nofollow noreferrer">This code snippet</a> provides such a policy along with a context manager for printing different time metrics.</p>
<pre><code>async def main():
    print("~ Correct IO management ~")
    with print_timing():
        await asyncio.sleep(1)
        sum(range(10**6))
    print()

    print("~ Incorrect IO management ~")
    with print_timing():
        time.sleep(0.2)
        await asyncio.sleep(0.8)
        sum(range(10**6))
    print()

asyncio.set_event_loop_policy(TimedEventLoopPolicy())
asyncio.run(main(), debug=True)
</code></pre>
<p>Note the difference between those two runs:</p>
<pre><code>~ Correct IO management ~
CPU time:      0.016 s
Select time:   1.001 s
Other IO time: 0.000 s
Real time:     1.017 s

~ Incorrect IO management ~
CPU time:      0.016 s
Select time:   0.800 s
Other IO time: 0.200 s
Real time:     1.017 s
</code></pre>
<p>Also notice that the <a href="https://docs.python.org/3/library/asyncio-dev.html#asyncio-debug-mode" rel="nofollow noreferrer">asyncio debug mode</a> can detect those blocking operations:</p>
<pre><code>Executing &lt;Handle &lt;TaskWakeupMethWrapper object at 0x7fd4835864f8&gt;(&lt;Future finis...events.py:396&gt;) created at ~/miniconda/lib/python3.7/asyncio/futures.py:288&gt; took 0.243 seconds
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you only want to measure performance of "your" code, you could used approach similar to unit testing - just monkey-patch (even patch + Mock) the nearest IO coroutine with Future of expected result. </p>
<p>The main drawback is that e.g. http client is fairly simple, but let's say momoko (pg client)... it could be hard to do without knowing its internals, it won't include library overhead. </p>
<p>The pro are just like in ordinary testing:</p>
<ul>
<li>it's easy to implement, </li>
<li>it measures something ;), mostly one's implementation without overhead of third party libraries,</li>
<li>performance tests are isolated, easy to re-run,</li>
<li>it's to run with many payloads</li>
</ul>
</div>
<span class="comment-copy">I'll upvote because it will work, but it's very hacky and can break at the next python upgrade.</span>
<span class="comment-copy">@e-satis See my edit for another approach, less hackish.</span>
<span class="comment-copy">That's brilliant, thanks.</span>
