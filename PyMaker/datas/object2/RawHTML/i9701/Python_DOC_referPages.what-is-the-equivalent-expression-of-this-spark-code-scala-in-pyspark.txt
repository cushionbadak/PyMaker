<div class="post-text" itemprop="text">
<pre><code>val aggregatedBigrams = bigramTokens.reduceByKey({(x:(Int,Int), y:(Int,Int)) =&gt; (x._1+y._1, x._2+y._2)}, numReducers)
</code></pre>
<p>I've seen a lot of spark code writing as above, however I am truly confused by the use of underscore such as <code>x._1</code>, I searched on internet and was told the underscore means getting the element of a tuple, so I assume <code>x._1 = x(0)</code>, so in pyspark, should I write as <code>x[0]</code>?</p>
<p>Also, why should I write out the type like <code>x:(Int,Int)</code>? Do I have to do the similar thing in pyspark?</p>
</div>
<div class="post-text" itemprop="text">
<p>In Scala, the syntax <code>(x: T) =&gt; y</code> denotes an anonymous function, where the part before <code>=&gt;</code>, here <code>(x: T)</code>, determines the function's arguments, and the part after, here <code>y</code>, is the return value. In your example, the arguments are <code>(x:(Int,Int), y:(Int,Int))</code>, which means the function takes two arguments, <code>x</code> and <code>y</code>, both of which are expected to be 2-tuples on integer values. The return value is another 2-tuple of integer values.</p>
<p>The equivalent to a Scala anonymous function in Python is a <code>lambda</code> function. Defining a lambda function with two arguments looks like <code>lambda x, y: ...</code>. Python doesn't need specific types, so you don't have to specify the argument types explicitly to be tuples of integers like in Scala. Actually, with Python's duck typing philosophy you just care that whatever is passed in support the operators you use (indexing and addition). You can still give <a href="https://docs.python.org/3/library/typing.html" rel="nofollow noreferrer">type <em>hints</em></a> nowadays, but you don't have to.</p>
<p>As you said, tuple indexing in Python is done with <code>[i]</code>, so your full code would look like:</p>
<pre><code>aggregatedBigrams = bigramTokens.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]), numReducers)
</code></pre>
</div>
<span class="comment-copy">In general these questions indicate you have some reading up to do about Scala and Python, the difference between strong/weak, dynamic/static typing, etc.</span>
<span class="comment-copy">Thank you so much! I've also seen the type transformation in Scala like <code>val (totalUnigramsFG, _) = processedUnigrams.map{ x =&gt; (x._2._1.toLong, x._2._2.toLong) } .reduce{(x:(Long,Long), y:(Long,Long)) =&gt; (x._1+y._1, x._2+y._2) }</code>, do I have to explicitly do that in Python?</span>
<span class="comment-copy">I assume we also don't need <code>.toLong</code> in pyspark?</span>
<span class="comment-copy">Exactly, Python doesn't care about types so much and unified int and long, so you don't need that. Again, reading some basic introductions to Python and Scala answer these questions too ;)</span>
