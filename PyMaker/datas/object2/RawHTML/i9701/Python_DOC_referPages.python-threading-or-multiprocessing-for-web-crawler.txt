<div class="post-text" itemprop="text">
<p>I've made simple web-crawler with Python. So far everything it does it creates set of urls that should be visited, set of urls that was already visited. While parsing page it adds all the links on that page to the should be visited set and page url to the already visited set and so on while length of should_be_visited is &gt; 0. So far it does everything in one thread.</p>
<p>Now I want to add parallelism to this application, so I need to have same kind of set of links and few threads / processes, where each will pop one url from should_be_visited and update already_visited. I'm really lost at threading and multiprocessing, which I should use, do I need some Pools, Queues?</p>
</div>
<div class="post-text" itemprop="text">
<p>The rule of thumb when deciding whether to use threads in Python or not is to ask the question, whether the task that the threads will be doing, is that CPU intensive or I/O intensive. If the answer is I/O intensive, then you can go with threads. </p>
<p>Because of the GIL, the Python interpreter will run only one thread at a time. If a thread is doing some I/O, it will block waiting for the data to become available (from the network connection or the disk, for example), and in the meanwhile the interpreter will context switch to another thread. On the other hand, if the thread is doing a CPU intensive task, the other threads will have to wait till the interpreter decides to run them.</p>
<p>Web crawling is mostly an I/O oriented task, you need to make an HTTP connection, send a request, wait for response. Yes, after you get the response you need to spend some CPU to parse it, but besides that it is mostly I/O work. So, I believe, threads are a suitable choice in this case.</p>
<p>(And of course, respect the robots.txt, and don't storm the servers with too many requests :-)</p>
</div>
<div class="post-text" itemprop="text">
<p>Another alternative is asynchronous I/O, which is much better for this kind of I/O-bound tasks (unless processing a page is <em>really</em> expensive). You can try both with <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow noreferrer">asyncio</a> or <a href="http://www.tornadoweb.org/en/stable/" rel="nofollow noreferrer">Tornado</a>, using its <a href="http://www.tornadoweb.org/en/stable/httpclient.html" rel="nofollow noreferrer">httpclient</a>.</p>
</div>
<span class="comment-copy">For the set of URLs that are to be visited, are many of them for the same server, or are they all from different servers?</span>
<span class="comment-copy">All from the same</span>
<span class="comment-copy">OK, in that case you probably should not parallelise at all, unless the server is under your control and your server/network is able to handle the load. Rather than speeding up your requests, which will appear to the remote administrator as a denial of service attack, you should be putting a small delay between each request, perhaps of a second or two. The broad rule of scraping is <b>it should be done slowly, not quickly</b>, and a failure to observe this may put your server on an IP blocklist.</span>
