<div class="post-text" itemprop="text">
<p>I'm trying to scrape data from a website using Python. When I look at the source code in my browser, I can see all the things I'm looking for. However, when I download the code with BeautifulSoup, I only get parts of the data that I want.</p>
<p>I know BeautifulSoup doesn't work with javascript, (I don't know javascript at all) and I was wondering if there's a way to download all the raw source code (basically get a hard copy) even if parts of the site are done with javascript.</p>
<p>This is my code thus far:</p>
<pre><code> r = requests.get('https://www.example.com/example/example')
 data = BeautifulSoup(r.content)
 example1 = data.find_all("class_="example2") 
 examples = []
 for example in example1:
     examples.append(link.get('href'))
</code></pre>
<p>I know it's hard to thoroughly answer my question without the actual source code. Unfortunately I can't show the code! Hope it isn't too big of a problem.</p>
</div>
<div class="post-text" itemprop="text">
<p>Maybe not the best solution, but here are my 2 cents if you only want to parse links. </p>
<pre><code>import requests
from BeautifulSoup import BeautifulSoup
r = requests.get('http://www.examples.com')
data = BeautifulSoup(r.content)
examples = []
for d in data.findAll('a'):
    examples.append(d)
</code></pre>
<p>Of course you can search for class like: </p>
<pre><code>my_as = soup.findAll("a", { "class" : "someclass" })
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>It's hard to get a full copy of a dynamic (JavaScript based) website without a browser, that's why the easiest way of getting such a copy is with a browser.</p>
<p>You can look into <a href="http://phantomjs.org/" rel="nofollow noreferrer">PhantomJS</a> and <a href="https://selenium-python.readthedocs.io/" rel="nofollow noreferrer">Selenium</a> to control the browser and download the HTML, or just run PhantomJS with <a href="https://docs.python.org/3/library/subprocess.html" rel="nofollow noreferrer"><code>subprocess</code></a>.</p>
<p>A very simple PhantomJS script downloading the contents of a website:</p>
<pre><code>"use strict";
var fs = require('fs');
var system = require('system');
var webpage = require('webpage');
if (system.args.length != 2) {
    console.log('Usage: gethtml.js &lt;url&gt;');
    phantom.exit(1);
} else {
    var url = system.args[1];
    var page = webpage.create();
    page.open(url, function(status) {
        if (status !== 'success') {
            phantom.exit(2);
        }
        setTimeout(function() {
            console.log(page.content);
            phantom.exit();
        }, 500);
    });
}
</code></pre>
<p>This waits 500ms before "downloading", depending on site and internet connection this is not enough or way too much, you can improve it by waiting until PhantomJS stops downloading data for X seconds using the <code>onResourceReceived</code> callback.</p>
</div>
<div class="post-text" itemprop="text">
<p>Your code is wrong and makes no sense: </p>
<pre><code>r = requests.get('https://www.example.com/example/example')
data = BeautifulSoup(r.content)
aes = data.find_all("class_="example2") 
result = []
for a in aes:
    result.append(a.attrs['href'])
</code></pre>
</div>
<span class="comment-copy">you can use scrapy</span>
<span class="comment-copy">for example in examples when you declare examples as an empty list, will no make any loop</span>
<span class="comment-copy">You can use Python Selenium</span>
<span class="comment-copy">BeautifulSoup has no problem with javascript. It's just that it won't execute any of it. That's all.</span>
<span class="comment-copy">MadMike is correct, but it should be explained that sometimes content is rendered via the executed Javascript.  If that is the case, it would explain why you don't see it in BeautifulSoup.</span>
<span class="comment-copy">I got mixed up with the examples there sorry. my actual code work is similar to yours.</span>
<span class="comment-copy"><code>aes = data.find_all("class_="example2")</code> is missing a <code>"</code> after <code>class_</code></span>
