<div class="post-text" itemprop="text">
<p>Versions of this question have already been asked but I have not found a satisfactory answer.</p>
<p><strong>Problem</strong>: given a large numpy vector, find indices of the vector elements which are duplicated (a variation of that could be comparison with tolerance). </p>
<p>So the problem is ~O(N^2) and memory bound (at least from the current algorithm point of view). I wonder why whatever I tried Python is 100x or more slower than an equivalent C code.</p>
<pre><code>import numpy as np
N = 10000
vect = np.arange(float(N))
vect[N/2] = 1
vect[N/4] = 1
dupl = []
print("init done")
counter = 0
for i in range(N):
    for j in range(i+1, N):
        if vect[i] == vect[j]:
            dupl.append(j)
            counter += 1

print("counter =", counter)
print(dupl)
# For simplicity, this code ignores repeated indices 
# which can be trimmed later. Ref output is
# counter = 3
# [2500, 5000, 5000]
</code></pre>
<p>I tried using numpy iterators but they are even worse (~ x4-5)
<a href="http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html" rel="nofollow">http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html</a></p>
<p>Using N=10,000 I'm getting 0.1 sec in C, 12 sec in Python (code above), 40 sec in Python using np.nditer, 50 sec in Python using np.ndindex. I pushed it to N=160,000 and the timing scales as N^2 as expected.</p>
</div>
<div class="post-text" itemprop="text">
<p>Since the answers have stopped coming and none was totally satisfactory, for the record I post my own solution.</p>
<p>It is my understanding that it's the assignment which makes Python slow in this case, not the nested loops as I thought initially. Using a library or compiled code eliminates the need for assignments and performance improves dramatically.</p>
<pre><code>from __future__ import print_function
import numpy as np
from numba import jit

N = 10000
vect = np.arange(N, dtype=np.float32)

vect[N/2] = 1
vect[N/4] = 1
dupl = np.zeros(N, dtype=np.int32)

print("init done")
# uncomment to enable compiled function
#@jit
def duplicates(i, counter, dupl, vect):
    eps = 0.01
    ns = len(vect)
    for j in range(i+1, ns):
        # replace if to use approx comparison
        #if abs(vect[i] - vect[j]) &lt; eps:
        if vect[i] == vect[j]:
            dupl[counter] = j
            counter += 1
    return counter

counter = 0
for i in xrange(N):
    counter = duplicates(i, counter, dupl, vect)

print("counter =", counter)
print(dupl[0:counter])
</code></pre>
<p>Tests</p>
<pre><code># no jit
$ time python array-test-numba.py
init done
counter = 3
[2500 5000 5000]

elapsed 10.135 s

# with jit
$ time python array-test-numba.py
init done
counter = 3
[2500 5000 5000]

elapsed 0.480 s
</code></pre>
<p>The performance of compiled version (with @jit uncommented) is close to C code performance ~0.1 - 0.2 sec. Perhaps eliminating the last loop could improve the performance even further. The difference in performance is even stronger when using approximate comparison using eps while there is very little difference for the compiled version.</p>
<pre><code># no jit
$ time python array-test-numba.py
init done
counter = 3
[2500 5000 5000]

elapsed 109.218 s

# with jit
$ time python array-test-numba.py
init done
counter = 3
[2500 5000 5000]

elapsed 0.506 s
</code></pre>
<p>This is ~ 200x difference. In the real code, I had to put both loops in the function as well as use a function template with variable types so it was a bit more complex but not very much.</p>
</div>
<div class="post-text" itemprop="text">
<p>Python itself is a highly-dynamic, slow, language. The idea in numpy is to use <a href="https://www.safaribooksonline.com/library/view/python-for-data/9781449323592/ch04.html" rel="nofollow">vectorization</a>, and avoid explicit loops. In this case, you can use <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.ufunc.outer.html" rel="nofollow"><code>np.equal.outer</code></a>. You can start with</p>
<pre><code>a = np.equal.outer(vect, vect)
</code></pre>
<p>Now, for example, to find the sum:</p>
<pre><code> &gt;&gt;&gt; np.sum(a)
 10006
</code></pre>
<p>To find the indices of <em>i</em> that are equal, you can do</p>
<pre><code>np.fill_diagonal(a, 0)

&gt;&gt;&gt; np.nonzero(np.any(a, axis=0))[0]
array([   1, 2500, 5000])
</code></pre>
<hr/>
<p><strong>Timing</strong></p>
<pre><code>def find_vec():
    a = np.equal.outer(vect, vect)
    s = np.sum(a)
    np.fill_diagonal(a, 0)
    return np.sum(a), np.nonzero(np.any(a, axis=0))[0]

&gt;&gt;&gt; %timeit find_vec()
1 loops, best of 3: 214 ms per loop

def find_loop():
    dupl = []
    counter = 0
    for i in range(N):
        for j in range(i+1, N):
             if vect[i] == vect[j]:
                 dupl.append(j)
                 counter += 1
    return dupl

&gt;&gt;&gt; % timeit find_loop()
1 loops, best of 3: 8.51 s per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The obvious question is why you want to do this in this way. NumPy arrays are intended to be opaque data structures â€“ by this I mean NumPy arrays are intended to be created inside the NumPy system and then operations sent in to the NumPy subsystem to deliver a result. i.e. NumPy should be a black box into which you throw requests and out come results.</p>
<p>So given the code above I am not at all suprised that NumPy performance is worse than dreadful.</p>
<p>The following should be effectively what you want, I believe, but done the NumPy way:</p>
<pre><code>import numpy as np

N = 10000
vect = np.arange(float(N))
vect[N/2] = 1
vect[N/4] = 1

print([np.where(a == vect)[0] for a in vect][1])

# Delivers [1, 2500, 5000]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This solution using the <a href="https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP" rel="nofollow">numpy_indexed</a> package has complexity n Log n, and is fully vectorized; so not terribly different from C performance, in all likelihood.</p>
<pre><code>import numpy_indexed as npi
dpl = np.flatnonzero(npi.multiplicity(vect) &gt; 1)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>As an alternative to Ami Tavory's answer, you can use a <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="nofollow">Counter</a> from the collections package to detect duplicates. On my computer it seems to be even faster. See the function below which can also find different duplicates.</p>
<pre><code>import collections
import numpy as np

def find_duplicates_original(x):
    d = []
    for i in range(len(x)):
        for j in range(i + 1, len(x)):
            if x[i] == x[j]:
                d.append(j)
    return d

def find_duplicates_outer(x):
    a = np.equal.outer(x, x)
    np.fill_diagonal(a, 0)
    return np.flatnonzero(np.any(a, axis=0))

def find_duplicates_counter(x):
    counter = collections.Counter(x)
    values = (v for v, c in counter.items() if c &gt; 1)
    return {v: np.flatnonzero(x == v) for v in values}


n = 10000
x = np.arange(float(n))
x[n // 2] = 1
x[n // 4] = 1

&gt;&gt;&gt;&gt; find_duplicates_counter(x)
{1.0: array([   1, 2500, 5000], dtype=int64)}

&gt;&gt;&gt;&gt; %timeit find_duplicates_original(x)
1 loop, best of 3: 12 s per loop

&gt;&gt;&gt;&gt; %timeit find_duplicates_outer(x)
10 loops, best of 3: 84.3 ms per loop

&gt;&gt;&gt;&gt; %timeit find_duplicates_counter(x)
1000 loops, best of 3: 1.63 ms per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>Approach #1</strong></p>
<p>You can simulate that iterator dependency criteria for a vectorized solution using a <code>triangular matrix</code>. This is based on <a href="https://stackoverflow.com/a/36045511/3293881"><code>this post</code></a> that dealt with multiplication involving <code>iterator dependency</code>. For performing the elementwise equality of each element in <code>vect</code> against its all elements, we can use <code>NumPy broadcasting</code>. Finally, we can use <code>np.count_nonzero</code> to get the count, as it's supposed to be very efficient in summing purposes on boolean arrays.</p>
<p>So, we would have a solution like so -</p>
<pre><code>mask = np.triu(vect[:,None] == vect,1)
counter = np.count_nonzero(mask)
dupl = np.where(mask)[1]
</code></pre>
<p>If you only care about the count <code>counter</code>, we could have two more approaches as listed next.</p>
<p><strong>Approach #2</strong></p>
<p>We can avoid the use of the triangular matrix and simply get the entire count and just subtract the contribution from diagonal elements and consider just one of either lower of upper triangular regions by just halving the remaining count as the contributions from either ones would be identical.</p>
<p>So, we would have a modified solution like so -</p>
<pre><code>counter = (np.count_nonzero(vect[:,None] == vect) - vect.size)//2
</code></pre>
<p><strong>Approach #3</strong></p>
<p>Here's an entirely different approach that uses the fact the count of each unique element plays a cumsumed contribution to the final total. </p>
<p>So, with that idea in mind, we would have a third approach like so -</p>
<pre><code>count = np.bincount(vect) # OR np.unique(vect,return_counts=True)[1]
idx = count[count&gt;1]
id_arr = np.ones(idx.sum(),dtype=int)
id_arr[0] = 0
id_arr[idx[:-1].cumsum()] = -idx[:-1]+1
counter = np.sum(id_arr.cumsum())
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This runs in 8 ms compared to 18 s for your code and doesn't use any strange libraries. It's similar to the approach by @vs0, but I like <code>defaultdict</code> more.  It should be approximately O(N).</p>
<pre><code>from collections import defaultdict
dupl = []
counter = 0
indexes = defaultdict(list)
for i, e in enumerate(vect):
    indexes[e].append(i)
    if len(indexes[e]) &gt; 1:
        dupl.append(i)
        counter += 1
</code></pre>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>I wonder why whatever I tried Python is 100x or more slower than an equivalent C code.</p>
</blockquote>
<p>Because Python programs are usually 100x slower than C programs.</p>
<p>You can either implement critical code paths in C and provide Python-C bindings, or change the algorithm. You can write an O(N) version by using a <code>dict</code> that reverses the array from value to index.</p>
<pre><code>import numpy as np
N = 10000
vect = np.arange(float(N))
vect[N/2] = 1
vect[N/4] = 1
dupl = {}
print("init done")
counter = 0
for i in range(N):
    e = dupl.get(vect[i], None)
    if e is None:
        dupl[vect[i]] = [i]
    else:
        e.append(i)
        counter += 1

print("counter =", counter)
print([(k, v) for k, v in dupl.items() if len(v) &gt; 1])
</code></pre>
<p>Edit:</p>
<p>If you need to test against an eps with abs(vect[i] - vect[j]) &lt; eps you can then normalize the values up to eps</p>
<pre><code>abs(vect[i] - vect[j]) &lt; eps -&gt;
abs(vect[i] - vect[j]) / eps &lt; (eps / eps) -&gt;
abs(vect[i]/eps - vect[j]/eps) &lt; 1
int(abs(vect[i]/eps - vect[j]/eps)) = 0
</code></pre>
<p>Like this:</p>
<pre><code>import numpy as np
N = 10000
vect = np.arange(float(N))
vect[N/2] = 1
vect[N/4] = 1
dupl = {}
print("init done")
counter = 0
eps = 0.01
for i in range(N):
    k = int(vect[i] / eps)
    e = dupl.get(k, None)
    if e is None:
        dupl[k] = [i]
    else:
        e.append(i)
        counter += 1

print("counter =", counter)
print([(k, v) for k, v in dupl.items() if len(v) &gt; 1])
</code></pre>
</div>
<span class="comment-copy">Because Python is slow?</span>
<span class="comment-copy">numpy arrays are efficient when using built-in numpy function (that are implemented in C). Python loops are slow whether you use numpy or not.  Try to implement your algorithm using only numpy functions. Using built-in Python functions and/or comprehensions should also increase performance (less than numpy but more than plain loops).</span>
<span class="comment-copy">As such, loops in Python are not bad. And what's so difficult about looping anyway. I suspect it's nested looping which is killing this code (creating another context?)</span>
<span class="comment-copy">I hate to answer my own question but I finally solved by resorting to Numba. Remembered it initially but then forgot. The timing is pretty much what C compiled code gives me and there is still one Python loop. So the outer loop is not a problem. I appreciate all the comments about using libraries. Indeed, they must be used in order to get the best performance. However I find it hard to remember all these calls and thinking in terms of code rather than libs is easier for me.</span>
<span class="comment-copy">That <code>nditer</code> page ends with a <code>cython</code> example.  That's were you gain some speed.  Otherwise <code>nditer</code> is just a way of handling broadcasting for multiple inputs and outputs.</span>
<span class="comment-copy">What is the speedup gain by using this approach?</span>
<span class="comment-copy">@vz0 Relative to the loop?</span>
<span class="comment-copy">OP posted in his question that it takes 12 seconds to run the pure python version. How much time would it take to run with your version?</span>
<span class="comment-copy">@vz0 See edit - loop takes by me around 8.5 secs, vs. 214 ms for the vectorized version.</span>
<span class="comment-copy">Is the vectorized version still O(N**2) ?</span>
<span class="comment-copy">nice one. i thought about np.where and tried it too. the problem is, i don't need 1 in the answer, only 2500 and 5000, ie repeats. secondly, the generalisation to near similarity is not so neat and a bit slower. but, yes, your one liner is as quick as C.</span>
<span class="comment-copy">Unfortunately this is not suitable on the same grounds as Ami Tavory's suggestion. Try N=160000</span>
<span class="comment-copy">unfortunately the dictionary approach won't work for me because actually i need to compare array elements within certain tolerance      abs(vect[i] - vect[j]) &lt; eps</span>
<span class="comment-copy">You can then normalize the values in the dict to eps.  abs(vect[i] - vect[j]) &lt; eps -&gt;  abs(vect[i] - vect[j]) / eps &lt; eps / eps -&gt;  abs(vect[i]/eps - vect[j]/eps) &lt; 1.</span>
