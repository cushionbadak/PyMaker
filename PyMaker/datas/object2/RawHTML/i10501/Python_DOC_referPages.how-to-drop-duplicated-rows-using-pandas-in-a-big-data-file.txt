<div class="post-text" itemprop="text">
<p>I have a csv file that too big to load to memory.I need to drop duplicated rows of the file.So I follow this way:</p>
<pre><code>chunker = pd.read_table(AUTHORS_PATH, names=['Author ID', 'Author name'],      encoding='utf-8', chunksize=10000000)

for chunk in chunker:
    chunk.drop_duplicates(['Author ID'])
</code></pre>
<p>But if duplicated rows distribute in different chunk seems like above script can't get the expected results.</p>
<p>Is there any better wayï¼Ÿ</p>
</div>
<div class="post-text" itemprop="text">
<p>You could try something like this.</p>
<p>First, create your chunker.</p>
<pre><code>chunker = pd.read_table(AUTHORS_PATH, names=['Author ID', 'Author name'], encoding='utf-8', chunksize=10000000)
</code></pre>
<p>Now create a set of ids:</p>
<pre><code>ids = set()
</code></pre>
<p>Now iterate over the chunks:</p>
<pre><code>for chunk in chunker:
    chunk.drop_duplicates(['Author ID'])
</code></pre>
<p>However, now, within the body of the loop, drop also ids already in the set of known ids:</p>
<pre><code>    chunk = chunk[~chunk['Author ID'].isin(ids)]
</code></pre>
<p>Finally, still within the body of the loop, add the new ids</p>
<pre><code>    ids.update(chunk['Author ID'].values)
</code></pre>
<hr/>
<p>If <code>ids</code> is too large to fit into main memory, you might need to use some disk-based database.</p>
</div>
<span class="comment-copy">Thanks! I tried it,but the memory still not enough.</span>
<span class="comment-copy">@yangxg Are you sure the set is the entity taking up your memory? What is its maximum <code>len</code>? If that is the problem, we need to escalate gradually. The next thing I would try is <a href="https://docs.python.org/3/library/shelve.html" rel="nofollow noreferrer"><code>shelve</code></a>.</span>
<span class="comment-copy">would addthing something like this to the loop potentially help?  chunk['Author ID'] = pd.to_numeric(chunk['Author ID'], downcast='integer')</span>
