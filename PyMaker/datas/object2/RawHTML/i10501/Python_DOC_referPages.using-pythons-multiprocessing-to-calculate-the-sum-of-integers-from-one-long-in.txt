<div class="post-text" itemprop="text">
<p>I want to use Python's multiprocessing module for the following:
Map an input line to a list of integers and calculate the sum of this list.</p>
<p>The input line is initially a string where the items to be summed are separated by spaces.</p>
<p>What I have tried is this:</p>
<pre><code>from itertools import imap

my_input = '1000000000 ' * int(1e6)
print sum(imap(int, my_input.split()))
</code></pre>
<p>This takes about 600ms on my machine, but I would like to make it faster with multiprocessing.</p>
<p>It seems that the bottleneck is in the mapping-part since the sum-method is pretty fast when applied to a ready list of integers:</p>
<pre><code>&gt;&gt;&gt; int_list = [int(1e9)] * int(1e6)
&gt;&gt;&gt; %time sum(int_list)
CPU times: user 7.38 ms, sys: 5 Âµs, total: 7.38 ms
Wall time: 7.4 ms
&gt;&gt;&gt; 1000000000000000
</code></pre>
<p>I tried to apply the instructions from <a href="https://stackoverflow.com/questions/29785427/how-to-parallel-sum-a-loop-using-multiprocessing-in-python">this question</a> but as I'm quite new to using multiprocessing, I couldn't fit the instructions to this problem.</p>
</div>
<div class="post-text" itemprop="text">
<p>So, this seems to roughly boil down to three steps:</p>
<ol>
<li>Make a <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer">pool</a></li>
<li>Map int() across the list within that pool</li>
<li>Sum the results.</li>
</ol>
<p>So:</p>
<pre><code>if __name__ == '__main__':
    import multiprocessing
    my_input = '1000000000 ' * int(1e6)
    string_list = my_input.split()
    # Pool over all CPUs
    int_list = multiprocessing.Pool().map(int, string_list)
    print sum(int_list)
</code></pre>
<p>It may be more efficient for time to use generators where possible:</p>
<pre><code>if __name__ == '__main__':
    import multiprocessing
    import re
    my_input = '1000000000 ' * int(1e6)
    # use a regex iterator matching whitespace
    string_list = (x.group(0) for x in re.finditer(r'[^\s]+\s', my_input))
    # Pool over all CPUs
    int_list = multiprocessing.Pool().imap(int, string_list)
    print sum(int_list)
</code></pre>
<p>The regex will likely be slower than <code>split</code>, but using <code>re.finditer</code> should allow the <code>Pool</code> to start mapping as fast as individual splits are made, and using <code>imap</code> rather than <code>map</code> should do similarly for <code>sum</code> (allowing it to start adding numbers as they become available). Credit to <a href="https://stackoverflow.com/a/9770397/6051861">this answer</a> for the <code>re.finditer</code> idea.</p>
<p>It may or may not be more efficient to multiprocess than doing it in a single process. You might end up losing more time making new processes and passing the results back from them than you gain in doing things all at once. The same goes for if you were to try putting the adding into the pool as well.</p>
<p>On the system I'm testing this on, which has two CPUs, I get the one-process solution to run in about half a second, the non-generator multiprocess solution in about 1 second, and the generator solution in 12-13 seconds.</p>
</div>
<div class="post-text" itemprop="text">
<p>Using a feature of Unix systems called <a href="https://en.wikipedia.org/wiki/Fork_(system_call)" rel="nofollow">forking</a>, you can read (not write) data from the parent process with zero overhead. Normally, you would have to copy the data over, but forking a process in Unix allows you to circumvent this.</p>
<p>Using this, the job in the pool can access the whole input string and extract the part that it will work on. It can then split and parse this section of the string on its own and return the sum of the integers in its section. </p>
<pre><code>from multiprocessing import Pool, cpu_count
from time import time


def serial(data):
    return sum(map(int, data.split()))


def parallel(data):
    processes = cpu_count()

    with Pool(processes) as pool:
        args = zip(
            ["input_"] * processes, # name of global to access
            range(processes), # job number
            [processes] * processes # total number of jobs 
        )
        return sum(pool.map(job, args, chunksize=1))


def job(args):
    global_name, job_number, total_jobs = args
    data = globals()[global_name]
    chunk = get_chunk(data, job_number, total_jobs)

    return serial(chunk)


def get_chunk(string, job_number, total_jobs):
    """This function may mess up if the number of integers in each chunk is low (1-2).
    It also assumes there is only 1 space separating integers."""
    approx_chunk_size = len(string) // total_jobs

    # initial estimates
    start = approx_chunk_size * job_number
    end = start + approx_chunk_size

    if start and not string.startswith(" ", start - 1):
        # if string[start] is not beginning of a number, advance to start of next number
        start = string.index(" ", start) + 1

    if job_number == total_jobs:
        # last job
        end = None
    elif not string.startswith(" ", end - 1):
        # if string[end] is part of a number, then advance to end of number
        end = string.index(" ", end - 1)

    return string[start:end]


def timeit(func, *args, **kwargs):
    "Simple timing function"
    start = time()
    result = func(*args, **kwargs)
    end = time()
    print("{} took {} seconds".format(func.__name__, end - start))
    return result


if __name__ == "__main__":
#    from multiprocessing.dummy import Pool # uncomment this for testing

    input_ = "1000000000 " * int(1e6)

    actual = timeit(parallel, input_)
    expected = timeit(serial, input_)
    assert actual == expected
</code></pre>
</div>
<span class="comment-copy">What OS are you using? One of the bigger problems is that splitting the string will be time consuming, as will sending the split substrings to the process pool. On Unix, you can avoid directly sending the substrings to the child process, and even delegate the splitting to the child process.</span>
<span class="comment-copy">@Dunes I'm using OS X. How can I avoid sending the substrings and delegate the splitting?</span>
<span class="comment-copy"><code>multiprocessing</code> won't help here due to serialisation overhead. You should only use <code>multiprocessing</code> to improve performance when one call of your target function is very expensive, far more expensive than a call to <code>pickle.dump</code> given your machines IO-performance.</span>
<span class="comment-copy">Thank you! Apart from missing quotas around <b>main</b> this worked, but as you implied, this was actually much slower (execution takes now about 3 seconds).</span>
<span class="comment-copy">@starkbot Fixed quotes. Also, was working on other additions when you posted that - try the iterator version? Might vary by the number of CPUs available.</span>
<span class="comment-copy">I think it's not very surprising that the <code>multiprocessing</code>-based version is a lot slower given that the only thing you can do concurrently here is a single <code>int</code> call. :-]</span>
<span class="comment-copy">@FrerichRaabe Well, yes. This makes a good basic example of how to use multiprocessing and pools, though, even if the specific job isn't a good one for it.</span>
<span class="comment-copy">@DavidHeyman the second version gives this error:  ValueError: invalid literal for int() with base 10: ''</span>
