<div class="post-text" itemprop="text">
<p>I would like to use multiple processes (<code>not threads</code>) to do some preprocessing and enqueue the results to a tf.RandomShuffleQueue which can be used by my main graph for training.</p>
<p>Is there a way to do that ?</p>
<h2>My actual problem</h2>
<p>I have converted my dataset into TFRecords split across 256 shards. I want to start 20 processes using <code>multiprocessing</code> and let each process a range of shards. Each process should read images and then augment them and push them into a <code>tf.RandomShuffleQueue</code> from which the input can be given to a graph for training.</p>
<p>Some people advised me to go through the <code>inception</code> example in <code>tensorflow</code>. However, it is a very different situation because there only reading of the data shards is done by multiple threads (<code>not processes</code>), while the preprocessing (e.g - augmentation) takes place in the main thread.</p>
</div>
<div class="post-text" itemprop="text">
<p>(<em>This aims to solve your actual problem</em>)</p>
<p>In another topic, someone told you that Python has the global interpreter lock (GIL) and therefore there would be no speed benefits from multi-core, unless you used multiple processes.</p>
<p>This was probably what prompted your desire to use <code>multiprocessing</code>.</p>
<p>However, with TF, Python is normally used only to construct the "graph". The actual execution happens in native code (or GPU), where GIL plays no role whatsoever.</p>
<p>In light of this, I recommend simply letting TF use multithreading. This can be controlled using the <code>intra_op_parallelism_threads</code> argument, such as:</p>
<pre><code>with tf.Session(graph=graph, 
    config=tf.ConfigProto(allow_soft_placement=True, 
    intra_op_parallelism_threads=20)) as sess:
    # ...
</code></pre>
<p>(Side note: if you have, say, a 2-CPU, 32-core system, the best argument may very well be <code>intra_op_parallelism_threads=16</code>, depending on a lot of factors)</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p><strong>Comment</strong>: The pickling of TFRecords is not that important.
  I can pass a list of lists containing names of ranges of sharded TFRecord files.</p>
</blockquote>
<p><strong>Therebe I have to restart Decision process!</strong></p>
<blockquote>
<p><strong>Comment</strong>: I can pass it to a Pool.map() as an argument.  </p>
</blockquote>
<p><strong>Verify</strong>, if a <code>multiprocesing.Queue()</code> can handle this.<br/>
Results of Tensor functions are a <code>Tensor object</code>.<br/>
Try the following:</p>
<pre><code>tensor_object = func(TFRecord)
q = multiprocessing.Manager().Queue()
q.put(tensor_object)
data = q.get()
print(data)
</code></pre>
<blockquote>
<p><strong>Comment</strong>: how do I make sure that all the processes enqueue to the same queue ?   </p>
</blockquote>
<p>This is simple done <code>enqueue</code> the results from <code>Pool.map(...</code> 
after all <code>process</code> finished.<br/>
Alternate we can <code>enqueue</code> parallel, <code>queueing</code> data from <strong>all</strong> <code>processes</code>.  </p>
<p>But doing so, depends on <strong>pickleabel</strong> data as described above.</p>
<hr/>
<p>For instance:</p>
<pre><code>import multiprocessing as mp
def func(filename):
    TFRecord = read(filename)
    tensor_obj = tf.func(TFRecord)
    return tensor_obj

def main_Tensor(tensor_objs):
    tf = # ... instantiat Tensor Session
    rsq = tf.RandomShuffleQueue(...)
    for t in tensor_objs:
        rsq.enqueue(t)

if __name__ == '__main__':
    sharded_TFRecords = ['file1', 'file2']
    with mp.Pool(20) as pool:
        tensor_objs = pool.map(func, sharded_TFRecords)
        pool.join()

    main_Tensor(tensor_objs)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>It seems the recommended way to run <code>TF</code> with <code>multiprocessing</code> is via creating a separate <code>tf.Session</code> for each child as sharing it across processes is unfeasible.</p>
<p>You can take a look at <a href="https://gist.github.com/yaroslavvb/ea1b1bae0a75c4aae593df7eca72d9ca" rel="nofollow noreferrer">this example</a>, I hope it helps. </p>
<p>[EDIT: Old answer]</p>
<p>You can use a <a href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.pool" rel="nofollow noreferrer"><code>multiprocessing.Pool</code></a> and rely on its callback mechanism to put results in the <code>tf.RandomShuffleQueue</code> as soon as they are ready.</p>
<p>Here's a very simple example on how to do it.</p>
<pre><code>from multiprocessing import Pool


class Processor(object):
    def __init__(self, random_shuffle_queue):
        self.queue = random_shuffle_queue
        self.pool = Pool()

    def schedule_task(self, task):
        self.pool.apply_async(processing_function, args=[task], callback=self.task_done)

    def task_done(self, results):
        self.queue.enqueue(results)
</code></pre>
<p>This assumes Python 2, for Python 3 I'd recommend to use a <a href="https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor" rel="nofollow noreferrer"><code>concurrent.futures.ProcessPoolExecutor</code></a>.</p>
</div>
<span class="comment-copy">Are you doing the augmentation in TF or using non-TF libs?</span>
<span class="comment-copy">I am using only TF based augmentations</span>
<span class="comment-copy">But preprocessing is intended for CPUs. Would it not make sense to use <code>multiprocessing</code> ? Why would GIL not play any role when I for example try to do data augmentation for N times using a <code>for</code> loop of python (while using TF libs inside the <code>for</code> loop ?</span>
<span class="comment-copy">@Ujjwal if the <code>for</code> loop only constructs the graph, GIL not relevant. The time-consuming actual execution happens when you run <code>sess.run</code>. At this point, the native code that gets eventually called doesn't even "know" you called it from Python. You could have called it from C++ or Java.</span>
<span class="comment-copy">@Ujjwal this has some great examples to follow: <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues" rel="nofollow noreferrer">tensorflow.org/programmers_guide/threading_and_queues</a></span>
<span class="comment-copy">1. Following your snippet, I got the output as the name of the TFRecord file. No errors were produced. 2. I work on a cluster and have 64 cores.  You should read the other answer by @noxdafox  I have used multiprocessing previously, but I do not know how to use it with TensorFlow as it has the concept of lazy evaluation inside <code>tf.Session()</code></span>
<span class="comment-copy">The pickling of TFRecords is not that important. I can pass a list of lists containing names of ranges of sharded TFRecord files. Within each function I can open the TFRecord files separately and do the work. However each function needs to enqueue data to <code>tf.RandomShuffleQueue</code> which is not picklable using <code>multiprocessing</code>. However using <code>pathos</code> module, I can pass it to a <code>Pool.map()</code> as an argument. However how do I make sure that all the processes enqueue to the same queue ? I am not able to wrap my mind around this due to the lazy execution principle of TensorFlow.</span>
<span class="comment-copy">I think it won't work. <code>tf.RandomShuffleQueue</code> does not have any <code>put()</code> method. Moreover, in TF, executions happen inside a session. Can <code>tf.Session()</code> be passed or shared between different processes ?</span>
<span class="comment-copy">The above code was ofc an example, I am not familiar with TF APIs. It's important when dealing with processes to keep stateful logic away from the processes themselves. Are you using any stateful logic during "data preprocessing"? Please share the preprocessing logic in your question.</span>
<span class="comment-copy">Can I ask the reason for the downvote?</span>
<span class="comment-copy">I assure you, I did not downvote your answer. However, Tensorflow does not work in the way, which would allow for the above snippet to work properly.</span>
<span class="comment-copy">Now it is more clear. It seems a <code>tf.Tensor</code> is a computation encapsulation unit which will be lazily evaluated later on. Therefore, even if you split the pre-processing in multiple processes the actual processing might happen on the other side of the <code>tf.RandomShuffleQueue</code>. I would see if I could de-couple the preprocessing logic from <code>TF</code> and run that into separate processes (receive raw data from the processes, encapsulate them into a <code>tf.Tensor</code> and enqueue it to <code>tf.RandomShuffleQueue</code>). If not, there's little you can do I guess. I would stay away from trying to pickle them myself.</span>
