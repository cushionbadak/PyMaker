<div class="post-text" itemprop="text">
<p>I need to organized a data file with chunks of named data. Data is NUMPY arrays. But I don't want to use numpy.save or numpy.savez function, because in some cases, data have to be sent on a server over a pipe or other interface. So I want to dump numpy array into memory, zip it, and then, send it into a server.</p>
<p>I've tried simple pickle, like this:</p>
<pre><code>try:
    import cPickle as pkl
except:
    import pickle as pkl
import ziplib
import numpy as np

def send_to_db(data, compress=5):
     send( zlib.compress(pkl.dumps(data),compress) )
</code></pre>
<p>.. but this is extremely slow process.</p>
<p>Even with compress level 0 (without compression), the process is very slow and just because of pickling.</p>
<p>Is there any way to dump numpy array into string without pickle? I know that numpy allows to get buffer <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.getbuffer.html#numpy.getbuffer" rel="nofollow noreferrer">numpy.getbuffer</a>, but it isn't obvious to me, how to use this dumped buffer to obtaine an array back.</p>
</div>
<div class="post-text" itemprop="text">
<p>You should definitely use <code>numpy.save</code>, you can still do it in-memory:</p>
<pre><code>&gt;&gt;&gt; import io
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import zlib
&gt;&gt;&gt; f = io.BytesIO()
&gt;&gt;&gt; arr = np.random.rand(100, 100)
&gt;&gt;&gt; np.save(f, arr)
&gt;&gt;&gt; compressed = zlib.compress(f.getvalue())
</code></pre>
<p>And to decompress, reverse the process:</p>
<pre><code>&gt;&gt;&gt; np.load(io.BytesIO(zlib.decompress(compressed)))
array([[ 0.80881898,  0.50553303,  0.03859795, ...,  0.05850996,
         0.9174782 ,  0.48671767],
       [ 0.79715979,  0.81465744,  0.93529834, ...,  0.53577085,
         0.59098735,  0.22716425],
       [ 0.49570713,  0.09599001,  0.74023709, ...,  0.85172897,
         0.05066641,  0.10364143],
       ...,
       [ 0.89720137,  0.60616688,  0.62966729, ...,  0.6206728 ,
         0.96160519,  0.69746633],
       [ 0.59276237,  0.71586014,  0.35959289, ...,  0.46977027,
         0.46586237,  0.10949621],
       [ 0.8075795 ,  0.70107856,  0.81389246, ...,  0.92068768,
         0.38013495,  0.21489793]])
&gt;&gt;&gt;
</code></pre>
<p>Which, as you can see, matches what we saved earlier:</p>
<pre><code>&gt;&gt;&gt; arr
array([[ 0.80881898,  0.50553303,  0.03859795, ...,  0.05850996,
         0.9174782 ,  0.48671767],
       [ 0.79715979,  0.81465744,  0.93529834, ...,  0.53577085,
         0.59098735,  0.22716425],
       [ 0.49570713,  0.09599001,  0.74023709, ...,  0.85172897,
         0.05066641,  0.10364143],
       ...,
       [ 0.89720137,  0.60616688,  0.62966729, ...,  0.6206728 ,
         0.96160519,  0.69746633],
       [ 0.59276237,  0.71586014,  0.35959289, ...,  0.46977027,
         0.46586237,  0.10949621],
       [ 0.8075795 ,  0.70107856,  0.81389246, ...,  0.92068768,
         0.38013495,  0.21489793]])
&gt;&gt;&gt;
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>THe default pickle method provides a pure ascii output. To get (much) better performance, use the latest version available. Versions 2 and above are binary and, if memory serves me right, allows numpy arrays to dump their buffer directly into the stream without addtional operations. </p>
<p>To select version to use, add the optional argument while pickling (no need to specify it while unpickling), for instance <code>pkl.dumps(data, 2)</code>.
To pick the latest possible version, use <code>pkl.dumps(data, -1)</code></p>
<p>Note that if you use different python versions, you need to specify the lowest supported version. 
See <a href="https://docs.python.org/3/library/pickle.html" rel="nofollow noreferrer">Pickle documentation</a> for details on the different versions</p>
</div>
<span class="comment-copy">Wait, why don't you want to use <code>numpy.save</code>? It will be the fastest, most portable way...</span>
<span class="comment-copy">@juanpa.arrivillaga I need to stream results of simulations from cluster nodes into head node, to save there. So I want zip it before sending. numpy.save is very fast, you right, but unfortunately it works only with files, not with memory?</span>
<span class="comment-copy">A "file" is just an abstraction. You can still do it in-memory. See my answer.</span>
<span class="comment-copy">The 'pickle' method for an <code>ndarray</code> is its <code>save</code> function.</span>
<span class="comment-copy">@hpaulj are you sure? why when juanpa.arrivillaga solution is so wonderfully fast and pickle code is so horrible slow?</span>
<span class="comment-copy">Ah! io.BytesIO! Wonderful.... Thank you!</span>
<span class="comment-copy">@rth yup, it's super handy.</span>
<span class="comment-copy">But can you give an example for decompression?</span>
<span class="comment-copy">@rth sure, check it out</span>
<span class="comment-copy">Note: Unless your data has a lot of repetition, compression probably won't gain you much, and may slow you down. It's worth checking if compression is worth it on total transfer time; it might be faster (and less memory intensive), if you're using a stream-oriented pipe/socket/whatever, to pass it as the argument for <code>numpy.save</code>; depending on implementation, you might manage to begin writing to the socket immediately, without additional memory overhead.</span>
<span class="comment-copy">Note: This not only runs much faster, it also produces <i>much</i> smaller pickles. In local microbenchmarks, for a 10x10 <code>numpy</code> array (produced with <code>numpy.random.random_sample</code>), pickling with protocol 0 took ~80 µs and produced output 2419 bytes of output, while protocol 2 took ~10 µs and produced only 934 bytes of output (100 C <code>double</code>s would need a minimum of 800 bytes to store). For 1000x1000, the times go to 6.24 <b><i>ms</i></b> for protocol 0 and 220 KB, vs. 22 µs for protocol 2 and 80,134 bytes (looks like a fixed 134 bytes to store type and shape).</span>
