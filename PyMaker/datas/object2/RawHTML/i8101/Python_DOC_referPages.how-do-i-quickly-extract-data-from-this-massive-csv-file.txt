<div class="post-text" itemprop="text">
<p>I have genomic data from 16 nuclei. The first column represents the nucleus, the next two columns represent the scaffold (section of genome) and the position on the scaffold respectively, and the last two columns represent the nucleotide and coverage respectively. There can be equal scaffolds and positions in different nuclei.</p>
<p>Using input for start and end positions (scaffold and position of each), I'm supposed to output a csv file which shows the data (nucleotide and coverage) of each nucleus within the range from start to end. I was thinking of doing this by having 16 columns (one for each nucleus), and then showing the data from top to bottom. The leftmost region would be a reference genome in that range, which I accessed by creating a dictionary for each of its scaffolds.</p>
<p>In my code, I have a defaultdict of lists, so the key is a string which combines the scaffold and the location, while the data is an array of lists, so that for each nucleus, the data can be appended to the same location, and in the end each location has data from every nucleus.</p>
<p>Of course, this is very slow. How should I be doing it instead?</p>
<p>Code:</p>
<pre><code>#let's plan this
#input is start and finish - when you hit first, add it and keep going until you hit next or larger
#dictionary of arrays
#loop through everything, output data for each nucleus

import csv
from collections import defaultdict

inrange = 0
start = 'scaffold_41,51335'
end = 'scaffold_41|51457'
locations = defaultdict(list)
count = 0

genome = defaultdict(lambda : defaultdict(dict))
scaffold = ''
for line in open('Allpaths_SL1_corrected.fasta','r'):
    if line[0]=='&gt;':
        scaffold = line[1:].rstrip()
    else:
        genome[scaffold] = line.rstrip()
print('Genome dictionary done.')

with open('automated.csv','rt') as read:
    for line in csv.reader(read,delimiter=','):
        if line[1] + ',' + line[2] == start:
            inrange = 1
        if inrange == 1:
            locations[line[1] + ',' + line[2]].append([line[3],line[4]])
        if line[1] + ',' + line[2] == end:
            inrange = 0
        count += 1
        if count%1000000 == 0:
            print('Checkpoint '+str(count)+'!')

with open('region.csv','w') as fp:
    wr = csv.writer(fp,delimiter=',',lineterminator='\n')
    for key in locations:
        nuclei = []
        for i in range(0,16):
            try:
                nuclei.append(locations[key][i])
            except IndexError:
                nuclei.append(['',''])
        wr.writerow([genome[key[0:key.index(',')][int(key[key.index(',')+1:])-1],key,nuclei])
print('Done!')
</code></pre>
<p>Files:
<a href="https://drive.google.com/file/d/0Bz7WGValdVR-bTdOcmdfRXpUYUE/view?usp=sharing" rel="nofollow noreferrer">https://drive.google.com/file/d/0Bz7WGValdVR-bTdOcmdfRXpUYUE/view?usp=sharing</a>
<a href="https://drive.google.com/file/d/0Bz7WGValdVR-aFdVVUtTbnI2WHM/view?usp=sharing" rel="nofollow noreferrer">https://drive.google.com/file/d/0Bz7WGValdVR-aFdVVUtTbnI2WHM/view?usp=sharing</a></p>
</div>
<div class="post-text" itemprop="text">
<p>(Only focusing on the CSV section in the middle of your code)</p>
<p>The example csv file you supplied is over 2GB and 77,822,354 lines. Of those lines, you seem to only be focused on 26,804,253 lines or about 1/3.</p>
<p>As a general suggestion, you can speed thing up by:</p>
<ol>
<li>Avoid processing the data you are not interested in (2/3 of the file);</li>
<li>Speed up identifying the data you are interested in;</li>
<li>Avoid the things that repeated millions of times that tend to be slower (processing each line as csv, reassembling a string, etc);</li>
<li>Avoid reading all data when you can break it up into blocks or lines (memory will get tight)</li>
<li>Use faster tools like <code>numpy</code>, <code>pandas</code> and <code>pypy</code></li>
</ol>
<p>You data is block oriented, so you can use a <code>FlipFlop</code> type object to sense if you are in a block or not.</p>
<p>The first column of your csv is numeric, so rather than splitting the line apart and reassembling two columns, you can use the faster Python <code>in</code> operator to find the start and end of the blocks:</p>
<pre><code>start = ',scaffold_41,51335,'
end = ',scaffold_41,51457,'

class FlipFlop: 
    def __init__(self, start_pattern, end_pattern):
        self.patterns = start_pattern, end_pattern
        self.state = False
    def __call__(self, st):
        rtr=True if self.state else False
        if self.patterns[self.state] in st:
            self.state = not self.state
        return self.state or rtr

lines_in_block=0    
with open('automated.csv') as f:
    ff=FlipFlop(start, end)
    for lc, line in enumerate(f):
        if ff(line):
            lines_in_block+=1

print lines_in_block, lc
</code></pre>
<p>Prints:</p>
<pre><code>26804256 77822354
</code></pre>
<p>That runs in about 9 seconds in PyPy and 46 seconds in Python 2.7. </p>
<p>You can then take the portion that reads the source csv file and turn that into a generator so you only need to deal with one block of data at a time.</p>
<p>(Certainly not correct, since I spent no time trying to understand your files overall..):</p>
<pre><code>def csv_bloc(fn, start_pat, end_pat):
    from itertools import ifilter 
    with open(fn) as csv_f:
    ff=FlipFlop(start_pat, end_pat)
    for block in ifilter(ff, csv_f):
        yield block
</code></pre>
<p>Or, if you need to combine all the blocks into one dict:</p>
<pre><code>def csv_line(fn, start, end):
    with open(fn) as csv_in:
        ff=FlipFlop(start, end)
        for line in csv_in:
            if ff(line):
                yield line.rstrip().split(",")              

di={}               
for row in csv_line('/tmp/automated.csv', start, end):
    di.setdefault((row[2],row[3]), []).append([row[3],row[4]])
</code></pre>
<p>That executes in about 1 minute on my (oldish) Mac in PyPy and about 3 minutes in cPython 2.7. </p>
<p>Best</p>
</div>
<span class="comment-copy">Here's a tip: abstract your question <i>away from the specifics of your problem</i>. Create a toy dataset with the same properties as your real dataset that people can quickly run. The more jargon you use that is specific to your field, the less likely you will get people to want to dig into your question.</span>
<span class="comment-copy">Probably you need to use <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html" rel="nofollow noreferrer">pandas.read_csv</a> or  <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html" rel="nofollow noreferrer">pandas.read_table</a>. After you read the dataset there are lots of methods  to transform it in Pandas.</span>
<span class="comment-copy">If you need to do 'filtration' of the same file many times, I would suggest to create a additional file  with beginnings and ends for each scaffold  (hash table) and search only in sub tables, reading them into a memory. Creating a table will be quite a long process, but after that, search may be done very fast.</span>
<span class="comment-copy">Have you considered using <a href="https://docs.python.org/3/library/sqlite3.html" rel="nofollow noreferrer">sqlite</a>? Your data can remain on disk and 2GB databases can be searched fairly quickly.</span>
<span class="comment-copy">You have <code>if line[1] + ',' + line[2] == end: inrange=0</code>  but you also have <code>end = 'scaffold_41|51457'</code>. <code>end</code> does not have a comma so the entire file after <code>start</code> will be read. Is this intended?</span>
