<div class="post-text" itemprop="text">
<p>The problem: I need to send many HTTP requests to a server. I can only use one connection (non-negotiable server limit). The server's response time plus the network latency is too high – I'm falling behind.</p>
<p>The requests typically don't change server state and don't depend on the previous request's response. So my idea is to simply send them on top of each other, enqueue the response objects, and depend on the Content-Length: of the incoming responses to feed incoming replies to the next-waiting response object. In other words: Pipeline the requests to the server.</p>
<p>This is of course not entirely safe (any reply without Content-Length: means trouble), but I don't care -- in that case I can always retry any queued requests. (The safe way would be to wait for the header before sending the next bit. That'd might help me enough. No way to test beforehand.)</p>
<p>So, ideally I want the following client code (which uses client delays to mimic network latency) to run in three seconds.</p>
<p>Now for the $64000 question: Is there a Python library which already does this, or do I need to roll my own? My code uses gevent; I could use Twisted if necessary, but Twisted's standard connection pool does not support pipelined requests. I also could write a wrapper for some C library if necessary, but I'd prefer native code.</p>
<pre><code>#!/usr/bin/python

import gevent.pool
from gevent import sleep
from time import time

from geventhttpclient import HTTPClient

url = 'http://local_server/100k_of_lorem_ipsum.txt'
http = HTTPClient.from_url(url, concurrency=1)

def get_it(http):
    print time(),"Queueing request"
    response = http.get(url)
    print time(),"Expect header data"
    # Do something with the header, just to make sure that it has arrived
    # (the greenlet should block until then)
    assert response.status_code == 200
    assert response["content-length"] &gt; 0
    for h in response.items():
        pass

    print time(),"Wait before reading body data"
    # Now I can read the body. The library should send at
    # least one new HTTP request during this time.
    sleep(2)
    print time(),"Reading body data"
    while response.read(10000):
        pass
    print time(),"Processing my response"
    # The next request should definitely be transmitted NOW.
    sleep(1)
    print time(),"Done"

# Run parallel requests
pool = gevent.pool.Pool(3)
for i in range(3):
    pool.spawn(get_it, http)

pool.join()
http.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://pypi.python.org/pypi/dugong/" rel="nofollow noreferrer">Dugong</a> is an HTTP/1.1-only client which claims to support real HTTP/1.1 pipelining. <a href="https://pythonhosted.org/dugong/tutorial.html" rel="nofollow noreferrer">The tutorial</a> includes several examples on how to use it, including one <a href="https://pythonhosted.org/dugong/tutorial.html#pipelining-with-threads" rel="nofollow noreferrer">using threads</a> and another <a href="https://pythonhosted.org/dugong/tutorial.html#pipelining-with-coroutines" rel="nofollow noreferrer">using asyncio</a>.</p>
<p>Be sure to verify that the server you're communicating with actually supports HTTP/1.1 pipelining—some servers claim to support HTTP/1.1 but don't implement pipelining.</p>
</div>
<div class="post-text" itemprop="text">
<p>I think txrequests could get you most of what you are looking for, using the background_callback to en-queue processing of responses on a separate thread. Each request would still be it's own thread but using a session means by default it would reuse the same connection.</p>
<p><a href="https://github.com/tardyp/txrequests#working-in-the-background" rel="nofollow noreferrer">https://github.com/tardyp/txrequests#working-in-the-background</a></p>
</div>
<div class="post-text" itemprop="text">
<p>It is not an answer to your library question but could you not use something as selenium and their selenium.webdriver.support.ui import WebDriverWait
to wait for your requests to be processed for some time, then taking your next step, be it storing the response for later use or sending next request if you did not have any relevant answer?<br/>
The use of this interface would also allow use of a proxy for bypassing (reasonably, depending on your application and needs) the server limit (either 3 or 5 is help much speed), if you don't need authentication for this connection.  </p>
</div>
<div class="post-text" itemprop="text">
<p>It seems you are running python2.</p>
<p>For python3 &gt;= 3.5 
you could use async/await loop
See <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow noreferrer">asyncio</a></p>
<p>Also, there is a library built on top for better, easier use
called <a href="https://github.com/python-trio/trio" rel="nofollow noreferrer">Trio</a>, available on pip.</p>
<hr/>
<p>Another thing I can think of is multiple threads with locks.
I will think on how to better explain this or could it even work.</p>
</div>
<span class="comment-copy">Note: as for C libraries, I already found serf at <a href="http://code.google.com/p/serf/" rel="nofollow noreferrer">code.google.com/p/serf</a> . Writing working Python bindings for <i>that</i> library unfortunately is not something I'm currently paid for. :-/</span>
<span class="comment-copy">Your code looks a bit like what <a href="https://github.com/kennethreitz/grequests/blob/master/grequests.py" rel="nofollow noreferrer">grequests</a> does. Have you taken a look to it? If you have, could you explain why it is not a good fit? (I probably haven't fully, fully understood the question)</span>
<span class="comment-copy">grequests is a simple async wrapper for requests, i.e. one thread per request, and each is still a send/receive/send-next-bit lockstep that uses its own connection. I need something which opens a single TCP connection and then spawns one thread that sends the request headers, and another which receives the responses and associates them with "their" requests.</span>
<span class="comment-copy">this example ( <a href="http://code.activestate.com/recipes/576673-python-http-pipelining/" rel="nofollow noreferrer">code.activestate.com/recipes/576673-python-http-pipelining</a> ) works well. But only in Python2 - if it's converted to Python3 it works only in like 50% cases...</span>
<span class="comment-copy">A more fun thing to do is to get your server to support http/2, then use something like grpc or <a href="https://hyper.readthedocs.io/en/latest/" rel="nofollow noreferrer">hyper</a>.</span>
<span class="comment-copy">Please explain which async HTTP library is able to queue more than one request onto a single connection. aiohttp can't do it.</span>
<span class="comment-copy">the anser has nothing to do with pipelining</span>
<span class="comment-copy"><i>aiohttp</i> might add requests pipelining in <code>ClientSession</code>: <a href="https://github.com/aio-libs/aiohttp/issues/1740" rel="nofollow noreferrer">github.com/aio-libs/aiohttp/issues/1740</a></span>
