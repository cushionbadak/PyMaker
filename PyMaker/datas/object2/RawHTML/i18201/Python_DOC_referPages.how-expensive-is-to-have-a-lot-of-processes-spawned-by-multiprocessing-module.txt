<div class="post-text" itemprop="text">
<p>I have an application where I have 2 sub-processes (<code>RabbitMQ consumer</code> and <code>RabbitMQ producer</code>) that are constantly running ( most of the time they idle). But also I need to spawn another <code>N</code> number of sub-processes (lets call it <code>Worker process</code>) based on condition of <code>RabbitMQ consumer</code> process. </p>
<p>So <code>Worker processes</code> are very lightweight and don't do a lot of computation, but the work whey do takes a long time ( up to an hour ). The machine itself would have no more then 4 CPU cores. I'm planning to run the app on <code>CentOS</code>. So the question: is it OK to have a lot ( I expect from 1 to 20 ) lightweight processes like this that would come and go, and most of the time would idle? </p>
<p>My first thought was to create one sub-process (<code>Worker process</code> ) and than use threads inside. But I've heard people where having difficulties mixing <code>multiprocessing</code> and <code>threading</code> module together. is it true? </p>
<p>By the way, my app is in Python 2.7 and to spawn sub-processes I use <code>multiprocessing</code> module.</p>
</div>
<div class="post-text" itemprop="text">
<p>Based on your description, I would suggest that you should just go ahead and create multiple processes for your ~20 jobs.  The <code>multiprocessing</code> API makes this very easy, and your most precious resource is your own time.  The complexity of concurrent programming can get out of hand very quickly so you need all the help you can get.</p>
<h2>Details</h2>
<p>If your worker processes are <strong>I/O bound</strong>, then there is (arguably) no CPU impact to having many processes.  My Windows currently lists 145 processes running, though we would consider the machine to be idle.  Just make sure that either <em>your</em> code calls <code>time.sleep(x)</code> periodically, where <code>x</code> is a "reasonable" pause time for polling, or you are using a library that does it for you, such as multiprocessing's connection object with its <code>.poll(x)</code> method.</p>
<p>If your worker processes are <strong>CPU bound</strong>, then I'm afraid you are better off setting up a process pool with a size equal to your free CPUs, and then push jobs onto a queue and let the processes in the pool take jobs off the queue.  <code>multiprocessing</code> supports this paradigm pretty well.</p>
<p>It gets tricky when the workers could be <strong>both CPU bound and I/O bound</strong> at different times.  In this case I would suggest that you keep one process reserved (dedicated) for CPU work, and let it take jobs off a queue, and then let many other (I/O) processes create the jobs and push them onto the work queue. If the work is coming in faster than your one CPU core can handle, you either add a second dedicated core, or you set a <em>maxsize</em> on the queue and let your I/O workers monitor queue size to know whether new work can be added.  </p>
<p>If you have a <strong>great many workers that are I/O bound</strong>, then you have to start looking at event-based frameworks such as <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow noreferrer">asyncio</a>, <a href="http://twistedmatrix.com/trac/" rel="nofollow noreferrer">Twisted</a>, <a href="http://www.gevent.org/" rel="nofollow noreferrer">gevent</a>, <a href="http://eventlet.net/" rel="nofollow noreferrer">eventlet</a>, <a href="http://greenlet.readthedocs.org/en/latest/" rel="nofollow noreferrer">greenlet</a> and so on.  This is because there is a reserve memory cost for each OS thread or process spawned, and once you get into thousands of instances that reserve space begins to add up; the event-based systems on the other hand do not spawn multiple threads, they just loop over the I/O device interface and accumulate data based on events. You can support a really huge number of concurrent connections with event-based networking.</p>
<p>On Windows, there is an excellent article of the measured limits of multiple threads and processes <a href="http://blogs.technet.com/b/markrussinovich/archive/2009/07/08/3261309.aspx" rel="nofollow noreferrer">here</a>.  A quick scan of the document tells me a limit of ~10k was found for maximum number of processes.  I have seen this mentioned elsewhere as <em>The 10k problem</em> but I don't have a reference available to me right now.</p>
<p>If you have a <strong>great many workers that are CPU bound</strong>, then you have to work with distributed computing, pushing jobs to various separate machines.  <code>multiprocessing</code> supports this too via the <code>Manager</code> API, but I have no personal experience with this. ZeroMQ appears to be popular right now for handling distributed messaging.</p>
</div>
<span class="comment-copy">20 processes is nothing, even more so since they are lightweight.</span>
<span class="comment-copy">I was in a situation like yours. Making different test I found that 20 were nothing. So I rised up the number until my PC couldn't create the pool in less than two minutes, about 4000 sub-processes. Based in the tests results, the perfect number was 500</span>
<span class="comment-copy">THANK YOU VERY VERY VERY MUCH!</span>
