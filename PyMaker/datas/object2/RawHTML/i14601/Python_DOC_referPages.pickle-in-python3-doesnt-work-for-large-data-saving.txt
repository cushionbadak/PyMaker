<div class="post-text" itemprop="text">
<p>I am trying to apply <code>_pickle</code> to save data onto disk. But when calling <code>_pickle.dump</code>, I got an error</p>
<pre><code>OverflowError: cannot serialize a bytes object larger than 4 GiB
</code></pre>
<p>Is this a hard limitation to use <code>_pickle</code>? (<code>cPickle</code> for python2)</p>
</div>
<div class="post-text" itemprop="text">
<p>Not anymore in Python 3.4 which has PEP 3154 and Pickle 4.0<br/>
<a href="https://www.python.org/dev/peps/pep-3154/">https://www.python.org/dev/peps/pep-3154/</a></p>
<p>But you need to say you want to use version 4 of the protocol:<br/>
<a href="https://docs.python.org/3/library/pickle.html">https://docs.python.org/3/library/pickle.html</a></p>
<pre><code>pickle.dump(d, open("file", 'w'), protocol=4)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Yes, this is a hard-coded limit; from <a href="https://hg.python.org/cpython/file/2d8e4047c270/Modules/_pickle.c#l1958" rel="nofollow"><code>save_bytes</code> function</a>:</p>
<pre class="lang-c prettyprint-override"><code>else if (size &lt;= 0xffffffffL) {
    // ...
}
else {
    PyErr_SetString(PyExc_OverflowError,
                    "cannot serialize a bytes object larger than 4 GiB");
    return -1;          /* string too large */
}
</code></pre>
<p>The protocol uses 4 bytes to write the size of the object to disk, which means you can only track sizes of up to 2<sup>32</sup> == 4GB.</p>
<p>If you can break up the <code>bytes</code> object into multiple objects, each smaller than 4GB, you can still save the data to a pickle, of course.</p>
</div>
<div class="post-text" itemprop="text">
<p>There is a great answers above for why pickle doesn't work.
But it still doesn't work for Python 2.7, which is a problem
if you are are still at Python 2.7 and want to support large
files, especially NumPy (NumPy arrays over 4G fail).</p>
<p>You can use OC serialization, which has been updated to work for data over
4Gig.   There is a Python C Extension module available from:</p>
<p><a href="http://www.picklingtools.com/Downloads" rel="nofollow">http://www.picklingtools.com/Downloads</a></p>
<p>Take a look at the Documentation:</p>
<p><a href="http://www.picklingtools.com/html/faq.html#python-c-extension-modules-new-as-of-picklingtools-1-6-0-and-1-3-3" rel="nofollow">http://www.picklingtools.com/html/faq.html#python-c-extension-modules-new-as-of-picklingtools-1-6-0-and-1-3-3</a></p>
<p>But, here's a quick summary:  there's ocdumps and ocloads, very much like
pickle's dumps and loads::</p>
<pre><code>from pyocser import ocdumps, ocloads
ser = ocdumps(pyobject)   : Serialize pyobject into string ser
pyobject = ocloads(ser)   : Deserialize from string ser into pyobject
</code></pre>
<p>The OC Serialization is 1.5-2x faster and also works with C++ (if you are mixing langauges).  It works with all built-in types, but not classes
(partly because it is cross-language and it's hard to build C++ classes
from Python).</p>
</div>
<span class="comment-copy">is it a good way to open file this way? I mean without closing it.</span>
<span class="comment-copy">@1a1a11a It would be good practice to open the file using a 'with' statement to ensure that the file gets closed. However, the reference count to the file object drops to zero as soon as the call to pickle.dump returns, so it will get garbage collected right away, and the file will be closed anyway.</span>
<span class="comment-copy">Thank you! is it possible to save large file on disk and circumvent this limit?</span>
<span class="comment-copy">@Jake0x32: not with pickle; this is a hard limit in the protocol. Break up your <code>bytes</code> object into smaller pieces.</span>
<span class="comment-copy">Gotcha! Thank you!</span>
<span class="comment-copy">@MartijnPieters I have the same problem while trying to pickle a classifier <code>from sklearn.svm import SVC</code>. How would I break the object into bytes and then pickle?</span>
