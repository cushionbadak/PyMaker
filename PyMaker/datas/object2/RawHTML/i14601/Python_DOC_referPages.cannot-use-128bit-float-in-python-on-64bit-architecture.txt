<div class="post-text" itemprop="text">
<p>I checked the size of a pointer in my python terminal (in Enthought Canopy IDE)
via </p>
<pre><code>import ctypes
print (ctypes.sizeof(ctypes.c_voidp) * 8)
</code></pre>
<p>I've a 64bit architecture and working with <code>numpy.float64</code> is just fine. But I cannot use <code>np.float128</code>?</p>
<pre><code>np.array([1,1,1],dtype=np.float128)
</code></pre>
<p>or </p>
<pre><code>np.float128(1)
</code></pre>
<p>results in:</p>
<pre><code>AttributeError: 'module' object has no attribute 'float128'
</code></pre>
<p>I'm running the following version:</p>
<pre><code>sys.version_info(major=2, minor=7, micro=6, releaselevel='final', serial=0)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Update: From the comments, it seems pointless to even have a 128 bit float on a 64 bit system. </p>
<p>I am using <code>anaconda</code> on a 64-bit Ubuntu 14.04 system with 
<code>sys.version_info(major=2, minor=7, micro=9, releaselevel='final', serial=0)</code></p>
<p>and 128 bit floats work fine:</p>
<pre><code>import numpy
a = numpy.float128(3)
</code></pre>
<p>This might be an distribution problem. Try:</p>
<ul>
<li><a href="http://docs.continuum.io/anaconda/install.html" rel="nofollow noreferrer">Install Anaconda</a></li>
<li><a href="http://docs.enthought.com/canopy/quick-start/updating_canopy.html" rel="nofollow noreferrer">Update canopy</a></li>
<li>Check that the version of python in the path is the one supplied by anaconda or <a href="http://docs.enthought.com/canopy/configure/canopy-cli.html#scenario-creating-a-system-wide-canopy-install" rel="nofollow noreferrer">canopy</a></li>
</ul>
<p>EDIT: 
Update from the comments:</p>
<blockquote>
<p>Not my downvote, but this post doesn't really answer the "why doesn't
  np.float128 exist on my machine" implied question. The true answer is
  that this is platform specific: float128 exists on some platforms but
  not others, and on those platforms where it does exist it's almost
  certainly simply the 80-bit x87 extended precision type, padded to 128
  bits.  – Mark Dickinson</p>
</blockquote>
</div>
<span class="comment-copy">@Matthias: Unless you've got a very unusual platform (e.g., IBM mainframe), NumPy almost certainly doesn't give you access to true 128-bit floats.  On some platforms, NumPy supports the x87 80-bit floating-point format defined in the 1985 version of the IEEE 754 standard, and on some of <i>those</i> platforms, that format is reported as <code>float128</code> (while on others it's reported as <code>float96</code>).  But all that's going on there is that you have an 80-bit format with 48 bits (or 16 bits) of padding.</span>
<span class="comment-copy">@PadraicCunningham <code>np.longdouble</code> results in <code>np.float64</code></span>
<span class="comment-copy"><a href="http://stackoverflow.com/questions/9062562/what-is-the-internal-precision-of-numpy-float128" title="what is the internal precision of numpy float128">stackoverflow.com/questions/9062562/…</a></span>
<span class="comment-copy">@PadraicCunningham the exact size does not really matter as long as I have a higher precision than a float64 (for comparing quadrature rules)</span>
<span class="comment-copy">@Matthias: Then you're probably out of luck.  Are you on Windows?  IIRC, the Windows platform defines <code>long double</code> to be the same type as <code>double</code>, so <code>np.longdouble</code> doesn't give you any extra precision.</span>
<span class="comment-copy">That's almost certainly <i>not</i> a 128-bit float, at least not in the sense of the IEEE 754 binary128 format.  It's an 80-bit float with 48 bits of padding.</span>
<span class="comment-copy">Try doing <code>numpy.float128(1) + numpy.float128(2**-64) - numpy.float128(1)</code>.  I suspect you'll get an answer of <code>0.0</code>, indicating that the <code>float128</code> type contains no more than 64 bits of precision.</span>
<span class="comment-copy">@CharlieParker: Yes, absolutely expected. In normal double precision, <code>1.0 + 2**-64</code> is not exactly representable (not enough significand bits), so the result of the addition is the closest double-precision float which <i>is</i> exactly representable, which is <code>1.0</code> again. And now of course subtracting <code>1.0</code> gives <code>0.0</code>. And for regular double precision, the same is true with <code>1.0 + 2**-53 - 1.0</code> (the binary precision is 53). For extended x87-style precision, with the usual round-ties-to-even, <code>1.0 + 2**-64 - 1.0</code> will give zero, while <code>1.0 + 2**-63 - 1.0</code> will be nonzero.</span>
<span class="comment-copy">@CharlieParker: Not my downvote, but this post doesn't really answer the "why doesn't np.float128 exist on my machine" implied question. The true answer is that this is platform specific: <code>float128</code> exists on some platforms but not others, and on those platforms where it does exist it's almost certainly simply the 80-bit x87 extended precision type, padded to 128 bits.</span>
<span class="comment-copy">@CharlieParker: Because floating-point means floating (binary) point! The ability to move the point allows representations of values at a wide range of scales, but doesn't magically give extra precision. See any of the <a href="https://docs.python.org/3/tutorial/floatingpoint.html" rel="nofollow noreferrer">many</a> <a href="http://floating-point-gui.de" rel="nofollow noreferrer">floating-point</a> guides out there for more information. These comments aren't really the right place for this discussion ...</span>
