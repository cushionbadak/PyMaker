<div class="post-text" itemprop="text">
<p>I'm reading through a large file, and processing it.
I want to be able to jump to the middle of the file without it taking a long time.</p>
<p>right now I am doing:</p>
<pre><code>f = gzip.open(input_name)
for i in range(1000000):
    f.read() # just skipping the first 1M rows

for line in f:
    do_something(line)
</code></pre>
<p>is there a faster way to skip the lines in the zipped file?
If I have to unzip it first, I'll do that, but there has to be a way.</p>
<p>It's of course a text file, with <code>\n</code> separating lines.</p>
</div>
<div class="post-text" itemprop="text">
<p>The nature of gzipping is such that there is no longer the concept of lines when the file is compressed -- it's just a binary blob. Check out <a href="http://www.gzip.org/deflate.html" rel="noreferrer">this</a> for an explanation of what gzip does. </p>
<p>To read the file, you'll need to decompress it -- the <code>gzip</code> module does a fine job of it. Like other answers, I'd also recommend <code>itertools</code> to do the jumping, as it will carefully make sure you don't pull things into memory, and it will get you there as fast as possible.</p>
<pre><code>with gzip.open(filename) as f:
    # jumps to `initial_row`
    for line in itertools.slice(f, initial_row, None):
        # have a party
</code></pre>
<p>Alternatively, if this is a CSV that you're going to be working with, you could also try clocking <code>pandas</code> parsing, as it can handle decompressing <code>gzip</code>. That would look like: <code>parsed_csv = pd.read_csv(filename, compression='gzip')</code>.</p>
<p>Also, to be extra clear, when you iterate over file objects in python -- i.e. like the <code>f</code> variable above -- you iterate over lines. You do not need to think about the '\n' characters.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use <a href="https://docs.python.org/2/library/itertools.html#itertools.islice" rel="nofollow">itertools.islice</a>, passing a file object <code>f</code> and  starting point, it will still advance the iterator but more efficiently than calling next 1000000 times:</p>
<pre><code>from itertools import islice

for line in islice(f,1000000,None):
     print(line)
</code></pre>
<p>Not overly familiar with gzip but I imagine <code>f.read()</code> reads the whole file so the next 999999 calls are doing nothing. If you wanted to manually advance the iterator you would call next on the file object i.e <code>next(f)</code>. </p>
<p>Calling <code>next(f)</code> won't mean all the lines are read into memory at once either, it advances the iterator one line at a time so if you want to skip a line or two in a file or a header it can be useful.</p>
<p>The <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow">consume</a>  recipe as @wwii suggested recipe is also worth checking out</p>
</div>
<div class="post-text" itemprop="text">
<p>Not really.</p>
<p>If you know the number of bytes you want to skip, you can use <code>.seek(amount)</code> on the file object, but in order to skip a number of lines, Python has to go through the file byte by byte to count the newline characters.</p>
<p>The only alternative that comes to my mind is if you handle a certain static file, that won't change. In that case, you can index it once, i.e. find out and remember the positions of each line. If you have that in e.g. a dictionary that you save and load with <code>pickle</code>, you can skip to it in quasi-constant time with <code>seek</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>It is not possible to randomly seek within a gzip file. Gzip is a stream algorithm and so it must always be uncompressed from the start until where your data of interest lies.</p>
<p>It is not possible to jump to a specific line without an index. Lines can be scanned forward or scanned backwards from the end of the file in continuing chunks.</p>
<p>You should consider a different storage format for your needs. What are your needs?</p>
</div>
<span class="comment-copy">You should consider a different storage format for your needs. What are your needs?</span>
<span class="comment-copy">How many bytes per line?</span>
<span class="comment-copy">I acknowledged you mentioned itertools @PadraicCunningham, I'm not competing.</span>
<span class="comment-copy">@PadraicCunningham, I was trying to explain things in a useful manner. If you disagree, you're welcome to down vote me. That's all I'm going to say.</span>
<span class="comment-copy">Why not add a pandas example? That would be useful.</span>
<span class="comment-copy">@PadraicCunningham, happily.</span>
<span class="comment-copy">Maybe use the <code>consume</code> function from the <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow noreferrer">Itertools Recipes</a> and pass it <code>iter(file object)</code>.</span>
