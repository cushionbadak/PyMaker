<div class="post-text" itemprop="text">
<p>I'm looking to run over a list of ids and return a list of any ids that occurred more than once. This was what I set up that is working:</p>
<pre><code>singles = list(ids)
duplicates = []
while len(singles) &gt; 0:
    elem = singles.pop()
    if elem in singles:
        duplicates.append(elem)
</code></pre>
<p>But the ids list is likely to get quite long, and I realistically don't want a while loop predicated on an expensive len call if I can avoid it. (I could go the inelegant route and call len once, then just decrement it every iteration but I'd rather avoid that if I could).</p>
</div>
<div class="post-text" itemprop="text">
<p>The smart way to do this is to use a data structure that makes it easy and efficient, like <a href="https://docs.python.org/3/library/collections.html#collections.Counter"><code>Counter</code></a>:</p>
<pre><code>&gt;&gt;&gt; ids = [random.randrange(100) for _ in range(200)]
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; counts = Counter(ids)
&gt;&gt;&gt; dupids = [id for id in ids if counts[id] &gt; 1]
</code></pre>
<p>Building the <code>Counter</code> takes O(N) time, as opposed to O(N log N) time for sorting, or O(N^2) for counting each element from scratch every time.</p>
<hr/>
<p>As a side note:</p>
<blockquote>
<p>But the ids list is likely to get quite long, and I realistically don't want a while loop predicated on an expensive len call if I can avoid it.</p>
</blockquote>
<p><code>len</code> is not expensive. It's constant time, and (at least on builtin types list <code>list</code>) it's about as fast as a function can possibly get in Python short of doing nothing at all.</p>
<p>The part of your code that's expensive is calling <code>elem in singles</code> inside the loop—that means for every element, you have to compare it against potentially every other element, meaning quadratic time.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could do like this,</p>
<pre><code>&gt;&gt;&gt; ids = [1,2,3,2,3,5]
&gt;&gt;&gt; set(i for i in ids if ids.count(i) &gt; 1)
{2, 3}
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I presume this will work faster:</p>
<pre><code>occasions = {}
for id in ids:
    try:
        occasions[id] += 1
    except KeyError:
        occasions[id] = 0
result = [id for id in ids if occasions[id] &gt; 1]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Or use <code>itertools.groupby</code>:</p>
<pre><code>&gt;&gt;&gt; l=[1,1,2,2,2,3]
&gt;&gt;&gt; from itertools import groupby
&gt;&gt;&gt; print([key for key,group in groupby(l) if len(list(group)) &gt; 1])
[1, 2]
&gt;&gt;&gt; 
</code></pre>
<p>Just check if the group (in loop) is bigger than one, if it is keep it, otherwise don't.</p>
<p>Or use <code>pandas</code>:</p>
<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; s=pd.Series(l)
&gt;&gt;&gt; s[s.duplicated()].unique().tolist()
[1, 2]
&gt;&gt;&gt; 
</code></pre>
<p>It's very fast, because <code>pandas</code> is super fast.</p>
<p>Documentation:</p>
<blockquote>
<p><a href="https://pandas.pydata.org/pandas-docs/stable/10min.html" rel="nofollow noreferrer">https://pandas.pydata.org/pandas-docs/stable/10min.html</a></p>
<p>Put cursor on the yellow part to view links.</p>
<blockquote class="spoiler">
<p> <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.duplicated.html#pandas.Series.duplicated" rel="nofollow noreferrer">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.duplicated.html#pandas.Series.duplicated</a>, and <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html#pandas.Series.unique" rel="nofollow noreferrer">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html#pandas.Series.unique</a></p>
</blockquote>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<p>If you don't care about the order in which these ids are retrieved, an efficient approach would consist in a sorting step (which is O(N log(N))) followed by keeping ids that are followed by themselves (which is O(N)). So this approach is overall O(N log(N)).</p>
</div>
<span class="comment-copy">I think it would be more efficient with <code>itertools.ifilter</code>. (i was writing an answer with both <code>Counter</code> and <code>ifilter</code> but you do it faster ;))</span>
<span class="comment-copy">@Kasra: Why? Using <code>filter</code>/<code>ifilter</code> instead of a generator expression/list comprehension means you have to wrap the test inside a function, which adds extra cost. (If you were just referring to not building the list if we don't need it, the easier way to do that is to just change the listcomp to a genexpr—but since he specifically says he needs to return a list, I don't think you can avoid building the list.)</span>
<span class="comment-copy">@Kasra: The first list is just creating the test data. You need to get input data from somewhere.</span>
<span class="comment-copy">@Kasra: Anyway, instead of guessing, from a quick timeit test on 10000 elements with 100 unique values, <code>[listcomp]</code> takes 1.88ms, <code>list(genexpr)</code> takes 2.25ms, <code>filter</code> takes 2.71ms, and <code>list(ifilter)</code> takes 2.91ms. So, using <code>ifilter</code> makes it 55% slower. At any rate, these kinds of micro-optimizations are rarely worth worrying about; once you get from quadratic behavior to linear, usually you can stop optimizing…</span>
<span class="comment-copy">Why not  <code>[i for i,j counts.items() if j &gt; 1]</code>?</span>
<span class="comment-copy">Just a small addition if you want to end with a list:      duplicates = list( set(i for i in ids if ids.count(i) &gt; 1) )</span>
<span class="comment-copy"><code>list(set(i for i in ids if ids.count(i) &gt; 1))</code></span>
<span class="comment-copy">This is even slower than his existing code (although only by a constant factor of 2-ish). It does remove the <code>len</code> call, but that part didn't matter; replacing the <code>elem in singles</code> with <code>ids.count</code> means you're now searching every element every time, instead of searching just for the first match in just the duplicates.</span>
<span class="comment-copy">Also, this destroys the order, and collapses all duplicates into 1, neither of which his original code did, so I don't think you can assume that's acceptable.</span>
<span class="comment-copy">Yeah, this is basically just a simple version of <code>Counter</code> implemented manually, so it'll be just as fast. (Or at least very close to it; <code>Counter</code> may be marginally faster by using <code>__missing__</code> instead of <code>except KeyError:</code>, but that's only going to make a small constant difference.) I think <code>occasions = Counter(ids)</code> is easier to read and harder to get wrong, but this is fine too—especially since it may be clearer to a novice who hasn't gotten used to thinking in terms of dicts why it helps.</span>
<span class="comment-copy">Okay, I totally agree with you. Thanks for your answer, I was unaware of such a feature.</span>
<span class="comment-copy">But the sort itself is O(N log N), so the O(N) step doesn't matter. Plus, that destroys the order, and we don't know if that's acceptable, so you can't just assume it is.</span>
<span class="comment-copy">Yes, it doesn't work if the result must be in the same order.</span>
