<div class="post-text" itemprop="text">
<p>I'm trying to create examples on how to manipulate massive databases composed of CSV tables using only Python.</p>
<p>I'd like to find out a way to emulate efficient indexed queries in tables spread through some <code>list()</code></p>
<p>The example below takes 24 seconds in a 3.2Ghz Core i5</p>
<pre><code>#!/usr/bin/env python
import csv
MAINDIR = "../"
pf = open (MAINDIR+"atp_players.csv")
players = [p for p in csv.reader(pf)]
rf = open (MAINDIR+"atp_rankings_current.csv")
rankings = [r for r in csv.reader(rf)]
for i in rankings[:10]:
    player = filter(lambda x: x[0]==i[2],players)[0]
    print "%s(%s),(%s) Points: %s"%(player[2],player[5],player[3],i[3])
</code></pre>
<p>For <a href="https://github.com/JeffSackmann/tennis_atp" rel="nofollow">this dataset</a>.</p>
<p>A more efficient, or more <em>pythonic</em> way would be greatly appreciated.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can <code>itertools.islice</code> instead of reading all rows and use <code>itertools.ifilter</code>:</p>
<pre><code>import csv
from itertools import islice,ifilter

MAINDIR = "../"
with  open(MAINDIR + "atp_players.csv") as pf,  open(MAINDIR + "atp_rankings_current.csv") as rf:
    players = list(csv.reader(pf))
    rankings = csv.reader(rf)
    # only get first ten rows using islice
    for i in islice(rankings, None, 10):
        # ifilter won't create a list, gives values in the fly
        player = next(ifilter(lambda x: x[0] == i[2], players),"")
</code></pre>
<p>Not quite sure what <code>filter(lambda x: x[0]==i[2],players)[0]</code> is doing, you seem to be searching the whole players list each time and just keeping the first element. It might pay to sort the list once with the first element as the key and use bisection search or build a dict with the first element as the key and the row as the value then simply do lookups.</p>
<pre><code>import csv
from itertools import islice,ifilter
from collections import OrderedDict

MAINDIR = "../"
with  open(MAINDIR + "atp_players.csv") as pf,  open(MAINDIR + "atp_rankings_current.csv") as rf:
    players = OrderedDict((row[0],row) for row in csv.reader(pf))
    rankings = csv.reader(rf)
    for i in islice(rankings, None, 10):
        # now constant work getting row as opposed to 0(n)    
        player = players.get(i[2])
</code></pre>
<p>What default value you use or indeed if any is needed you will have to decide.</p>
<p>If you have repeating elements at the start of each row but just want to return the first occurrence:</p>
<pre><code>with  open(MAINDIR + "atp_players.csv") as pf, open(MAINDIR + "atp_rankings_current.csv") as rf:
    players = {}
    for row in csv.reader(pf):
        key = row[0]
        if key in players:
            continue
        players[key] = row
    rankings = csv.reader(rf)
    for i in islice(rankings, None, 10):
        player = players.get(i[2])
</code></pre>
<p>Output:</p>
<pre><code>Djokovic(SRB),(R) Points: 11360
Federer(SUI),(R) Points: 9625
Nadal(ESP),(L) Points: 6585
Wawrinka(SUI),(R) Points: 5120
Nishikori(JPN),(R) Points: 5025
Murray(GBR),(R) Points: 4675
Berdych(CZE),(R) Points: 4600
Raonic(CAN),(R) Points: 4440
Cilic(CRO),(R) Points: 4150
Ferrer(ESP),(R) Points: 4045
</code></pre>
<p>Timing for the code on ten players shows ifilter to be the fastest but we will see the dict winning when we increase rankings and just how badly your code scales:</p>
<pre><code>In [33]: %%timeit
MAINDIR = "tennis_atp-master/"
pf = open ("/tennis_atp-master/atp_players.csv")                                                          players = [p for p in csv.reader(pf)]
rf =open( "/tennis_atp-master/atp_rankings_current.csv")
rankings = [r for r in csv.reader(rf)]
for i in rankings[:10]:
    player = filter(lambda x: x[0]==i[2],players)[0]
   ....: 
10 loops, best of 3: 123 ms per loop

In [34]: %%timeit
with  open("/tennis_atp-master/atp_players.csv") as pf, open( "/tennis_atp-master/atp_rankings_current.csv") as rf:                     players = list(csv.reader(pf))
    rankings = csv.reader(rf)    # only get first ten rows using islice
    for i in islice(rankings, None, 10):
        # ifilter won't create a list, gives values in the fly
        player = next(ifilter(lambda x: x[0] == i[2], players),"")
   ....: 
10 loops, best of 3: 43.6 ms per loop

In [35]: %%timeit                           
with  open("/tennis_atp-master/atp_players.csv") as pf, open( "/tennis_atp-master/atp_rankings_current.csv") as rf:
    players = {}
    for row in csv.reader(pf):
        key = row[0]
        if key in players:
            continue
        players[row[0]] = row
    rankings = csv.reader(rf)
    for i in islice(rankings, None, 10):
        player = players.get(i[2])
        pass
   ....: 
10 loops, best of 3: 50.7 ms per loop
</code></pre>
<p>Now with 100 players you will see the dict is as fast as it was for 10. The cost of building the dict has been offset by constant time lookups:</p>
<pre><code>In [38]: %%timeit
with  open("/tennis_atp-master/atp_players.csv") as pf, open("/tennis_atp-master/atp_rankings_current.csv") as rf:
    players = list(csv.reader(pf))
    rankings = csv.reader(rf)
    # only get first ten rows using islice
    for i in islice(rankings, None, 100):
        # ifilter won't create a list, gives values in the fly
        player = next(ifilter(lambda x: x[0] == i[2], players),"")
   ....: 
10 loops, best of 3: 120 ms per loop

In [39]: %%timeit
with  open("/tennis_atp-master/atp_players.csv") as pf, open( "/tennis_atp-master/atp_rankings_current.csv") as rf:
    players = {}                  
    for row in csv.reader(pf):
        key = row[0]
        if key in players:
            continue                                          
        players[row[0]] = row                                     
    rankings = csv.reader(rf)
    for i in islice(rankings, None, 100):
        player = players.get(i[2])
        pass
   ....: 
10 loops, best of 3: 50.7 ms per loop

In [40]: %%timeit
MAINDIR = "tennis_atp-master/"
pf = open ("/tennis_atp-master/atp_players.csv")
players = [p for p in csv.reader(pf)]
rf =open( "/tennis_atp-master/atp_rankings_current.csv")
rankings = [r for r in csv.reader(rf)]
for i in rankings[:100]:
    player = filter(lambda x: x[0]==i[2],players)[0]
   ....: 
1 loops, best of 3: 806 ms per loop
</code></pre>
<p>For 250 players:</p>
<pre><code># your code
1 loops, best of 3: 1.86 s per loop

# dict
10 loops, best of 3: 50.7 ms per loop

# ifilter
10 loops, best of 3: 483  ms per loop
</code></pre>
<p>The final test looping over the whole rankings:</p>
<pre><code># your code

1 loops, best of 3: 2min 40s per loop

# dict 
10 loops, best of 3: 67 ms per loop

# ifilter
1 loops, best of 3: 1min 3s per loop
</code></pre>
<p>So you can see as we loop over more rankings  the <strong>dict</strong> option is by far the most efficient as far as runtime goes and will scale extremely well. </p>
</div>
<div class="post-text" itemprop="text">
<p>Consider putting your data in an <a href="https://docs.python.org/3/library/sqlite3.html" rel="nofollow">SQLite database</a>. This meets your requirement of using only Python, since it is built into the standard Python library and supported in (almost) all Python interpreters. SQLite is a database library that allows you to do processing on data using the SQL syntax. It gives you features like indexing and foreign key relationships.</p>
<p>If you need to do multiple queries on the data, doing some pre-computation (i.e. indexes and data normalization) is the most sensible route.</p>
</div>
<div class="post-text" itemprop="text">
<p>This code doesn't take that much time to run. So I'm going to assume that you were really running through more of the rankings that just 10.  When I run through them all it takes a long time.  If that is what you are interested in doing, then a dictionary would shorten the search time.  For a bit of overhead to setup the dictionary, you can search it very fast.  Here's how I've modified your for loop:</p>
<pre><code>play_dict = {}
for index, player in enumerate(players):
    play_dict[player[0]] = index
for i in rankings[:10]:
    player = players[play_dict[i[2]]]
</code></pre>
<p>With this code you can process all the rankings instantaneously.</p>
</div>
<span class="comment-copy">Estimating 5000 lines per file, your computer is processing 5000*47 / 24 = about <i>10000 lines per second</i>. I know you want it faster, but perhaps the dataset is just too large.</span>
<span class="comment-copy">why not use <a href="http://pandas.pydata.org/" rel="nofollow noreferrer" title="pandas">pandas</a>? video on how to use pandas can be found <a href="https://player.vimeo.com/video/59324550" rel="nofollow noreferrer">here</a>.</span>
<span class="comment-copy">@PadraicCunningham i believe our discussion below makes clear his logic (at least for <code>rankings</code>).  he makes the list so that he can iterate over just a subset of the rankings. (your method is better.)</span>
<span class="comment-copy">I was going to suggest iterating over an enumerated reader, then breaking -- this is nicer.</span>
<span class="comment-copy">why keep all of <code>rankings</code> in memory while you're iterating over just a small subset of it?</span>
<span class="comment-copy">@dbliss, where am I keeping all rankings in memory?</span>
<span class="comment-copy">@dbliss, I think you don't really understand what a csv.reader object is</span>
<span class="comment-copy">@dbliss <code>csv.reader</code> returns a file iterator, so it will only consume data from the file on demand, akin to lazy evaluation from a functional language or similar. When you say "<code>csv.reader</code> returns something, and the whole of what it returns is not needed" this belies an assumption that it is pre-emptively fetching data, which it isn't. The actual <code>reader</code> object itself is extremely minimal and has nothing to do with the data, other than iterating it on demand.</span>
