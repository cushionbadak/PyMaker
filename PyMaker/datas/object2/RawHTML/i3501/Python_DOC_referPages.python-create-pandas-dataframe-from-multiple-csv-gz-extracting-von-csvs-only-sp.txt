<div class="post-text" itemprop="text">
<p>I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:</p>
<pre><code>import glob
import pandas as pd

# get data file names
path =r'C:\DRO\DCL_rawdata_files'
filenames = glob.glob(path + "/*.csv")

dfs = []
for filename in filenames:
    dfs.append(pd.read_csv(filename))

# Concatenate all data into one DataFrame
big_frame = pd.concat(dfs, ignore_index=True)
</code></pre>
<p>I guess I need some help within the for loop???</p>
</div>
<div class="post-text" itemprop="text">
<p>If you have same columns in all your <code>csv</code> files then you can try the code below.
I have added <code>header=0</code> so that after reading <code>csv</code> first row can be assigned as the column names.</p>
<pre><code>import glob

path = r'C:\DRO\DCL_rawdata_files' # use your path
all_files = glob.glob(path + "/*.csv")

li = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

frame = pd.concat(li, axis=0, ignore_index=True)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>An alternative to <a href="https://stackoverflow.com/a/21232849/3888455">darindaCoder's answer</a>:</p>
<pre><code>path = r'C:\DRO\DCL_rawdata_files'                     # use your path
all_files = glob.glob(os.path.join(path, "*.csv"))     # advisable to use os.path.join as this makes concatenation OS independent

df_from_each_file = (pd.read_csv(f) for f in all_files)
concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)
# doesn't create a list, nor does it append to one
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>import glob, os    
df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', "my_files*.csv"))))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Edit: I googled my way into <a href="https://stackoverflow.com/a/21232849/186078">https://stackoverflow.com/a/21232849/186078</a>.
However of late I am finding it faster to do any manipulation using numpy and then assigning it once to dataframe rather than manipulating the dataframe itself on an iterative basis and it seems to work in this solution too.</p>
<p>I do sincerely want anyone hitting this page to consider this approach, but don't want to attach this huge piece of code as a comment and making it less readable.  </p>
<p>You can leverage numpy to really speed up the dataframe concatenation. </p>
<pre><code>import os
import glob
import pandas as pd
import numpy as np

path = "my_dir_full_path"
allFiles = glob.glob(os.path.join(path,"*.csv"))


np_array_list = []
for file_ in allFiles:
    df = pd.read_csv(file_,index_col=None, header=0)
    np_array_list.append(df.as_matrix())

comb_np_array = np.vstack(np_array_list)
big_frame = pd.DataFrame(comb_np_array)

big_frame.columns = ["col1","col2"....]
</code></pre>
<p>Timing stats:</p>
<pre><code>total files :192
avg lines per file :8492
--approach 1 without numpy -- 8.248656988143921 seconds ---
total records old :1630571
--approach 2 with numpy -- 2.289292573928833 seconds ---
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The Dask library can read a dataframe from multiple files: </p>
<pre><code>&gt;&gt;&gt; import dask.dataframe as dd
&gt;&gt;&gt; df = dd.read_csv('data*.csv')
</code></pre>
<p>(Source: <a href="http://dask.pydata.org/en/latest/examples/dataframe-csv.html" rel="noreferrer">http://dask.pydata.org/en/latest/examples/dataframe-csv.html</a>)</p>
<p>The Dask dataframes implement a subset of the Pandas dataframe API. If all the data fits into memory, you can <a href="http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.compute" rel="noreferrer">call <code>df.compute()</code></a> to convert the dataframe into a Pandas dataframe.</p>
</div>
<div class="post-text" itemprop="text">
<p>Almost all of the answers here are either unnecessarily complex (glob pattern matching) or rely on additional 3rd party libraries. You can do this in 2 lines using everything Pandas and python (all versions) already have built in.</p>
<p>For a few files - 1 liner:</p>
<pre><code>df = pd.concat(map(pd.read_csv, ['data/d1.csv', 'data/d2.csv','data/d3.csv']))
</code></pre>
<p>For many files:</p>
<pre><code>from os import listdir

filepaths = [f for f in listdir("./data") if f.endswith('.csv')]
df = pd.concat(map(pd.read_csv, filepaths))
</code></pre>
<hr/>
<p>This pandas line which sets the df utilizes 3 things:</p>
<ol>
<li><a href="https://docs.python.org/3.5/library/functions.html#map" rel="noreferrer">Python's map (function, iterable)</a> sends to the function (the
<code>pd.read_csv()</code>) the iterable (our list) which is every csv element
in filepaths).</li>
<li>Panda's <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html" rel="noreferrer">read_csv()</a> function reads in each CSV file as normal.</li>
<li>Panda's <a href="http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html" rel="noreferrer">concat()</a> brings all these under one df variable.</li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p>If you want to <strong>search recursively</strong> (<em>Python 3.5 or above</em>), you can do the following:</p>
<pre><code>from glob import iglob
import pandas as pd

path = r'C:\user\your\path\**\*.csv'

all_rec = iglob(path, recursive=True)     
dataframes = (pd.read_csv(f) for f in all_rec)
big_dataframe = pd.concat(dataframes, ignore_index=True)
</code></pre>
<p>Note that the three last lines can be expressed in one <strong>single line</strong>:</p>
<pre><code>df = pd.concat((pd.read_csv(f) for f in iglob(path, recursive=True)), ignore_index=True)
</code></pre>
<p>You can find the documentation of <code>**</code> <a href="https://docs.python.org/3/library/glob.html#glob.glob" rel="nofollow noreferrer">here</a>. Also, I used <code>iglob</code>instead of <code>glob</code>, as it returns an <strong>iterator</strong> instead of a list.</p>
<hr/>
<hr/>
<p><strong>EDIT: Multiplatform recursive function:</strong></p>
<p>You can wrap the above into a <strong>multiplatform function</strong> (Linux, Windows, Mac), so you can do:</p>
<pre><code>df = read_df_rec('C:\user\your\path', *.csv)
</code></pre>
<p>Here is the function:</p>
<pre><code>from glob import iglob
from os.path import join
import pandas as pd

def read_df_rec(path, fn_regex=r'*.csv'):
    return pd.concat((pd.read_csv(f) for f in iglob(
        join(path, '**', fn_regex), recursive=True)), ignore_index=True)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If the multiple csv files are zipped, you may use zipfile to read all and concatenate as below: </p>
<pre><code>import zipfile
import numpy as np
import pandas as pd

ziptrain = zipfile.ZipFile('yourpath/yourfile.zip')

train=[]

for f in range(0,len(ziptrain.namelist())):
    if (f == 0):
        train = pd.read_csv(ziptrain.open(ziptrain.namelist()[f]))
    else:
        my_df = pd.read_csv(ziptrain.open(ziptrain.namelist()[f]))
        train = (pd.DataFrame(np.concatenate((train,my_df),axis=0), 
                          columns=list(my_df.columns.values)))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I found this method pretty elegant.</p>
<pre><code>import pandas as pd
import os

big_frame = pd.DataFrame()

for file in os.listdir():
    if file.endswith('.csv'):
        df = pd.read_csv(file)
        big_frame = big_frame.append(df, ignore_index=True)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>one liner using <code>map</code>, but if you'd like to specify additional args, you could do:</p>
<pre class="lang-py prettyprint-override"><code>import pandas as pd
import glob
import functools

df = pd.concat(map(functools.partial(pd.read_csv, sep='|', compressed=None), 
                    glob.glob("data/*.csv")))
</code></pre>
<p>Note: <code>map</code> be itself does not let you supply additional args.</p>
</div>
<div class="post-text" itemprop="text">
<p>Another on-liner with list comprehension which allows to use arguments with read_csv.</p>
<pre><code>df = pd.concat([pd.read_csv(f'dir/{f}') for f in os.listdir('dir') if f.endswith('.csv')])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<h1>Easy and Fast</h1>
<p>Import 2 or more <code>csv</code>'s without having to make a list of names. </p>
<pre><code>import glob

df = pd.concat(map(pd.read_csv, glob.glob('data/*.csv'))
</code></pre>
</div>
<span class="comment-copy">your code does nothing because you are not appending to your <code>dfs</code> list, don't you want to replace the line <code>data = pd.read_csv(filename)</code> with <code>dfs.append(pd.read_csv(filename)</code>. You would then need to loop over the list and <code>concat</code>, I don't think <code>concat</code> will work on a list of <code>df</code>s.</span>
<span class="comment-copy">also you are mixing an alias for the module with the module name in your last line, shouldn't it be <code>big_frame = pd.concat(dfs, ignore_index=True)</code>?, anyway once you have a list of dataframes you will need to iterate over the list and concat to <code>big_frame</code></span>
<span class="comment-copy">Yes, I edited the code, but i'm still not able to build a concatenated dataframe from the csv-files, I'm new to python so I need some more help on this</span>
<span class="comment-copy">you need to loop over <code>dfs</code> now, so something like <code>for df in dfs: big_frame.concat(df, ignore_index=True)</code> should work, you could also try <code>append</code> instead of <code>concat</code> also.</span>
<span class="comment-copy">Can you tell more exactly what is not working? Because <code>concat</code> should handle a list of DataFrames just fine like you did. I think this is a very good approach.</span>
<span class="comment-copy">What is the purpose of the initial <code>frame = pd.DataFrame()</code>?</span>
<span class="comment-copy">I think you should add "ignore_index=True" to pd.concat(list)</span>
<span class="comment-copy">The same thing more concise, and perhaps faster as it doesn't use a list:  <code>df = pd.concat((pd.read_csv(f) for f in all_files))</code>  Also, one should perhaps use <code>os.path.join(path, "*.csv")</code> instead of <code>path + "/*.csv"</code>, which makes it OS independent.</span>
<span class="comment-copy">@Sid make this an answer already so that the current atrocity can lose its lead.</span>
<span class="comment-copy">@curtisp you can still do that with Sid's answer, just use <code>pandas.read_csv(f).assign(filename = foo)</code> inside the generator.  <code>assign</code> will return the entire dataframe including the new column <code>filename</code></span>
<span class="comment-copy">@bongbang Using parentheses returns a generator instead of a list.</span>
<span class="comment-copy">@Sid the nested parens to form a generator and wrap the arguments to a function are are redundant, i.e. you can just do <code>pd.concat(pd.read_csv(f) for f in all_files)</code>.</span>
<span class="comment-copy">@Mike that's amazing, wasn't aware, editing my answer accordingly.</span>
<span class="comment-copy">@Sid you'd need ignore_index=True unless by luck the csv files have an index column and that index is unique across files.</span>
<span class="comment-copy">I recommend using <code>glob.iglob</code> instead of <code>glob.glob</code>; The first one returns and <a href="https://docs.python.org/3/library/glob.html#glob.iglob" rel="nofollow noreferrer">iterator (instead of a list)</a>.</span>
<span class="comment-copy">Excellent one liner, specially useful if no read_csv arguments are needed!</span>
<span class="comment-copy">If, on the other hand, arguments are needed, this can be done with lambdas: <code>df = pd.concat(map(lambda file: pd.read_csv(file, delim_whitespace=True), data_files))</code></span>
<span class="comment-copy">Any numbers to back the "speed up"? Specifically, is it faster than <a href="http://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe/36073025#comment57648172_21232849" title="import multiple csv files into pandas and concatenate into one dataframe">stackoverflow.com/questions/20906474/â€¦</a> ?</span>
<span class="comment-copy">I don't see the OP asking for a way to speed up his concatenation, this just looks like a rework of a pre-existing accepted answer.</span>
<span class="comment-copy">@ivan, see my edit with timing</span>
<span class="comment-copy">That won't work if the data has mixed columns types.</span>
<span class="comment-copy">@SKG perfect.. this is the only working solution for me. 500 files 400k rows total in 2 secs. Thanks for posting it.</span>
<span class="comment-copy">or just <code>df = pd.concat(map(pd.read_csv, glob.glob('data/*.csv))</code></span>
