<div class="post-text" itemprop="text">
<p>I wrote a spider in Scrapy to extract data from quotes.toscrape.com, but when I exported the extracted data to csv, the " (quote symbol) is converting itself to characters like â€</p>
<p>Here is the code written under spider as can be seen on sublime text3 on a windows machine.</p>
<pre><code># -*- coding: utf-8 -*-
import scrapy


class TestSpider(scrapy.Spider):
    name = 'Test'
    allowed_domains = ['quotes.toscrape.com']
    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        quotes = response.xpath('//*[@class="quote"]')
        for quote in quotes:
            text = quote.xpath('.//*[@class="text"]/text()').extract_first()
            author = quote.xpath('.//*[@class="author"]/text()').extract_first()
            tags = quote.xpath('.//*[@itemprop="keywords"]/@content').extract_first()
            yield{"Text": text, "Author": author, "Tags": tags}
        next_p = response.xpath('//*[@class="next"]/a/@href').extract_first()
        absolute_n = response.urljoin(next_p)
        yield scrapy.Request(absolute_n)
</code></pre>
<p>Also, here is the command I used to export the data which is defined as in class dictionary to a csv file.(This was run via scrapy shell under windows command prompt)</p>
<pre><code>scrapy crawl Test -o scraped.csv
</code></pre>
<p>And this is how, I have received the data in csv file.
<img alt="" src="https://i.stack.imgur.com/9dzyN.png"/></p>
<p>Please help me resolve that treating me like a beginner.</p>
</div>
<div class="post-text" itemprop="text">
<p>That sequence of mojibake looks like what you get if you encode smart quotes (like '“`, U+201C) as UTF-8 and then try to decode them as ISO Latin 9, Windows-1252, or something else that's similar to Latin-1 but has a Euro symbol. For example:</p>
<pre><code>&gt;&gt;&gt; print('\u201c'.encode('utf-8').decode('iso-8859-9')
â
</code></pre>
<hr/>
<p>There are two likely places things could be going wrong. Since you haven't shown us the raw bytes at any step in the process, or any of your code, it's impossible to know which of the two is going wrong, but I can explain how to deal with both of them.</p>
<hr/>
<p>First, you could be decoding the HTML response that contains these quotes as Latin-9 or whatever, even though it's encoded in UTF-8.</p>
<p>If you're doing this explicitly, just stop doing that. </p>
<p>But more likely, you're getting, e.g, a <a href="https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse" rel="nofollow noreferrer"><code>TextResponse</code></a> from Scrapy and just accessing <code>resp.text</code>, and the page had an incorrect header or <code>meta</code> tag or the like, causing Scrapy to mis-decode it. </p>
<p>To fix this, you want to access the raw bytes and decode them explicitly. So, if you were using <code>resp.text</code>, you'd do <code>resp.body.decode('utf8')</code> instead.</p>
<hr/>
<p>Alternatively, you could be decoding the HTML fine, and encoding the CSV fine, and you're just opening that CSV as Latin-9 instead of UTF-8. In which case there's nothing to change in your code; you just need to look at the settings of your spreadsheet program.</p>
<p>However, if you're on Windows, a lot of Windows software (especially from Microsoft) makes some weird assumptions. By default, a text file is assumed to be encoded in the OEM codepage, which is usually something like Windows-1252. To override this and force UTF-8, you're expected to include a "byte order mark". This isn't really a byte order mark (because that makes no sense for 8-bit encodings), and it's strongly discouraged by the standards for UTF-8, but Microsoft does it anyway).</p>
<p>So, if you're using Excel on Windows, and you don't want to change the settings, you can work around Microsoft's problem by writing the file with the <a href="https://docs.python.org/3/library/codecs.html#encodings-and-unicode" rel="nofollow noreferrer"><code>utf-8-sig</code></a> encoding instead of <code>utf-8</code>, which will force this "BOM" to be written:</p>
<pre><code>with open('outfile.csv', 'w', encoding='utf-8-sig') as f:
    writer = csv.writer(f)
    # etc.
</code></pre>
<p>Since you appear to be creating your export pipeline just by passing <code>-o csv</code> to the <code>scrapy crawl</code> command, I believe you need to set <a href="https://doc.scrapy.org/en/latest/topics/feed-exports.html#feed-export-encoding" rel="nofollow noreferrer"><code>FEED_EXPORT_ENCODING</code></a> either in your config file (by editing <code>settings.py</code> or using the <code>scrapy settings</code> command), on the <code>crawl</code> command line (<code>-set FEED_EXPORT_ENDCODING=utf-8-sig</code>), or in an environment variable (<code>SET FEED_EXPORT_ENDCODING=utf-8-sig</code> in the <code>cmd</code> console window before you <code>scrapy crawl</code>).</p>
</div>
<span class="comment-copy">Looks like those are probably smart quotes (<code>“</code> and <code>”</code>) instead of straight quotes (<code>"</code>), so check your text encoding.</span>
<span class="comment-copy">Yes, they are as per availability on the website "<a href="http://quotes.toscrape.com/" rel="nofollow noreferrer">quotes.toscrape.com</a>".</span>
<span class="comment-copy">Thanks,I have edited the question to provide the additional information.Also, it must be noted that I haven't messed up with encoding at any point so that according to me makes the issue one with Windows MS Excel.Please if possible elaborate the corrective steps(also how) I can take and shall that be taken before exporting the data into csv or afterwards.  Will be greatly thankful.(I'm just a beginner so don't know much)</span>
<span class="comment-copy">The easiest thing to do is to just tell Excel to open the file as UTF-8 instead of the default, or to configure Excel to default to UTF-8 instead of the default default. But if you want to change the CSV files from your Python script, that's the <code>utf-8-sig</code> encoding, as already explained in my answer. If there's some part of that you don't understand, you'll need to tell me what it is.</span>
<span class="comment-copy">Tell me a way to force the exported data to get saved into a excel csv in utf-8 encoding. if that can be possible by changing my excel encoding permanently to utf-8(please elaborate the steps).</span>
<span class="comment-copy">@VishalSharma I don't have a Windows machine to test on, and you can read the docs as well as I can, but I've added a paragraph that will hopefully explain all you need here.</span>
<span class="comment-copy">Thanks for your time (Highly appreciated)... It did the work for me. To summarize, I did two things: 1- Changed the default encoding of excel to utf-8 through registry keys. 2- Set the Feed_Export_Encoding to 'utf-8-sig' by going to default_settings.py in C:\Intel\Lib\site-packages\scrapy\settings.</span>
