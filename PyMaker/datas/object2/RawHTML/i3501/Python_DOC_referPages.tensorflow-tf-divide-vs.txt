<div class="post-text" itemprop="text">
<p>In tensorflow tutorials, I see both codes like <code>tf.add(tf.matmul(X, W), b)</code> and <code>tf.matmul(X, W) + b</code>, what is the difference between using the math function <code>tf.add()</code>, <code>tf.assign()</code>, etc and the operators <code>+</code> and <code>=</code>, etc, in precision or other aspects? </p>
</div>
<div class="post-text" itemprop="text">
<p>There's no difference in precision between <code>a+b</code> and <code>tf.add(a, b)</code>. The former translates to <code>a.__add__(b)</code> which gets mapped to <code>tf.add</code> by means of <a href="https://github.com/tensorflow/tensorflow/blob/c43a32d5d0929170a057862e2cd0b59308421444/tensorflow/python/ops/math_ops.py#L845" rel="noreferrer">following line</a> in math_ops.py </p>
<p><code>_OverrideBinaryOperatorHelper(gen_math_ops.add, "add")</code></p>
<p>The only difference is that node name in the underlying Graph is <code>add</code> instead of <code>Add</code>. You can generally compare things by looking at the underlying Graph representation like this</p>
<pre><code>tf.reset_default_graph()
dtype = tf.int32
a = tf.placeholder(dtype)
b = tf.placeholder(dtype)
c = a+b
print(tf.get_default_graph().as_graph_def())
</code></pre>
<p>You could also see this directly by inspecting the <code>__add__</code> method. There's an extra level of indirection because it's a closure, but you can get the underlying function as follows</p>
<pre><code>real_function = tf.Tensor.__add__.im_func.func_closure[0].cell_contents
print(real_function.__module__ + "." + real_function.__name__)
print(tf.add.__module__ + "." + tf.add.__name__)
</code></pre>
<p>And you'll see output below which means that they call same underlying function</p>
<pre><code>tensorflow.python.ops.gen_math_ops.add
tensorflow.python.ops.gen_math_ops.add
</code></pre>
<p>You can see from <code>tf.Tensor.OVERLOADABLE_OPERATORS</code> that following Python special methods are potentially overloaded by appropriate TensorFlow versions</p>
<pre><code>{'__abs__',
 '__add__',
 '__and__',
 '__div__',
 '__floordiv__',
 '__ge__',
 '__getitem__',
 '__gt__',
 '__invert__',
 '__le__',
 '__lt__',
 '__mod__',
 '__mul__',
 '__neg__',
 '__or__',
 '__pow__',
 '__radd__',
 '__rand__',
 '__rdiv__',
 '__rfloordiv__',
 '__rmod__',
 '__rmul__',
 '__ror__',
 '__rpow__',
 '__rsub__',
 '__rtruediv__',
 '__rxor__',
 '__sub__',
 '__truediv__',
 '__xor__'}
</code></pre>
<p>Those methods are described in <a href="https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types" rel="noreferrer">Python reference 3.3.7</a>: emulating numeric types. Note that Python data model does not provide a way to overload assignment operator <code>=</code> so assignment always uses native Python implementation.</p>
</div>
<div class="post-text" itemprop="text">
<p>Yaroslav nicely explained that there is no real difference. I will just add when using <code>tf.add</code> is beneficial.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/math/add" rel="nofollow noreferrer">tf.add</a> has one important parameter which is <code>name</code>. It allows you to name the operation in a graph which will be visible in tensorboard. So my rule of thumb, if it will be beneficial to name an operation in tensorboard, I use <code>tf.</code> equivalent, otherwise I go for brevity and use overloaded version.</p>
</div>
<div class="post-text" itemprop="text">
<pre><code>a = [1,1,1,1]
b = [1,1,1,1]
w = tf.add(a, b)


with tf.Session() as sess:
    p = sess.run(w)
    print(p)

a+b
</code></pre>
<p>Now, the value of <code>p</code> printed will be <code>[2,2,2,2]</code> and simple <code>a+b</code> printed will be <code>[1,1,1,1,1,1,1,1]</code>.</p>
</div>
<span class="comment-copy">Possible duplicate of <a href="https://stackoverflow.com/questions/35094899/tensorflow-operator-overloading">TensorFlow operator overloading</a></span>
<span class="comment-copy">So why these tensorflow methods are defined at all?</span>
<span class="comment-copy">@Hossein Because we at Google love being extra.</span>
