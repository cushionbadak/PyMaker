<div class="post-text" itemprop="text">
<p>i have been dong a simple webscraping program to learn how to code and i made it work but i wanted to see how to make it faster. I wanted to ask how could i implement multi-threading to this program? all that the program does is open the stock symbols file and searches for the price for that stock online.</p>
<p>Here is my code</p>
<pre><code>import urllib.request
import urllib
from threading import Thread

symbolsfile = open("Stocklist.txt")

symbolslist = symbolsfile.read()

thesymbolslist = symbolslist.split("\n")

i=0


while i&lt;len (thesymbolslist):
    theurl = "http://www.google.com/finance/getprices?q=" + thesymbolslist[i] + "&amp;i=10&amp;p=25m&amp;f=c"
    thepage = urllib.request.urlopen(theurl)
    # read the correct character encoding from `Content-Type` request header
    charset_encoding = thepage.info().get_content_charset()
    # apply encoding
    thepage = thepage.read().decode(charset_encoding)
    print(thesymbolslist[i] + " price is " + thepage.split()[len(thepage.split())-1])
    i= i+1
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you just iterate a function on a list, i recommend you the <code>multiprocessing.Pool.map(function, list)</code>.</p>
<p><a href="https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing%20map#multiprocessing.pool.Pool.map" rel="nofollow">https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing%20map#multiprocessing.pool.Pool.map</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You need to use <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" rel="nofollow">asyncio</a>. That's quite neat package that could also help you with scrapping. I have created a small snippet of how to <a href="https://anvileight.com/blog/2016/05/24/fetch-linkedin-profile-api-using-asyncio/" rel="nofollow">integrate with linkedin</a> with asyncio but you can adopt it to your needs quite easily.</p>
<pre><code>import asyncio
import requests

def scrape_first_site():
    url = 'http://example.com/'
    response = requests.get(url)


def scrape_another_site():
    url = 'http://example.com/other/'
    response = requests.get(url)

loop = asyncio.get_event_loop()

tasks = [
    loop.run_in_executor(None, scrape_first_site),
    loop.run_in_executor(None, scrape_another_site)
]

loop.run_until_complete(asyncio.wait(tasks))
loop.close()
</code></pre>
<p>Since default executor is ThreadPoolExecutor it will run each task in the separate thread. You can use ProcessPoolExecutor if you'd like to run tasks in process rather than threads (GIL related issues maybe).</p>
</div>
<span class="comment-copy">Can you show what stocklist.txt looks like</span>
<span class="comment-copy">it is just a text document with all the stock names. goes something like this: ABY ABEO ABEOW ABIL ABMD AXAS ACIA ACTG  and so on, they all have an ENTER after each one</span>
<span class="comment-copy">Also try using requests. It's better than urllib</span>
<span class="comment-copy">isnt urllib.request the same?</span>
