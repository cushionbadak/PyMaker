<div class="post-text" itemprop="text">
<p>I have to make numerous (thousands) of HTTP GET requests to a great deal of websites. This is pretty slow, for reasons that some websites may not respond (or take long to do so), while others time out. As I need as many responses as I can get, setting a small timeout (3-5 seconds) is not in my favour.</p>
<p>I have yet to do any kind of multiprocessing or multi-threading in Python, and I've been reading the documentation for a good while. Here's what I have so far:</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from multiprocessing import Process, Pool

errors = 0

def get_site_content(site):
    try :
        # start = time.time()
        response = requests.get(site, allow_redirects = True, timeout=5)
        response.raise_for_status()
        content = response.text
    except Exception as e:
        global errors
        errors += 1
        return ''
    soup = BeautifulSoup(content)
    for script in soup(["script", "style"]):
        script.extract()
    text = soup.get_text()

    return text

sites = ["http://www.example.net", ...]

pool = Pool(processes=5)
results = pool.map(get_site_content, sites)
print results
</code></pre>
<p>Now, I want the results that are returned to be joined somehow. This allows two variation:</p>
<ol>
<li><p>Each process has a local list/queue that contains the content it has accumulated is joined with the other queues to form a single result, containing all the content for all sites.</p></li>
<li><p>Each process writes to a single global queue as it goes along. This would entail some locking mechanism for concurrency checks.</p></li>
</ol>
<p>Would multiprocessing or multithreading be the better choice here? How would I accomplish the above with either of the approaches in Python?</p>
<hr/>
<p><strong>Edit</strong>:</p>
<p>I did attempt something like the following:</p>
<pre><code># global
queue = []
with Pool(processes = 5) as pool:
    queue.append(pool.map(get_site_contents, sites))

print queue
</code></pre>
<p>However, this gives me the following error:</p>
<pre><code>with Pool(processes = 4) as pool:
AttributeError: __exit__
</code></pre>
<p>Which I don't quite understand. I'm having a little trouble understanding <em>what</em> exactly pool.map does, past applying the function on every object in the iterable second parameter. Does it return anything? If not, do I append to the global queue from within the function?</p>
</div>
<div class="post-text" itemprop="text">
<p>I had a similar assignment in university (to implement a multiprocess web crawler) and used a multiprocessing-safe Queue class from python multiprocessing library, which will do all the magic with locks and concurrency checks. The example from docs states:        </p>
<pre><code>import multiprocessing as mp

def foo(q):
    q.put('hello')

if __name__ == '__main__':
    mp.set_start_method('spawn')
    q = mp.Queue()
    p = mp.Process(target=foo, args=(q,))
    p.start()
    print(q.get())
    p.join()
</code></pre>
<p>However, I had to write a separate process class for this to work, as I wanted it to work. And I hadn't used a Pool of processes. Instead I tried to check memory usage and spawn a process until a preset memory threshold was reached.</p>
</div>
<div class="post-text" itemprop="text">
<p><code>pool.map</code> starts 'n' number of processes that take a function and runs it with an item from the iterable. When such a process finishes and returns, the returned value is stored in a result list in the same position as the input item in the input variable.</p>
<p>eg: if a function is written to calculate square of a number and then a <code>pool.map</code> is used to run this function on a list of numbers.
    def square_this(x):
        square = x**2
        return square</p>
<pre><code>input_iterable = [2, 3, 4]
pool = Pool(processes=2) # Initalize a pool of 2 processes
result = pool.map(square_this, input_iterable) # Use the pool to run the function on the items in the iterable
pool.close() # this means that no more tasks will be added to the pool
pool.join() # this blocks the program till function is run on all the items
# print the result
print result

...&gt;&gt;[4, 9, 16]
</code></pre>
<p>The <code>Pool.map</code> technique may not be ideal in your case since it will block till all the processes finishes. i.e. If a website does not respond or takes too long to respond your program will be stuck waiting for it. Instead try sub-classing the <code>multiprocessing.Process</code> in your own class which polls these websites and use Queues to access the results. When you have a satisfactory number of responses you can stop all the processes without having to wait for the remaining requests to finish.</p>
</div>
<span class="comment-copy">Did you read the <a href="https://docs.python.org/3/library/multiprocessing.html#introduction" rel="nofollow noreferrer">introduction on <code>multiprocessing</code></a>? It describes how this can be done.</span>
<span class="comment-copy">@LutzHorn Yes, specifically [<a href="https://docs.python.org/3/library/multiprocessing.html#using-a-pool-of-workers](this" rel="nofollow noreferrer">docs.python.org/3/library/â€¦</a> section). I have updated my question with some more specific information pertaining to it, as I found the documentation slightly confusing, unfortunately.</span>
<span class="comment-copy">The exception occurs because you use a python version that does not support context managers for Pool. So just don't use the with statement with pools or switch to the newest version. Other than that, Pool.map returns a list with all results, so with your code you create a list queue containing the actual list of results. Otherwise your code seems fine.</span>
<span class="comment-copy">Ah, okay. In that case though (without using <code>with</code>) how do I know when  all processes have finished and the objects are safe to access (i.e. no writes are being done)? Basically, is there anything similar to threads' <code>join()</code> in this case? Or would it be better to simply stick to the pre-implemented <code>Queue</code> class like someone else suggested?</span>
<span class="comment-copy">On another note, check out concurrent.futures.threadpoolexecutor. It basically does the same thing with threads, which means you won't create a new process for each function call.</span>
<span class="comment-copy">I got AttributeError: 'module' object has no attribute 'set_start_method'</span>
