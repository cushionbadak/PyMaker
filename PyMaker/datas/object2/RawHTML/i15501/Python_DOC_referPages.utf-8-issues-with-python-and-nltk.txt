<div class="post-text" itemprop="text">
<p>I have this little script, which is basically just a test I'm doing for a larger program. I have a problem with the encoding. When I try to write to file the utf-8 characters, such as øæå, are not encoded properly. Why is that, and how can I solve this issue?</p>
<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import nltk
from nltk.collocations import *

collocations = open('bam.txt', 'w')
bigram_measures = nltk.collocations.BigramAssocMeasures()
tokens = nltk.wordpunct_tokenize("Hei på deg øyerusk, du er meg en gammel dust, neida neida, det er ikke helt sant da."
                                 "Men du, hvorfor så brusk, ikke klut i din susk på en runkete lust")
finder = BigramCollocationFinder.from_words(tokens)
# finder.apply_freq_filter(3)
scored = finder.score_ngrams(bigram_measures.raw_freq)
for i in scored:
    print i[0][0] + ' ' + i[0][1] + ': ' + str(i[1]) + '\n'
    collocations.write(i[0][0] + ' ' + i[0][1] + ': ' + str(i[1]) + '\n')

collocations.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The thing is <code>nltk.wordpunct_tokenize</code> doesn't work with non-ascii data. It is better to use <code>PunktWordTokenizer</code> from <code>nltk.tokenize.punkt</code>. So import is as:</p>
<pre><code>from nltk.tokenize.punkt import PunktWordTokenizer as PT
</code></pre>
<p>and replace:</p>
<pre><code>tokens = nltk.wordpunct_tokenize("Hei på deg øyerusk, du er meg en gammel dust, neida neida, det er ikke helt sant da."
                             "Men du, hvorfor så brusk, ikke klut i din susk på en runkete lust")
</code></pre>
<p>with:</p>
<pre><code>tokens = PT().tokenize("Hei på deg øyerusk, du er meg en gammel dust, neida neida, det er ikke helt sant da." 
                             "Men du, hvorfor så brusk, ikke klut i din susk på en runkete lust")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>There are any number of reasons why the encoding isn't working properly. Unicode is a vast and varied mess. The Python HOWTO on Unicode is somewhat helpful for background info: <a href="https://docs.python.org/3/howto/unicode.html" rel="nofollow">https://docs.python.org/3/howto/unicode.html</a></p>
<p>When I just need stuffy to work, I've had success forcing encodings into unicode by using <code>ftfy</code>,  available on PyPi: <a href="https://pypi.python.org/pypi/ftfy/3.3.0" rel="nofollow">https://pypi.python.org/pypi/ftfy/3.3.0</a></p>
<p>Example usage: </p>
<pre><code>&gt;&gt;&gt;import ftfy
&gt;&gt;&gt; print(ftfy.fix_text('uÌˆnicode'))
ünicode

&gt;&gt;&gt;print(ftfy.fix_text_encoding('AHÅ™, the new sofa from IKEA®'))
AHÅ™, the new sofa from IKEA®
</code></pre>
</div>
<span class="comment-copy">if you downvote, provide a reason.</span>
<span class="comment-copy">Should this work on non-english? (I didn't downvote), seems all results (in <code>print ..</code>) are the same.</span>
<span class="comment-copy">I assume it works with non-english. The results are the same because there are no collocations, but a different string would give different result in the frequencies.</span>
<span class="comment-copy">worked like a charm, thanks!</span>
<span class="comment-copy">cant import punkt anymore any help ?</span>
<span class="comment-copy">@ArdaNalbant share the error traceback</span>
<span class="comment-copy"><a href="http://stackoverflow.com/questins/36836481/nltk-remove-tags-from-parsed-chunks">stackoverflow.com/questins/36836481/…</a>  there is no error traceback but you can help maybe thanks</span>
