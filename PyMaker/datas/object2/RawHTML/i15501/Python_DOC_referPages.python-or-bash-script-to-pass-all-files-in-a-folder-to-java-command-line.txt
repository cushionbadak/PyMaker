<div class="post-text" itemprop="text">
<p>I have the following Java command line working fine Mac os. </p>
<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer file.txt &gt; output.txt
</code></pre>
<p>Multiple files can be passed as input with spaces as follows.</p>
<pre><code>java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer file1.txt file2.txt &gt; output.txt
</code></pre>
<p>Now I have 100 files in a folder. All these files I have to pass as input to this command. I used </p>
<p>python os.system in a for loop of directories as follows .</p>
<pre><code>for i,f in enumerate(os.listdir(filedir)):

     os.system('java -cp "stanford-ner.jar" edu.stanford.nlp.process.PTBTokenizer "%s" &gt;        "annotate_%s.txt"' %(f,i))
</code></pre>
<p>This works fine only for the first file. But for all othe outputs like annotate_1,annotate_2 it creates only the file with nothing inside that. I thought of using for loop the files and pass it to subprocess.popen() , but that seems of no hope.</p>
<p>Now I am thinking of passing the files in a loop one by one to execute the command sequentially by passing each file in a bash script. I am also wondering whether I can parallely executes 10 files (atleast) in different terminals at a time. Any solution is fine, but I think this question will help me to gain some insights into different this.</p>
</div>
<div class="post-text" itemprop="text">
<p>To pass all <code>.txt</code> files in the current directory at once to the java subprocess:</p>
<pre><code>#!/usr/bin/env python
from glob import glob
from subprocess import check_call

cmd = 'java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer'.split()
with open('output.txt', 'wb', 0) as file:
    check_call(cmd + glob('*.txt'), stdout=file)
</code></pre>
<p>It is similar to running the shell command but without running the shell:</p>
<pre><code>$ java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer *.txt &gt; output.txt
</code></pre>
<hr/>
<p>To run no more than 10 subprocesses at a time, passing no more than 100 files at a time, you could use <code>multiprocessing.pool.ThreadPool</code>:</p>
<pre><code>#!/usr/bin/env python
from glob import glob
from multiprocessing.pool import ThreadPool
from subprocess import call
try:
    from threading import get_ident # Python 3.3+
except ImportError: # Python 2
    from thread import get_ident

cmd = 'java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer'.split()
def run_command(files):
    with open('output%d.txt' % get_ident(), 'ab', 0) as file:
        return files, call(cmd + files, stdout=file)

all_files = glob('*.txt')
file_groups = (all_files[i:i+100] for i in range(0, len(all_files), 100))
for _ in ThreadPool(10).imap_unordered(run_command, file_groups):
   pass
</code></pre>
<p>It is similar to this <code>xargs</code> command (<a href="https://stackoverflow.com/a/27544953/4279">suggested by @abarnert</a>):</p>
<pre><code>$ ls *.txt | xargs --max-procs=10 --max-args=100 java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer &gt;&gt;output.txt
</code></pre>
<p>except that each thread in the Python script writes to its own output file to avoid corrupting the output due to parallel writes.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you want to do this from the shell instead of Python, the <a href="http://unixhelp.ed.ac.uk/CGI/man-cgi?xargs" rel="nofollow"><code>xargs</code></a> tool can <em>almost</em> do everything you want.</p>
<p>You give it a command with a fixed list of arguments, and feed it input with a bunch of filenames, and it'll run the command multiple times, using the same fixed list plus a different batch of filenames from its input. The <code>--max-args</code> option sets the size of the biggest group. If you want to run things in parallel, the <code>--max-procs</code> option lets you do that.</p>
<p>But that's not quite there, because it doesn't do the output redirection. But… do you really need 10 separate files instead of 1 big one? Because if 1 big one is OK, you can just redirect all of them to it:</p>
<pre><code>ls | xargs --max-args=10 --max-procs=10 java -cp stanford-ner.jar\
    edu.stanford.nlp.process.PTBTokenizer &gt;&gt; output.txt
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you have 100 files, and you want to kick off 10 processes, each handling 10 files, all in parallel, that's easy.</p>
<p>First, you want to group them into chunks of 10. You can do this with slicing or with zipping iterators; in this case, since we definitely have a list, let's just use slicing:</p>
<pre><code>files = os.listdir(filedir)
groups = [files[i:i+10] for i in range(0, len(files), 10)]
</code></pre>
<p>Now, you want to kick off process for each group, and then wait for all of the processes, instead of waiting for each one to finish before kicking off the next. This is impossible with <code>os.system</code>, which is one of the many reasons <a href="https://docs.python.org/3/library/os.html#os.system" rel="nofollow"><code>os.system</code></a> says "The <code>subprocess</code> module provides more powerful facilities for spawning new processes…"</p>
<pre><code>procs = [subprocess.Popen(…) for group in groups]
for proc in procs:
    proc.wait()
</code></pre>
<p>So, what do you pass on the command line to give it 10 filenames instead of 1? If none of the names have spaces or other special characters, you can just <code>' '.join</code> them. But otherwise, it's a nightmare. Another reason <code>subprocess</code> is better: you can just pass a list of arguments:</p>
<pre><code>procs = [subprocess.Popen(['java', '-cp', 'stanford-ner.jar',
                           'edu.stanford.nlp.process.PTBTokenizer'] + group)
         for group in groups]
</code></pre>
<p>But now how to do you get all of the results?</p>
<p>One way is to go back to using a shell command line with the <code>&gt;</code> redirection. But a better way is to do it in Python:</p>
<pre><code>procs = []
files = []
for i, group in enumerate(groups):
    file = open('output_{}'.format(i), 'w')
    files.append(file)
    procs.append(subprocess.Popen([…same as before…], stdout=file)
for proc in procs:
    proc.wait()
for file in files:
    file.close()
</code></pre>
<p>(You might want to use a <code>with</code> statement with <code>ExitStack</code>, but I wanted to make sure this didn't require Python 2.7/3.3+, so I used explicit <code>close</code>.)</p>
</div>
<div class="post-text" itemprop="text">
<p>Inside your input file directory you can do the following in bash:</p>
<pre><code>#!/bin/bash
for file in *.txt
do
    input=$input" \"$file\""
done
java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer $input &gt; output.txt
</code></pre>
<p>If you want to run it as a script. Save the file with some name, my_exec.bash:</p>
<pre><code>#!/bin/bash
if [ $# -ne 2 ]; then
    echo "Invalid Input. Enter a directory and a output file"
    exit 1
fi
if [ ! -d $1 ]; then
    echo "Please pass a valid directory"
    exit 1
fi
for file in $1*.txt
do
    input=$input" \"$file\""
done
java -cp stanford-ner.jar edu.stanford.nlp.process.PTBTokenizer $input &gt; $2
</code></pre>
<p>Make it an executable file</p>
<pre><code>chmod +x my_exec.bash
</code></pre>
<p><strong>USAGE:</strong></p>
<pre><code> ./my_exec.bash &lt;folder&gt; &lt;output_file&gt;
</code></pre>
</div>
<span class="comment-copy">First, you really shouldn't be using <code>os.system</code>. You've added the <code>subprocess</code> tag, which is about using the <code>subprocess</code> module. Use that; then you can pass multiple arguments just by passing a list with multiple elements, without having to worry about how to quote them or join them up or anything.</span>
<span class="comment-copy">Also, unlike <code>os.system</code>, <code>subprocess</code> lets you start a process up and then check whether it's finished later, instead of waiting for each one to finish before you can do the next.</span>
<span class="comment-copy">@abarnert I dint get the last part . But I will see the first part right away .</span>
<span class="comment-copy">All it need a slight modification , a simple cutomise according to the application.</span>
<span class="comment-copy">hey man it worked. This was very useful from your part. thanks alot . Thanks .</span>
<span class="comment-copy">upvoted for xargs. Are you sure that <code>&gt;&gt; output.txt</code> and <code>&gt; output.txt</code> differ in this case (assuming there is no output.txt before the <code>xargs</code> is run)? Also the output might be garbled if <code>max-procs &gt; 1</code>.</span>
<span class="comment-copy">let me check man . i am so damn confused .</span>
<span class="comment-copy">Hey . Where should i mention my directory path . Should it be in quotes ?</span>
<span class="comment-copy">This is a really complicated way of doing the same thing as just passing <code>*.txt</code> on the command line (but less robustly—e.g., if there are quotes or other special characters in any of the filenames, this will break, but <code>*.txt</code> will not).</span>
<span class="comment-copy">a file name will not have special characters. As far as spaces are concerned the double quotes will handle that.</span>
<span class="comment-copy">@user3368375: you can pass it as a command line parameter.</span>
<span class="comment-copy">@pratZ: But you don't <i>need</i> any of that. In every case this whole mess works, <code>*.txt</code> works too. And in many cases where this mess <i>doesn't</i> work, <code>*.txt</code> works. So why do this?</span>
