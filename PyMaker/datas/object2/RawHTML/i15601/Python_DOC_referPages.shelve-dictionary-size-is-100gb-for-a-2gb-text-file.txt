<div class="post-text" itemprop="text">
<p>I am creating a shelve file of sequences from a genomic FASTA file:</p>
<pre><code># Import necessary libraries
import shelve
from Bio import SeqIO

# Create dictionary of genomic sequences
genome = {}
with open("Mus_musculus.GRCm38.dna.primary_assembly.fa") as handle:
    for record in SeqIO.parse(handle, "fasta"):
        genome[str(record.id)] = str(record.seq)

# Shelve genome sequences
myShelve = shelve.open("Mus_musculus.GRCm38.dna.primary_assembly.db")
myShelve.update(genome)
myShelve.close()
</code></pre>
<p>The file itself is 2.6Gb, however when I try and shelve it, a file of &gt;100Gb is being produced, plus my computer will throw out a number of complaints about being out of memory and the start up disk being full. This only seems to happen when I try to run this under OSX Yosemite, on Ubuntu it works as expected. Any suggestions why this is not working? I'm using Python 3.4.2</p>
</div>
<div class="post-text" itemprop="text">
<p>Verify what interface is used for dbm by <code>import dbm; print(dbm.whichdb('your_file.db')</code> The file format used by shelve depends on the best installed binary package available on your system and its interfaces. The newest is <code>gdbm</code>, while <code>dumb</code> is a fallback solution if no binary is found, <code>ndbm</code> is something between.<br/>
<a href="https://docs.python.org/3/library/shelve.html" rel="nofollow">https://docs.python.org/3/library/shelve.html</a><br/>
<a href="https://docs.python.org/3/library/dbm.html" rel="nofollow">https://docs.python.org/3/library/dbm.html</a></p>
<p>It is not favourable to have all data in the memory if you lose all memory for filesystem cache. Updating by smaller blocks is better. I even don't see a slowdown if items are updated one by one.</p>
<pre><code>myShelve = shelve.open("Mus_musculus.GRCm38.dna.primary_assembly.db")
with open("Mus_musculus.GRCm38.dna.primary_assembly.fa") as handle:
    for i, record in enumerate(SeqIO.parse(handle, "fasta")):
        myShelve.update([(str(record.id), str(record.seq))])
myShelve.close()
</code></pre>
<p>It is known that dbm databases became fragmented if the app fell down after updates without calling database <code>close</code>. I think that this was your case. Now you probably have no important data yet in the big file, but in the future you can defragment a database by <code>gdbm.reorganize()</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>I had the very same problem: On a macOS system with a shelve with about 4 Megabytes of data grew to the enormous size of 29 Gigabytes on disk! This obviously happened because I updated the same key value pairs in the shelve over and over again.</p>
<p>As my shelve was based on GNU dbm I was able to use his hint about reorganizing. Here is the code that brought my shelve file back to normal size within seconds:</p>
<pre><code>import dbm
db = dbm.open(shelfFileName, 'w')
db.reorganize()
db.close()
</code></pre>
<p>I am not sure whether this technique will work for other (non GNU) dbms as well. To test your dbm system, remember the code shown by @hynekcer:</p>
<pre><code>import dbm
print( dbm.whichdb(shelfFileName) )
</code></pre>
<p>If GNU dbm is used by your system this should output 'dbm.gnu' (which is the new name for the older gdbm).</p>
</div>
<span class="comment-copy">which python versions do you use?</span>
<span class="comment-copy">@Daniel Python 3.4.2</span>
<span class="comment-copy">May be related to this one? <a href="http://stackoverflow.com/questions/26574954/virtualenv-fails-on-os-x-yosemite-with-oserror" title="virtualenv fails on os x yosemite with oserror">stackoverflow.com/questions/26574954/â€¦</a></span>
