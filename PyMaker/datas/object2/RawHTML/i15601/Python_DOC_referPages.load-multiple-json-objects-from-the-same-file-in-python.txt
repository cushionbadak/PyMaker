<div class="post-text" itemprop="text">
<p>I have a multi-gigabyte JSON file. The file is made up of JSON objects that are no more than a few thousand characters each, but there are no line breaks between the records.</p>
<p>Using Python 3 and the <code>json</code> module, how can I read one JSON object at a time from the file into memory?</p>
<p>The data is in a plain text file. Here is an example of a similar record.  The actual records contains many nested dictionaries and lists.</p>
<p>Record in readable format:</p>
<pre><code>{
    "results": {
      "__metadata": {
        "type": "DataServiceProviderDemo.Address"
      },
      "Street": "NE 228th",
      "City": "Sammamish",
      "State": "WA",
      "ZipCode": "98074",
      "Country": "USA"
    }
  }
}
</code></pre>
<p>Actual format. New records start one after the other without any breaks.</p>
<pre><code>{"results": { "__metadata": {"type": "DataServiceProviderDemo.Address"},"Street": "NE 228th","City": "Sammamish","State": "WA","ZipCode": "98074","Country": "USA" } } }{"results": { "__metadata": {"type": "DataServiceProviderDemo.Address"},"Street": "NE 228th","City": "Sammamish","State": "WA","ZipCode": "98074","Country": "USA" } } }{"results": { "__metadata": {"type": "DataServiceProviderDemo.Address"},"Street": "NE 228th","City": "Sammamish","State": "WA","ZipCode": "98074","Country": "USA" } } }
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Generally speaking, putting more than one JSON object into a file makes that file <em>invalid, broken JSON</em>. That said, you can still parse data in chunks using the <a href="http://docs.python.org/3/library/json.html#json.JSONDecoder.raw_decode" rel="nofollow noreferrer"><code>JSONDecoder.raw_decode()</code> method</a>.</p>
<p>The following will yield complete objects as the parser finds them:</p>
<pre><code>from json import JSONDecoder
from functools import partial


def json_parse(fileobj, decoder=JSONDecoder(), buffersize=2048):
    buffer = ''
    for chunk in iter(partial(fileobj.read, buffersize), ''):
         buffer += chunk
         while buffer:
             try:
                 result, index = decoder.raw_decode(buffer)
                 yield result
                 buffer = buffer[index:]
             except ValueError:
                 # Not enough data to decode, read more
                 break
</code></pre>
<p>This function will read chunks from the given file object in <code>buffersize</code> chunks, and have the <code>decoder</code> object parse whole JSON objects from the buffer. Each parsed object is yielded to the caller.</p>
<p>Use it like this:</p>
<pre><code>with open('yourfilename', 'r') as infh:
    for data in json_parse(infh):
        # process object
</code></pre>
<p>Use this only if your JSON objects are written to a file back-to-back, with no newlines in between. If you <em>do</em> have newlines, and each JSON object is limited to a single line, you have a <a href="http://jsonlines.org/" rel="nofollow noreferrer">JSON Lines document</a>, in which case you can use <a href="https://stackoverflow.com/questions/12451431/loading-parsing-json-file-in-python/12451465#12451465">Loading and parsing a JSON file with multiple JSON objects in Python</a> instead.</p>
</div>
<div class="post-text" itemprop="text">
<p>If your JSON documents contains a list of objects, and you want to read one object one-at-a-time, you can use the iterative JSON parser <em><a href="https://pypi.python.org/pypi/ijson/" rel="nofollow">ijson</a></em> for the job. It will only read more content from the file when it needs to decode the next object.</p>
<p>Note that you should use it with the <a href="http://lloyd.github.io/yajl/" rel="nofollow">YAJL</a> library, otherwise you will likely not see any performance increase.</p>
<p>That being said, unless your file is <em>really big</em>, reading it completely into memory and then parsing it with the normal JSON module will probably still be the best option.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here is a slight modification of <a href="https://stackoverflow.com/a/21709058/190597">Martijn Pieters' solution</a>, which will handle JSON strings separated with whitespace.</p>
<pre><code>def json_parse(fileobj, decoder=json.JSONDecoder(), buffersize=2048, 
               delimiters=None):
    remainder = ''
    for chunk in iter(functools.partial(fileobj.read, buffersize), ''):
        remainder += chunk
        while remainder:
            try:
                stripped = remainder.strip(delimiters)
                result, index = decoder.raw_decode(stripped)
                yield result
                remainder = stripped[index:]
            except ValueError:
                # Not enough data to decode, read more
                break
</code></pre>
<hr/>
<p><a href="https://stackoverflow.com/q/27027395/190597">For example,</a> if <code>data.txt</code> contains JSON strings separated by a space:</p>
<pre><code>{"business_id": "1", "Accepts Credit Cards": true, "Price Range": 1, "type": "food"} {"business_id": "2", "Accepts Credit Cards": true, "Price Range": 2, "type": "cloth"} {"business_id": "3", "Accepts Credit Cards": false, "Price Range": 3, "type": "sports"}
</code></pre>
<p>then</p>
<pre><code>In [47]: list(json_parse(open('data')))
Out[47]: 
[{u'Accepts Credit Cards': True,
  u'Price Range': 1,
  u'business_id': u'1',
  u'type': u'food'},
 {u'Accepts Credit Cards': True,
  u'Price Range': 2,
  u'business_id': u'2',
  u'type': u'cloth'},
 {u'Accepts Credit Cards': False,
  u'Price Range': 3,
  u'business_id': u'3',
  u'type': u'sports'}]
</code></pre>
</div>
<span class="comment-copy">Post a sample of the data, at least few <i>objects</i>.</span>
<span class="comment-copy">You mean the JSON file is an array of objects, and you want to lazily read those objects?</span>
<span class="comment-copy">And did you already search for other posts on this very subject, here on Stack Overflow? There is at least one listed in the 'related' sidebar here that I can see. How did those posts not address your specific situation?</span>
<span class="comment-copy">@poke I'm not sure what you mean by 'lazily', but yes I think that is what I want.</span>
<span class="comment-copy">@MartijnPieters None of the other posts I could find addressed the same problem. Could you share the link with the solution you found?</span>
<span class="comment-copy">This worked great, thank you.  Yes, the file i was dealing with had back to back JSON objects.  Also for the try/except, I used 'pass' instead of 'break'.  Was the break intentional?  I couldn't get it to work with it.</span>
<span class="comment-copy">@user3281420: yes, the <code>break</code> was intentional; it breaks the <code>while</code> loop so we move on the next chunk read from the file. The <code>break</code> is only triggered if there is no JSON object to decode in the current buffer.</span>
<span class="comment-copy">@user3281420: <code>pass</code> would only work if the buffer was empty as that is the other termination condition for the <code>while</code> loop.</span>
<span class="comment-copy">@RonanDejhero: it's easy enough to add a <code>.strip()</code> call to the <code>buffer</code>: <code>result, index = decoder.raw_decode(buffer.strip())</code> to remove whitespace, <code>buffer.strip(' \n|')</code> to remove an explicit set of characters.</span>
<span class="comment-copy">@user2441441: <code>data</code> is the JSON data decoded to a Python object. It depends on what JSON object you decoded how to get key-value pairs from it.</span>
