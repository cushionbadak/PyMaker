<div class="post-text" itemprop="text">
<p>Below is my code to get file size in ascending order. </p>
<pre><code>def Create_Files_Structure(directoryname):
   for path, subdirs, files in os.walk(directoryname,followlinks=False):
        subdirs[:] = [d for d in subdirs if not d[0] == '.']
        try:
           files_list.extend([(os.path.join(path, file),os.path.getsize(os.path.join(path, file))) for file in files ])
        except Exception as e:
            print()
   files_list.sort(key=lambda s: s[1], reverse=True)
   for pair in files_list:
     print(pair)
   print(len(files_list))

start=time.time()
Create_Files_Structure("/home/&lt;username&gt;")
end=time.time()
print(end-start)
</code></pre>
<p>This code is working however performance is slow if size of a directory is in TB or PB. Any suggestion to improve the code to get faster result please.</p>
</div>
<div class="post-text" itemprop="text">
<ol>
<li>To get a feel for how fast you can get, try running and timing <code>du -k</code> on the directory. You probably won't be getting faster than that with Python for a full listing.</li>
<li>If you're running on Python &lt; 3.5, try upgrading or using <a href="https://github.com/benhoyt/scandir" rel="nofollow noreferrer">scandir</a> for a nice performance improvement.</li>
<li>If you don't really need the whole list of files but can live with e.g the largest 1000 files:</li>
</ol>
<p>Avoid keeping the list and use <a href="https://docs.python.org/3/library/heapq.html#heapq.nlargest" rel="nofollow noreferrer">heapq.nlargest</a> with a generator</p>
<pre><code>def get_sizes(root):
  for path, dirs, files in os.walk(root):
    dirs[:] = [d for d in dirs if not d.startswith('.')]
    for file in files:
        full_path = os.path.join(path, file)
        try:
          # keeping the size first means no need for a key function
          # which can affect performance
          yield (os.path.getsize(full_path), full_path)
        except Exception:
          pass

import heapq
for (size, name) in heapq.nlargest(1000, get_sizes(r"c:\some\path")):
  print(name, size)
</code></pre>
<p>EDIT - to get even faster on Windows - <code>os.scandir</code> yields entries that already contain the size helping avoid another system call.</p>
<p>This means using <code>os.scandir</code> and recursing yourself instead of relying on <code>os.walk</code> which doesn't yield that information.</p>
<p>There's a similar working example <code>get_tree_size()</code> function in the <a href="https://www.python.org/dev/peps/pep-0471/#examples" rel="nofollow noreferrer">scandir PEP 471</a> that can be easily modified to yield names and sizes instead. Each entry's size is accessible with <code>entry.stat(follow_symlinks=False).st_size</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Nice Question</strong> Try This :</p>
<pre><code>import time, os

def create_files_structuredire_2(ctoryname):

    files_list = []
    counter = 0

    for dirpath, _, filenames in os.walk(ctoryname):

        for items in filenames:

            file_full_path = os.path.abspath(os.path.join(dirpath, items))
            get_size = os.path.getsize(file_full_path)
            files_list.append((file_full_path, get_size))
            counter += 1

    files_list.sort(key=lambda s: s[1], reverse=True)
    [print(f) for f in files_list]
    print(counter)


start = time.time()
create_files_structuredire_2("your_target_folder")
end = time.time()
print(end-start)
</code></pre>
<p><strong>NOTE : your time is 0.044736385345458984, my time is 0.001501321792602539 !!!!!!</strong></p>
<p>Good Luck ...</p>
</div>
<span class="comment-copy">as your code is working, a better place for these type of questions is <a href="https://codereview.stackexchange.com/">Code Review</a></span>
<span class="comment-copy">You're not filtering the '.' subdirs from the scan - try timing it with that to compare</span>
