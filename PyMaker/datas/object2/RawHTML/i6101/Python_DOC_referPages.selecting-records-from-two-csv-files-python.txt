<div class="post-text" itemprop="text">
<p>I have two csv files. train_all.csv :</p>
<pre><code>    msno                                         is_churn
0   waLDQMmcOu2jLDaV1ddDkgCrB/jl6sD66Xzs0Vqax1Y=    1
1   QA7uiXy8vIbUSPOkCf9RwQ3FsT8jVq2OxDr8zqa7bRQ=    1
2   fGwBva6hikQmTJzrbz/2Ezjm5Cth5jZUNvXigKK2AFA=    1
3   mT5V8rEpa+8wuqi6x0DoVd3H5icMKkE9Prt49UlmK+4=    1
4   XaPhtGLk/5UvvOYHcONTwsnH97P4eGECeq+BARGItRw=    1
5   GBy8qSz16X5iYWD+3CMxv/Hm6OPSrXBYtmbnlRtknW0=    1
6   lYLh7TdkWpIoQs3i3o6mIjLH8/IEgMWP9r7OpsLX0Vo=    1
</code></pre>
<p>It has 1963891 records, but only 1082190 unique records. Thats why i sorted unique msno-s with pandas. 
The other csv is about 30GB, cointaining more than 900 million records with user logs. </p>
<pre><code>msno    date    num_25  num_50  num_75  num_985 num_100 num_unq total_secs
rxIP2f2aN0rYNp+toI0Obt/N/FYQX8hcO1fTmmy2h34=    20150513    0   0   0   0   1   1   280.335
rxIP2f2aN0rYNp+toI0Obt/N/FYQX8hcO1fTmmy2h34=    20150709    9   1   0   0   7   11  1658.948
yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=    20150105    3   3   0   0   68  36  17364.956
yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=    20150306    1   0   1   1   97  27  24667.317
yxiEWwE9VR5utpUecLxVdQ5B7NysUPfrNtGINaM2zA8=    20150501    3   0   0   0   38  38  9649.029
</code></pre>
<p>I would like to iterate through the unique msno-s in train_all.csv and search for 5 user_log records in user_logs.csv for each msno. My code stops after about 20 minutes with only 104 records in the results.csv - desired output file with matched msno - user_logs.</p>
<pre><code>import pandas as pd
import csv

reader = csv.reader(open('user_logs/user_logs.csv','r'))
writer = csv.writer(open('results.csv','w',newline=''))

data = pd.read_csv("train_all.csv")
unique_msnos = data["msno"].unique()

i = 0

for msno in range(len(unique_msnos)):
    counter = 0

    for row in reader:
        results_row = row

        if unique_msnos[msno] == row[0]:
            writer.writerow(results_row)
            counter+=1
            if counter == 5:
                i+=1
                break
            else:
                continue
        else:
            continue
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Probable reason why it stops without finding everything:</p>
<ul>
<li>the inner loop advances the position in the file every time it reads a row.</li>
<li>when you break out of the inner loop the the file counter/pointer/position(?) remains at the place it left off</li>
<li>during the next iteration of the outer loop, the inner loop starts at the last file position instead of at the beginning. So it cannot find any valid msno's in the previous lines/rows of the file. You can verify this by printing the file's position (<code>file.tell()</code>) just before the inner loop (remember ctrl-c to break out of it.)</li>
</ul>
<p>You can <em>fix</em> this by sending the file back to the beginning after the inner loop has completed (whether by StopIteration or a break) so that the inner loop can search through the whole file looking for the msno. Be prepared for a much longer wait.</p>
<pre><code>with open('user_logs/user_logs.csv','r') as infile:
    reader = csv.reader(infile)
    for msno in range(len(unique_msnos)):
        counter = 0
        #print(infile.tell())
        for row in reader:
            #inner_loop_stuff...
            #inner_loop_stuff...
            #inner_loop_stuff...
        infile.seek(0)
</code></pre>
<hr/>
<p><code>unique_msnos</code> has 1.1E6 records, <code>reader</code> has 9E8 records.  For each record in <code>unique_msnos</code> you will iterate 9e8 times - that would come to 1e15-ish  iterations.  You have to be careful with nested loops.</p>
<p>When <a href="https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset" rel="nofollow noreferrer">checking for membership</a> you should use a set.</p>
<pre><code>unique_msnos = set(unique_msnos)
</code></pre>
<p>You want to keep track of the number of records you find so maybe make <code>unique_msnos</code> a dictionary with the value holding the count.</p>
<pre><code>unique_msnos = dict.fromkeys(unique_msnos, 0)
</code></pre>
<p>Then iterate over <code>reader</code> check for membership and adding logic to see if there are already 5:</p>
<pre><code>for row in reader:
    c = unique_msnos.get(row[0], None)
    if c in (5, None):
        continue
    writer.write(row)
    unique_msnos[row[0]] += 1
    ...
</code></pre>
<p>This should alleviate it somewhat - 9e8 iterations.  There are probably other optimizations.  </p>
<hr/>
<p>If the msno-s are guaranteed to all be the same length you may get an incremental improvement using slicing instead of using a csv.Reader.  I really don't know how fast csv readers are but slicing is pretty fast.</p>
<p>Something like</p>
<pre><code># unique_msnos is a dictionary
with open(open('user_logs/user_logs.csv','r') as f:
    for line in f:
        c = unique_msnos.get(line[:44], None)
        ....
</code></pre>
<p>It won't reduce the iteration count but if if you get (a_small_improvement * 9e8) improvement it might be noticeable.</p>
<hr/>
<p>Looks like you are making a DataFrame then throwing it away. It may be faster to just make <code>unique_msnos</code> directly from the file:</p>
<pre><code>unique_msnos = {}
with open("train_all.csv") as f:
    for line in f:
        _, msno, churn = line.split()
        unique_msnos[msno] = 0
</code></pre>
<p>You would need to try it to see if it is faster.</p>
<hr/>
<p>You may want to make yourself some smaller files to work with while you are trying to get it to work and making optimizations. Just do it once for each file then use the small files for debuging.  Using <a href="https://docs.python.org/3/library/profile.html#the-python-profilers" rel="nofollow noreferrer">cprofile</a> or <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow noreferrer">timeit</a> to try and figure out where the bottlenecks are</p>
<pre><code>limit = 10000  # or whatever
with open(file_path, 'r') as infile, open(small_filepath, 'w') as outfile:
    for n, line in enumerate(infile):
        outfile.write(line)
        if n == limit:
            break
</code></pre>
</div>
<span class="comment-copy">I see your point but the biggest problem is the code stops after i have 104 records in results.csv . My question is why the code stops?</span>
<span class="comment-copy">with open(open('user_logs/user_logs.csv','r') as f:     for line in f:         if line[:44] in unique_msnos:  If i do this it iterate the log file, not the msno-s needed. This way it is possible not every not every msnos will have a pair.</span>
<span class="comment-copy">@hk_03 - see edit.</span>
