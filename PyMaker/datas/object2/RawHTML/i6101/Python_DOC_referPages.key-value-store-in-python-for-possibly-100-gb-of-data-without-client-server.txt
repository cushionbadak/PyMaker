<div class="post-text" itemprop="text">
<p>There are many solutions to serialize a small dictionary: <code>json.loads</code>/<code>json.dumps</code>, <code>pickle</code>, <code>shelve</code>, <code>ujson</code>, or even by using <code>sqlite</code>.</p>
<p>But when dealing with possibly 100 GB of data, it's not possible anymore to use such modules that would possibly rewrite the whole data when closing / serializing.</p>
<p><code>redis</code> is not really an option because it uses a client/server scheme.</p>
<p>Question: <strong>Which key:value store, serverless, able to work with 100+ GB of data, are frequently used in Python?</strong></p>
<p>I'm looking for a solution with a standard "Pythonic" <code>d[key] = value</code> syntax:</p>
<pre><code>import mydb
d = mydb.mydb('myfile.db')
d['hello'] = 17          # able to use string or int or float as key
d[183] = [12, 14, 24]    # able to store lists as values (will probably internally jsonify it?)
d.flush()                # easy to flush on disk 
</code></pre>
<p>Note: <a href="https://docs.python.org/2/library/bsddb.html#module-bsddb" rel="nofollow noreferrer">BsdDB</a> (BerkeleyDB) seems to be deprecated. There seems to be a <a href="https://github.com/jtolds/leveldb-py" rel="nofollow noreferrer">LevelDB for Python</a>, but it doesn't seem well-known - and I <a href="https://github.com/google/leveldb/issues/528#issuecomment-347626773" rel="nofollow noreferrer">haven't found</a> a version which is ready to use on Windows. Which ones would be the most common ones?</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use <a href="https://pypi.python.org/pypi/sqlitedict" rel="noreferrer">sqlitedict</a> which provides key-value interface to SQLite database.</p>
<p>SQLite <a href="https://www.sqlite.org/limits.html" rel="noreferrer">limits page</a> says that theoretical maximum is 140 TB depending on <code>page_size</code> and <code>max_page_count</code>. However, default values for Python 3.5.2-2ubuntu0~16.04.4 (<code>sqlite3</code> 2.6.0), are <code>page_size=1024</code> and <code>max_page_count=1073741823</code>. This gives ~1100 GB of maximal database size which fits your requirement.</p>
<p>You can use the package like:</p>
<pre><code>from sqlitedict import SqliteDict

mydict = SqliteDict('./my_db.sqlite', autocommit=True)
mydict['some_key'] = any_picklable_object
print(mydict['some_key'])
for key, value in mydict.items():
    print(key, value)
print(len(mydict))
mydict.close()
</code></pre>
<h1>Update</h1>
<p>About memory usage. SQLite doesn't need your dataset to fit in RAM. By default it caches up to <code>cache_size</code> pages, which is barely 2MiB (the same Python as above). Here's the script you can use to check it with your data. Before run:</p>
<pre><code>pip install lipsum psutil matplotlib psrecord sqlitedict
</code></pre>
<p><em>sqlitedct.py</em></p>
<pre><code>#!/usr/bin/env python3

import os
import random
from contextlib import closing

import lipsum
from sqlitedict import SqliteDict

def main():
    with closing(SqliteDict('./my_db.sqlite', autocommit=True)) as d:
        for _ in range(100000):
            v = lipsum.generate_paragraphs(2)[0:random.randint(200, 1000)]
            d[os.urandom(10)] = v

if __name__ == '__main__':
    main()
</code></pre>
<p>Run it like <code>./sqlitedct.py &amp; psrecord --plot=plot.png --interval=0.1 $!</code>. In my case it produces this chart:
<a href="https://i.stack.imgur.com/TVa6n.png" rel="noreferrer"><img alt="chart" src="https://i.stack.imgur.com/TVa6n.png"/></a></p>
<p>And database file:</p>
<pre><code>$ du -h my_db.sqlite 
84M my_db.sqlite
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I would consider <a href="https://support.hdfgroup.org/HDF5/whatishdf5.html" rel="nofollow noreferrer">HDF5</a> for this.  It has several advantages:</p>
<ul>
<li>Usable from many programming languages.</li>
<li>Usable from Python via the excellent <a href="http://www.h5py.org/" rel="nofollow noreferrer">h5py</a> package.</li>
<li>Battle tested, including with large data sets.</li>
<li>Supports variable-length string values.</li>
<li>Values are addressable by a filesystem-like "path" (<code>/foo/bar</code>).</li>
<li>Values can be arrays (and usually are), but do not have to be.</li>
<li>Optional built-in compression.</li>
<li>Optional "chunking" to allow writing chunks incrementally.</li>
<li>Does not require loading the entire data set into memory at once.</li>
</ul>
<p>It does have some disadvantages too:</p>
<ul>
<li>Extremely flexible, to the point of making it hard to define a single approach.</li>
<li>Complex format, not feasible to use without the official HDF5 C library (but there are many wrappers, e.g. <code>h5py</code>).</li>
<li>Baroque C/C++ API (the Python one is not so).</li>
<li>Little support for concurrent writers (or writer + readers).  Writes might need to lock at a coarse granularity.</li>
</ul>
<p>You can think of HDF5 as a way to store values (scalars or N-dimensional 
 arrays) inside a hierarchy inside a single file (or indeed multiple such files).  The biggest problem with just storing your values in a single disk file would be that you'd overwhelm some filesystems; you can think of HDF5 as a filesystem within a file which won't fall down when you put a million values in one "directory."</p>
</div>
<div class="post-text" itemprop="text">
<p>First, bsddb (or under it's new name Oracle BerkeleyDB) is not deprecated.</p>
<p>From experience LevelDB / RocksDB / bsddb are slower than <a href="http://source.wiredtiger.com/" rel="nofollow noreferrer">wiredtiger</a>, that's why I recommend wiredtiger.</p>
<p>wiredtiger is the storage engine for mongodb so it's well tested in production. There is little or no use of wiredtiger in Python outside my AjguDB project; I use wiredtiger (via AjguDB) to store and query wikidata and concept which around 80GB.</p>
<p>Here is an example class that allows mimick the python2 <a href="https://docs.python.org/2/library/shelve.html" rel="nofollow noreferrer">shelve</a> module. Basically,
it's a wiredtiger backend dictionary where keys can only be strings:</p>
<pre><code>import json

from wiredtiger import wiredtiger_open


WT_NOT_FOUND = -31803


class WTDict:
    """Create a wiredtiger backed dictionary"""

    def __init__(self, path, config='create'):
        self._cnx = wiredtiger_open(path, config)
        self._session = self._cnx.open_session()
        # define key value table
        self._session.create('table:keyvalue', 'key_format=S,value_format=S')
        self._keyvalue = self._session.open_cursor('table:keyvalue')

    def __enter__(self):
        return self

    def close(self):
        self._cnx.close()

    def __exit__(self, *args, **kwargs):
        self.close()

    def _loads(self, value):
        return json.loads(value)

    def _dumps(self, value):
        return json.dumps(value)

    def __getitem__(self, key):
        self._session.begin_transaction()
        self._keyvalue.set_key(key)
        if self._keyvalue.search() == WT_NOT_FOUND:
            raise KeyError()
        out = self._loads(self._keyvalue.get_value())
        self._session.commit_transaction()
        return out

    def __setitem__(self, key, value):
        self._session.begin_transaction()
        self._keyvalue.set_key(key)
        self._keyvalue.set_value(self._dumps(value))
        self._keyvalue.insert()
        self._session.commit_transaction()
</code></pre>
<p>Here the adapted test program from @saaj answer:</p>
<pre><code>#!/usr/bin/env python3

import os
import random

import lipsum
from wtdict import WTDict


def main():
    with WTDict('wt') as wt:
        for _ in range(100000):
            v = lipsum.generate_paragraphs(2)[0:random.randint(200, 1000)]
            wt[os.urandom(10)] = v

if __name__ == '__main__':
    main()
</code></pre>
<p>Using the following command line:</p>
<pre><code>python test-wtdict.py &amp; psrecord --plot=plot.png --interval=0.1 $!
</code></pre>
<p>I generated the following diagram:</p>
<p><a href="https://i.stack.imgur.com/ZxRHk.png" rel="nofollow noreferrer"><img alt="wt performance without wal" src="https://i.stack.imgur.com/ZxRHk.png"/></a></p>
<pre><code>$ du -h wt
60M wt
</code></pre>
<p>When write-ahead-log is active:</p>
<p><a href="https://i.stack.imgur.com/JKskc.png" rel="nofollow noreferrer"><img alt="wt performance with wal" src="https://i.stack.imgur.com/JKskc.png"/></a></p>
<pre><code>$ du -h wt
260M    wt
</code></pre>
<p>This is without performance tunning and compression.</p>
<p>Wiredtiger has no known limit until recently, the documentation was updated to the following:</p>
<blockquote>
<p>WiredTiger supports petabyte tables, records up to 4GB, and record numbers up to 64-bits.</p>
</blockquote>
<p><a href="http://source.wiredtiger.com/1.6.4/architecture.html" rel="nofollow noreferrer">http://source.wiredtiger.com/1.6.4/architecture.html</a></p>
</div>
<span class="comment-copy">SQLite should work great. Did you have any problems using it? Its the DBMS that is small but the DB itself can be large. See <a href="https://stackoverflow.com/questions/14451624/will-sqlite-performance-degrade-if-the-database-size-is-greater-than-2-gigabytes" title="will sqlite performance degrade if the database size is greater than 2 gigabytes">stackoverflow.com/questions/14451624/â€¦</a></span>
<span class="comment-copy">@Himanshu It's the fact the usage with SQLite is not as simple as <code>db[key] = value</code> or <code>db.put('key', 'value')</code>, but uses SQL instead... And I'd like to avoid INSERT into TABLE or SELECT ... for just a simple key:value <code>db[key] = value</code> set/get.</span>
<span class="comment-copy">Can you describe the data more?  100 GB of what?  How large is the smallest/median/largest value?  How many key/value pairs make up the 100 GB?</span>
<span class="comment-copy">You may be able to get this working in dask but I've never actually used it, it's on my to-do. Apparently it runs on a single system too. Or you can always have MongoDB - there's nothing stopping you running that on localhost. I'm not sure what your requirement for serverless stems from, you might not have a choice for such large data stores on a single PC.</span>
<span class="comment-copy">@JohnZwinck the keys are always 10 bytes, the values is a string of length 200 to 1000. It should be able to handle 100 millions keys/values for example</span>
<span class="comment-copy">Very nice benchmark, thank you @saaj! Being curious: what does <code>with closing(...) as ...:</code> do?</span>
<span class="comment-copy">About the graph CPU y-axis between 0 and 200%, average 150%, is the y-axis correct?</span>
<span class="comment-copy">@Basj 1) <a href="https://docs.python.org/3/library/contextlib.html#contextlib.closing" rel="nofollow noreferrer"><code>contextlib.closing</code></a>. 2) I think it is, as <code>sqlite3</code> creates own thread which releases GIL when operating in <code>_sqlite3</code> binary. So it gets over 100%.</span>
<span class="comment-copy">Excellent answer, I updated mine accordingly <a href="https://stackoverflow.com/a/48298904/140837">stackoverflow.com/a/48298904/140837</a></span>
<span class="comment-copy">There's also <a href="http://www.grantjenks.com/docs/diskcache/" rel="nofollow noreferrer">diskcache</a> which is pure-Python, requires no server, <a href="http://www.grantjenks.com/docs/diskcache/cache-benchmarks.html" rel="nofollow noreferrer">fast</a> and also built atop SQLite. The largest known diskcache database is 75GB.</span>
<span class="comment-copy">HDF5 is not a database, it's serialization format. It's requires to load the whole in memory.</span>
<span class="comment-copy">Thank you. Can you include 3 or 4 lines of code showing how to use it, like in the (edited) question? i.e. <code>import ...</code> then creation of DB then <code>d[key] = value</code> then flush it to disk.</span>
<span class="comment-copy">@amirouche: Obviously HDF5 is not a database.  The question did not ask for a database.  HDF5 does not require loading the whole of anything into memory--you can load slices, "hyperslabs", single arrays or attributes in a hierarchical file, etc.  It absolutely does not require loading anything more than you want into memory.  Anyway OP's data is on the order of 100 GB, and 100 GB of main memory is easily found on commodity servers and even some desktops these days.</span>
<span class="comment-copy">@Basj: Please see h5py's excellent Quick Start tutorial here: <a href="http://docs.h5py.org/en/latest/quick.html" rel="nofollow noreferrer">docs.h5py.org/en/latest/quick.html</a> - it has code for exactly what you want.</span>
<span class="comment-copy">Thanks. Can you give an example of code using <code>wiredtiger</code>? Is it possible to use it with a simple API like <code>import wiredtiger</code> <code>wt = wiredtiger.wiredtiger('myfile.db')</code> <code>wt['hello'] = 17</code> <code>wt[183] = [12, 14, 24]</code> <code>wt.flush()</code> i.e. the main requirements are: 1) <code>wt[key] = value</code> syntax 2) able to use string or int or float as key 3) able to store lists as values 4) easy to flush on disk</span>
<span class="comment-copy">Is it for Python 2 or Python 3?</span>
<span class="comment-copy">I use Python 2 @amirouche.</span>
<span class="comment-copy">Then this works with python 2.</span>
