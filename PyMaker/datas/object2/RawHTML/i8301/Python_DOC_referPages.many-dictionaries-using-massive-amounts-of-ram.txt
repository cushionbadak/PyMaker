<div class="post-text" itemprop="text">
<p>I have a very simple Python script to create (for test purposes), 35 million dictionary objects within a list. Each dictionary object contains two key/value pairs. eg.</p>
<pre><code>{'Name': 'Jordan', 'Age': 35}
</code></pre>
<p>The script very simply take a query on name and age, searches through the list of dictionaries and returns a new list containing the index of all matching dictionary entries.</p>
<p>However as you can see below, <em>an insane amount of memory is consumed</em>. I presume I am making a very naive mistake somewhere.</p>
<p><img alt="screenshot of code and task manager show ram usage" src="https://i.stack.imgur.com/xBg5X.png"/></p>
<p>My code is as follows: (can also be viewed in the image if more readable).</p>
<pre><code>import sys

# Firstly, we will create 35 million records in memory, all will be the same apart from one

def search(key, value, data, age):
    print("Searching, please wait")
    # Create list to store returned PKs
    foundPKS = []
    for index in range(0, len(data)):
        if key in data[index] and 'Age' in data[index]:
            if data[index][key] == value and data[index]['Age'] &gt;= age:
                foundPKS.append(index)
    results = foundPKS
    return results

def createdata():
    # Let's create our list for storing our dictionaries
    print("Creating database, please wait")
    dictList = []
    for index in range(0, 35000000):
        # Define dictionary
        record = {'Name': 'Jordan', 'Age': 25}
        if 24500123 &lt;= index &lt;= 24500200:
            record['Name'] = 'Chris'
            record['Age'] = 33
        # Add the dict to a list
        dictList.append(record)
    return dictList

datareturned = createdata()

keyname = input("For which key do you wish to search?")
valuename = input("Which values do you want to find?")
valueage = input("What is the minimum age?")

print("Full data set object size:" + str(sys.getsizeof(datareturned)))
results = search(keyname, valuename, datareturned, int(valueage))

if len(results) &gt; 0:
    print(str(len(results)) + " found. Writing to results.txt")
    fo = open("results.txt", "w")
    for line in range(0, len(results)):
        fo.write(str(results[line]) + "\n")
    fo.close()
</code></pre>
<p>What is causing the massive consumption of RAM?</p>
</div>
<div class="post-text" itemprop="text">
<p>The overhead for a <code>dict</code> object is quite large. It depends on your Python version and your system architechture, but on Python 3.5 64bit</p>
<pre><code>In [21]: sys.getsizeof({})
Out[21]: 288
</code></pre>
<p>So guesstimating:</p>
<pre><code>250*36e6*1e-9 == 9.0
</code></pre>
<p>So that is a lower limit on my ram usage in <strong>gigabytes</strong> if I created that many dictionaries, not factoring in the <code>list</code>! </p>
<p>Rather than use a dict as a record type, which isn't really the use case, use a <code>namedtuple</code>.</p>
<p>And to get a view of how this compares, let's set up an equivalent list of tuples:</p>
<pre><code>In [23]: Record = namedtuple("Record", "name age")

In [24]: records = [Record("john", 28) for _ in range(36000000)]

In [25]: getsizeof = sys.getsizeof
</code></pre>
<p>Consider:</p>
<pre><code>In [31]: sum(getsizeof(record)+ getsizeof(record.name) + getsizeof(record.age)  for record in records)
Out[31]: 5220000000

In [32]: _ + getsizeof(records)
Out[32]: 5517842208

In [33]: _ * 1e-9
Out[33]: 5.517842208
</code></pre>
<p>So 5 gigs is an upper limit that is quite conservative. For example, it assumes that there is no small-int caching going on, which for a record-type of <em>ages</em> will totally matter. On my own system, the python process is registering 2.7 gigs of memory usage (via <code>top</code>).</p>
<p>So, what is actually going on in my machine is better modeled by being conservative for strings assuming -- unique strings that have an average size of 10, so no string interning -- but liberal for ints, assuming int-caching is taking care of our <code>int</code> objects for us, so we just have to worry about the 8-byte pointers!</p>
<pre><code>In [35]: sum(getsizeof("0123456789") + 8  for record in records)
Out[35]: 2412000000

In [36]: _ + getsizeof(records)
Out[36]: 2709842208

In [37]: _ * 1e-9
Out[37]: 2.709842208
</code></pre>
<p>Which is a good model for what I'm observing from <code>top</code>.</p>
<h3>If you really want efficient storage</h3>
<p>Now, if you really want to cram data into ram, you are going to have to lose the flexibility of Python. You could use the <code>array</code> module in combination with <code>struct</code>, to get C-like memory efficiency. An easier world to wade into might be <code>numpy</code> instead, which allows for similar things. For example:</p>
<pre><code>In [1]: import numpy as np

In [2]: recordtype = np.dtype([('name', 'S20'),('age', np.uint8)])

In [3]: records = np.empty((36000000), dtype=recordtype)

In [4]: records.nbytes
Out[4]: 756000000

In [5]: records.nbytes*1e-9
Out[5]: 0.756
</code></pre>
<p>Note, we are now allowed to be quite compact. I can use 8-bit unsigned integers (i.e. a single byte) to represent age. However, immediately I am faced with some inflexibility: if I want efficient storage of strings I must define a maximum size. I've used <code>'S20'</code>, which is 20 characters. These are ASCII bytes, but a field of 20 ascii characters might very well suffice for names. </p>
<p>Now, <code>numpy</code> gives you a lot of fast methods wrapping C-compiled code. So, just to play around with it, let's fill our records with some toy data. Names will simply be string of digits from a simple count, and age will be selected from a normal distribution with a mean of 50 and a standard deviation of 10. </p>
<pre><code>In [8]: for i in range(1, 36000000+1):
   ...:     records['name'][i - 1] = b"%08d" % i
   ...:

In [9]: import random
   ...: for i in range(36000000):
   ...:     records['age'][i] = max(0, int(random.normalvariate(50, 10)))
   ...:
</code></pre>
<p>Now, we can use numpy to query our <code>records</code>. For example, if you want the indices of your records <em>given some condition</em>, use <code>np.where</code>:</p>
<pre><code>In [10]: np.where(records['age'] &gt; 70)
Out[10]: (array([      58,      146,      192, ..., 35999635, 35999768, 35999927]),)

In [11]: idx = np.where(records['age'] &gt; 70)[0]

In [12]: len(idx)
Out[12]: 643403
</code></pre>
<p>So <code>643403</code> records that have an age <code>&gt; 70</code>. Now, let's try <code>100</code>:</p>
<pre><code>In [13]: idx = np.where(records['age'] &gt; 100)[0]

In [14]: len(idx)
Out[14]: 9

In [15]: idx
Out[15]:
array([ 2315458,  5088296,  5161049,  7079762, 15574072, 17995993,
       25665975, 26724665, 28322943])

In [16]: records[idx]
Out[16]:
array([(b'02315459', 101), (b'05088297', 102), (b'05161050', 101),
       (b'07079763', 104), (b'15574073', 101), (b'17995994', 102),
       (b'25665976', 101), (b'26724666', 102), (b'28322944', 101)],
      dtype=[('name', 'S20'), ('age', 'u1')])
</code></pre>
<p>Of course, one major inflexibility is that <code>numpy</code> arrays are <em>sized</em>. Resizing operations are expensive. Now, you could maybe wrap a <code>numpy.array</code> in some class and it will act as an efficient backbone, but at that point, you might as well use a real data-base. Lucky for you, Python comes with <code>sqlite</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>Let's look at this</p>
<pre><code>&gt;&gt;&gt; import sys 
&gt;&gt;&gt; sys.getsizeof({'Name': 'Jordan', 'Age': 25}) * 35000000
10080000000
</code></pre>
<p>So ~10 GB. Python is doing exactly what you are asking it to do.</p>
<p>You need to split this up into chucks and check them sequentially.Try <a href="https://stackoverflow.com/questions/8991506/iterate-an-iterator-by-chunks-of-n-in-python">this</a> as a starting point</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>... 35 million dictionary objects within a list. Each dictionary object
  contains two key/value pairs. eg.  {'Name': 'Jordan', 'Age': 35}</p>
</blockquote>
<p>You're right that this manner of storage has considerable overhead. </p>
<p>The <a href="https://en.wikipedia.org/wiki/Flyweight_pattern" rel="nofollow noreferrer">Flyweight Design Pattern</a> suggests that the solution involves factoring-out the commonalities.  Here are two ideas for alternative storage of the same data with better space utilization.</p>
<p>You can use <a href="https://docs.python.org/3/reference/datamodel.html#slots" rel="nofollow noreferrer">__slots__</a> to save space on instances of classes (this suppresses the creation of per-instance dictionaries):</p>
<pre><code>class Person(object):
    __slots__ = ['Name', 'Age']

s = [Person('Jordan', 35), Person('Martin', 31), Person('Mary', 33)]
</code></pre>
<p>It is even more space-efficient to use dense data structures like a pair of parallel lists:</p>
<pre><code>s_name = ['Jordan', 'Martin', 'Mary']
s_age = [35, 31, 33]
</code></pre>
<p>If there duplicates in the data, you save even more space by <a href="https://en.wikipedia.org/wiki/String_interning" rel="nofollow noreferrer">interning</a> the values:</p>
<pre><code>s_name = map(intern, s_name)
</code></pre>
<p>Or in Python 3:</p>
<pre><code>s_name = list(map(sys.intern, s_name)
</code></pre>
</div>
<span class="comment-copy">Yeah, don't use 36 million dicts. Use a tuple or better yet, a namedtuple</span>
<span class="comment-copy">I presume the RAM usage is caused by the overhead required by a dict, as the raw string data only makes about 250 - 300MB?</span>
<span class="comment-copy">Looks like a namedtuple is going to be a winner if I do want to maintain the data in-memory. Very useful, thanks.</span>
<span class="comment-copy">@Jordan I'm about to add a comparison</span>
<span class="comment-copy">@Jordan comparison added.</span>
<span class="comment-copy">That's a worthy improvement indeed. Thanks for adding. I'm also going to investigate some compression techniques for further reducing RAM utilisation.</span>
<span class="comment-copy">@Jordan Well, you can use <code>numpy</code> if you want to cram as much into memory as possible. Or the <code>array</code> module, in combinations with <code>struct</code>, which can give you the compactness of C if you really need it, but a lot of the nice flexibility of Python is lost... but at least you are still writing code in Python!</span>
<span class="comment-copy">Very useful. Thanks. I do actually want the data to remain in-memory for performance. It seems that a Dict consumes quite a bit of overhead.</span>
<span class="comment-copy">@Jordan - <code>pandas</code> might also be a good option. It would also make the search and matching a lot easier since <code>pandas</code> has that functionality built it... or just buy moar RAM :)</span>
