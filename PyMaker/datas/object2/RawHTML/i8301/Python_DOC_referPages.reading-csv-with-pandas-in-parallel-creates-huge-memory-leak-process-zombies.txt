<div class="post-text" itemprop="text">
<p>I'm reading +1000 of ~200Mb CSVs in parallel and saving the modified CSV afterwards using pandas. This creates many zombie processes that accumulate to +128Gb of RAM which devastates performance.</p>
<pre><code>    csv_data = []
    c = zip(a, b)
    process_pool = Pool(cpu_count())
    for name_and_index in process_pool.starmap(load_and_process_csv, c):
        csv_data.append(name_and_index)
    process_pool.terminate()
    process_pool.close()
    process_pool.join()
</code></pre>
<p>This is my current solution. It doesn't seem to cause a problem until you process more than 80 CSVs or so.</p>
<p>PS: Even when pool is completed ~96Gb of RAM is still occupied and you can see the python processes occupying RAM but not doing anything nor being destoryed. Moreover, I know with certainty that the function the pool is executing itself is running to completion.</p>
<p>I hope that's descriptive enough.</p>
</div>
<div class="post-text" itemprop="text">
<p>Python's <code>multiprocessing</code> module is process-based. So it is natural that you have many processes.</p>
<p>Worse, these processes do not share memory, but communicate through <code>pickling/unpickling</code>. So they are very slow if large data need to be transferred between processed, which is happening here.</p>
<p>For this case, because the processing is <code>I/O</code> related, you may have better performance using multithread with <code>threading</code> module if <code>I/O</code> is the bottleneck. Threads share memory but they also 'share' 1 CPU core, so it's not guarantee to run faster, you should try it.</p>
<p><strong>Update:</strong> If multithread does not help, you don't have many options left. Because this case is exactly against the critical weakness of Python's parallel processing architecture. You may want to try dask (parallel pandas): <a href="http://dask.readthedocs.io/en/latest/" rel="nofollow noreferrer">http://dask.readthedocs.io/en/latest/</a></p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p><strong>Question</strong>:  </p>
<pre><code>process_pool = Pool(48)
for name_and_index in process_pool.starmap(load_and_process_csv, c):
</code></pre>
</blockquote>
<p>I tried your example code, but I'm not able to start more then <strong>one</strong> <code>process</code>.
Your code looks extraordinary, beside this <code>Pool(48)</code> are to much <code>processes</code>.
To start more than <strong>one</strong> <code>process</code> I have to change to</p>
<pre><code>process_pool = Pool(2)
c_list = [(a,b), (a,b)]
csv_data = process_pool.starmap(load_and_process_csv, c_list)
</code></pre>
<blockquote>
<p><strong>Python » 3.6.1 Documentation</strong> <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.starmap" rel="nofollow noreferrer">multiprocessing.pool.Pool.starmap</a><br/>
     starmap(func, iterable[, chunksize])<br/>
      Like map() except that the elements of the iterable are expected to be iterables that are unpacked as arguments.
      Hence an iterable of [(1,2), (3, 4)] results in [func(1,2), func(3,4)].</p>
</blockquote>
<p>As I know nothing about <code>(a, b)</code>, doublecheck that the following don't apply to you.</p>
<blockquote>
<p><strong>Python » 3.6.1 Documentation</strong> <a href="https://docs.python.org/3/library/multiprocessing.html#all-start-methods" rel="nofollow noreferrer">multiprocessing.html#all-start-methods</a><br/>
      Explicitly pass resources to child processes
      On Unix using the fork start method, a child process can make use of a shared resource created in a parent process using a global resource. However, it is better to pass the object as an argument to the constructor for the child process.
      Apart from making the code (potentially) compatible with Windows and the other start methods this also ensures that as long as the child process is still alive the object will not be garbage collected in the parent process. This might be important if some resource is freed when the object is garbage collected in the parent process.</p>
</blockquote>
<hr/>
<blockquote>
<p><strong>Question</strong>:<br/>
  I know with certainty that the function the pool is executing itself is running to completion.  </p>
<p>terminate()</p>
<pre><code>Stops the worker processes immediately without completing outstanding work.  
</code></pre>
</blockquote>
<p>Please explain, why do you call <code>process_pool.terminate()</code>?  </p>
</div>
<span class="comment-copy">is this on an instance?</span>
<span class="comment-copy">no runs on both a desktop and a server. Both Ubuntu</span>
<span class="comment-copy">I believe it is a <code>pandas</code> problem since the data frames are loaded into memory and then not flushed until the python process is closed. I have had this issue many times myself.</span>
<span class="comment-copy">Are you also saying that there is no known solution for this? If you've suffered this many times what was your general workflow to solve this?</span>
<span class="comment-copy">Well as someone mentioned this is an I/O related process I have begun using the new <code>i3</code> I/O optimized EC2 instances by Amazon. At that point with a 256GB+ RAM box having a few extra DataFrames in memory doesn't matter. You also have the option of potentially optimizing your code. Although I do not know what that entails.</span>
<span class="comment-copy">I added that in a attempt to fix it a while ago. With that line remove the outcome is the same</span>
<span class="comment-copy">@SARose: Tried your code, updated my Answer with the result.</span>
<span class="comment-copy"><code>Pool(48)</code>  is because I have 48 CPUs</span>
