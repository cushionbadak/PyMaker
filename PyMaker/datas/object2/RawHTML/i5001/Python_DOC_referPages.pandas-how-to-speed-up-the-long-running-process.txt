<div class="post-text" itemprop="text">
<p>I'm reading the data from a csv file into dataframe, the file has 5 columns and has about the 300k rows, now I have build another 3 dataframes which are about 1.2 million, 400k and 500k rows. I need to check for a particular values from these 2 dataframes and depending upon the result from the three I need to pick one. My is code is below. The process is running forever. I'm the reading the source data in chunks and I have even indexed the key columns even then performance is very slow.</p>
<p>Here is the pseudo code is below, where do you think the performance issue is.</p>
<p>ps: the look up dataframe size are approx 300 ~ 500 MB each.</p>
<pre><code>def lookup1(val, date):
    # some ecode to get the lookup_data1
    return lookup_data1[(lookup_data1['key'] == val) &amp; (date &gt;= lookup_data1['start_dt']) &amp; (lookup_data1['end_dt'] &gt;= date)].required_value

def lookup2(val, date):
    # some code to get the lookup_data2
    return lookup_data2[(lookup_data2['key'] == val) &amp; (date &gt;= lookup_data2['start_dt']) &amp; (lookup_data2['end_dt'] &gt;= date)].required_value

def lookup3(val, date):
    # some code to get the lookup_data3
    return lookup_data3[(lookup_data3['key'] == val) &amp; (date &gt;= lookup_data3['start_dt']) &amp; (lookup_data3['end_dt'] &gt;= date)].pd.required_value

# driver process

source_data = pd.read_csv("some file in chunks")
date = pd.datetime('today')
for chunk in source_data:
    chunk['result1'] = chunk['val1'].map(lambda x: lookup1(x, date))
    chunk['result2'] = chunk['val2'].map(lambda x: lookup2(x, date))
    chunk['result3'] = chunk['val2'].map(lambda x: lookup3(x, date))
    chunk['final'] = chunk['result1'].fillna(chunk['result2']).fillna(chunk['result3'])
    chunk.to_csv("somefile")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<h1>Vectorize</h1>
<p><code>&amp; (date &gt;= lookup_data3['start_dt']) &amp; (lookup_data3['end_dt'] &gt;= date)</code> always stays the same, so why not do</p>
<pre><code>source_data = pd.read_csv("some file in chunks")
date = pd.datetime('today')

lookup1_date = lookup_data1[(date &gt;= lookup_data1['start_dt']) &amp; (lookup_data1['end_dt'] &gt;= date)]
....
</code></pre>
<p>Like this the 3 lookups don't have to do that each time, and then you perhaps can even just use <code>.loc</code></p>
<h1>Groupby</h1>
<p>If there is a limited number of values in the <code>chunk['val1']</code>, a <code>groupby.transform</code> will work faster here than a <code>map</code></p>
</div>
<span class="comment-copy">Performance issues are often not where one <i>thinks</i> they are. That's why people invented <a href="https://docs.python.org/3/library/profile.html" rel="nofollow noreferrer">profilers</a>. Why don't you follow that link and see if you can <i>reliably</i> find the bottleneck for yourself?</span>
<span class="comment-copy">One of the first places people look for performance boosts in python is <a href="https://pypy.org/performance.html" rel="nofollow noreferrer">pypy</a>. It's an alternate distribution of python with a more limited interface (no advanced unpacking for example), but it runs much faster, and it does <a href="https://morepypy.blogspot.ie/2017/10/pypy-v59-released-now-supports-pandas.html" rel="nofollow noreferrer">support pandas and numpy</a>. Might be worth looking in to.</span>
<span class="comment-copy">I cant find a good example to use vectorization most of the ones I see are using merge operations. I tried doing chunk['result3'] = lookup3(chunk['val1].values, date) but this doesnt work. can you pls give me an example on how to do it for my use case. Thanks!</span>
