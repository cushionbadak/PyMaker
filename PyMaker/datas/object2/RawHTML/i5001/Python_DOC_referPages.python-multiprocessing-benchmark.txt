<div class="post-text" itemprop="text">
<p>I want to perform some benchmarking between 'multiprocessing' a file and sequential processing a file.<br/>
In basics it's a file that is read line by line (consists of 100 lines), and the first character is read from eachline and is put into the list if it doesn't exists.</p>
<pre><code>import multiprocessing as mp
import sys
import time
database_layout=[]

def get_first_characters(string):
    global database_layout
    if string[0:1] not in database_layout:
        database_layout.append(string[0:1])

if __name__ == '__main__':
    start_time = time.time()
    bestand_first_read=open('random.txt','r', encoding="latin-1")
    for line in bestand_first_read:
        p = mp.Process(target=get_first_characters, args=(line,))
        p.start()
    print(str(len(database_layout)))
    print("Finished part one: "+ str(time.time() - start_time))
    bestand_first_read.close()

    ###Part two
    database_layout_two=[]
    start_time = time.time()
    bestand_first_read_two=open('random.txt','r', encoding="latin-1")
    for linetwo in bestand_first_read_two:
        if linetwo[0:1] not in database_layout_two:
            database_layout_two.append(linetwo[0:1])
    print(str(len(database_layout_two)))
    print("Finished: part two"+ str(time.time() - start_time))
</code></pre>
<p>But when i execute this program i get the following result:</p>
<pre><code>python test.py
0
Finished part one: 17.105965852737427
10
Finished part two: 0.0
</code></pre>
<p>Two problems arise at this moment.<br>
1) Why does the multiprocessing takes much longer (+/- 17 sec) than the sequential processing (+/- 0 sec).<br/>
2) Why does the list 'database_layout' defined not get filled? (It is the same code)</br></p>
<p><strong>EDIT</strong>
A same example which works with Pools. </p>
<pre><code>import multiprocessing as mp
import timeit

def get_first_characters(string):
    return string

if __name__ == '__main__':
    database_layout=[]
    start = timeit.default_timer()
    nr = 0
    with mp.Pool(processes=4) as pool:
        for i in range(99999):
            nr += 1
            database_layout.append(pool.starmap(get_first_characters, [(str(i),)]))
    stop = timeit.default_timer()
    print("Pools: %s " % (stop - start))
    database_layout=[]
    start = timeit.default_timer()
    for i in range(99999):
        database_layout.append(get_first_characters(str(i)))
    stop = timeit.default_timer()
    print("Regular: %s " % (stop - start))
</code></pre>
<p>After running above example the following output is shown.</p>
<pre><code>Pools: 22.058468394726148
Regular: 0.051738489109649066
</code></pre>
<p>This shows that in such a case working with Pools is 440 times slower than using sequential processing. Any clou why this is?</p>
</div>
<div class="post-text" itemprop="text">
<p>Multiprocessing starts one <em>process</em> for each line of your input. That means that all the overhead of opening one new Python interpreter for <em>each line</em> of your (possibly very long) file. That accounts for the long time it takes to go through the file.</p>
<p>However, there are other issues with your code. While there is no synchronisation issue due to fighting for the file (since all reads are done in the main process, where the line iteration is going on), you have misunderstood how multiprocessing works.</p>
<p>First of all, your global variable is not global across processes. Actually processes don't usually share memory (like threads) and you have to use some interface to share objects (and hence why shared objects must be picklable). When your code opens each process, each interpreter instance starts by loading your file, which creates a new <code>database_layout</code> variable. Because of that, each interpreter starts with an empty list, which means it ends with a single-element list. For actually sharing the list, you might want to use a <a href="https://docs.python.org/3/library/multiprocessing.html#managers" rel="nofollow noreferrer">Manager</a> (also see <a href="https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow noreferrer">how to share state in the docs</a>).</p>
<p>Also because of the huge overhead of opening new interpreters, your script performance may benefit from <a href="https://docs.python.org/3/library/multiprocessing.html#using-a-pool-of-workers" rel="nofollow noreferrer">using a pool of workers</a>, since this will open just a few processes for sharing the work. Remember that resource contention will impact performance if opening more processes than you have CPU cores.</p>
<p>The second problem, besides the issue of sharing your variable, is that your code does not wait for the processing to finish. Hence, even if the state was shared, your processing might not have finished when you check the length of <code>database_layout</code>. Again, using a pool might help with that.</p>
<p><strong>PS:</strong> unless you want to preserve the insertion order, you might get even faster by using a <a href="https://docs.python.org/3/tutorial/datastructures.html#sets" rel="nofollow noreferrer">set</a>, though I'm not sure the Manager supports it.</p>
<p><strong>EDIT after the OP EDIT:</strong> Your pool code is still starting up the pool for each line (or number). As you did, you still have much of your processing in the main process, just looping and passing arguments to the other processes. Besides, you're still running each element in the pool individually and appending in the list, which pretty much uses only one worker process at a time (remember that map or starmaps waits until the work finishes to return). This is from Process Explorer running your code:</p>
<p><a href="https://i.stack.imgur.com/LBUt6.png" rel="nofollow noreferrer"><img alt="Process Explorer of running script" src="https://i.stack.imgur.com/LBUt6.png"/></a></p>
<p>Note how the main process is still doing all the hard work (22% in a quad-core machine means its CPU is maxed). What you need to do is pass the iterable to <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map" rel="nofollow noreferrer"><code>map()</code></a> in a <em>single call</em>, minimizing the work (specially switching between Python and the C side):</p>
<pre><code>import multiprocessing as mp
import timeit

def get_first_characters(number):
    return str(number)[0]

if __name__ == '__main__':
    start = timeit.default_timer()
    with mp.Pool(processes=4) as pool:
        database_layout1 = (pool.map(get_first_characters, range(99999)))
    stop = timeit.default_timer()
    print("Pools: %s " % (stop - start))
    database_layout2=[]
    start = timeit.default_timer()
    for i in range(99999):
        database_layout2.append(get_first_characters(str(i)))
    stop = timeit.default_timer()
    print("Regular: %s " % (stop - start))

    assert database_layout1 == database_layout2
</code></pre>
<p>This got me from this:</p>
<pre><code>Pools: 14.169268206710512 
Regular: 0.056271265139002935 
</code></pre>
<p>To this:</p>
<pre><code>Pools: 0.35610273658926417 
Regular: 0.07681461930314981  
</code></pre>
<p>It's still slower than the single-processing one, but that's mainly because of the message-passing overhead for a very simple function. If your function is more complex it'll make more sense.</p>
</div>
<span class="comment-copy">Please fix the indentation.</span>
<span class="comment-copy">@Norrius: Done, thanks!</span>
<span class="comment-copy">Different processes have different memory, that's why <code>database_layout</code> is not filled. 0 s looks plausible (I tried 100 lines and it took <code>0.000199</code> seconds on my laptop), while 17 s is probably due to the processes fighting for the file or some synchronisation side-effect.</span>
<span class="comment-copy">I didn't realise OP is spawning a process per line... Well, that explains it!</span>
