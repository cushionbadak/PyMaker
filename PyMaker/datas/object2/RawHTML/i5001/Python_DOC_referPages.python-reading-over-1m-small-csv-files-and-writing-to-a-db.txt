<div class="post-text" itemprop="text">
<p>I have over a Million times snapshots files that I need to merge and create a single file/db for analysis.</p>
<p>My attempt to do this in the code below. first, I read a small csv from a list of URLs, takes a few columns, parse date field from text to date and writes it to a sqlite database.</p>
<p>while this code works well enough over a small subset of files, is too slow to iterate over a million CSVs.</p>
<p>I'm not sure how to increase performance or even whether Python is the right tool for the job or not. any help in improving this code or suggestions will be much appreciated. </p>
<pre><code>import pandas as pd
from sqlalchemy import create_engine
import datetime
import requests
import csv
import io

csv_database2 = create_engine('sqlite:///csv_database_test.db')

col_num = [0,8,9,12,27,31]

with open('url.csv','r') as line_list:
     reader = csv.DictReader(line_list,)

for line in reader:

    data = requests.get(line['URL'])
    df = pd.read_csv(io.StringIO(data.text), usecols=col_num, infer_datetime_format=True)
    df.columns.values[0] = 'DateTime'
    df['ParseDateTime'] = [datetime.datetime.strptime(t, "%a %b %d %H:%M:%S %Y") for t in df.DateTime]
    df.to_sql('LineList', csv_database2, if_exists='append')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>IMHO python is well suited for this task and with simple modifications you can achieve your desired performance.</p>
<p>AFAICS there could be two bottlenecks that affect performance:</p>
<h2>downloading the urls</h2>
<p>you download a single file at a time, if download a file takes 0.2 sec to download 1M files it'll take &gt; 2 days!
I suggest you'll parallelize the download, example code using <a href="https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures" rel="nofollow noreferrer"><code>concurrent.futures</code></a>:</p>
<pre><code>from concurrent.futures import ThreadPoolExecutor
import requests


def insert_url(line):
    """download single csv url and insert it to SQLite"""
    data = requests.get(line['URL'])
    df = pd.read_csv(io.StringIO(data.text), usecols=col_num,
                     infer_datetime_format=True)
    df.columns.values[0] = 'DateTime'
    df['ParseDateTime'] = [
        datetime.datetime.strptime(t, "%a %b %d %H:%M:%S %Y") for t in
        df.DateTime]
    df.to_sql('LineList', csv_database2, if_exists='append')


with ThreadPoolExecutor(max_workers=128) as pool:
    pool.map(insert_url, lines)
</code></pre>
<h2>inserting to SQL</h2>
<p>try to take a look at how to optimize the SQL insertions at <a href="https://stackoverflow.com/questions/5942402/python-csv-to-sqlite/7137270#7137270">this</a> SO answer.</p>
<h2>Further guidance</h2>
<ul>
<li>I would start with the parallel requests as it seems larger bottleneck</li>
<li>run profiler to get better idea where your code spends most of the time</li>
</ul>
</div>
<span class="comment-copy">A million files is a million files..</span>
<span class="comment-copy">Can the 'files' be "accessed locally"? Network access is <i>relatively slow</i>, and can benefit from parallelization. Can the 'files' be "pre-merged" (ie. concatenated by other tools) or "batched" (ie. by the remote server)? Can the network 'file' fetch be <i>separated</i> from the processing? -- as should be obvious, I suspect the #1 performance bottleneck is 'requests.get`. Verify and/or disprove this.</span>
<span class="comment-copy">probably want to avoid using pandas as that's more overhead plus holding everything in memory</span>
<span class="comment-copy">Consider <a href="https://www.sqlite.org/cvstrac/wiki?p=ImportingFiles" rel="nofollow noreferrer">importing csv</a> directly into SQLite using its CLI. Set up a PowerShell/Bash script to run through a directory.</span>
<span class="comment-copy">Thank you! That was very helpful, there is definitely an improvement requesting access to  URL. I will look into improving the writing to sql code a bit further.</span>
