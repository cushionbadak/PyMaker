<div class="post-text" itemprop="text">
<p>I have a script that takes a list file of query IDs and extracts the organism and sequence from uniprot, the code works well, however it is very slow. I want to process approximately 4 million sequences through it, but it takes around 5 min to parse through 100 sequences:</p>
<pre><code>real    5m32.452s
user    0m0.651s
sys 0m0.135s
</code></pre>
<p>The code uses python's retrieve module. I've read online that I can use the .session() attribute, however when I try this I get the following error:</p>
<pre><code>Traceback (most recent call last):
File "retrieve.py", line 14, in &lt;module&gt;
result = session.get(baseURL, payload)
TypeError: get() takes exactly 2 arguments (3 given)
</code></pre>
<p>The code is listed here:</p>
<pre><code>import requests

baseURL = 'http://www.uniprot.org/uniprot/'

sample = open('sample.txt','r')
out = open('out','w')

for line in sample:
    query = line.strip()
    payload = {
        'query': query, 
        'format':'tab',
        'columns': 'id, entry_name, organism, sequence'
    }
    result = requests.get(baseURL, payload)
    if result.ok:
        out.write(query + '\t' + result.text[41:] + '\n')
</code></pre>
<p>Example input format:</p>
<pre><code>EDP09046
ONI31767
ENSFALT00000002630
EAS32469
ENSXETT00000048864
</code></pre>
<p>Example output format:</p>
<blockquote>
<p>EDP09046   R6X9    A0A251R6X9_PRUPE    Prunus persica (Peach) (Amygdalus persica)  MEENHAPALESIPNGDHEAATTTNDFNTHIHTNNDHGWQKVTAKRQRKTKPSKADSINNLNKLVPGVTIAGGEGVFRSLEKQSEDRRRRILEAQRAANADADSLAPVRSKLRSDDEDGEDSDDESVAQNVKAEEAKKSKPKKPKKPKVTVAEAAAKIDDANDLSAFLIDISASYESKEDIQLMRFADYFGRAFSAVTAAQFPWVKMFRESTVAKLADIPLSHISEAVYKTSVDWISQRSLEALGSFILWSLDSILADLASQVAGAKGSKKSVQNVSSKSQVAIFVVVAMVLRKKPDVLISILPTLRENSKYQGQDKLPVIVWAISQASQGDLAVGLHSWAHIVLPLVSGKGSNPQSRDLILQLAERILSTPKARTILVNGAVRKGERLVPPSAFEILIGVTFPAPSARVKATERFEAIYPTLKAVALAGSPRSKAMKQVSLQILSFAVKAAGESIPALSNEATGIFIWCLTQHADCFKQWDKVYQENLEASVAVLKKLSDQWKEHSAKLAPFDPMRETLKSFRHKNEKMLASGEDEAHQEKLIKDADKYCKTLLGKSSRGSGCKKSVALAVVALAVGAAVMSPNMESWDWDLEKLRVTISSFFD</p>
</blockquote>
<p>Can anyone suggest some ways that I may improve this code to make it faster?</p>
<p>Thanks in advance!</p>
</div>
<div class="post-text" itemprop="text">
<p>Requests are almost always the slowest portion of any networking code so you'll absolutely want to batch your IDs. Uniprot has a <a href="http://www.uniprot.org/help/api_batch_retrieval" rel="noreferrer">batching capability in it's API</a>. There's a Perl example on that page that should help you get started – I'd look at what the batch size limit is and go for the largest as a starting point (it's likely much smaller than 4,000,000). As noted on the Uniprot site, there's also an <a href="http://www.uniprot.org/help/uploadlists" rel="noreferrer">ID mapping service</a> that may fit the bill.</p>
</div>
<span class="comment-copy">Can you clarify, you want to use requests.get 4 million times? In other words, how many lines are in your samples.txt file</span>
<span class="comment-copy">Well, essentially yes. Unless there is a better way?</span>
<span class="comment-copy">Just a quick&amp;dirty solution ... use <code>xargs</code> to "paralelize" the work in a cheap way. Create a python file that accepts 1 sys.argv (the query), does "the stuff" and print the result. Use <code>cat</code> to spill out the queries and pipe it "|" to xargs <code>xargs -n 1 -P 10 python -u ./script.py</code> (the <code>-P</code> is the number of processes to run in parallel). Add some eventual extra "tinkering" with awk if necessary</span>
<span class="comment-copy"><a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example" rel="nofollow noreferrer">docs.python.org/3/library/…</a>  (try using maybe 15 threads (workers argument)). Also try calling requests with requests.get(url, params=payload) to see if it squashes the args error.</span>
<span class="comment-copy">@jrjames83 yes, this is a perfect candidate for threading in Python. See <a href="https://stackoverflow.com/questions/2632520/what-is-the-fastest-way-to-send-100-000-http-requests-in-python">this related question</a> However, since the OP is hitting some (likely rate-limited API) then really you'll likely just want to use serially batched API calls, as per the answer posted in this question.</span>
<span class="comment-copy">Thanks for this, I had a look at this before and I was advised against it as my Query IDs come from a wealth of databases (product of a multi-lab collaboration) it actually takes an age to convert IDs to ACC, the code I have above doesn't require the queries to be in ACC</span>
<span class="comment-copy">Apologies on the delay, sounds like this may be trickier that I originally expected. I’m unfamiliar with the acronym ACC in this context, can you provide some more detail on it?</span>
<span class="comment-copy">I just realized that ACC probably means accession number, did you ever find a solution that worked for you?</span>
