<div class="post-text" itemprop="text">
<p>I call repetitively - during a minimisation process - a function that requires big arrays. Here is a dummy example</p>
<pre><code>def foo(N,a):
    big_array = np.mgrid[0:N,0:N]
    b = np.fft.fft2(big_array[0]**a) #some heavy computation
    return b
</code></pre>
<p>During the minimisation process, the array size <code>N</code> doesn't change, so I would like to use the same array to avoid useless computation and memory allocation.</p>
<p>Also I would like the function <code>foo</code> to be self-consistent, meaning that I don't want another function to create the array and give it to <code>foo</code> during the minimisation process.</p>
<p>Given these requirements, I was thinking to use a callable object with the array as an attribute. What do you think about this? Is there a more pythonic way to do?</p>
</div>
<div class="post-text" itemprop="text">
<p>a self contained approach (without global variable) would be to use a mutable default argument (that you shouldn't call your function with) to memoize the previously allocated arrays given their size</p>
<p>if the array size isn't in the dictionary, create it, and add it.</p>
<pre><code>def foo(N,a,dict_container={}):
    if N in dict_container:
        big_array = dict_container[N]
    else:
        big_array = np.mgrid[0:N,0:N]
        dict_container[N] = big_array

    b = np.fft.fft2(big_array[0]**a) #some heavy computation
    return b
</code></pre>
<p>The main problem with this approach is that it disables the garbage collector for this array, so if <code>N</code> changes too much, you can have memory exhaustion. Same technique, but using a <a href="https://docs.python.org/3/library/functools.html" rel="noreferrer">LRU cache</a> can solve the issue:</p>
<pre><code>from functools import lru_cache
@lru_cache(maxsize=32)  # max 32 elements in cache
def get_matrix(N):
    return np.mgrid[0:N,0:N]

def foo(N,a):
    big_array = get_matrix(N)
    b = np.fft.fft2(big_array[0]**a) #some heavy computation
    return b
</code></pre>
<p>(don't define <code>get_matrix</code> inside <code>foo</code> or the cache is going to be reinitialized at each call)</p>
</div>
<span class="comment-copy">you mean you want to avoid the allocation or the computation?</span>
<span class="comment-copy">I want to avoid the useless repetition of the line <code>big_array = np.mgrid[0:N,0:N]</code>. The goal is to make the program faster</span>
<span class="comment-copy">What happens if <code>N</code> is not an integer but a numpy array??? Indeed my <code>get_matrix</code> function might take an array as input and return another array... Previous methods both return an <code>unhashable type</code> error</span>
<span class="comment-copy">if <code>N</code> is an array, then you want to build an array of zeros given the array dimensions right? in that case, the key would be a <code>tuple</code> of the array dimensions (which is hashable). If you need the contents of the array, then it's going to be difficult...</span>
<span class="comment-copy">Yes I need the content of the 2D input array to build a 2D output array :/</span>
<span class="comment-copy">Maybe it is possible to build a homemade decorator in the <code>lru_cache</code> fashion. However instead of using a key-value system as in <code>lru_cache</code>, it will parse one by one all the previous inputs. It might work if the number of cached inputs is not too high.</span>
<span class="comment-copy">If the same 2D input arrays are being passed to your function (and you don't make copies of those arrays) you could use <code>id</code> as the key of your dictionary (to identify existing objects). This is a trivial change in the first example (and also works with small integers since they're unique from -5 to 256 in CPython)</span>
