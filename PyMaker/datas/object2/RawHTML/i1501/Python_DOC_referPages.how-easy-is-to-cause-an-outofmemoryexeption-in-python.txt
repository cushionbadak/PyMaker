<div class="post-text" itemprop="text">
<p>Im dong a spelling bee program in python using pygame, and it works fine, but i have been testing it with 7 words, not more.<br/>
Im worried that, if used with 300 words it might cause the memory to fill.
remember there are 2 arrays: One holds the default list of words, and the other holds the randomized words.</p>
</div>
<div class="post-text" itemprop="text">
<p>One good way to find out is to try it.</p>
<p>You can put a line midway through your program to <a href="https://stackoverflow.com/questions/938733/total-memory-used-by-python-process">print out how much memory it is using</a>:</p>
<pre><code>import os
import psutil
process = psutil.Process(os.getpid())
print(process.memory_info().rss)
</code></pre>
<p>Try running your program with different numbers of words and plotting the results:</p>
<p><a href="https://i.stack.imgur.com/4vTdX.png" rel="nofollow noreferrer"><img alt="graph plotting total memory vs. number of words" src="https://i.stack.imgur.com/4vTdX.png"/></a></p>
<p>Then you can predict how many words it would take to use up all your memory.</p>
<p>A few other points to keep in mind:</p>
<ul>
<li>If you are using 32 bit Python, your total memory will be limited by the 32 bit address space to about 4 GB.</li>
<li>Your computer likely uses the disk to increase the virtual memory beyond the RAM size. So, even if you only have 1 GB RAM, you might find you can use 3 GB of memory in your program.</li>
<li>For small lists of words like you are using, you will almost <em>never</em> run out of memory unless your program has a bug. In my experience, OutOfMemory is almost always because I made a mistake.</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>You really do not need to worry. Python is not such a memory hog as to cause issues with a mere 600 words.</p>
<p>With a bit of care, you can measure memory requirements directly. The <a href="https://docs.python.org/3/library/sys.html#sys.getsizeof" rel="nofollow noreferrer"><code>sys.getsizeof()</code> function</a> lets you measure the direct memory requirements of a given Python object (only direct memory, not anything that it references!). You could use this to measure individual strings:</p>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.getsizeof("Hello!")
55
&gt;&gt;&gt; sys.getsizeof("memoryfootprint")
64
</code></pre>
<p>Exact sizes depend on the Python version and your OS. A Python string object needs a base amount of memory for a lot of book-keeping information, and then 1, 2 or 4 bytes per character, depending on the highest Unicode code point. For ASCII, that's just one byte per letter. Python 3.7, on my Mac OS X system uses 49 bytes for the bookkeeping portion.</p>
<p>Getting the size of a Python <code>list</code> object means you get <em>just the list object memory requirements</em>, not anything that's stored 'in' the list. You can repeatedly add the same object to a list and you'd not get copies, because <a href="https://nedbatchelder.com/text/names.html" rel="nofollow noreferrer">Python uses references for everything</a>, including list contents. Take that into account.</p>
<p>So lets load 300 random words, and create two lists, to see what the memory needs will be:</p>
<pre><code>&gt;&gt;&gt; import random
&gt;&gt;&gt; words = list(map(str.strip, open('/usr/share/dict/words')))  # big file of words, present on many computers
&gt;&gt;&gt; words = random.sample(words, 300)  # just use 300
&gt;&gt;&gt; words[:10]
['fourer', 'tampon', 'Minyadidae', 'digallic', 'euploid', 'Mograbi', 'sketchbook', 'annul', 'ambilogy', 'outtalent']
&gt;&gt;&gt; import statistics
&gt;&gt;&gt; statistics.mean(map(len, words))
9.346666666666666
&gt;&gt;&gt; statistics.median(map(len, words))
9.0
&gt;&gt;&gt; statistics.mode(map(len, words))
10
&gt;&gt;&gt; sys.getsizeof(words)
2464
&gt;&gt;&gt; sum(sys.getsizeof(word) for word in words)
17504
</code></pre>
<p>That's one list, with 300 unique words with an average length of just over 9 characters, and that required 2464 bytes for the list, and 17504 bytes for the words themselves. That's less that not even 20KB.</p>
<p>But, you say, you have 2 lists. But that second list will not have copies of your words, that's just more references to the existing words, so that'll only take another 2464 bytes, so 2KB.</p>
<p><em>For 300 random English words, in two lists, your total memory requirements are around 20KB of memory</em>.</p>
<p>On an 8GB machine, you will not have any problems. Note that I loaded the <em>whole</em> <code>words</code> file in one go into my computer, and then cut that back to 300 random words. Here is how much memory that whole initial list requires:</p>
<pre><code>&gt;&gt;&gt; words = list(map(str.strip, open('/usr/share/dict/words')))
&gt;&gt;&gt; len(words)
235886
&gt;&gt;&gt; sum(sys.getsizeof(word) for word in words)
13815637
&gt;&gt;&gt; sys.getsizeof(words)
2007112
</code></pre>
<p>That's about 15MB of memory, for close to 236 thousand words.</p>
<p>If you are worried about larger programs with more objects, that you can also use the <a href="https://docs.python.org/3/library/tracemalloc.html" rel="nofollow noreferrer"><code>tracemalloc</code> library</a> to get statistics about memory use:</p>
<pre><code>last = None
def display_memory_change(msg):
    global last
    snap = tracemalloc.take_snapshot()
    statdiff, last = snap.compare_to(last, 'filename', True), snap
    tot = sum(s.size for s in statdiff)
    change = sum(s.size_diff for s in statdiff)
    print('{:&gt;20} (Tot: {:6.1f} MiB, Inc: {:6.1f} MiB)'.format(
        msg, tot / 2 ** 20, change / 2 ** 20))


# at the start, get a baseline
tracemalloc.start()
last = tracemalloc.take_snapshot()

# create objects, run more code, etc.

display_memory_change("Some message as to what has been done")

# run some more code.

display_memory_change("Show some more statistics")
</code></pre>
<p>Using the above code to measure reading all those words:</p>
<pre><code>tracemalloc.start()
last = tracemalloc.take_snapshot()
display_memory_change("Baseline")

words = list(map(str.strip, open('/usr/share/dict/words')))

display_memory_change("Loaded words list")
</code></pre>
<p>gives an output of</p>
<pre><code>            Baseline (Tot:    0.0 MiB, Inc:    0.0 MiB)
   Loaded words list (Tot:   15.1 MiB, Inc:   15.1 MiB)
</code></pre>
<p>confirming my <code>sys.getsizeof()</code> measurements.</p>
</div>
<span class="comment-copy">It would likely depend on the computer it's being run on. 300 strings alone isn't going to cause memory problems though unless you're running it on a potato.</span>
<span class="comment-copy">It's <i>easy</i> to use all available memory:<code>a = 'a'*100000000000000000</code>. But 300 words is not going to cause a problem on any modern system.</span>
<span class="comment-copy">300 words will not use up all your memory, no.</span>
<span class="comment-copy">1): Remember they are 600 due to being the normal list and the randomized list.</span>
<span class="comment-copy">2) Lets say its an 8Gb RAM</span>
<span class="comment-copy">OS memory allocation is not a good way to measure this, as that happens in chunks and Python uses a heap model (meaning it'll request larger blocks).</span>
<span class="comment-copy">Instead, use <a href="https://docs.python.org/3/library/tracemalloc.html" rel="nofollow noreferrer"><code>tracemalloc</code> snapshots</a>.</span>
<span class="comment-copy">so how can i measure it?</span>
<span class="comment-copy">ok ill se if it works</span>
<span class="comment-copy">@MartijnPieters That's a good point. Of course, if you are nearing using your whole RAM, os memory usage will be a decent approximation.</span>
<span class="comment-copy">Very usefull too</span>
<span class="comment-copy">if i could choose both, i would</span>
<span class="comment-copy">Sorry, you can only mark one as the accepted answer. The choice is entirely yours, and not picking one is also an option.</span>
