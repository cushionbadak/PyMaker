<div class="post-text" itemprop="text">
<p>I have a folder which contains many csvs (100+) and each csv contains many rows.</p>
<p>I am using the following code to load the csv into data frame, but it take a fair amount of time. What is the quickest way to load this data?</p>
<pre><code>import os
import glob
import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

path = '/Users/DATA'
all_files = glob.glob(os.path.join(path,'*.csv'))

np_array_list = []
for file_ in all_files:
    df = pd.read_csv(file_, index_col = None, header = 0, low_memory = False,
                     usecols = [1, 6, 7, 8, 9, 10, 11, 14, 16, 17, 22, 23])
    np_array_list.append(df.as_matrix())

comb_np_array = np.vstack(np_array_list)
big_data = pd.DataFrame(comb_np_array)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Try this:</p>
<pre><code>dfs = []
for file_ in all_files:
    df = pd.read_csv(...)
    dfs.append(df)

big_data = pd.concat(dfs)
</code></pre>
<p>This avoids turning your DataFrames into NumPy arrays and back again.</p>
<p>If that's still not fast enough, use a <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor" rel="nofollow noreferrer"><code>ThreadPoolExecutor</code></a> to read several at a time, the concat them at the end.</p>
</div>
<div class="post-text" itemprop="text">
<p>I recommend using a generator expression to avoid loading all data into memory twice.</p>
<pre><code>dfs = (pd.read_csv(file_, **kwargs) for file_ in all_files)
pd.concat(dfs)
</code></pre>
<p>You can also try passing the <code>engine='c'</code> argument to the reader as well to speed things up a bit.</p>
</div>
<div class="post-text" itemprop="text">
<h3><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html#numpy-loadtxt" rel="nofollow noreferrer"><code>np.loadtxt</code></a></h3>
<p>If you wish to use NumPy and your data is clean, you can bypass <code>pd.read_csv</code> altogether:</p>
<pre><code>all_files = glob.glob(os.path.join(path,'*.csv'))
cols_to_use = [1, 6, 7, 8, 9, 10, 11, 14, 16, 17, 22, 23]

arr = np.vstack([np.loadtxt(fn, delimiter=',', usecols=cols_to_use) for fn in all_files])
df = pd.DataFrame(arr)
</code></pre>
</div>
<span class="comment-copy">Thanks John! that halved the processing time.</span>
<span class="comment-copy">Sorry, but I seem to get the name 'kwargs' is not defined.</span>
<span class="comment-copy">@h.choi **kwargs is a python convention for unpacking key word arguments. In your case, these are those optional arguments that you are passing to read_csv ie. (index_col = None, header = 0, low_memory = False) etc.</span>
<span class="comment-copy">Sorry, but my data must not be clean. (it says it could not convert string to float:</span>
<span class="comment-copy">@h.choi, You should give an example of your data <a href="https://stackoverflow.com/posts/53463553/edit">as an edit to your question</a> so we know what you mean.</span>
