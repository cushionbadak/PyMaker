<div class="post-text" itemprop="text">
<p><a href="http://docs.python.org/library/stdtypes.html#str.split" rel="noreferrer"><code>string.split()</code></a> returns a <em>list</em> instance. Is there a version that returns a <a href="http://docs.python.org/tutorial/classes.html#generators" rel="noreferrer"><em>generator</em></a> instead? Are there any reasons against having a generator version? </p>
</div>
<div class="post-text" itemprop="text">
<p>It is highly probable that <a href="http://docs.python.org/library/re.html#re.finditer" rel="noreferrer"><code>re.finditer</code></a> uses fairly minimal memory overhead.</p>
<pre><code>def split_iter(string):
    return (x.group(0) for x in re.finditer(r"[A-Za-z']+", string))
</code></pre>
<p>Demo:</p>
<pre><code>&gt;&gt;&gt; list( split_iter("A programmer's RegEx test.") )
['A', "programmer's", 'RegEx', 'test']
</code></pre>
<p><strong>edit:</strong> I have just confirmed that this takes constant memory in python 3.2.1, assuming my testing methodology was correct. I created a string of very large size (1GB or so), then iterated through the iterable with a <code>for</code> loop (NOT a list comprehension, which would have generated extra memory). This did not result in a noticeable growth of memory (that is, if there was a growth in memory, it was far far less than the 1GB string).</p>
</div>
<div class="post-text" itemprop="text">
<p>The most efficient way I can think of it to write one using the <code>offset</code> parameter of the <code>str.find()</code> method. This avoids lots of memory use, and relying on the overhead of a regexp when it's not needed.  </p>
<p><em>[edit 2016-8-2: updated this to optionally support regex separators]</em></p>
<pre><code>def isplit(source, sep=None, regex=False):
    """
    generator version of str.split()

    :param source:
        source string (unicode or bytes)

    :param sep:
        separator to split on.

    :param regex:
        if True, will treat sep as regular expression.

    :returns:
        generator yielding elements of string.
    """
    if sep is None:
        # mimic default python behavior
        source = source.strip()
        sep = "\\s+"
        if isinstance(source, bytes):
            sep = sep.encode("ascii")
        regex = True
    if regex:
        # version using re.finditer()
        if not hasattr(sep, "finditer"):
            sep = re.compile(sep)
        start = 0
        for m in sep.finditer(source):
            idx = m.start()
            assert idx &gt;= start
            yield source[start:idx]
            start = m.end()
        yield source[start:]
    else:
        # version using str.find(), less overhead than re.finditer()
        sepsize = len(sep)
        start = 0
        while True:
            idx = source.find(sep, start)
            if idx == -1:
                yield source[start:]
                return
            yield source[start:idx]
            start = idx + sepsize
</code></pre>
<p>This can be used like you want...</p>
<pre><code>&gt;&gt;&gt; print list(isplit("abcb","b"))
['a','c','']
</code></pre>
<p>While there is a little bit of cost seeking within the string each time find() or slicing is performed, this should be minimal since strings are represented as continguous arrays in memory.</p>
</div>
<div class="post-text" itemprop="text">
<p>This is generator version of <code>split()</code> implemented via <code>re.search()</code> that does not have the problem of allocating too many substrings.</p>
<pre><code>import re

def itersplit(s, sep=None):
    exp = re.compile(r'\s+' if sep is None else re.escape(sep))
    pos = 0
    while True:
        m = exp.search(s, pos)
        if not m:
            if pos &lt; len(s) or sep is not None:
                yield s[pos:]
            break
        if pos &lt; m.start() or sep is not None:
            yield s[pos:m.start()]
        pos = m.end()


sample1 = "Good evening, world!"
sample2 = " Good evening, world! "
sample3 = "brackets][all][][over][here"
sample4 = "][brackets][all][][over][here]["

assert list(itersplit(sample1)) == sample1.split()
assert list(itersplit(sample2)) == sample2.split()
assert list(itersplit(sample3, '][')) == sample3.split('][')
assert list(itersplit(sample4, '][')) == sample4.split('][')
</code></pre>
<p><strong>EDIT:</strong> Corrected handling of surrounding whitespace if no separator chars are given.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here is my implementation, which is much, much faster and more complete than the other answers here. It has 4 separate subfunctions for different cases.</p>
<p>I'll just copy the docstring of the main <code>str_split</code> function:</p>
<hr/>
<pre><code>str_split(s, *delims, empty=None)
</code></pre>
<p>Split the string <code>s</code> by the rest of the arguments, possibly omitting
empty parts (<code>empty</code> keyword argument is responsible for that).
This is a generator function.</p>
<p>When only one delimiter is supplied, the string is simply split by it.
<code>empty</code> is then <code>True</code> by default.</p>
<pre><code>str_split('[]aaa[][]bb[c', '[]')
    -&gt; '', 'aaa', '', 'bb[c'
str_split('[]aaa[][]bb[c', '[]', empty=False)
    -&gt; 'aaa', 'bb[c'
</code></pre>
<p>When multiple delimiters are supplied, the string is split by longest
possible sequences of those delimiters by default, or, if <code>empty</code> is set to
<code>True</code>, empty strings between the delimiters are also included. Note that
the delimiters in this case may only be single characters.</p>
<pre><code>str_split('aaa, bb : c;', ' ', ',', ':', ';')
    -&gt; 'aaa', 'bb', 'c'
str_split('aaa, bb : c;', *' ,:;', empty=True)
    -&gt; 'aaa', '', 'bb', '', '', 'c', ''
</code></pre>
<p>When no delimiters are supplied, <code>string.whitespace</code> is used, so the effect
is the same as <code>str.split()</code>, except this function is a generator.</p>
<pre><code>str_split('aaa\\t  bb c \\n')
    -&gt; 'aaa', 'bb', 'c'
</code></pre>
<hr/>
<pre><code>import string

def _str_split_chars(s, delims):
    "Split the string `s` by characters contained in `delims`, including the \
    empty parts between two consecutive delimiters"
    start = 0
    for i, c in enumerate(s):
        if c in delims:
            yield s[start:i]
            start = i+1
    yield s[start:]

def _str_split_chars_ne(s, delims):
    "Split the string `s` by longest possible sequences of characters \
    contained in `delims`"
    start = 0
    in_s = False
    for i, c in enumerate(s):
        if c in delims:
            if in_s:
                yield s[start:i]
                in_s = False
        else:
            if not in_s:
                in_s = True
                start = i
    if in_s:
        yield s[start:]


def _str_split_word(s, delim):
    "Split the string `s` by the string `delim`"
    dlen = len(delim)
    start = 0
    try:
        while True:
            i = s.index(delim, start)
            yield s[start:i]
            start = i+dlen
    except ValueError:
        pass
    yield s[start:]

def _str_split_word_ne(s, delim):
    "Split the string `s` by the string `delim`, not including empty parts \
    between two consecutive delimiters"
    dlen = len(delim)
    start = 0
    try:
        while True:
            i = s.index(delim, start)
            if start!=i:
                yield s[start:i]
            start = i+dlen
    except ValueError:
        pass
    if start&lt;len(s):
        yield s[start:]


def str_split(s, *delims, empty=None):
    """\
Split the string `s` by the rest of the arguments, possibly omitting
empty parts (`empty` keyword argument is responsible for that).
This is a generator function.

When only one delimiter is supplied, the string is simply split by it.
`empty` is then `True` by default.
    str_split('[]aaa[][]bb[c', '[]')
        -&gt; '', 'aaa', '', 'bb[c'
    str_split('[]aaa[][]bb[c', '[]', empty=False)
        -&gt; 'aaa', 'bb[c'

When multiple delimiters are supplied, the string is split by longest
possible sequences of those delimiters by default, or, if `empty` is set to
`True`, empty strings between the delimiters are also included. Note that
the delimiters in this case may only be single characters.
    str_split('aaa, bb : c;', ' ', ',', ':', ';')
        -&gt; 'aaa', 'bb', 'c'
    str_split('aaa, bb : c;', *' ,:;', empty=True)
        -&gt; 'aaa', '', 'bb', '', '', 'c', ''

When no delimiters are supplied, `string.whitespace` is used, so the effect
is the same as `str.split()`, except this function is a generator.
    str_split('aaa\\t  bb c \\n')
        -&gt; 'aaa', 'bb', 'c'
"""
    if len(delims)==1:
        f = _str_split_word if empty is None or empty else _str_split_word_ne
        return f(s, delims[0])
    if len(delims)==0:
        delims = string.whitespace
    delims = set(delims) if len(delims)&gt;=4 else ''.join(delims)
    if any(len(d)&gt;1 for d in delims):
        raise ValueError("Only 1-character multiple delimiters are supported")
    f = _str_split_chars if empty else _str_split_chars_ne
    return f(s, delims)
</code></pre>
<p>This function works in Python 3, and an easy, though quite ugly, fix can be applied to make it work in both 2 and 3 versions. The first lines of the function should be changed to:</p>
<pre><code>def str_split(s, *delims, **kwargs):
    """...docstring..."""
    empty = kwargs.get('empty')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Did some performance testing on the various methods proposed (I won't repeat them here). Some results:</p>
<ul>
<li><code>str.split</code> (default    = 0.3461570239996945</li>
<li>manual search (by character) (one of Dave Webb's answer's) = 0.8260340550004912</li>
<li><code>re.finditer</code> (ninjagecko's answer)     = 0.698872097000276</li>
<li><code>str.find</code> (one of Eli Collins's answers)      = 0.7230395330007013</li>
<li><code>itertools.takewhile</code> (Ignacio Vazquez-Abrams's answer) = 2.023023967998597</li>
<li><code>str.split(..., maxsplit=1)</code> recursion = N/A†</li>
</ul>
<p><em>†The recursion answers (<code>string.split</code> with <code>maxsplit = 1</code>) fail to complete in a reasonable time, given <code>string.split</code>s speed they may work better on shorter strings, but then I can't see the use-case for short strings where memory isn't an issue anyway.</em></p>
<p>Tested using <code>timeit</code> on:</p>
<pre><code>the_text = "100 " * 9999 + "100"

def test_function( method ):
    def fn( ):
        total = 0

        for x in method( the_text ):
            total += int( x )

        return total

    return fn
</code></pre>
<p>This raises another question as to why <code>string.split</code> is so much faster despite its memory usage.</p>
</div>
<div class="post-text" itemprop="text">
<p>No, but it should be easy enough to write one using <a href="http://docs.python.org/library/itertools.html#itertools.takewhile" rel="nofollow"><code>itertools.takewhile()</code></a>.</p>
<p><strong>EDIT:</strong></p>
<p>Very simple, half-broken implementation:</p>
<pre><code>import itertools
import string

def isplitwords(s):
  i = iter(s)
  while True:
    r = []
    for c in itertools.takewhile(lambda x: not x in string.whitespace, i):
      r.append(c)
    else:
      if r:
        yield ''.join(r)
        continue
      else:
        raise StopIteration()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strike>I don't see any obvious benefit to a generator version of <code>split()</code>.  The generator object is going to have to contain the whole string to iterate over so you're not going to save any memory by having a generator.</strike></p>
<p>If you wanted to write one it would be fairly easy though:</p>
<pre><code>import string

def gsplit(s,sep=string.whitespace):
    word = []

    for c in s:
        if c in sep:
            if word:
                yield "".join(word)
                word = []
        else:
            word.append(c)

    if word:
        yield "".join(word)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I wrote a version of @ninjagecko's answer that behaves more like string.split (i.e. whitespace delimited by default and you can specify a delimiter).</p>
<pre><code>def isplit(string, delimiter = None):
    """Like string.split but returns an iterator (lazy)

    Multiple character delimters are not handled.
    """

    if delimiter is None:
        # Whitespace delimited by default
        delim = r"\s"

    elif len(delimiter) != 1:
        raise ValueError("Can only handle single character delimiters",
                        delimiter)

    else:
        # Escape, incase it's "\", "*" etc.
        delim = re.escape(delimiter)

    return (x.group(0) for x in re.finditer(r"[^{}]+".format(delim), string))
</code></pre>
<p>Here are the tests I used (in both python 3 and python 2):</p>
<pre><code># Wrapper to make it a list
def helper(*args,  **kwargs):
    return list(isplit(*args, **kwargs))

# Normal delimiters
assert helper("1,2,3", ",") == ["1", "2", "3"]
assert helper("1;2;3,", ";") == ["1", "2", "3,"]
assert helper("1;2 ;3,  ", ";") == ["1", "2 ", "3,  "]

# Whitespace
assert helper("1 2 3") == ["1", "2", "3"]
assert helper("1\t2\t3") == ["1", "2", "3"]
assert helper("1\t2 \t3") == ["1", "2", "3"]
assert helper("1\n2\n3") == ["1", "2", "3"]

# Surrounding whitespace dropped
assert helper(" 1 2  3  ") == ["1", "2", "3"]

# Regex special characters
assert helper(r"1\2\3", "\\") == ["1", "2", "3"]
assert helper(r"1*2*3", "*") == ["1", "2", "3"]

# No multi-char delimiters allowed
try:
    helper(r"1,.2,.3", ",.")
    assert False
except ValueError:
    pass
</code></pre>
<p>python's regex module says that it <a href="https://docs.python.org/3/library/re.html" rel="nofollow">does "the right thing"</a> for unicode whitespace, but I haven't actually tested it.</p>
<p>Also available as a <a href="https://gist.github.com/davidshepherd7/2857bfc620a648a90e7f" rel="nofollow">gist</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you would also like to be able to <em>read</em> an iterator (as well as <em>return</em> one) try this:</p>
<pre><code>import itertools as it

def iter_split(string, sep=None):
    sep = sep or ' '
    groups = it.groupby(string, lambda s: s != sep)
    return (''.join(g) for k, g in groups if k)
</code></pre>
<p>Usage</p>
<pre><code>&gt;&gt;&gt; list(iter_split(iter("Good evening, world!")))
['Good', 'evening,', 'world!']
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I wanted to show how to use the find_iter solution to return a generator for given delimiters and then use the pairwise recipe from itertools to build a previous next iteration which will get the actual words as in the original split method.</p>
<hr/>
<pre><code>from more_itertools import pairwise
import re

string = "dasdha hasud hasuid hsuia dhsuai dhasiu dhaui d"
delimiter = " "
# split according to the given delimiter including segments beginning at the beginning and ending at the end
for prev, curr in pairwise(re.finditer("^|[{0}]+|$".format(delimiter), string)):
    print(string[prev.end(): curr.start()])
</code></pre>
<hr/>
<p>note: </p>
<ol>
<li>I use prev &amp; curr instead of prev &amp; next because overriding next in python is a very bad idea</li>
<li>This is quite efficient</li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://more-itertools.readthedocs.io/en/latest/api.html#more_itertools.split_at" rel="nofollow noreferrer"><code>more_itertools.spit_at</code></a> offers an analog to <code>str.split</code> for iterators.</p>
<pre><code>&gt;&gt;&gt; import more_itertools as mit


&gt;&gt;&gt; list(mit.split_at("abcdcba", lambda x: x == "b"))
[['a'], ['c', 'd', 'c'], ['a']]

&gt;&gt;&gt; "abcdcba".split("b")
['a', 'cdc', 'a']
</code></pre>
<p><code>more_itertools</code> is a third-party package.</p>
</div>
<div class="post-text" itemprop="text">
<pre><code>def split_generator(f,s):
    """
    f is a string, s is the substring we split on.
    This produces a generator rather than a possibly
    memory intensive list. 
    """
    i=0
    j=0
    while j&lt;len(f):
        if i&gt;=len(f):
            yield f[j:]
            j=i
        elif f[i] != s:
            i=i+1
        else:
            yield [f[j:i]]
            j=i+1
            i=i+1
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Need is for me, at least, with files used as generators.</p>
<p>This is version I did in preparation to some huge files with empty line separated blocks of text (this would need to be thoroughly tested for corner cases in case you would use it in production system):</p>
<pre><code>from __future__ import print_function

def isplit(iterable, sep=None):
    r = ''
    for c in iterable:
        r += c
        if sep is None:
            if not c.strip():
                r = r[:-1]
                if r:
                    yield r
                    r = ''                    
        elif r.endswith(sep):
            r=r[:-len(sep)]
            yield r
            r = ''
    if r:
        yield r


def read_blocks(filename):
    """read a file as a sequence of blocks separated by empty line"""
    with open(filename) as ifh:
        for block in isplit(ifh, '\n\n'):
            yield block.splitlines()           

if __name__ == "__main__":
    for lineno, block in enumerate(read_blocks("logfile.txt"), 1):
        print(lineno,':')
        print('\n'.join(block))
        print('-'*40)

    print('Testing skip with None.')
    for word in isplit('\tTony   \t  Jarkko \n  Veijalainen\n'):
        print(word)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>here is a simple response</p>
<pre><code>def gen_str(some_string, sep):
    j=0
    guard = len(some_string)-1
    for i,s in enumerate(some_string):
        if s == sep:
           yield some_string[j:i]
           j=i+1
        elif i!=guard:
           continue
        else:
           yield some_string[j:]
</code></pre>
</div>
<span class="comment-copy"><a href="http://stackoverflow.com/questions/3054604/">This question</a> might be related.</span>
<span class="comment-copy">The reason is that it's very hard to think of a case where it's useful.  Why do you want this?</span>
<span class="comment-copy">@Glenn: Recently I saw a question about splitting a long string into chunks of n words. One of the solutions <code>split</code> the string and then returned a generator working on the result of <code>split</code>. That got me thinking if there was a way for <code>split</code> to return a generator to start with.</span>
<span class="comment-copy">There is a relevant discussion on the Python Issue tracker: <a href="http://bugs.python.org/issue17343" rel="nofollow noreferrer">bugs.python.org/issue17343</a></span>
<span class="comment-copy">@GlennMaynard it can be useful for really large bare string/file parsing, but anybody can write generator parser himself very easy using self-brewed DFA and yield</span>
<span class="comment-copy">Excellent! I had forgotten about finditer. If one were interested in doing something like splitlines, I would suggest using this RE: '(.*\n|.+$)' str.splitlines chops off the trainling newline though (something that I don't really like...); if you wanted to replicated that part of the behavior, you could use grouping: (m.group(2) or m.group(3) for m in re.finditer('((.*)\n|(.+)$)', s)). PS: I guess the outer paren in the RE are not needed; I just feel uneasy about using | without paren :P</span>
<span class="comment-copy">What about performance? re matching should be slower that ordinary search.</span>
<span class="comment-copy">How would you rewrite this split_iter function to work like <code>a_string.split("delimiter")</code>?</span>
<span class="comment-copy">split accepts regular expressions anyway so it's not really faster, if you want to use the returned value in a prev next fashion, look at my answer at the bottom...</span>
<span class="comment-copy"><code>str.split()</code> does not accept regular expressions, that's <code>re.split()</code> you're thinking of...</span>
<span class="comment-copy">why is this any better than <code>re.finditer</code>?</span>
<span class="comment-copy">This is because memory is slower than cpu and in this case, the list is loaded by chunks where as all the others are loaded element by element. On the same note, many academics will tell you linked lists are faster and have less complexity while your computer will often be faster with arrays, which it finds easier to optimise. <b>You can't assume an option is faster than another, test it !</b> +1 for testing.</span>
<span class="comment-copy">@Ignacio: The example in docs uses a list of integers to illustrate the use of <code>takeWhile</code>. What would be a good <code>predicate</code> for splitting a string into words (default <code>split</code>) using <code>takeWhile()</code>?</span>
<span class="comment-copy">Look for presence in <code>string.whitespace</code>.</span>
<span class="comment-copy">The separator can have multiple characters, <code>'abc&lt;def&lt;&gt;ghi&lt;&gt;&lt;&gt;lmn'.split('&lt;&gt;') == ['abc&lt;def', 'ghi', '', 'lmn']</code></span>
<span class="comment-copy">@Ignacio: Can you add an example to your answer?</span>
<span class="comment-copy">Easy to write, but <i>many</i> orders of magnitude slower.  This is an operation that really should be implemented in native code.</span>
<span class="comment-copy">You'd halve the memory used, by not having to store a second copy of the string in each resulting part, plus the array and object overhead (which is typically more than the strings themselves).  That generally doesn't matter, though (if you're splitting strings so large that this matters, you're probably doing something wrong), and even a native C generator implementation would always be significantly slower than doing it all at once.</span>
<span class="comment-copy">@Glenn Maynard - I just realised that.  I for some reason I originally the generator would store a copy of the string rather than a reference.  A quick check with <code>id()</code> put me right.  And obviously as strings are immutable you don't need to worry about someone changing the original string while you're iterating over it.</span>
<span class="comment-copy">Isn't the main point in using a generator not the memory usage, but that you could save yourself having to split the whole string if you wanted to exit early? (That's not a comment on your particular solution, I was just surprised by the discussion about memory).</span>
<span class="comment-copy">@Scott: It's hard to think of a case where that's really a win--where 1: you want to stop splitting partway through, 2: you don't know how many words you're splitting in advance, 3: you have a large enough string for it to matter, and 4: you consistently stop early enough for it to be a significant win over str.split.  That's a very narrow set of conditions.</span>
<span class="comment-copy">You can have much higher benefit if your string is lazily generated as well (e.g. from network traffic or file reads)</span>
<span class="comment-copy">why do you yield <code>[f[j:i]]</code>and not <code>f[j:i]</code>?</span>
