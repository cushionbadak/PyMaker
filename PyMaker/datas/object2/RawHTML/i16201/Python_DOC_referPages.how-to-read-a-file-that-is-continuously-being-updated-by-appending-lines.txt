<div class="post-text" itemprop="text">
<p>In my terminal I am running:</p>
<pre><code>curl --user dhelm:12345 \https://stream.twitter.com/1.1/statuses/sample.json &gt; raw-data.txt
</code></pre>
<p>curl's output is live streaming Twitter data which is being written on to a file raw-data.txt</p>
<p>In python, </p>
<pre><code> import json
 posts = []

 for line in open("/Users/me/raw-data.txt"):
    try:
        posts.append(json.loads(line))
    except:
        pass
</code></pre>
<p>I am reading the file in python and using json decoder and appending the results to posts.</p>
<p>Now, the issue is I don't want my program to end when the python script reaches the end of file. instead I want to continue reading when the curl running on my terminal appends more posts to the file raw-data.txt.</p>
</div>
<div class="post-text" itemprop="text">
<p>I think this is an <a href="https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem">XY problem</a>. Because you couldn't think of a way to stream an HTTP request line by line from within Python, you decided to use <code>curl</code> to do a streaming download to a file, and then read that file from within Python. Because you did that, you have to deal with the possibility of running into EOF while the request is still going, just because you've caught up to <code>curl</code>. So you're making things harder on yourself for no reason.</p>
<p>While streaming downloads <em>can</em> be done with the stdlib, it's a bit painful; the <a href="http://docs.python-requests.org/en/latest/user/advanced/#streaming-requests" rel="nofollow noreferrer"><code>requests</code></a> library makes it a lot easier. So, let's use that:</p>
<pre><code>import json
import requests
from requests.auth import HTTPBasicAuth

posts = []
url = 'https://stream.twitter.com/1.1/statuses/sample.json'
r = requests.get(url, auth=('dhelm', '12345'), stream=True)
for line in r.iter_lines():
    try:
        posts.append(json.loads(line))
    except:
        pass
</code></pre>
<p>And that's the whole program.</p>
</div>
<div class="post-text" itemprop="text">
<p>I don't know if this is guaranteed anywhere by the language, but I do know that it works with <em>at least</em> CPython 2.x and 3.3+ on Unix. So if you don't care about 3.0-3.2 (or can test it yourself), and don't care about Windows (or can test it yourself)…</p>
<p>When you reach EOF, your <code>for line in f</code> loop will finish. But it doesn't close the file or anything; all it does is leave the file pointer sitting at EOF. If you try to loop again, and more data have been written, you'll get the new data.</p>
<p>So, you <em>could</em> do this:</p>
<pre><code>with open("/Users/me/raw-data.txt") as f:
    while True:
        for line in f:
            try:
                posts.append(json.loads(line))
            except:
                pass
</code></pre>
<p>The problem with this is that when you reach EOF, it will spin as fast as possible, verifying that it's still at EOF. So what you really want to do is block until there's more data. You can do that with <code>select</code> on some Unix platforms, but not all. You can use a platform-specific file notification API, or a cross-platform wrapper around such APIs.</p>
<p>If you're using Python 3.4+, you can use the <a href="https://docs.python.org/3/library/selectors.html" rel="nofollow"><code>selectors</code></a> module in the stdlib, which will give you something that works on Solaris, on Linux, on OS X and any other *BSD with <code>kqueue</code>, and on some Unix platforms with only <code>select</code>… but on Windows it will fail, and on some Unix systems it will spin as fast as possible. You can work around that by refusing to start if you can't find a good selector.</p>
<p>Or, if worst comes to worst, you can just sleep for a bit at EOF (possibly with some exponential backoff, but only up to a reasonably short limit). This is what <code>tail -f</code> does in ports to platforms that have no way to detect notifications.</p>
<p>So:</p>
<pre><code>import selectors
import time

if selectors.DefaultSelector in (selectors.SelectSelector, selectors.PollSelector):
    def init(f): pass
    def wait(): time.sleep(1)
else:
    def init(f):
        sel = selectors.DefaultSelector()
        sel.register(f, selectors.EVENT_READ, None)
    def wait():
        sel.select()

with open("/Users/me/raw-data.txt") as f:
    init(f)
    while True:
        for line in f:
            try:
                posts.append(json.loads(line))
            except:
                pass
         wait()
</code></pre>
</div>
<span class="comment-copy">Is there a reason you want to use <code>curl</code> instead of using, e.g., <code>requests</code> from within Python?</span>
<span class="comment-copy">@abarnert The data being returned is streaming data. So if I tried requests from within Python, its an infinite loop ... I need to stop at some point and work with the data. So I thought, I could run curl in the terminal and read the results from Python</span>
<span class="comment-copy"><code>requests</code> can do streaming downloads. Just read a line at a time off the socket, process that line, and come back for the next line.</span>
<span class="comment-copy">@abarnert do you suggest I use urllib2 to do that? or use requests package ?</span>
<span class="comment-copy">I'd suggest <code>requests</code> for this. By default, <code>urllib2</code> will try to block until it's downloaded the entire page… which in this case means forever. Of course the same is true with <code>requests</code>, but <code>urllib2</code> makes it hard to get under the covers, while with <code>requests</code> it's just a matter of adding <code>stream=True</code>.</span>
