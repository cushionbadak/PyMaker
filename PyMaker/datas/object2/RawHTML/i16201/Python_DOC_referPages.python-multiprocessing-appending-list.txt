<div class="post-text" itemprop="text">
<p>Have a quick question about a shared variable between multiple processes using Multiprocessing.Pool().</p>
<p>Will I run in to any issues if I am updating a global list from within multiple processes? I.e. if two of the processes were to try to update the list at the same time.</p>
<p>I have seen documentation about using a Lock for similar things but I was wondering if it was necessary.</p>
<p>EDIT:</p>
<p>The way I am sharing this variable is by using a global variable in my callback function,
'successes' in which i append all of the successful actions to after the target function has completed:</p>
<pre><code>TOTAL_SUCCESSES = []

def func(inputs):
    successes = []

    for input in inputs:
        result = #something with return code
        if result == 0:
            successes.append(input)
    return successes

def callback(successes):
    global TOTAL_SUCCESSES

    for entry in successes:
        TOTAL_SUCCESSES.append(entry)

def main():     
    pool = mp.Pool()
    for entry in myInputs:
         pool.apply_async(func, args=(entry,),callback=callback)         
</code></pre>
<p>Apologize for any syntax errors, wrote this up quickly but the program is working just wondering if I add the shared variable if I will have issues.</p>
<p>Thanks in advance!</p>
</div>
<div class="post-text" itemprop="text">
<p>With your current code, you're not actually sharing <code>CURRENT_SUCCESSES</code> between processes. <code>callback</code> is executed in the main process, in a result handling thread. There is only one result handling thread, so each <code>callback</code> will be run one at a time, not concurrently. So your code as written is process/thread safe. </p>
<p>However, you are forgetting to return <code>successes</code> from <code>func</code>, which you'll want to fix.</p>
<p><strong>Edit:</strong></p>
<p>Also, this could be much more succinctly written using <code>map</code>:</p>
<pre><code>def func(inputs):
    successes = []

    for input in inputs:
        result = #something with return code
        if result == 0:
            successes.append(input)
    return successes

def main():     
    pool = mp.Pool()
    total_successes = pool.map(func, myInputs) # Returns a list of lists
    # Flatten the list of lists
    total_successes = [ent for sublist in total_successes for ent in sublist]
</code></pre>
</div>
<span class="comment-copy">How are you sharing the list?  If you are passing a simple list in directly, each process will get a local copy of the list.  Are you using a queue?  A multiprocessing.Array?</span>
<span class="comment-copy">You'll glean a great deal by reading through the multiprocessing examples: <a href="https://docs.python.org/3/library/multiprocessing.html?highlight=multiprocessing#examples" rel="nofollow noreferrer">docs.python.org/3/library/â€¦</a>, but until you have some sample code for your implementation we can't tell you what pitfalls you might encounter.</span>
<span class="comment-copy">Hey guys, thanks for the responses, edited with some code.</span>
<span class="comment-copy">yup, sorry I am returning it (obviously). Thanks for the info! And yeah that makes perfect sense, I should have realized that.</span>
<span class="comment-copy">Side note - Is there a more appropriate way to be doing this or is this acceptable?</span>
<span class="comment-copy">@DJMcCarthy12 I edited my answer with a nicer way.</span>
<span class="comment-copy">Excellent, this is great! Appreciate it.</span>
