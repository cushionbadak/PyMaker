<div class="post-text" itemprop="text">
<p>I am using pandas to analyse the large data files here: <a href="http://www.nielda.co.uk/betfair/data/" rel="noreferrer">http://www.nielda.co.uk/betfair/data/</a> They are around 100 megs in size.</p>
<p>Each load from csv takes a few seconds, and then more time to convert the dates.</p>
<p>I have tried loading the files, converting the dates from strings to datetimes, and then re-saving them as pickle files. But loading those takes a few seconds as well.</p>
<p>What fast methods could I use to load/save the data from disk?</p>
</div>
<div class="post-text" itemprop="text">
<p>As @chrisb said, pandas' <code>read_csv</code> is probably faster than <code>csv.reader/numpy.genfromtxt/loadtxt</code>. I don't think you will find something better to parse the csv (as a note, <code>read_csv</code> is not a 'pure python' solution, as the CSV parser is implemented in C). </p>
<p>But, if you have to load/query the data often, a solution would be to parse the CSV only once and then store it in another format, eg HDF5. You can use <code>pandas</code> (with <code>PyTables</code> in background) to query that efficiently (<a href="http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables" rel="noreferrer">docs</a>).<br/>
See here for a comparison of the io performance of HDF5, csv and SQL with pandas: <a href="http://pandas.pydata.org/pandas-docs/stable/io.html#performance-considerations" rel="noreferrer">http://pandas.pydata.org/pandas-docs/stable/io.html#performance-considerations</a></p>
<p>And a possibly relevant other question: <a href="https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas/14268804#14268804">"Large data" work flows using pandas</a></p>
</div>
<div class="post-text" itemprop="text">
<p>One thing to check is the actual performance of the disk system itself. Especially if you use spinning disks (not SSD), your practical disk read speed may be one of the explaining factors for the performance. So, before doing too much optimization, check if reading the same data into memory (by, e.g., <code>mydata = open('myfile.txt').read()</code>) takes an equivalent amount of time. (Just make sure you do not get bitten by disk caches; if you load the same data twice, the second time it will be much faster because the data is already in RAM cache.)</p>
<p><strong>See the update below before believing what I write underneath</strong></p>
<p>If your problem is really parsing of the files, then I am not sure if any pure Python solution will help you. As you know the actual structure of the files, you do not need to use a generic CSV parser.</p>
<p>There are three things to try, though:</p>
<ol>
<li>Python <code>csv</code> package and <code>csv.reader</code></li>
<li>NumPy <code>genfromtext</code></li>
<li>Numpy <code>loadtxt</code></li>
</ol>
<p>The third one is probably fastest if you can use it with your data. At the same time it has the most limited set of features. (Which actually may make it fast.)</p>
<p>Also, the suggestions given you in the comments by <code>crclayton</code>, <code>BKay</code>, and <code>EdChum</code> are good ones. </p>
<p>Try the different alternatives! If they do not work, then you will have to do write something in a compiled language (either compiled Python or, e.g. C).</p>
<p><strong>Update:</strong> I do believe what <code>chrisb</code> says below, i.e. the <code>pandas</code> parser is fast.</p>
<p>Then the only way to make the parsing faster is to write an application-specific parser in C (or other compiled language). Generic parsing of CSV files is not straightforward, but if the exact structure of the file is known there may be shortcuts. In any case parsing text files is slow, so if you ever can translate it into something more palatable (HDF5, NumPy array), loading will be only limited by the I/O performance.</p>
</div>
<span class="comment-copy"><code>This Websense category is filtered: Adult Material. URL: http://www.nielda.co.uk/betfair/data</code> Oh jeez, please tell me what I just clicked.</span>
<span class="comment-copy">Just a guess, but maybe <a href="https://docs.python.org/3/library/csv.html" rel="nofollow noreferrer">csv.reader</a> would be more efficient to parse than Pandas.</span>
<span class="comment-copy">My experience with pickling pandas data frames is that it isn't faster than loading a CSV file with the same contents.  Have you tried using the parse_dates option of pd.read_csv?</span>
<span class="comment-copy">Are you converting to datetime during loading or after? You should be able to convert the columns as you read them in, I would expect pandas to be very efficient at reading the csvs in, as for storing perhaps you should consider pytables/hdf5?</span>
<span class="comment-copy">crclayton - the data contains historical data for betfair horse races. There is no naughty material there.</span>
<span class="comment-copy">Another alternative for HDF5 is to save the NumPy array as such. Then loading it is just loading binary stream into memory with no parsing or any extra operations. Using <code>memmap</code> there is zero overhead (but depending on the application it may be possible to perform calculations on the head while the tail is still loading, and then <code>memmap</code> is not a good solution as such). On the other hand, if HDF5 is fast enough, it is a standard format and should be preferred.</span>
<span class="comment-copy">For what it's worth, the pandas csv parser is quite a bit faster than any of those three.</span>
<span class="comment-copy">@chrisb: Thank you for the correction, I did not have any benchmarks. I revised my answer, so if you gave it -1, you may consider removing that. (The -1 was deserved, I am not complaining.)</span>
