<div class="post-text" itemprop="text">
<h1><strong>Huge plain-text data file</strong></h1>
<p>I read a huge file in chunks using python. Then I apply a regex on that chunk. Based on an identifier tag, I want to extract the corresponding value. Due to the chunk size, data is missing at the chunk boundaries.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>The file must be read in chunks.</li>
<li>The chunk sizes must be smaller than
or equal to 1 GiB.</li>
</ul>
<p><br/>
<strong>Python code example</strong></p>
<pre><code>identifier_pattern = re.compile(r'Identifier: (.*?)\n')
with open('huge_file', 'r') as f:
    data_chunk = f.read(1024*1024*1024)
    m = re.findall(identifier_pattern, data_chunk)
</code></pre>
<p><br/>
<strong>Chunk data examples</strong><br/><br/>
<strong>Good:</strong> number of tags equivalent to number of values</p>
<blockquote>
<p>Identifier: value<br/> Identifier: value<br/> Identifier: value<br/>
  Identifier: value<br/></p>
</blockquote>
<p><br/>Due to the chunk size, you get varying boundary issues as listed below.  The third identifier returns an incomplete value, "v" instead of "value". The next chunk contains "alue". This causes missing data after parsing.</p>
<p><strong>Bad:</strong> identifier value incomplete</p>
<blockquote>
<p>Identifier: value<br/> Identifier: value<br/> Identifier: v<br/></p>
</blockquote>
<p><br/>How do you solve chunk boundary issues like this?</p>
</div>
<div class="post-text" itemprop="text">
<p>Assuming this is your exact problem you could probably just adapt your regex and read line by line (which won't load the full file into memory):</p>
<pre><code>import re
matches = []
identifier_pattern = re.compile(r'Identifier: (.*?)$')
with open('huge_file') as f:
    for line in f:
        matches += re.findall(identifier_pattern, line)

print("matches", matches)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can control chunk forming and have it close to 1024 * 1024 * 1024, in that case you avoid missing parts:</p>
<pre><code>import re


identifier_pattern = re.compile(r'Identifier: (.*?)\n')
counter = 1024 * 1024 * 1024
data_chunk = ''
with open('huge_file', 'r') as f:
    for line in f:
        data_chunk = '{}{}'.format(data_chunk, line)
        if len(data_chunk) &gt; counter:
            m = re.findall(identifier_pattern, data_chunk)
            print m.group()
            data_chunk = ''
    # Analyse last chunk of data
    m = re.findall(identifier_pattern, data_chunk)
    print m.group()
</code></pre>
<p>Alternativelly, you can go two times over same file with different starting point of <code>read</code> (first time from: 0, second time from max length of matched string collected during first iteration), store results as dictionaries, where <code>key=[start position of matched string in file]</code>, that position would be same for each iteration, so it shall not be a problem to merge results, however I think it would be more accurate to do merge by start position and length of matched string.</p>
<p><strong>Good Luck !</strong></p>
</div>
<div class="post-text" itemprop="text">
<p>If the file is <strong>line-based</strong>, the <code>file</code> object is a <strong>lazy generator of lines</strong>, it will load the file into memory <strong>line by line</strong> (in chunks), based on that, you can use:</p>
<pre><code>import re
matches = []
for line in open('huge_file'):
    matches += re.findall("Identifier:\s(.*?)$", line)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I have a solution very similar to Jack's answer:</p>
<pre><code>#!/usr/bin/env python3

import re

identifier_pattern = re.compile(r'Identifier: (.*)$')

m = []
with open('huge_file', 'r') as f:
    for line in f:
        m.extend(identifier_pattern.findall(line))
</code></pre>
<p>You could use a another part of the regex API to have the same result:</p>
<pre><code>#!/usr/bin/env python3

import re

identifier_pattern = re.compile(r'Identifier: (.*)$')

m = []
with open('huge_file', 'r') as f:
    for line in f:
        pattern_found = identifier_pattern.search(line)
        if pattern_found:
            value_found = pattern_found.group(0)
            m.append(value_found)
</code></pre>
<p>Which we could simplify using a <a href="https://docs.python.org/3/tutorial/classes.html#generator-expressions" rel="nofollow noreferrer">generator expression</a> and a <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" rel="nofollow noreferrer">list comprehension</a></p>
<pre><code>#!/usr/bin/env python3

import re

identifier_pattern = re.compile(r'Identifier: (.*)$')

with open('huge_file', 'r') as f:
    patterns_found = (identifier.search(line) for line in f)
    m = [pattern_found.group(0) 
         for pattern_found in patterns_found if pattern_found]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If the matched result string's length is known, the easiest way I think is to cache the last chunk's bytes around the boundary.</p>
<p>Suppose the result's length is 3, keep the last 2 chars of the last chunk, then add it to the new chunk for matching.</p>
<p>Pseudo-code:</p>
<pre><code>regex  pattern
string boundary
int    match_result_len

for chunk in chunks:
    match(boundary + chunk, pattern)
    boundary = chunk[-(match_result_len - 1):]
</code></pre>
</div>
<span class="comment-copy">Maybe you can find your answer here: <a href="https://stackoverflow.com/questions/4634376/python-regex-parse-stream">Python regex parse stream</a></span>
<span class="comment-copy">Also here: <a href="https://stackoverflow.com/questions/13004359/regular-expression-on-stream-instead-of-string">regular expression on stream instead of string?</a></span>
<span class="comment-copy">Since your pattern appears on a line boundary, maybe you could just read a line at a time and matching on the line instead of chunk.</span>
<span class="comment-copy">Is the file line based?</span>
<span class="comment-copy">@PedroLobito: no, unfortunately the file is not line based.</span>
<span class="comment-copy">Good low-memory footprint solution. The file is not line based as the presented example suggests. I hadn't specified the requirement unambiguously.  I had to explicitly specify that the file must be read in chunks. In some way I have to find a solution at the chunk boundary, while avoiding accidental double-counts.</span>
<span class="comment-copy">This is a very clever approach, closest to what I want. I hadn't thought about it like this. The line-based reading will however form a new challenge on multi-processing chunks. That's why I would prefer the f.read() method and feeding the chunks to separate processes. Line-by-line synchronization will be very costly interprocess operations.</span>
<span class="comment-copy">@JodyK thanks for your comment, you are right, I've updated answer with an alternative approach</span>
<span class="comment-copy">This is indeed a great solution for line-based files. Is there also a solution where the file is not line based and where you 'must' read chunks?</span>
<span class="comment-copy">I agree that these are good solutions for line-based files. Assuming that we have a strict condition where we 'have to' read the file in chunks: is there a possible solution to get around the chunk boundary issue?</span>
<span class="comment-copy">These examples were based on your example. But for each iteration could you keep the last few characters from the previous chunk where the pattern could appeared ?</span>
<span class="comment-copy">I hadn't been clear in the chunk requirement. Your proposal comes close to Andriy's approach. I guess that is the closest way to solve this. I am afraid it is not possible to do a kind of look-ahead in the succeeding chunk or a look-behind in the preceding chunk. Line-by-line approaches take away the multi-processing benefits which one would have with large chunks.</span>
