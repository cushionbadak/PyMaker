<div class="post-text" itemprop="text">
<p>Please could I solicit some general advice regarding Python lists.  I know I shouldn't answer 'open' questions on here but I am worried about setting off on completely the wrong path.</p>
<p>My problem is that I have .csv files that are approximately 600,000 lines long each.  Each row of the .csv has 6 fields, of which the first field is a date-time stamp in the format DD/MM/YYYY HH:MM:SS.  The next two fields are blank and the last three fields contain float and integer values, so for example:</p>
<pre class="lang-none prettyprint-override"><code>23/05/2017 16:42:17,  ,   , 1.25545, 1.74733, 12 
23/05/2017 16:42:20,  ,   , 1.93741, 1.52387, 14 
23/05/2017 16:42:23,  ,   , 1.54875, 1.46258, 11
</code></pre>
<p>etc</p>
<p>No two values in column 1 (date-time stamp) will ever be the same.</p>
<p>I need to write a program that will do a few basic operations with the data, such as:</p>
<ol>
<li>read all of the data into a dictionary, list, set (?) etc as  appropriate.</li>
<li>search through the date time stamp column for a particular value.</li>
<li>read through the list and do basic calculations on the floats in columns 4 and 5. </li>
<li>write a new list based on the searches/calculations.</li>
</ol>
<p>My question is - how should I 'handle' the data and am I likely to run into problems due to the length of the dataset?</p>
<p>For example, should I import all of the data into a list, and each element of the list is a sublist of each rows data?  E.g:</p>
<p><code>[[23/05/2017 16:42:17,'','', 1.25545, 1.74733, 12],[23/05/2017 16:42:20,'','', 1.93741, 1.52387, 14], ...]</code></p>
<p>Or would it be better to make each date-time stamp the 'key' in a dictionary and make the dictionary 'value' a list with all the other values, e.g:</p>
<p><code>{'23/05/2017 16:42:17': [ , , 1.25545, 1.74733, 12], ...}</code>
etc</p>
<p>If I use the list approach, is there a way to get Python to 'search' in only the first column for a particular time stamp rather than making it search through 600,000 rows times 6 columns when we know that only the first column contains timestamps?</p>
<p>I apologize if my query is a little vague, but would appreciate any guidance that anyone can offer.</p>
</div>
<div class="post-text" itemprop="text">
<p>600000 lines aren't that many, your script should run fine with either a list or a dict.</p>
<p>As a test, let's use:</p>
<pre><code>data = [["2017-05-02 17:28:24", 0.85260, 1.16218, 7],
["2017-05-04 05:40:07", 0.72118, 0.47710, 15],
["2017-05-07 19:27:53", 1.79476, 0.47496, 14],
["2017-05-09 01:57:10", 0.44123, 0.13711, 16],
["2017-05-11 07:22:57", 0.17481, 0.69468, 0],
["2017-05-12 10:11:01", 0.27553, 0.47834, 4],
["2017-05-15 05:20:36", 0.01719, 0.51249, 7],
["2017-05-17 14:01:13", 0.35977, 0.50052, 7],
["2017-05-17 22:05:33", 1.68628, 1.90881, 13],
["2017-05-18 14:44:14", 0.32217, 0.96715, 14],
["2017-05-18 20:24:23", 0.90819, 0.36773, 5],
["2017-05-21 12:15:20", 0.49456, 1.12508, 5],
["2017-05-22 07:46:18", 0.59015, 1.04352, 6],
["2017-05-26 01:49:38", 0.44455, 0.26669, 13],
["2017-05-26 18:55:24", 1.33678, 1.24181, 7]]
</code></pre>
<h1>dict</h1>
<p>If you're looking for exact timestamps, a lookup will be much faster with a dict than with a list. You have to know exactly what you're looking for though: <code>"23/05/2017 16:42:17"</code> has a completely different hash than <code>"23/05/2017 16:42:18"</code>.</p>
<pre><code>data_as_dict = {l[0]: l[1:] for l in data}
print(data_as_dict)
# {'2017-05-21 12:15:20': [0.49456, 1.12508, 5], '2017-05-18 14:44:14': [0.32217, 0.96715, 14], '2017-05-04 05:40:07': [0.72118, 0.4771, 15], '2017-05-26 01:49:38': [0.44455, 0.26669, 13], '2017-05-17 14:01:13': [0.35977, 0.50052, 7], '2017-05-15 05:20:36': [0.01719, 0.51249, 7], '2017-05-26 18:55:24': [1.33678, 1.24181, 7], '2017-05-07 19:27:53': [1.79476, 0.47496, 14], '2017-05-17 22:05:33': [1.68628, 1.90881, 13], '2017-05-02 17:28:24': [0.8526, 1.16218, 7], '2017-05-22 07:46:18': [0.59015, 1.04352, 6], '2017-05-11 07:22:57': [0.17481, 0.69468, 0], '2017-05-18 20:24:23': [0.90819, 0.36773, 5], '2017-05-12 10:11:01': [0.27553, 0.47834, 4], '2017-05-09 01:57:10': [0.44123, 0.13711, 16]}

print(data_as_dict.get('2017-05-17 14:01:13'))
# [0.35977, 0.50052, 7]

print(data_as_dict.get('2017-05-17 14:01:10'))
# None
</code></pre>
<p>Note that your <code>DD/MM/YYYY HH:MM:SS</code> format isn't very convenient : sorting the cells lexicographically won't sort them by datetime. You'd need to use <a href="https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime" rel="nofollow noreferrer"><code>datetime.strptime()</code></a> first:</p>
<pre><code>from datetime import datetime
data_as_dict = {datetime.strptime(l[0], '%Y-%m-%d %H:%M:%S'): l[1:] for l in data}    
print(data_as_dict.get(datetime(2017,5,17,14,1,13)))
# [0.35977, 0.50052, 7]

print(data_as_dict.get(datetime(2017,5,17,14,1,10)))
# None
</code></pre>
<h1>list with binary search</h1>
<p>If you're looking for timestamps ranges, a dict won't help you much. A binary search (e.g. with <a href="https://docs.python.org/3/library/bisect.html" rel="nofollow noreferrer"><code>bisect</code></a>) on a list of timestamps should be very fast.</p>
<pre><code>import bisect
timestamps = [datetime.strptime(l[0], '%Y-%m-%d %H:%M:%S') for l in data]
i = bisect.bisect(timestamps, datetime(2017,5,17,14,1,10))
print(data[i-1])
# ['2017-05-15 05:20:36', 0.01719, 0.51249, 7]
print(data[i])
# ['2017-05-17 14:01:13', 0.35977, 0.50052, 7]
</code></pre>
<h1>Database</h1>
<p>Before reinventing the wheel, you might want to dump all your CSVs into a small database (sqlite, Postgresql, ...) and use the corresponding queries.</p>
<h1>Pandas</h1>
<p>If you don't want the added complexity of a database but are ready to invest some time learning a new syntax, you should use <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html" rel="nofollow noreferrer"><code>pandas.DataFrame</code></a>. It does exactly what you want, and then some.</p>
</div>
<span class="comment-copy">For searching, dicts scale O(log(n)), lists O(n) (except when the entrys are ordered, than you can make it O(log(n)) too). Is that an argument for you? And you might be able to use a shelve to minimize the required memory if you scale up.</span>
<span class="comment-copy">When you say "write a new list based on the searches/calculations." do you mean to write it to the file? Basically, are you trying to edit a row in the file?</span>
<span class="comment-copy">It sounds like you should definitely go with the dictionary option. You might want to take a look at the <code>csv</code> python module that can help you read the data to a dict easily.</span>
<span class="comment-copy">Are your timestamps exact or are you searching for a 'closest' timestamp? Is your CSV file ordered by timestamp? If you are looking for <i>exact</i> timestamps, then finding a key in a dictionary takes constant time (O(1)), there is no scanning needed. If you are not looking for an exact match but your data is ordered, use a list and use bisection (see the <code>bisect</code> module).</span>
<span class="comment-copy">I think the pandas library is worth a try here.</span>
<span class="comment-copy">Thank you @Eric Duminil for such a comprehensive and helpful response - this definitely gives me something to get started with.  Per my comments above, the 'csv data' is always in chronological order and typically has a data point every 3 seconds, but not always.  I will be searching for a specific time (down to the second), but since the datastamps are only every ~3 seconds it's possible that the particular stamp Im looking for doesn't exist in the raw data, in which case I would settle for finding a datapoint thats "nearby".  Thank you so much again for taking time to help me!</span>
