<div class="post-text" itemprop="text">
<p>I'm trying to run some tests for upload speeds on AWS S3 for very large files (500GB-5TB). I'm currently using <code>boto3</code>, the AWS SDK for Python. Rather than creating and storing massive files on my own hard drive, I'd prefer to stream directly from <code>/dev/urandom</code> (or at least <code>/dev/zero</code>). <code>boto3</code>'s <em>put_object()</em> can upload data from a stream, but it seems to have a hard limit of 5GB, which is far less than I need to test. </p>
<p>I tried <code>boto3</code>'s <em>upload_fileobj()</em>, which handles larger objects by using multipart uploads automatically. It works just fine on actual files, but I can't seem to figure out a way to get it to upload data directly from a stream. I also looked at using the AWS S3 Command Line Interface (CLI) instead of the <code>boto3</code> SDK, but again couldn't figure out a way to upload data directly from a stream.</p>
<p>Is there a comparatively easy way to upload a large amount of data to AWS S3 directly from <code>/dev/urandom</code>?</p>
</div>
<div class="post-text" itemprop="text">
<p>You don't want to stream directly from <code>/dev/urandom</code>, because it is actually CPU-limited rather than IO-limited (you can see this by running <code>top</code> while using <code>dd</code> to stream random data into a file, or by comparing times to copy an existing 1GB file that's not already in disk cache).</p>
<p>Using Boto3, the calls you want are <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_multipart_upload" rel="nofollow noreferrer">create_multipart_upload</a> to initiate the upload, <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_part" rel="nofollow noreferrer">upload_part</a> to send each part, and <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.complete_multipart_upload" rel="nofollow noreferrer">complete_multipart_upload</a> to finish the upload. You can either pass a file or a byte array to <code>upload_part</code>, so you can either generate a byte array using the built-in random number generator (which will be sufficiently random to avoid GZip compression), or repeatedly read the same file (in similar tests I use a 1GB file containing data from urandom -- Gzip isn't going to give you any compression over that large an input space).</p>
<p>However, the entire exercise is pointless. Unless you have a gigabit pipe directly into the Internet backbone, AWS is going to be faster than your network. So all you're really testing is how fast your network can push bytes into the Internet, and there are a bunch of "speed test" sites that will tell you that throughput. Plus, you won't learn much more sending 1 TB than sending 1 GB: the entire point of S3 is that it can handle <em>anything</em>.</p>
</div>
<span class="comment-copy">The SDK should never use Gzip, because S3 does not decode a gzipped upload -- if you set <code>Content-Encoding: gzip</code> when uploading an object to S3, S3 assumes you want it to <i>serve</i> the Gzipped object, still Gzipped, with that same <code>Content-Encoding</code> header. It stores the actual bytes you sent and completely assumes you know what you're doing -- it doesn't even verify whether what you stored is a valid decodable gzip stream.</span>
<span class="comment-copy">@Michael-sqlbot - OK; based on the docs, I was assuming that the object's content type was specified in the InitiateMultipartUpload call, but UploadPart was free to use it to specify transport-level compression. I'll remove that from the answer.</span>
<span class="comment-copy">@guest Thanks for the suggestions! For purposes of this question, I do in fact have access to a massive NAS with ridiculously fast CPU and a 10-gigabit fibre channel to the Internet. Also, when testing with files 5G and under, larger files definitely had a lower average transfer speed (for example, a smaller file might be 300MB/sec, while a larger file might fall to 50MB/sec). Does any of this affect your opinion at all?</span>
<span class="comment-copy">@B.Shefter - that was a rather important thing to leave out from your original question. And it changes my answer in that you should be talking to your IT support team. It's entirely possible that they have restrictions to limit high-bandwidth connections to approved uses (at that volume Internet isn't free). And if not, then they have the tools to help you find the slowdown. It might be something that AWS does, but I'd exhaust options on your end first.</span>
