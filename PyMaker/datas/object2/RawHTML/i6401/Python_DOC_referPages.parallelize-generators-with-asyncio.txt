<div class="post-text" itemprop="text">
<p>My application reads data from a slow i/o source, does some processing and then writes it to a local file. I've implemented this with generators like so:</p>
<pre><code>import time

def io_task(x):
    print("requesting data for input %s" % x)
    time.sleep(1)   # this simulates a blocking I/O task
    return 2*x

def producer(xs):
    for x in xs:
        yield io_task(x)

def consumer(xs):
    with open('output.txt', 'w') as fp:
        for x in xs:
            print("writing %s" % x)
            fp.write(str(x) + '\n')

data = [1,2,3,4,5]
consumer(producer(data))
</code></pre>
<p>Now I'd like to parallelize this task with the help of asyncio, but I can't seem to figure out how. The main issue for me is to directly feed data through a generator from the producer to the consumer while letting asyncio make multiple parallel requests to <code>io_task(x)</code>. Also, this whole <code>async def</code> vs. <code>@asyncio.coroutine</code> thing is confusing me.</p>
<p>Can someone show me how to build a minimal working example that uses <code>asyncio</code> from this sample code?</p>
<p>(Note: It is <em>not</em> ok to just make calls to <code>io_task()</code>, buffer the results and then write them to a file. I need a solution that works on large data sets that can exceed the main memory, that's why I've been using generators so far. It is however safe to assume that the consumer is always faster than all producers combined)</p>
</div>
<div class="post-text" itemprop="text">
<p>Since python 3.6 and <a href="https://www.python.org/dev/peps/pep-0525/" rel="nofollow noreferrer">asynchronous generators</a>, very few changes need be applied to make your code compatible with asyncio.</p>
<p>The <code>io_task</code> function becomes a coroutine:</p>
<pre><code>async def io_task(x):
    await asyncio.sleep(1)
    return 2*x
</code></pre>
<p>The <code>producer</code> generator becomes an asynchronous generator:</p>
<pre><code>async def producer(xs):
    for x in xs:
        yield await io_task(x)
</code></pre>
<p>The <code>consumer</code> function becomes a coroutine and uses <a href="https://pypi.python.org/pypi/aiofiles" rel="nofollow noreferrer">aiofiles</a>, asynchronous context management and asynchronous iteration:</p>
<pre><code>async def consumer(xs):
    async with aiofiles.open('output.txt', 'w') as fp:
        async for x in xs:
            await fp.write(str(x) + '\n')
</code></pre>
<p>And the main coroutine runs in an event loop:</p>
<pre><code>data = [1,2,3,4,5]
main = consumer(producer(data))
loop = asyncio.get_event_loop()
loop.run_until_complete(main)
loop.close()
</code></pre>
<p>Also, you may consider using <a href="https://github.com/vxgmichel/aiostream" rel="nofollow noreferrer">aiostream</a> to pipeline some processing operations between the producer and the consumer.</p>
<hr/>
<p>EDIT: The different I/O tasks can easily be run concurrently on the producer side by using <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.as_completed" rel="nofollow noreferrer">as_completed</a>:</p>
<pre><code>async def producer(xs):
    coros = [io_task(x) for x in xs]
    for future in asyncio.as_completed(coros):
        yield await future
</code></pre>
</div>
<span class="comment-copy">this is awesome, now I definitely have to upgrade to python 3.6. thanks :)</span>
<span class="comment-copy">@Klamann Yes, your example brought most of the cool asynchronous features of python 3.5 and 3.6. It almost looks like a tutorial ;)</span>
<span class="comment-copy">I've tried your code now, but it doesn't seem to run asynchronously. When I add print statements before <code>await asyncio.sleep</code> and <code>await fp.write</code>, all of them are executed sequentially and the total run time of the program is 5 seconds. Is this an issue on my platform or do you see the same behaviour?</span>
<span class="comment-copy">@Klamann Yes, that's the expected behavior. The point of converting this program to asyncio is that you could potentially run other tasks at the same time. For instance, you could have another producer/consumer pair working on another dataset. If you also wish to separate the producer from the consumer to run them concurrently, you could use a <a href="https://asyncio.readthedocs.io/en/latest/producer_consumer.html" rel="nofollow noreferrer">queue</a>. However, the total run time will still be 5 seconds since the producer itself is sequential. Hope that helps.</span>
<span class="comment-copy">Unfortunately, that doesn't solve my problem at all. I know that asyncio can be used to parallelize i/o-bound tasks, e.g. using <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.gather" rel="nofollow noreferrer"><code>asyncio.gather</code></a>, but then again, I have no idea how to include this in my producer/consumer setup or what other ways there are to make this work.</span>
