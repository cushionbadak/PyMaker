<div class="post-text" itemprop="text">
<p>I have a problem that's easy enough to do in an ugly way, but I'm wondering if there's a more Pythonic way of doing it.</p>
<p>Say I have three lists, <code>A</code>, <code>B</code> and <code>C</code>.</p>
<pre><code>A = [1, 1, 2, 3, 4, 4, 5, 5, 3]
B = [1, 2, 3, 4, 5, 6, 7, 8, 9]
C = [1, 2, 3, 4, 5, 6, 7, 8, 9]
# The actual data isn't important.
</code></pre>
<p>I need to remove all duplicates from list <code>A</code>, but when a duplicate entry is deleted, I would like the corresponding indexes removed from <code>B</code> and <code>C</code>:</p>
<pre><code>A = [1, 2, 3, 4, 5]
B = [1, 3, 4, 5, 7]
C = [1, 3, 4, 5, 7]
</code></pre>
<p>This is easy enough to do with longer code by moving everything to new lists:</p>
<pre><code>new_A = []
new_B = []
new_C = []
for i in range(len(A)):
  if A[i] not in new_A:
    new_A.append(A[i])
    new_B.append(B[i])
    new_C.append(C[i])
</code></pre>
<p>But is there a more elegant and efficient (and less repetitive) way of doing this? This could get cumbersome if the number of lists grows, which it might.</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://docs.python.org/3/library/functions.html#zip" rel="nofollow">Zip</a> the three lists together, uniquify based on the first element, then unzip:</p>
<pre><code>from operator import itemgetter
from more_itertools import unique_everseen

abc = zip(a, b, c)
abc_unique = unique_everseen(abc, key=itemgetter(0))
a, b, c = zip(*abc_unique)
</code></pre>
<p>This is a very common pattern. Whenever you want to do anything in lock step over a bunch of lists (or other iterables), you zip them together and loop over the result.</p>
<p>Also, if you go from 3 lists to 42 of them ("This could get cumbersome if the number of lists grows, which it might."), this is trivial to extend:</p>
<pre><code>abc = zip(*list_of_lists)
abc_unique = unique_everseen(abc, key=itemgetter(0))
list_of_lists = zip(*abc_unique)
</code></pre>
<hr/>
<p>Once you get the hang of <code>zip</code>, the "uniquify" is the only hard part, so let me explain it.</p>
<p>Your existing code checks whether each element has been seen by searching for each one in <code>new_A</code>. Since <code>new_A</code> is a list, this means that if you have N elements, M of them unique, on average you're going to be doing M/2 comparisons for each of those N elements. Plug in some big numbers, and NM/2 gets pretty big—e.g., 1 million values, a half of them unique, and you're doing 250 billion comparisons.</p>
<p>To avoid that quadratic time, you use a <code>set</code>. A <code>set</code> can test an element for membership in constant, rather than linear, time. So, instead of 250 billion comparisons, that's 1 million hash lookups.</p>
<p>If you don't need to maintain order or decorate-process-undecorate the values, just copy the list to a <code>set</code> and you're done. If you need to decorate, you can use a <code>dict</code> instead of a set (with the key as the <code>dict</code> keys, and everything else hidden in the values). To preserve order, you could use an <code>OrderedDict</code>, but at that point it's easier to just use a <code>list</code> and a <code>set</code> side by side. For example, the smallest change to your code that works is:</p>
<pre><code>new_A_set = set()
new_A = []
new_B = []
new_C = []
for i in range(len(A)):
    if A[i] not in new_A_set:
        new_A_set.add(A[i])
        new_A.append(A[i])
        new_B.append(B[i])
        new_C.append(C[i])
</code></pre>
<p>But this can be generalized—and should be, especially if you're planning to expand from 3 lists to a whole lot of them.</p>
<p>The <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow">recipes in the <code>itertools</code> documentation</a> include a function called <code>unique_everseen</code> that generalizes exactly what we want. You can copy and paste it into your code, write a simplified version yourself, or <code>pip install more-itertools</code> and use someone else's implementation (as I did above).</p>
<hr/>
<p>PadraicCunningham asks:</p>
<blockquote>
<p>how efficient is <code>zip(*unique_everseen(zip(a, b, c), key=itemgetter(0)))</code>?</p>
</blockquote>
<p>If there are N elements, M unique, it's O(N) time and O(M) space.</p>
<p>In fact, it's effectively doing the same work as the 10-line version above. In both cases, the only work that's not obviously trivial inside the loop is <code>key in seen</code> and <code>seen.add(key)</code>, and since both operations are amortized constant time for <code>set</code>, that means the whole thing is O(N) time. In practice, for N=<code>1000000, M=100000</code> the two versions are about 278ms and 297ms (I forget which is which) compared to minutes for the quadratic version. You could probably micro-optimize that down to 250ms or so—but it's hard to imagine a case where you'd need that, but wouldn't benefit from running it in PyPy instead of CPython, or writing it in Cython or C, or numpy-izing it, or getting a faster computer, or parallelizing it.</p>
<p>As for space, the explicit version makes it pretty obvious. Like any conceivable non-mutating algorithm, we've got the three <code>new_Foo</code> lists around at the same time as the original lists, and we've also added <code>new_A_set</code> of the same size. Since all of those are length <code>M</code>, that's 4M space. We could cut that in half by doing one pass to get indices, then doing the same thing mu 無's answer does:</p>
<pre><code>indices = set(zip(*unique_everseen(enumerate(a), key=itemgetter(1))[0])
a = [a[index] for index in indices]
b = [b[index] for index in indices]
c = [c[index] for index in indices]
</code></pre>
<p>But there's no way to go lower than that; you have to have at least a set and a list of length <code>M</code> alive to uniquify a list of length <code>N</code> in linear time.</p>
<p>If you really need to save space, you can mutate all three lists in-place. But this is a lot more complicated, and a bit slower (although still linear*).</p>
<p>Also, it's worth noting another advantage of the <code>zip</code> version: it works on any iterables. You can feed it three lazy iterators, and it won't have to instantiate them eagerly. I don't think it's doable in 2M space, but it's not too hard in 3M:</p>
<pre><code>indices, a = zip(*unique_everseen(enumerate(a), key=itemgetter(1))
indices = set(indices)
b = [value for index, value in enumerate(b) if index in indices]
c = [value for index, value in enumerate(c) if index in indices]
</code></pre>
<hr/>
<p>* Note that just <code>del c[i]</code> will make it quadratic, because deleting from the middle of a list takes linear time. Fortunately, that linear time is a giant memmove that's orders of magnitude faster than the equivalent number of Python assignments, so if <code>N</code> isn't <em>too</em> big you can get away with it—in fact, at <code>N=100000, M=10000</code> it's twice as fast as the immutable version… But if <code>N</code> might be too big, you have to instead replace each duplicate element with a sentinel, then loop over the list in a second pass so you can shift each element only once, which is instead 50% slower than the immutable version.</p>
</div>
<div class="post-text" itemprop="text">
<p>How about this - basically get a set of all unique elements of A, and then get their indices, and create a new list based on these indices.</p>
<pre><code>new_A = list(set(A))
indices_to_copy = [A.index(element) for element in new_A]
new_B = [B[index] for index in indices_to_copy]
new_C = [C[index] for index in indices_to_copy]
</code></pre>
<p>You can write a function for the second statement, for reuse:</p>
<pre><code>def get_new_list(original_list, indices):
    return [original_list[idx] for idx in indices]
</code></pre>
</div>
<span class="comment-copy">In this specific form, I would say to go ahead with how you are doing it. There may be an underlying pattern to the problem that you are describing, but as described I don't see any.</span>
<span class="comment-copy">This will get slow if the lists are huge, because you're checking each element in A against each of the unique elements seen so far—if A is 10000 elements long, that 10000*10000/2 comparisons. Keeping the values seen so far in a set will solve that—instead of 10000*10000/2 comparisons, it's just 10000 hash lookups.</span>
<span class="comment-copy">Also, you probably want <code>for i, a in enumerate(A):</code> instead of <code>for i in range(len(A))</code> and then <code>A[i]</code> within the loop. But other than those two things, there's really nothing wrong with what you're doing, it's just that there are ways to make it simpler.</span>
<span class="comment-copy">are your lists sorted?</span>
<span class="comment-copy">@PadraicCunningham no, not necessarily within themselves, but the indices between lists correspond to each other.</span>
<span class="comment-copy">This is a clever solution, but it looks like it would be more code than what I have now. I'd have to time-test the two methods side by side.</span>
<span class="comment-copy">@TheSoundDefense I'm pretty sure for a large number of lists, for a large number of elements, my solution will be more optimal. Also, not to be pedantic but its 4 lines shorter than the current solution you have :P</span>
<span class="comment-copy">This is still quadratic, because <code>A.index(element)</code> is going to take linear time for each element. You have to remember the original indices, not look them up again. For example, you could do <code>indices = {value: index for index, value in enumerate(A)}</code>, then <code>new_A = list(indices)</code>, then <code>new_b = [B[index] for index in indices.values()]</code>, and that's linear.</span>
<span class="comment-copy">Also, his original version guarantees to keep the first copy of each duplicated element. I have no idea whether that's important, but it might not be a good idea to break that without at least explaining that it's being broken.</span>
<span class="comment-copy">@abarnert Thanks for your insights :) Really appreciate them, especially the quadratic running time.</span>
