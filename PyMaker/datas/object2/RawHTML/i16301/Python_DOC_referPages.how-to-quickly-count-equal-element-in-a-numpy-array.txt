<div class="post-text" itemprop="text">
<p>I have a python matrix</p>
<pre><code>leafs = np.array([[1,2,3],[1,2,4],[2,3,4],[4,2,1]])
</code></pre>
<p>I would like to compute for each couple of rows the number of time they have the same element. </p>
<p>In this case I would get a 4x4 matrix proximity </p>
<pre><code>proximity = array([[3, 2, 0, 1],
                   [2, 3, 1, 1],
                   [0, 1, 3, 0],
                   [1, 1, 0, 3]])
</code></pre>
<p>This is the code that I am currently using. </p>
<pre><code>proximity = []

for i in range(n):
 print(i)
 proximity.append(np.apply_along_axis(lambda x: sum(x==leafs[i, :]), axis=1,
                                      arr=leafs))
</code></pre>
<p>I need a faster solution</p>
<p>EDIT: 
The accepted solution does not work in this example </p>
<pre><code>    &gt;&gt;&gt; type(f.leafs)
&lt;class 'numpy.ndarray'&gt;
&gt;&gt;&gt; f.leafs.shape
(7210, 1000)
&gt;&gt;&gt; f.leafs.dtype
dtype('int64')

&gt;&gt;&gt; f.leafs.reshape(7210, 1, 1000) == f.leafs.reshape(1, 7210, 1000)
False
&gt;&gt;&gt; f.leafs
array([[ 19,  32,  16, ..., 143, 194, 157],
       [ 19,  32,  16, ..., 143, 194, 157],
       [ 19,  32,  16, ..., 143, 194, 157],
       ..., 
       [139,  32,  16, ...,   5, 194, 157],
       [170,  32,  16, ...,   5, 194, 157],
       [170,  32,  16, ...,   5, 194, 157]])
&gt;&gt;&gt; 
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Warren Weckesser offered a very beautiful solution using broadcasting. However, even a straightforward approach using a loop can have comparable performance. <code>np.apply_along_axis</code> is slow in your initial solution because it does not take advantage of vectorization. However the following fixes it:</p>
<pre><code>def proximity_1(leafs):
    n = len(leafs)
    proximity = np.zeros((n,n))
    for i in range(n):
        proximity[i] = (leafs == leafs[i]).sum(1)  
    return proximity
</code></pre>
<p>You could also use a list comprehension to make the above code more concise. The difference is that <code>np.apply_along_axis</code> would loop through all the rows in a non-optimized manner, while <code>leafs == leafs[i]</code> will take advantage of <code>numpy</code> speed. </p>
<p>The solution from Warren Weckesser truly shows <code>numpy</code>'s beauty. However, it includes the overhead of creating an intermediate 3-d array of size <code>nrows*nrows*ncols</code>. So if you have large data, the simple loop might be more efficient.</p>
<p>Here's an example. Below is code offered by Warren Weckesser, wrapped in a function. (I don't know what are the code copyright rules here, so I assume this reference is enough <code>:)</code>)</p>
<pre><code>def proximity_2(leafs):
    nrows, ncols = leafs.shape    
    eq = leafs.reshape(nrows,1,ncols) == leafs.reshape(1,nrows,ncols)
    proximity = eq.sum(axis=-1)  
    return proximity
</code></pre>
<p>Now let's evaluate the performance on an array of random integers of size 10000 x 100.</p>
<pre><code>leafs = np.random.randint(1,100,(10000,100))
time proximity_1(leafs)
&gt;&gt; 28.6 s
time proximity_2(leafs) 
&gt;&gt; 35.4 s 
</code></pre>
<p>I ran both examples in an <code>IPython</code> environment on the same machine. </p>
</div>
<div class="post-text" itemprop="text">
<p>Here's one way, using broadcasting.  Be warned: the temporary array <code>eq</code> has shape <code>(nrows, nrows, ncols)</code>, so if <code>nrows</code> is 4000 and <code>ncols</code> is 1000, <code>eq</code> will require 16GB of memory.</p>
<pre><code>In [38]: leafs
Out[38]: 
array([[1, 2, 3],
       [1, 2, 4],
       [2, 3, 4],
       [4, 2, 1]])

In [39]: nrows, ncols = leafs.shape

In [40]: eq = leafs.reshape(nrows,1,ncols) == leafs.reshape(1,nrows,ncols)

In [41]: proximity = eq.sum(axis=-1)

In [42]: proximity
Out[42]: 
array([[3, 2, 0, 1],
       [2, 3, 1, 1],
       [0, 1, 3, 0],
       [1, 1, 0, 3]])
</code></pre>
<p>Also note that this solution is inefficient: <code>proximity</code> is symmetric, and the diagonal is always equal to <code>ncols</code>, but this solution computes the full array, so it does more than twice as much work as necessary. </p>
</div>
<span class="comment-copy">Itertools (<a href="https://docs.python.org/2/library/itertools.html" rel="nofollow noreferrer">python2</a> | <a href="https://docs.python.org/3/library/itertools.html" rel="nofollow noreferrer">python3</a>) should have something..? Try investigating this answer for <a href="http://stackoverflow.com/a/21960058/1762224">Generate all unique permutations of 2d array</a></span>
<span class="comment-copy">Can you be a bit more precise about what your output should be?  Maybe I'm tired but for 'each couple of rows' I'm not seeing how a 4x3 maps to a 4x4</span>
<span class="comment-copy">The final matrix is squared and has the same number of rows of the initial one. Each row of the matrix is compared with all the others. `similarity[i,j] = sum(leafs[i,k] == leafs[j,k] for k in range(len(leafs[i,:]))'</span>
<span class="comment-copy">What are the elements on the array? Integers, strings, floats?</span>
<span class="comment-copy">The elements are integers.</span>
<span class="comment-copy">This is a fantastic solution and it is very very fast! Can you explain command 40 and 41 .  Thanks!</span>
<span class="comment-copy">@Donbeo, 4x1x3 against 1x4x3 =&gt; 4x4x3. <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" rel="nofollow noreferrer">doc</a></span>
<span class="comment-copy">@Warren Weckesser There is a case in which your solution does not work. I have edited the question. This can probably be due to an excessive use of memory</span>
<span class="comment-copy">As I noted in my answer, I expected memory would be an issue if the the dimensions of <code>leafs</code> were large.  Broadcasting+reduction is handy, but the the excessive memory use of intermediate results can be an issue.  @ojy's answer is a reasonable way to break up the calculation into small steps.</span>
<span class="comment-copy">I have edited the question. I am just curious to know why this is not working. The ojy answer is probably better in my situation</span>
