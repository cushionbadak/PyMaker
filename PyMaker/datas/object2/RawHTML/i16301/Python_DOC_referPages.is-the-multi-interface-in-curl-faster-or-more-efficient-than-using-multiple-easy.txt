<div class="post-text" itemprop="text">
<p>I am making something which involves pycurl since pycurl depends on libcurl, I was reading through its documentation and came across this Multi interface where you could perform several transfers using a single multi object. I was wondering if this is faster/more memory efficient than having miltiple easy interfaces ? I was wondering what is the advantage with this approach since the site barely says,</p>
<p>"Enable multiple simultaneous transfers in the same thread without making it complicated for the application."</p>
</div>
<div class="post-text" itemprop="text">
<p>You are trying to optimize something that doesn't matter at all.</p>
<p>If you want to download 200 URLs as fast as possible, you are going to spend 99.99% of your time waiting for those 200 requests, limited by your network and/or the server(s) you're downloading from. The key to optimizing that is to make the right number of concurrent requests. Anything you can do to cut down the last 0.01% will have no visible effect on your program. (See <a href="http://en.wikipedia.org/wiki/Amdahl%27s_Law" rel="nofollow">Amdahl's Law</a>.)</p>
<p>Different sources give different guidelines, but typically it's somewhere between 6-12 requests, no more than 2-4 to the same server. Since you're pulling them all from Google, I'd suggest starting 4 concurrent requests, then, if that's not fast enough, tweaking that number until you get the best results.</p>
<p>As for space, the cost of storing 200 pages is going to far outstrip the cost of a few dozen bytes here and there for overhead. Again, what you want to optimize is those 200 pages—by storing them to disk instead of in memory, by parsing them as they come in instead of downloading everything and then parsing everything, etc.</p>
<p>Anyway, instead of looking at what command-line tools you have and trying to find a library that's similar to those, look for libraries directly. <code>pycurl</code> can be useful in some cases, e.g., when you're trying to do something complicated and you already know how to do it with <code>libcurl</code>, but in general, it's going to be a lot easier to use either stdlib modules like <code>urllib</code> or third-party modules designed to be as simple as possible like <code>requests</code>.</p>
<p>The <a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example" rel="nofollow">main example for <code>ThreadPoolExecutor</code></a> in the docs shows how to do exactly what you want to do. (If you're using Python 2.x, you'll have to <code>pip install futures</code> to get the backport for <code>ThreadPoolExecutor</code>, and use <code>urllib2</code> instead of <code>urllib.request</code>, but otherwise the code will be identical.)</p>
</div>
<div class="post-text" itemprop="text">
<p>Having multiple easy interfaces running concurrently in the same thread means building your own reactor and driving curl at a lower level. That's painful in C, and just as painful in Python, which is why <code>libcurl</code> offers, and recommends, multi.</p>
<p>But that "in the same thread" is key here. You can also create a pool of threads and throw the easy instances into that. In C, that can still be painful; in Python, it's dead simple. In fact, the first example in the docs for using a <code>concurrent.futures.ThreadPoolExecutor</code> does something similar, but actually more complicated than you need here, and it's still just a few lines of code.</p>
<p>If you're comparing multi vs. easy with a manual reactor, the simplicity is the main benefit. In C, you could easily implement a more efficient reactor than the one <code>libcurl</code> uses; in Python, that may or may not be true. But in either language, the performance cost of switching among a handful of network requests is going to be so tiny compared to everything else you're doing—especially waiting for those network requests—that it's unlikely to ever matter.</p>
<p>If you're comparing multi vs. easy with a thread pool, then a reactor can definitely outperform threads (except on platforms where you can tie a thread pool to a proactor, as with Windows I/O completion ports), especially for huge numbers of concurrent connections. Also, each thread needs its own stack, which typically means about 1MB of memory pages allocated (although not all of them used), which can be a serious problem in 32-bit land for huge numbers of connections. That's why very few serious servers use threads for connections. But in a client making a handful of connections, none of this matters; again, the costs incurred by wasting 8 threads vs. using a reactor will be so small compared to the real costs of your program that they won't matter.</p>
</div>
<span class="comment-copy">Out of curiosity, is there a reason you're using <code>pycurl</code> instead of the stdlib, <code>requests</code>, or something else more Pythonic? There are sometimes good reasons for it, but most common is "I already know how to do this well in <code>libcurl</code>, and I'm expecting that will translate to <code>pycurl</code>", which doesn't seem to be the case here.</span>
<span class="comment-copy">@abarnert I am looking for the most efficient(Execution time wise or memory) solution. I only know Curl and Wget commands from unix. So I started looking ways I could use Curl in Python and I came across PyCurl. Since I only spent the last one hour on PyCurl and haven't yet written any code, I am ready to change to something else. Since PyCurl uses Libcurl which is written in C, I kinda assumed that it will be very efficent. I know it might not be true. The thing I am building is a word meaning lookup utility. Which "googles" the meanings of about 200 or so words. "</span>
<span class="comment-copy">Why are you looking for the most efficient solution? The limiting factor on time is going to be your network or the remote server; any time wasted in your server is irrelevant. The limiting factor on space is going to be holding 200 complete pages; any wasted dozen bytes here or there are irrelevant. This is <a href="http://en.wikipedia.org/wiki/Premature_optimization#When_to_optimize" rel="nofollow noreferrer">premature optimization</a> at its worst.</span>
<span class="comment-copy">Very well put sir. Reading your answer made me curious about the whole thing and I wanted to read more about how the limit 2-4 requests to the same server was established.</span>
<span class="comment-copy">This is not always the case. It all depends on the task. Having to scrape 1000 pages at the same time with a gigabit  (or 10 gigabit connection) is typical scenario for modern scrapers. But you cannot afford running  1000 workers in parallel. Curl multi does that with ease in a single thread!</span>
