<div class="post-text" itemprop="text">
<p>I built a scraper to run through a job site and save all potential job data to a <strong>csv</strong> file, then my MySQL database. For some reason, the scraper stops running after pulling jobs from the first city in the list. Here's what I mean: </p>
<h3>City List Code:</h3>
<pre><code>Cities = {
    'cities':[  'washingtondc',
                'newyork',
                'sanfrancisco',
                '...',
                '...']
            }
</code></pre>
<h3>Scrapy Spider Code:</h3>
<pre><code># -*- coding: utf-8 -*-
from city_list import Cities
import scrapy, os, csv, glob, pymysql.cursors

class JobsSpider(scrapy.Spider):
    name = 'jobs'
    c_list = Cities['cities']
    for c in c_list:
        print(f'Searching {c} for jobs...')
        allowed_domains = [f'{c}.jobsite.com']
        start_urls = [f'https://{c}.jobsite.com/search/jobs/']

        def parse(self, response):
            listings = response.xpath('//li[@class="listings-path"]')
            for listing in listings:
                date = listing.xpath('.//*[@class="date-path"]/@datetime').extract_first()
                link = listing.xpath('.//a[@class="link-path"]/@href').extract_first()
                text = listing.xpath('.//a[@class="text-path"]/text()').extract_first()

                yield scrapy.Request(link,
                                    callback=self.parse_listing,
                                    meta={'date': date,
                                        'link': link,
                                        'text': text})

            next_page_url = response.xpath('//a[text()="next-path "]/@href').extract_first()
            if next_page_url:
                yield scrapy.Request(response.urljoin(next_page_url), callback=self.parse)

        def parse_listing(self, response):
            date = response.meta['date']
            link = response.meta['link']
            text = response.meta['text']
            compensation = response.xpath('//*[@class="compensation-path"]/span[1]/b/text()').extract_first()
            employment_type = response.xpath('//*[@class="employment-type-path"]/span[2]/b/text()').extract_first()
            images = response.xpath('//*[@id="images-path"]//@src').extract()
            address = response.xpath('//*[@id="address-path"]/text()').extract()

            yield {'date': date,
                'link': link,
                'text': text,
                'compensation': compensation,
                'type': employment_type,
                'images': images,
                'address': address}

        def close(self, reason):
            csv_file = max(glob.iglob('*.csv'), key=os.path.getctime)


            conn = pymysql.connect(host='localhost',
                                user='root',
                                password='**********',
                                db='jobs_database',
                                charset='utf8mb4',
                                cursorclass=pymysql.cursors.DictCursor)

            cur = conn.cursor()
            csv_data = csv.reader(open('jobs.csv'))

            for row in csv_data: 
                cur.execute('INSERT INTO jobs_table(date, link, text, compensation, type, images, address)' 'VALUES(%s, %s, %s, %s, %s, %s, %s)', row)

            conn.commit()
            conn.close()
            print("Done Importing!")
</code></pre>
<p>The scraper works fine, but it stops running after grabbing jobs from washingtondc and exits. </p>
<p>How do I solve this problem? </p>
<p>UPDATE - 
I changed the code above to </p>
<pre><code>class JobsSpider(scrapy.Spider):
    name = 'jobs'
    allowed_domains = []
    start_urls = []

    def __init__(self, *args, **kwargs):
        super().__init__(self, *args, **kwargs)
        c_list = Cities['cities']
        for c in c_list:
            print(f'Searching {c} for jobs...')
            self.allowed_domains.append(f'{c}.jobsearch.com')
            self.start_urls.append(f'https://{c}.jobsearch.com/search/jobs/')


    def parse(self, response):
        ...
</code></pre>
<p>and am now getting "RecursionError: maximum recursion depth exceeded while calling a Python object"</p>
<p>Here's the traceback:</p>
<pre><code>Traceback (most recent call last):
  File "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py", line 1034, in emit
    msg = self.format(record)
  File "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py", line 880, in format
    return fmt.format(record)
  File "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py", line 619, in format
    record.message = record.getMessage()
  File "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py", line 380, in getMessage
    msg = msg % self.args
  File "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 107, in __str__
    return "&lt;%s %r at 0x%0x&gt;" % (type(self).__name__, self.name, id(self))
  File "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 107, in __str__
    return "&lt;%s %r at 0x%0x&gt;" % (type(self).__name__, self.name, id(self))
  File "/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 107, in __str__
    return "&lt;%s %r at 0x%0x&gt;" % (type(self).__name__, self.name, id(self))
  [Previous line repeated 479 more times]
RecursionError: maximum recursion depth exceeded while calling a Python object
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The first problem is that your spider variables and methods are inside the for loop. Instead, you need to set those member variables in <code>__init__()</code>. Without testing the rest of your logic, here's a rough idea of what you need to do instead:</p>
<pre><code>class JobsSpider(scrapy.Spider):
    name = 'jobs'
    # Don't do the for loop here.
    allowed_domains = []
    start_urls = []

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        c_list = Cities['cities']
        for c in c_list:
            self.allowed_domains.append(f'{c}.jobsite.com')
            self.start_urls.append(f'https://{c}.jobsite.com/search/jobs/')

    def parse(self, request):
        # ...
</code></pre>
<p>If you still have problems after this, update your question and I will try to update the answer.</p>
<hr/>
<p>To explain what's going wrong: When you have a for loop like in your question, it's going to end up overwriting the variables and functions. Here is an example directly in Python's shell:</p>
<pre><code>&gt;&gt;&gt; class SomeClass:
...     for i in range(3):
...         print(i)
...         value = i
...         def get_value(self):
...             print(self.value)
... 
0
1
2
&gt;&gt;&gt; x = SomeClass()
&gt;&gt;&gt; x.value
2
&gt;&gt;&gt; x.get_value()
2
</code></pre>
<p>Basically the for loop is executed before you even use the class. So this doesn't end up <em>running</em> the function multiple times, but rather <em>redefining</em> it multiple times. The end result is that your functions and variables point to whatever was set last.</p>
</div>
<span class="comment-copy">Thanks for that! It's going through them now, but I'm getting a new error that I believe is related to the dunder edits. I updated the question with those details.</span>
<span class="comment-copy">@Haytorade I noticed the mistake and fixed the answer. When you call <code>super().__init__()</code>, don't pass in <code>self</code>.</span>
<span class="comment-copy">Thanks @malberts! Worked like a charm.</span>
