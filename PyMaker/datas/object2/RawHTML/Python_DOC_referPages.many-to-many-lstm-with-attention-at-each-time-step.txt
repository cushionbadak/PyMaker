<div class="post-text" itemprop="text">
<p>I am working on time series image classification, where I need to output a classification at each time step (many to many). </p>
<p>My Tensorflow graph takes [Batch size, Time step, Image] and implements a deep CNN-LSTM, which currently goes to a time distributed dense layer before classification. </p>
<p>In my previous work, I've had a lot of success adding attention to better model time dependencies by weighting the hidden states of time steps. However, I cannot find any implementations that attempt to use attention in a many-to-many RNN. </p>
<p>I have tried with the following code, which compiles and runs but performs worse than the model without. The idea here is to learn attention weights at each step to weight every other time step based on the current time step. I have 7.8 million training samples so I am not worried that this is overfitting - in fact it is increasing the <em>training</em> error over the model without!</p>
<pre><code>def create_multi_attention(inputs, attention_size, time_major=False):
hidden_size = inputs.shape[2].value
print("Attention In: {}".format(inputs.shape))

w_omegas, b_omegas, u_omegas = [], [], []
for i in range(0, MAX_TIME_STEPS):
    w_omegas.append(create_weights([hidden_size, attention_size]))
    b_omegas.append(tf.Variable(tf.constant(0.05, shape = [attention_size])))
    u_omegas.append(create_weights([attention_size]))

# Trainable parameters
layers_all = []
for i in range(0, MAX_TIME_STEPS):  
    v = tf.tanh(tf.tensordot(inputs, w_omegas[i], axes=1) + b_omegas[i])       
    vu = tf.tensordot(v, u_omegas[i], axes=1, name='vu')
    alphas = tf.nn.softmax(vu, name='alphas')  

    output = tf.reshape(tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1), (-1, 1, hidden_size))        
    layers_all.append(output)

output = tf.concat(layers_all, axis = 1) #[Batch, Time steps, LSTM output size]
print("Attention Out: {}".format(output.shape))
return output
</code></pre>
<p>Would love any input or ideas or points to papers! I have thought about trying a seq2seq attention model, but this seems to be a bit of a stretch.</p>
</div>
<div class="post-text" itemprop="text">
<p>Seems that this code worked fine. The error was downstream. If anyone uses this code to implement many-to-many attention, note that it will take a very long time to train as you are learning two additional weight matrixes for each time step.</p>
</div>
