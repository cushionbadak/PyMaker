<div class="post-text" itemprop="text">
<p>In Python, I have a function that takes a list of URL's(url_list) and add headers. The list can have up to 25,000 urls so I wanted to try and make use multiprocessing. I tried the following code but I think it isn't truely doing multilprocessing because of the join. How can I make this truely mutliprocessing? </p>
<pre><code>def do_it(url_list, headers):
    for i in url_list:
        print "adding header to: \n" + i
        requests.post(i, headers=headers)
        print "done!"



  value = raw_input("Proceed? Enter [Y] for yes: ")
    if value == "Y":
        p = Process(target=do_it, args = (url_list, headers))
        p.start()
        p.join()
    else:
        print "Operation Failed!"
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You definitely don't want to create 25000 child processes that each handles 1 URL. But you also don't want 1 process that handles 25000 (which is what you've built). You probably want, say, 8 processes, each handling about 1/8th of 25000.</p>
<p>You could do this by creating a <code>Queue</code> full of URLs, creating 8 processes to loop around servicing that queue by pulling the next URL and doing the work, and then joining all 8 processes.</p>
<p>But what you'd be doing is building a process pool. And there's already one built in to <a href="http://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.pool" rel="nofollow"><code>multiprocessing</code></a>. On top of being already built and debugged, it also has features you probably wouldn't think of yourself. It can also pass results back, in various different ways. It lets you chunk up batches of a few URLs at a time (if you give each process a full 1/8th of the list, there's no load balancing; if you give each process 1 at a time, you're wasting time on inter-process communication that could be spent doing real work). And so on.</p>
<p>So, let's just use that:</p>
<pre><code>def do_it(url, headers):
    print "adding header to: \n" + i
    requests.post(i, headers=headers)
    print "done!"

pool = multiprocessing.Pool(max_workers=8)
results = pool.map(lambda url: do_it(url, headers), urls)
pool.join()
</code></pre>
<p>The only real problem with this code is that you're waiting for it to build up a list of 25000 results, which are all <code>None</code>. There are other ways you can wait on it without building up return results, but really, the cost of this list isn't worth the extra complexity to deal with it.</p>
<hr/>
<p>The reason I used a <code>lambda</code> there is that you need a function that just takes each <code>url</code>, and you only have a function that takes each <code>url</code> <em>plus</em> a <code>headers</code> argument. You can create that function by defining a new wrapper with <code>lambda</code> or <code>def</code>, or by calling a higher-order function that does it for you like <code>partial</code>. These are basically equivalent:</p>
<pre><code>results = pool.map(lambda url: do_it(url, headers), urls)

def wrapper(url):
    return do_it(url, headers)
results = pool.map(wrapper, urls)

results = pool.map(partial(do_it, headers=headers), urls)
</code></pre>
<hr/>
<p>You may also want to consider using an <a href="http://docs.python.org/3/library/concurrent.futures.html#executor-objects" rel="nofollow"><code>Executor</code></a> instead of a plain pool. An <code>Executor</code> returns smarter result objects called <code>Future</code>s that in many cases are easier to deal with. (For example, instead of having to decide between the four different <code>map</code> variants in <code>Pool</code>, you can just create a comprehension over the simple <code>submit</code> method, and then call <code>as_completed</code> or <code>wait</code> on the resulting <code>Future</code>s.) Since this wasn't added to Python until 3.2, for 2.x you'll have to install the backport library <a href="https://pypi.python.org/pypi/futures" rel="nofollow"><code>futures</code></a> to use it. Anyway, for a trivial case like this, it isn't going to be much different:</p>
<pre><code>with futures.ProcessPoolExecutor(max_workers=8) as executor:
    results = executor.map(lambda url: do_it(url, headers), urls)
</code></pre>
<p>… or:</p>
<pre><code>with futures.ProcessPoolExecutor(max_workers=8) as executor:
    fs = [executor.submit(do_it, url, headers) for url in urls]
    futures.wait(fs)
</code></pre>
</div>
<span class="comment-copy">As a side note, why do you think you need multiprocessing here instead of just multithreading? Your code is almost entirely I/O bound, waiting on the 25000 servers to respond, so it will run just as fast, and slightly more simply, with threads. (Or, for that matter, coroutines or greenlets… but you'd need a third-party library for either of those.)</span>
<span class="comment-copy">I tried the lamba but it errored out. So I tried pool = multiprocessing.Pool(processes=8);pool.map(do_it(link, headers));        pool.join().... but having a issue with the list with: "requests.exceptions.MissingSchema: Invalid URL u'h': No schema supplied"</span>
<span class="comment-copy">Without knowing any more than "it errored out", it's very hard to debug anything. You can't put <code>do_it(link, headers)</code> directly as an argument, because that will call <code>do_it(link, headers)</code> and pass the return value as the thing to be called 25000 times; you want to pass a <i>function</i> to be called 25000 times. If you can't figure out how to write <code>lambda</code>s, you can use <code>partial</code> or, simplest of all, a local function with a closure. I'll edit the answer to show how.</span>
