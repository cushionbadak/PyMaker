<div class="post-text" itemprop="text">
<p>My dataset is in the following form:</p>
<p><strong>Training Data</strong></p>
<p>A numpy array of size (7855, 448, 448, 3) where (448, 448, 3) is the numpy version of an RGB image. Because the purpose of the network is regression, I have not yet found a solution to use the ImageDataGenerator. So, I've converted the entire image dataset to a numpy array.</p>
<p><strong>Training Target</strong></p>
<p>The training target is a 1-dimensional numpy array of size 7855. The entries correspond to that of the training data.</p>
<p>To get hold of the numpy arrays, I have to load the entire dataset into memory into a variable and then pass it to fit and predict. This takes up to 5 - 6 gigs of RAM alone.</p>
<p>When fitting the model, the RAM quickly overflows, and the runtime crashes. How do I feed the numpy array elements in batches, or is there an alternate way of loading a dataset with the format:</p>
<pre><code>|list of images |
|labelled       |
|1, 2, 3...     |
|n              |


|csv file with: |
|1   target1    |
|2   target2    |
|3   target3... |
</code></pre>
<p>CODE
<a href="https://colab.research.google.com/drive/1FUvPcpYiDtli6vwIaTwacL48RwZ0sq-9" rel="nofollow noreferrer">https://colab.research.google.com/drive/1FUvPcpYiDtli6vwIaTwacL48RwZ0sq-9</a></p>
<p>[I've been using Google Colab as this is an academic research project, and have not yet invested in a high-end server. ]</p>
</div>
<div class="post-text" itemprop="text">
<p>You need to use Dataset API. 
When you created your numpy arrays, train_images, train_target, use <code>tf.data.Dataset.from_tensor_slices</code></p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices((train_images, train_target))
</code></pre>
<p>This will create dataset object, which can be fed into <code>model.fit</code>
You can shuffle, batch, and map any parse function to this dataset. You can control how many examples will be preloaded with shuffle buffer. Repeat controls epoch count and better be left <code>None</code>, so it will repeat indefinitely.</p>
<pre><code>dataset = dataset.shuffle().repeat()
dataset = dataset.batch()
</code></pre>
<p>Remember that batching goes on inside this pipeline, so you don't need to use batch in <code>model.fit</code>, but you need to pass number of epochs and steps per epoch. The latter can be a little tricky cause you can't do smth like <code>len(dataset)</code> so it should be calculated in advance.</p>
<pre><code>model.fit(dataset, epochs, steps_per_epoch)
</code></pre>
<p>If you will encounter graphdef limit error, it's better to save several smaller numpy arrays and pass them as a list </p>
<p>Make yourself familiar with this giude
<a href="https://www.tensorflow.org/guide/datasets" rel="nofollow noreferrer">https://www.tensorflow.org/guide/datasets</a> 
Hope this helps.</p>
</div>
<span class="comment-copy">What code do you use now?</span>
<span class="comment-copy">May I know the code for which part do I need to specify so I can post it? The layers in the model, the conversion image dataset to np-array, the loading of arrays and giving as input?</span>
<span class="comment-copy">conversion image dataset to np-array, the loading of arrays and giving as input</span>
<span class="comment-copy">@Sharky edited to link to the colab file.</span>
<span class="comment-copy">Hi, thanks so much for the response! For using tensor-slices, I'd still need to have the numpy array in the memory. Or can I directly use the numpy file from the storage, like the images are fetched in batches from storage to memory?</span>
<span class="comment-copy">I'm not 100% sure about how colab works, but this should be controlled with <code>dataset.shuffle</code> which loads only buffer size into memory</span>
