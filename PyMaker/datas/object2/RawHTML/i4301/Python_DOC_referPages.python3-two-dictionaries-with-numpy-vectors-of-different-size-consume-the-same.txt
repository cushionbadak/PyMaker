<div class="post-text" itemprop="text">
<p>I have two python dictionaries <code>{word: np.array(float)}</code>, in the first dictionary I use 300-dimensional numpy vectors, in the second (keys are the same) - 150-dimensional. File size of the first one is 4.3 GB, of the second one - 2.2 GB.</p>
<p>When I check loaded objects with <code>sys.getsizeof()</code> I get:</p>
<pre><code>import sys
import pickle
import numpy as np
</code></pre>
<p>For big dictionary:</p>
<pre><code>with open("big.pickle", 'rb') as f:
    source = pickle.load(f)

sys.getsizeof(source)
#201326688

all(val.size==300 for key, val in source.items())
#True
</code></pre>
<p>Linux <code>top</code> command shows 6.22GB:</p>
<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                   
 4669 hcl       20   0 6933232 6,224g  15620 S   0,0 19,9   0:11.74 python3
</code></pre>
<p>For small dictionary:</p>
<pre><code>with open("small.pickle", 'rb') as f:
    source = pickle.load(f)

sys.getsizeof(source)
# 201326688  # Strange!

all(val.size==150 for key, val in source.items())
#True
</code></pre>
<p>But when I look at the python3 process with linux <code>top</code> command I see 6.17GB:</p>
<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                   
 4515 hcl       20   0 6875596 6,170g  16296 S   0,0 19,7   0:08.77 python3 
</code></pre>
<p>Both dictionaries were saved using <code>pickle.HIGHEST_PROTOCOL</code> in Python3, I do not want to use json because of possible errors with encoding and slow loading. Also, using numpy arrays is important for me, as I compute <code>np.dot</code> for these vectors.</p>
<p>How can I shrink RAM for the dictionary with smaller vectors in it?</p>
<p>More precise memory measurement:</p>
<pre><code>#big:
sum(val.nbytes for key, val in source.items())
4456416000


#small:

sum(val.nbytes for key, val in source.items())
2228208000
</code></pre>
<hr/>
<p>EDIT: Thanks to @etene's hint, I've managed to save and load my model using hdf5:</p>
<p>Saving:</p>
<pre><code>import pickle
import numpy as np
import h5py


with open("reduced_150_normalized.pickle", 'rb') as f:
    source = pickle.load(f)

# list to save order
keys = []
values = []

for k, v in source.items():
    keys.append(k)
    values.append(v)

values = np.array(values)
print(values.shape)

with open('model150_keys.pickle',"wb") as f:
    pickle.dump(keys, f,protocol=pickle.HIGHEST_PROTOCOL) # do not store stings in h5! Everything will hang

h5f = h5py.File('model150_values.h5', 'w')
h5f.create_dataset('model_values', data=values)


h5f.close()
</code></pre>
<p>Which produces keyphrases list with length <code>3713680</code> and vectors array with the shape <code>(3713680, 150)</code>.</p>
<p>Loading:</p>
<pre><code>import pickle
import numpy as np
import h5py

with open('model150_keys.pickle',"rb") as f:
    keys = pickle.load(f) # do not store stings in h5! Everything will hang

# we will construct model by reading h5 file line-by-line
h5f = h5py.File('model150_values.h5','r')
d=h5f['model_values']

print(len(keys))
print(d.shape)

model = {}

for i,key in enumerate(keys):    
    model[key]=np.array(d[i,:])

h5f.close()
</code></pre>
<p>Now I indeed have only 3GB of RAM consumed:</p>
<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                   
 5012 hcl       20   0 3564672 2,974g  17800 S   0,0  9,5   4:25.27 python3 
</code></pre>
<p>@etene, you can write your comment as answer, I will choose it.</p>
<p>The only problem left is that loading now takes considerable time (5min), perhaps because of lookup made in hdf5 file for each position in numpy array. If I can iterate hdf5 by the second coordinate somehow, without loading into RAM, that will be great.</p>
<hr/>
<p>EDIT2: Following @hpaulj 's suggestion, I loaded file in chunks, and it is now as fast as pickle or even quicker (4s) when chunk in 10k is used:</p>
<pre><code>import pickle
import numpy as np
import h5py

with open('model150_keys.pickle',"rb") as f:
    keys = pickle.load(f) # do not store stings in h5! Everything will hang

# we will construct model by reading h5 file line-by-line
h5f = h5py.File('model150_values.h5','r')
d=h5f['model_values']

print(len(keys))
print(d.shape)

model = {}

# we will load in chunks to speed up loading
for i,key in enumerate(keys):    
    if i%10000==0:
        data = d[i:i+10000,:]

    model[key]=data[i%10000,:]

h5f.close()

print(len(model))
</code></pre>
<p>Thanks everyone !!!</p>
</div>
<div class="post-text" itemprop="text">
<p>Summarizing what we found out in the comments:</p>
<ul>
<li><a href="https://docs.python.org/3/library/sys.html#sys.getsizeof" rel="nofollow noreferrer">sys.getsizeof</a> returning the same value for two <code>dict</code>s with the same keys is normal behavior. From the docs: "Only the memory consumption directly attributed to the object is accounted for, not the memory consumption of objects it refers to."</li>
<li>Deserializing all your data at once is what eats up so much RAM; <a href="http://numpy-discussion.10968.n7.nabble.com/Memory-usage-of-numpy-arrays-td22072.html" rel="nofollow noreferrer">this Numpy discussion thread</a> mentions the HDF5 file format as a solution to read data in smaller batches, reducing memory usage.</li>
<li>However, reading in smaller batches can also have an impact on performance due to the disk i/o. Thanks to @hpaulj, @slowpoke was able to determine a larger batch size that worked for him.</li>
</ul>
<p><strong>TL;DR</strong> for future readers: If it's really large, don't deserialize your whole dataset at once, that can take unpredictable amounts of RAM. Use a specialized format such as HDF5 and chop your data in reasonably-sized batches, keeping in mind that smaller reads = more disk i/o and larger reads = more memory usage.</p>
</div>
<span class="comment-copy">The fact that the <code>sys.getsizeof</code> returns the same value for both dictionaries is not strange; it is normal and documented behavior assuming they have the same keys. From the docs: "Only the memory consumption directly attributed to the object is accounted for, not the memory consumption of objects it refers to."</span>
<span class="comment-copy">Since numpy is an external library, and Python memory management is not really straightforward (it has a small objects cache which can eat up RAM although Python3 is better in that regard), it's not going to be easy to reduce memory usage the way you want it. Would it be possible for you to read your data in smaller batches and process them individually, in order to avoid deserializing everything at once ?</span>
<span class="comment-copy"><a href="http://numpy-discussion.10968.n7.nabble.com/Memory-usage-of-numpy-arrays-td22072.html" rel="nofollow noreferrer">This numpy thread</a> might also be of interest to you.</span>
<span class="comment-copy">The hdf5 specific part is a bit outside my knowledge, but consider this: you'll probably have to chose a tradeoff between speed and memory usage. If you store all your data in RAM at once it'll take a lot of space but calculations on it will be quicker, and if you read it from disk in smaller parts it won't take as much RAM but will be slower.</span>
<span class="comment-copy">@etene Ok, great thanks nevertheless. But I think that I use hdf5 in a very inefficient way now, because saving was very quick. We reload our backend periodically because of some memory leaks in libraries used, and it is loading in much less than a minute with pickle (but we need to use more expensive Amazon node because of RAM). I'm telling this to explain why 5min is not very comfortable for me.</span>
