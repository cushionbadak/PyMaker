<div class="post-text" itemprop="text">
<p>I'm having the following dilemma, implementing LRU algorithm.
For every entry I have to store (key, value, timestamp)
As you guessed the cache structure can be :</p>
<pre><code> cache[key] = [ value, tstamp ]
</code></pre>
<p>so far so good. The problem is that LRU algorithm requires that 
when I have to purge element (because the cache is full), I have to do
linear search on the time stamp to delete the least used item, which is slow.</p>
<p>On the other hand I can't build reverse hash tstamp =&gt; key, because the
key is 1KByte big and the additional memory usage will negate the use of the cache.</p>
<p>Any ideas to preserve the speed but not increase the memory usage ?</p>
<p>If I could have something as reference I could probably reference cache-key w/o allocating additional memory !</p>
<hr/>
<p>I can't use LRU lib, because the key is unhashable by default I have to transform it beforehand. I'm also using 2.7</p>
</div>
<div class="post-text" itemprop="text">
<p>If you want to use a out-of-box LRU cache, refer to <a href="http://docs.python.org/3/library/functools.html#functools.lru_cache" rel="nofollow noreferrer">functools.lru_cache</a></p>
<p>If you need to implement it yourself, use a linked list to keep track of the function call history, please refer to <a href="https://stackoverflow.com/questions/49883177/how-does-lru-cache-from-functools-work/49883487#49883487">my other answer</a> regards to how <code>functools.lru_cache</code> works internally.</p>
</div>
<span class="comment-copy">Use <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache" rel="nofollow noreferrer">docs.python.org/3/library/functools.html#functools.lru_cache</a> ?</span>
<span class="comment-copy">this still doesn't resolve the large-key dilemma, right ?</span>
