<div class="post-text" itemprop="text">
<p>I have been messing around with Multiprocessing on and off for months now trying to figure out an elegant repeatable solution to my issue of wanting multiple processes to write to the same file without messing each other up.</p>
<p>I have used the Multiprocessing Producer/Consumer relationship to overcome these hurdles in the past.  Good articles and posts I've found include:</p>
<ul>
<li><p><a href="https://stackoverflow.com/questions/11196367/processing-single-file-from-multiple-processes-in-python">Processing single file from multiple processes in python</a></p></li>
<li><p><a href="http://sebastianraschka.com/Articles/2014_multiprocessing_intro.html" rel="nofollow noreferrer">http://sebastianraschka.com/Articles/2014_multiprocessing_intro.html</a></p></li>
</ul>
<p>I've tried implementing a function similar to a shared counter described here:</p>
<ul>
<li><a href="http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing" rel="nofollow noreferrer">http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing</a></li>
</ul>
<p>I have become a big fan of the simplicity of the concurrent.Futures ProcessPoolExecutor and using map on each executor as described here:</p>
<ul>
<li><p><a href="http://www.dalkescientific.com/writings/diary/archive/2012/01/19/concurrent.futures.html" rel="nofollow noreferrer">http://www.dalkescientific.com/writings/diary/archive/2012/01/19/concurrent.futures.html</a></p></li>
<li><p><a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">https://docs.python.org/3/library/concurrent.futures.html</a></p></li>
</ul>
<p>Tonight, I thought I had found the answer to my search with discovering a module called fasteners for readwrite locks, but apparently this approach only works on threading.</p>
<ul>
<li><a href="http://fasteners.readthedocs.org/en/latest/examples.html#reader-writer-shared-locks" rel="nofollow noreferrer">http://fasteners.readthedocs.org/en/latest/examples.html#reader-writer-shared-locks</a></li>
</ul>
<p>QUESTION: 
<strong>IS there an elegant, simple solution to sharing a lock so that all Processes from ProcessPoolExecutor do not overwrite eachother when writing to a file?</strong></p>
<p>NOTE: I'm writing about 800M rows of ~200 fields to one file using csv.DictWriter.  Other recommendations are welcome.</p>
</div>
<div class="post-text" itemprop="text">
<p>You are looking at the solution from the wrong angle. Instead of sharing a lock to protect the access over a file, give file access to a single process. The other processes will just tell to it what to write.</p>
<p>From that perspective, there are plenty of questions similar to your on stackoverflow.</p>
<p><a href="https://stackoverflow.com/questions/13446445/python-multiprocessing-safely-writing-to-a-file">Python multiprocessing safely writing to a file</a></p>
<p><a href="https://stackoverflow.com/questions/6524635/writing-to-a-file-with-multiprocessing">Writing to a file with multiprocessing</a></p>
<p><a href="https://stackoverflow.com/questions/15530563/python-multiprocessing-using-queue-to-write-to-same-file">Python Multiprocessing using Queue to write to same file</a></p>
</div>
<span class="comment-copy">I just tried using global variables and setting the lock if not set as a global variable.  Still open to better solutions.  Maybe joblib?</span>
