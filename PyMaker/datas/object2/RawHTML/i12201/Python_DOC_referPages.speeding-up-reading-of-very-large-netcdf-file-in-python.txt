<div class="post-text" itemprop="text">
<p>I have a very large netCDF file that I am reading using netCDF4 in python</p>
<p>I cannot read this file all at once since its dimensions (1200 x 720 x 1440) are too big for the entire file to be in memory at once. The 1st dimension represents time, and the next 2 represent latitude and longitude respectively.</p>
<pre><code>import netCDF4 
nc_file = netCDF4.Dataset(path_file, 'r', format='NETCDF4')
for yr in years:
    nc_file.variables[variable_name][int(yr), :, :]
</code></pre>
<p>However, reading one year at a time is excruciatingly slow. How do I speed this up for the use cases below?</p>
<p>--EDIT</p>
<p>The chunksize is 1</p>
<ol>
<li><p>I can read a range of years: nc_file.variables[variable_name][0:100, :, :]</p></li>
<li><p>There are several use-cases:</p>
<p>for yr in years:</p>
<pre><code>numpy.ma.sum(nc_file.variables[variable_name][int(yr), :, :])
</code></pre></li>
</ol>
<hr/>
<pre><code># Multiply each year by a 2D array of shape (720 x 1440)
for yr in years:
    numpy.ma.sum(nc_file.variables[variable_name][int(yr), :, :] * arr_2d)
</code></pre>
<hr/>
<pre><code># Add 2 netcdf files together 
for yr in years:
    numpy.ma.sum(nc_file.variables[variable_name][int(yr), :, :] + 
                 nc_file2.variables[variable_name][int(yr), :, :])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I highly recommend that you take a look at the <a href="https://github.com/pydata/xarray"><code>xarray</code></a> and <a href="https://github.com/dask/dask"><code>dask</code></a> projects.   Using these powerful tools will allow you to easily split up the computation in chunks.  This brings up two advantages: you can compute on data which does not fit in memory, and you can use all of the cores in your machine for better performance.  You can optimize the performance by appropriately choosing the chunk size (see <a href="http://xarray.pydata.org/en/stable/dask.html">documentation</a>).</p>
<p>You can load your data from netCDF by doing something as simple as </p>
<pre><code>import xarray as xr
ds = xr.open_dataset(path_file)
</code></pre>
<p>If you want to chunk your data in years along the time dimension, then you specify the <code>chunks</code> parameter (assuming that the year coordinate is named 'year'):</p>
<pre><code>ds = xr.open_dataset(path_file, chunks={'year': 10})
</code></pre>
<p>Since the other coordinates do not appear in the <code>chunks</code> dict,  then a single chunk will be used for them.  (See more details in the documentation <a href="http://xarray.pydata.org/en/stable/dask.html">here</a>.).  This will be useful for your first requirement, where you want to multiply each year by a 2D array.  You would simply do: </p>
<pre><code>ds['new_var'] = ds['var_name'] * arr_2d
</code></pre>
<p>Now, <code>xarray</code> and <code>dask</code> are computing your result <em>lazily</em>.  In order to trigger the actual computation, you can simply ask <code>xarray</code> to save your result back to netCDF:</p>
<pre><code>ds.to_netcdf(new_file)
</code></pre>
<p>The computation gets triggered through <code>dask</code>, which takes care of splitting the processing out in chunks and thus enables working with data that does not fit in memory.  In addition, <code>dask</code> will take care of using all your processor cores for computing chunks.  </p>
<p>The <code>xarray</code> and <code>dask</code> projects still don't handle nicely situations where chunks do not "align" well for parallel computation.  Since in this case we chunked only in the 'year' dimension, we expect to have no issues. </p>
<p>If you want to add two different netCDF files together, it is as simple as:</p>
<pre><code>ds1 = xr.open_dataset(path_file1, chunks={'year': 10})
ds2 = xr.open_dataset(path_file2, chunks={'year': 10})
(ds1 + ds2).to_netcdf(new_file)
</code></pre>
<p></p>
<p>I have provided a fully working example using <a href="https://www.unidata.ucar.edu/software/netcdf/examples/ECMWF_ERA-40_subset.nc">a dataset available online</a>. </p>
<pre><code>In [1]:

import xarray as xr
import numpy as np

# Load sample data and strip out most of it:
ds = xr.open_dataset('ECMWF_ERA-40_subset.nc', chunks = {'time': 4})
ds.attrs = {}
ds = ds[['latitude', 'longitude', 'time', 'tcw']]
ds

Out[1]:

&lt;xarray.Dataset&gt;
Dimensions:    (latitude: 73, longitude: 144, time: 62)
Coordinates:
  * latitude   (latitude) float32 90.0 87.5 85.0 82.5 80.0 77.5 75.0 72.5 ...
  * longitude  (longitude) float32 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 ...
  * time       (time) datetime64[ns] 2002-07-01T12:00:00 2002-07-01T18:00:00 ...
Data variables:
    tcw        (time, latitude, longitude) float64 10.15 10.15 10.15 10.15 ...

In [2]:

arr2d = np.ones((73, 144)) * 3.
arr2d.shape

Out[2]:

(73, 144)

In [3]:

myds = ds
myds['new_var'] = ds['tcw'] * arr2d

In [4]:

myds

Out[4]:

&lt;xarray.Dataset&gt;
Dimensions:    (latitude: 73, longitude: 144, time: 62)
Coordinates:
  * latitude   (latitude) float32 90.0 87.5 85.0 82.5 80.0 77.5 75.0 72.5 ...
  * longitude  (longitude) float32 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 ...
  * time       (time) datetime64[ns] 2002-07-01T12:00:00 2002-07-01T18:00:00 ...
Data variables:
    tcw        (time, latitude, longitude) float64 10.15 10.15 10.15 10.15 ...
    new_var    (time, latitude, longitude) float64 30.46 30.46 30.46 30.46 ...

In [5]:

myds.to_netcdf('myds.nc')
xr.open_dataset('myds.nc')

Out[5]:

&lt;xarray.Dataset&gt;
Dimensions:    (latitude: 73, longitude: 144, time: 62)
Coordinates:
  * latitude   (latitude) float32 90.0 87.5 85.0 82.5 80.0 77.5 75.0 72.5 ...
  * longitude  (longitude) float32 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 ...
  * time       (time) datetime64[ns] 2002-07-01T12:00:00 2002-07-01T18:00:00 ...
Data variables:
    tcw        (time, latitude, longitude) float64 10.15 10.15 10.15 10.15 ...
    new_var    (time, latitude, longitude) float64 30.46 30.46 30.46 30.46 ...

In [6]:

(myds + myds).to_netcdf('myds2.nc')
xr.open_dataset('myds2.nc')

Out[6]:

&lt;xarray.Dataset&gt;
Dimensions:    (latitude: 73, longitude: 144, time: 62)
Coordinates:
  * time       (time) datetime64[ns] 2002-07-01T12:00:00 2002-07-01T18:00:00 ...
  * latitude   (latitude) float32 90.0 87.5 85.0 82.5 80.0 77.5 75.0 72.5 ...
  * longitude  (longitude) float32 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 ...
Data variables:
    tcw        (time, latitude, longitude) float64 20.31 20.31 20.31 20.31 ...
    new_var    (time, latitude, longitude) float64 60.92 60.92 60.92 60.92 ...
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Check chunking of file. <code>ncdump -s &lt;infile&gt;</code> will give the answer. If chunk size in time dimension is larger than one, You should read the same amount of years at once, otherwise You are reading several years at once from disk and using only one at a time.
How slow is slow? Max few seconds per timestep sounds reasonable for an array of this size.
Giving more info on what You do with the data later may give us more guidance on where the problem may be.</p>
</div>
<div class="post-text" itemprop="text">
<p>This is Kinda Hacky, but may be the simplest solution:</p>
<p>Read subsets of the file into memory, then cPickle (<a href="https://docs.python.org/3/library/pickle.html" rel="nofollow">https://docs.python.org/3/library/pickle.html</a>) the file back to disk for future use. Loading your data from a pickled data structure is likely to be faster than parsing netCDF every time.</p>
</div>
<span class="comment-copy">Are you sure reading in any other matter (e.g the entire file at once) would be any faster? Can you try with a cropped file?</span>
<span class="comment-copy">Any <a href="http://stackoverflow.com/questions/582336/how-can-you-profile-a-python-script">essential profiling</a> done?</span>
<span class="comment-copy">Are you doing anything with the year's data once you read it?  Can you read a range of years, e.g. <code>[1997:2007,:,:]</code>?</span>
<span class="comment-copy">thanks @hapulj, I can read a range of years. There are several use-cases. Edited question to reflect them.</span>
<span class="comment-copy">thanks @Pedro, will be looking closer at xarray</span>
<span class="comment-copy">thanks @kakk11, chunksize = 1</span>
<span class="comment-copy">1 in time dimension and what is it in other dimensions? Can You also clarify how slow is "slow" in your case!</span>
<span class="comment-copy">chunk size is 720 and 1440 for other dimensions. It takes a fraction of a second for each iteration of the loop. But it adds up when you have to iterate over 1200 years</span>
<span class="comment-copy">Then You may already be at the speed for current file and hardware. If You have an option to rewrite the data, You may try PyTables and convert the files to blosc compressed HDF5. This should be faster that zlib compressed NetCDF4, though file will be slightly bigger. As rewriting the file was not an option in Your question, I'll not add it to answer yet, but as I've recently converted NetCDF to PyTables, I could give You some hints.</span>
<span class="comment-copy">thanks @kakk11, how slow/fast is the rewriting option? i.e. does it take so long to rewrite netcdf into hdf5 that subsequent speed benefits are useless?</span>
<span class="comment-copy">It is quite likely that writing/reading hdf5 with blosc compression, like in PyTables, is actually faster than cPickle. Not to mention the file size which can get very big for uncompressed numeric data!</span>
