<div class="post-text" itemprop="text">
<p>I ran into a pickle (literally) in parallelizing the following Python code and could really need some help.</p>
<p>First of all the input is a CSV file consisting of a list of website links that I need to scrape with the function <code>scrape_function()</code>. The original code is as follows and runs perfectly</p>
<pre><code>with open('C:\\links.csv','r') as source:
    reader=csv.reader(source)
    inputlist=list(reader)

m=[]

for i in inputlist:
    m.append(scrape_code(re.sub("\'|\[|\]",'',str(i)))) #remove the quotes around the link strings otherwise it results in URLError

print(m)
</code></pre>
<p>I then tried to parallelize this code using <code>joblib</code> as follows:</p>
<pre><code>from joblib import Parallel, delayed
import multiprocessing

with open('C:\\links.csv','r') as source:
        reader=csv.reader(source)
        inputlist=list(reader)

cores = multiprocessing.cpu_count()
results = Parallel(n_jobs=cores)(delayed(m.append(scrape_code(re.sub("\'|\[|\]",'',str(i))))) for i in inputlist)
</code></pre>
<p>However, this would result in a weird error:</p>
<pre class="lang-none prettyprint-override"><code>  File "C:\Users\...\joblib\pool.py", line 371, in send
    CustomizablePickler(buffer, self._reducers).dump(obj)
AttributeError: Can't pickle local object 'delayed.&lt;locals&gt;.delayed_function'
</code></pre>
<p>Any idea what I did wrong here? If I try to put the append in a separate function like below then the error would go away, but the execution would then freeze and hang indefinitely:</p>
<pre><code>def process(k):
    a=[]
    a.append(scrape_code(re.sub("\'|\[|\]",'',str(k))))
    return a

cores = multiprocessing.cpu_count()
results = Parallel(n_jobs=cores)(delayed(process)(i) for i in inputlist)
</code></pre>
<p>The input list has 10000s of pages so parallel processing would be a huge benefit.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you really need it in separate processes, the easiest way is to just create a <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer">process pool</a> and let it deal with distributing the links to your function, e.g.:</p>
<pre><code>import csv
from multiprocessing import Pool

if __name__ == "__main__":  # multiprocessing guard
    with open("c:\\links.csv", "r", newline="") as f:  # open the CSV
        reader = csv.reader(f)  # create a reader
        links = [r[0] for r in reader]  # collect only the first column
    with Pool() as pool:  # create a pool, it will make a pool with all your CPU cores...
        results = pool.map(scrape_code, links)  # distribute your links to scrape_code
    print(results)
</code></pre>
<p>NOTE: I'm assuming your <code>links.csv</code> actually holds the link in its first column based on how you're pre-processing the links in your code.</p>
<p>However, as I've stated in my comment, this doesn't have to be necessarily faster than plain threading so I'd first try it using threads. Fortunately, the <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer"><code>multiprocessing</code></a> module includes a <a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.dummy" rel="nofollow noreferrer">threading interfrace dummy</a> so you just need to replace <code>from multiprocessing import Pool</code> with <code>from multiprocessing.dummy import Pool</code> and see in what regime your code works faster.</p>
</div>
<span class="comment-copy">I know nothing about joblib so I can't help, but why not just stick with the multiprocessing library?</span>
<span class="comment-copy">What's wrong with using the built-in <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer"><code>multiprocessing</code></a> module? That being said, the majority of time here will be spent on network I/O so instead of dealing with shared memory and context switching just use regular threads to do your bidding - it's very likely they'll end up faster than multiprocessing.</span>
