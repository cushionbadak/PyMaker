<div class="post-text" itemprop="text">
<p>I have 6 large files which each of them contains a dictionary object that I saved in a hard disk using pickle function. It takes about 600 seconds to load all of them in sequential order. I want to start loading all them at the same time to speed up the process. Suppose all of them have the same size, I hope to load them in 100 seconds instead. I used multiprocessing and apply_async to load each of them separately but it runs like sequential. This is the code I used and it doesn't work. 
The code is for 3 of these files but it would be the same for six of them. I put the 3rd file in another hard disk to make sure the IO is not limited.</p>
<pre><code>def loadMaps():    
    start = timeit.default_timer()
    procs = []
    pool = Pool(3)
    pool.apply_async(load1(),)
    pool.apply_async(load2(),)
    pool.apply_async(load3(),)
    pool.close()
    pool.join()
    stop = timeit.default_timer()
    print('loadFiles takes in %.1f seconds' % (stop - start))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If your code is primarily limited by IO and the files are on multiple disks, you <em>might</em> be able to speed it up using threads:</p>
<pre><code>import concurrent.futures
import pickle

def read_one(fname):
    with open(fname, 'rb') as f:
        return pickle.load(f)

def read_parallel(file_names):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(read_one, f) for f in file_names]
        return [fut.result() for fut in futures]
</code></pre>
<p>The <a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow noreferrer">GIL</a> will not force IO operations to run serialized because Python consistently releases it when doing IO.</p>
<p>Several remarks on alternatives:</p>
<ul>
<li><p><code>multiprocessing</code> is unlikely to help because, while it guarantees to do its work in multiple processes (and therefore free of the GIL), it also requires the content to be transferred between the subprocess and the main process, which takes additional time.</p></li>
<li><p><code>asyncio</code> will not help you at all because it doesn't natively support asynchronous file system access (and neither do the popular OS'es). While it can <a href="https://pypi.org/project/aiofile/" rel="nofollow noreferrer">emulate it with threads</a>, the effect is the same as the code above, only with much more ceremony.</p></li>
<li><p>Neither option will speed up loading the six files by a factor of six. Consider that at least <em>some</em> of the time is spent creating the dictionaries, which will be serialized by the GIL. If you want to really speed up startup, a better approach is not to create the whole dictionary upfront and switch to an <a href="https://docs.python.org/3.6/library/sqlite3.html" rel="nofollow noreferrer">in-file database</a>, possibly using the dictionary to cache access to its content.</p></li>
</ul>
</div>
<span class="comment-copy">If the files are stored in the same volume, you are limited by the I/O. So, loading files in parallel won’t speed up the process, worse it will slow down because you may increase the number of random access to the hard drive. It’s better to load files in sequence, sorry.</span>
<span class="comment-copy">Your bottleneck is likely the mass storage. Even with parallel processing, it will remain the bottleneck and likely will get worse.</span>
<span class="comment-copy">I have multiple hard disks on this machine and I put the 3rd file in a different hard disk which is actually a parallel filesystem and using OpenMP I can read large file with many cores at the same time. I want to achieve the same performance using python.</span>
<span class="comment-copy">I ran it with 3 files and 6 files. Multiple cores are utilized. 3 files took 171 seconds and 6 files took 361 seconds. Thanks for explaining the details. Would you please elaborate more on the last bullet point about an in-file database?</span>
<span class="comment-copy">@EhsanSadr I think the suggestion is to use databases that are stored in files instead of dictionaries. This answer suggests the sqlite3 module, and I've used the <a href="https://docs.python.org/3/library/shelve.html" rel="nofollow noreferrer">shelve</a> module to do this before. The idea is that instead of loading 6 massive dictionaries all into memory at the start of your script, you instead leave all the data on disk and access it as needed. You'll incur a runtime penalty for this, but eliminate the startup latency.</span>
<span class="comment-copy">@EhsanSadr Exactly as skrrgwasme described it. Whether this suggestion makes sense really depends on your application - if it will eventually need all these entries, then it might be better to load them upfront and be done with it. Otherwise, a lightweight data store might be precisely what you need. Depending on when you need the dicts, you could even create hybrid scenarios where the files are being read in the background while the application is doing something else.</span>
<span class="comment-copy">Thanks for clarification, it would be useful for my other applications later. In this application, I need all of the dictionaries for fast lookups and as you mentioned I am willing to pay for slower start up.</span>
