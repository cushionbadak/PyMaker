<div class="post-text" itemprop="text">
<p>I want to apply a function in parallel using multiprocessing.Pool.
The problem is that if one function call triggers a segmentation fault the Pool hangs forever.
Has anybody an idea how I can make a Pool that detects when something like this happens and raises an error?</p>
<p>The following example shows how to reproduce it (requires scikit-learn &gt; 0.14)</p>
<pre><code>import numpy as np
from sklearn.ensemble import gradient_boosting
import time

from multiprocessing import Pool

class Bad(object):
    tree_ = None


def fit_one(i):
    if i == 3:
        # this will segfault                                                    
        bad = np.array([[Bad()] * 2], dtype=np.object)
        gradient_boosting.predict_stages(bad,
                                         np.random.rand(20, 2).astype(np.float32),
                                         1.0, np.random.rand(20, 2))
    else:
        time.sleep(1)
    return i


pool = Pool(2)
out = pool.imap_unordered(fit_one, range(10))
# we will never see 3
for o in out:
    print o
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This is a <a href="http://bugs.python.org/issue22393" rel="nofollow">known bug, issue #22393, in Python</a>. There is no meaningful workaround as long as you're using <code>multiprocessing.pool</code> until it's fixed. A patch is available at that link, but it has not been integrated into the main release as yet, so no stable release of Python fixes the problem.</p>
</div>
<div class="post-text" itemprop="text">
<p>Instead of using <code>Pool().imap()</code> maybe you would rather manually create child processes yourself with <code>Process()</code>. I bet the object returned would allow you to get liveness status of any child. You will know if they hang up.</p>
</div>
<div class="post-text" itemprop="text">
<p>As described in the comments, this just works in Python 3 if you use <code>concurrent.Futures.ProcessPoolExecutor</code> instead of <code>multiprocessing.Pool</code>.</p>
<p>If you're stuck on Python 2, the best option I've found is to use the <code>timeout</code> argument on the result objects returned by <code>Pool.apply_async</code> and <code>Pool.map_async</code>. For example:</p>
<pre><code>pool = Pool(2)
out = pool.map_async(fit_one, range(10))
for o in out:
    print o.get(timeout=1000)  # allow 1000 seconds max
</code></pre>
<p>This works as long as you have an upper bound for how long a child process should take to complete a task.</p>
</div>
<div class="post-text" itemprop="text">
<p>I haven't run your example to see if it can handle the error, but try concurrent futures. Simply replace my_function(i) with your fit_one(i). Keep the <code>__name__=='__main__':</code> structure. concurrent futures seems to need this. The code below is tested on my machine so will hopefully work straight up on yours.</p>
<pre><code>import concurrent.futures

def my_function(i):
    print('function running')
    return i

def run():
    number_processes=4
    executor = concurrent.futures.ProcessPoolExecutor(number_processes)
    futures = [executor.submit(my_function,i) for i in range(10)]
    concurrent.futures.wait(futures)

    for f in futures:
        print(f.result())

if __name__ == '__main__':
    run()
</code></pre>
</div>
<span class="comment-copy">Fix the segmentation fault? Usually segfaults are caused by invalid memory access, which is <i>undefined</i> behavior and not guarenteed to cause a segfault at all.</span>
<span class="comment-copy">No answers, but I can say that joblib.Parallel seems to hang forever. From what I can tell, there is no way to return the segfault or add a "watchdog" timeout in multiprocessing.</span>
<span class="comment-copy">Actually, maybe you can add a timeout decorator? Such as shown here: <a href="http://code.activestate.com/recipes/577028/" rel="nofollow noreferrer">code.activestate.com/recipes/577028</a></span>
<span class="comment-copy">See this answer: <a href="http://stackoverflow.com/a/24396655/2073595">stackoverflow.com/a/24396655/2073595</a>. It's a bit messy, but you can monitor the individual processes in your pool to see if one has restarted unexpectedly.</span>
<span class="comment-copy">Also of note here: <a href="https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor" rel="nofollow noreferrer"><code>concurrent.futures.ProcessPoolExecutor</code></a> does detect when a process has been killed unexpectedly, and will raise a <code>BrokenProcessPool</code> exception on any outstanding tasks when it happens. There is also <a href="http://bugs.python.org/issue22393" rel="nofollow noreferrer">a bug</a> filed against <code>multiprocessing</code>, which has a working patch, to add that same behavior to <code>multiprocessing.Pool</code>.</span>
<span class="comment-copy">Requires python 3.2</span>
<span class="comment-copy">I'm just thinking it might work because you can call all sorts of methods on the itterable 'futures' which is returned after completion of all the processes. So it may be able to take the error in its stride.</span>
