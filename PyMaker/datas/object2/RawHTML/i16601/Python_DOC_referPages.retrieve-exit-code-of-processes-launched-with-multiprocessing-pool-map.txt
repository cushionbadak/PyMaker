<div class="post-text" itemprop="text">
<p>I'm using python <code>multiprocessing</code> module to parallelize some computationally heavy tasks.
 The obvious choice is to use a <code>Pool</code> of workers and then use the <code>map</code> method.</p>
<p>However, processes can fail. For instance, they may be silently killed for instance by the <code>oom-killer</code>. Therefore I would like to be able to retrieve the exit code of the processes launched with <code>map</code>.</p>
<p>Additionally, for logging purpose, I would like to be able to know the PID of the process launched to execute each value in the the iterable.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you're using <code>multiprocessing.Pool.map</code> you're generally not interested in the <em>exit code</em> of the sub-processes in the pool, you're interested in what value they returned from their work item. This is because under normal conditions, the processes in a <code>Pool</code> won't exit until you <code>close</code>/<code>join</code> the pool, so there's no exit codes to retrieve until all work is complete, and the <code>Pool</code> is about to be destroyed. Because of this, there is no public API to get the exit codes of those sub-processes.</p>
<p>Now, you're worried about exceptional conditions, where something out-of-band kills one of the sub-processes while it's doing work. If you hit an issue like this, you're probably going to run into some strange behavior. In fact, in my tests where I killed a process in a <code>Pool</code> while it was doing work as part of a <code>map</code> call, <code>map</code> never completed, because the killed process didn't complete. Python did, however, immediately launch a new process to replace the one I killed.</p>
<p>That said, you can get the pid of each process in your pool by accessing the <code>multiprocessing.Process</code> objects inside the pool directly, using the private <code>_pool</code> attribute:</p>
<pre><code>pool = multiprocessing.Pool()
for proc in pool._pool:
  print proc.pid
</code></pre>
<p>So, one thing you could do to try to detect when a process had died unexpectedly (assuming you don't get stuck in a blocking call as a result). You can do this by examining the list of processes in the pool before and after making a call to <code>map_async</code>:</p>
<pre><code>before = pool._pool[:]  # Make a copy of the list of Process objects in our pool
result = pool.map_async(func, iterable)  # Use map_async so we don't get stuck.
while not result.ready():  # Wait for the call to complete
    if any(proc.exitcode for proc in before):  # Abort if one of our original processes is dead.
        print "One of our processes has exited. Something probably went horribly wrong."
        break
    result.wait(timeout=1)
else:  # We'll enter this block if we don't reach `break` above.
    print result.get() # Actually fetch the result list here.
</code></pre>
<p>We have to make a copy of the list because when a process in the <code>Pool</code> dies, Python immediately replaces it with a new process, and removes the dead one from the list.</p>
<p>This worked for me in my tests, but because it's relying on a private attribute of the <code>Pool</code> object (<code>_pool</code>) it's risky to use in production code. I would also suggest that it may be overkill to worry too much about this scenario, since it's very unlikely to occur and complicates the implementation significantly.</p>
</div>
<span class="comment-copy">are you on Python2  or Python3?</span>
<span class="comment-copy">I use mostly python 2.</span>
<span class="comment-copy">You're right: when such problems arise, I get strange results. In the current code, processes are launched by a special loop. My idea was to replace this loop by a (much simpler) call to map. However your solution is about as complex as the current one so I don't think it's worth. Also, processes write their results to the disk (so I don't really need their result). However you answer is interesting.</span>
<span class="comment-copy">Also of note here: <a href="https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor" rel="nofollow noreferrer"><code>concurrent.futures.ProcessPoolExecutor</code></a> does detect when a process has been killed unexpectedly, and will raise a <code>BrokenProcessPool</code> on any outstanding tasks when it happens. There is <a href="http://bugs.python.org/issue22393" rel="nofollow noreferrer">a bug</a> filed against <code>multiprocessing</code>, which has a working patch, to add this behavior to <code>multiprocessing.Pool</code> as well.</span>
<span class="comment-copy">Billiard is a multiprocessing fork that seems to handle this case <a href="https://pypi.org/project/billiard/" rel="nofollow noreferrer">pypi.org/project/billiard</a></span>
