<div class="post-text" itemprop="text">
<p>I don't have much knowledge of image processing. I am trying to implement a ConvNet. I downloaded some images as data set and made their  height and width equal. Then I tried loading them into np.array by this code:</p>
<pre><code>train_list = glob.glob('A:\Code\Machine 
Learning\CNN\ConvolutionalNN1\TrainImg\*.jpg')
X_train_orig = np.array([np.array(Image.open(file)) for file in train_list])
</code></pre>
<p>But it gave me error that cannot broadcast (420,310) to (420,310,3). Then I printed the shape of array, some were (420,310,3) others (410,320,4). Why is so? And how can I change that to fit it in array? </p>
</div>
<div class="post-text" itemprop="text">
<h2>Problem</h2>
<p>So basically what is happening over here is you are playing with three different formats of images (at least those that appear in your question). They are respectively:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/RGB_color_model" rel="nofollow noreferrer">RGB</a> (of dimension
<code>(420, 310, 3)</code>), <em>three channels</em></li>
<li><a href="https://en.wikipedia.org/wiki/RGBA_color_space" rel="nofollow noreferrer">RGB-A</a> (of dimension
<code>(420, 310, 4)</code>), <em>four channels</em></li>
<li><a href="https://en.wikipedia.org/wiki/Grayscale" rel="nofollow noreferrer">Grayscale</a>  (of dimension
<code>(420, 310)</code>), <em>single channel</em></li>
</ul>
<p>The third dimension that you are seeing is what represents the number of channels in your image (the first two being the height and width respectively).</p>
<p>An example will further clear it up. I downloaded random images from the internet each belonging to one of the three formats mentioned above.</p>
<p>RGB image <code>dog.png</code></p>
<p><a href="https://i.stack.imgur.com/zb58G.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/zb58G.png"/></a></p>
<p>RGB-A image <code>fish.png</code></p>
<p><a href="https://i.stack.imgur.com/HGTaF.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/HGTaF.png"/></a></p>
<p>Grayscale image <code>lena.png</code></p>
<p><a href="https://i.stack.imgur.com/UWonP.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/UWonP.png"/></a></p>
<p>Here's a python script to load each of them using <code>PIL</code> and display their shape:</p>
<pre><code>from PIL import Image
import numpy as np

dog = Image.open('dog.png')
print('Dog shape is ' + str(np.array(dog).shape))

fish = Image.open('fish.png')
print('Fish shape is ' + str(np.array(fish).shape))

lena = Image.open('lena.png')
print('Lena shape is ' + str(np.array(lena).shape))
</code></pre>
<p>And here is the output:</p>
<pre><code>Dog shape is (250, 250, 3)
Fish shape is (501, 393, 4)
Lena shape is (512, 512)
</code></pre>
<p>Hence, when you are trying to iteratively assign all the images to an array (<code>np.array</code>), you are getting the shape mis-match error. </p>
<h2>Solution</h2>
<p>The easiest way to resolve this is to convert all the images to one particular format before saving it in the array. Assuming you will be using a pre-trained ImageNet model, we will convert them to <code>RGB</code> format (you can similarly choose a format of your choice also).</p>
<p>We will convert <code>RGB-A</code> to <code>RGB</code> using the following code:</p>
<pre><code>fish = Image.open('fish.png')
print('Fish RGB-A shape is ' + str(np.array(fish).shape))
rgb = fish.convert('RGB')
print('Fish RGB shape is ' + str(np.array(rgb).shape))
</code></pre>
<p>Output is:</p>
<pre><code>Fish RGB-A shape is (501, 393, 4)
Fish RGB shape is (501, 393, 3)
</code></pre>
<p>Similarly you can do for all your images, and then you have a consistent number of channels (<em>three</em> in this case) for all your images.</p>
<p><strong>NOTE</strong>: In my example, the spatial dimensions vary for the images also. In your case that is not an issue as all are of consistent dimension <code>(420, 310)</code>.</p>
<p>Hope this clarifies your doubt.</p>
</div>
<span class="comment-copy">The alpha value of RGBA</span>
<span class="comment-copy">To drop the fourth dimension, <code>img[:, :, :3]</code></span>
<span class="comment-copy">@MateenUlhaq where should I write that? and there are not four but three elements in the tuple of shape. so what fourth dimensiopn are you refering to</span>
<span class="comment-copy">But does removing the alpha parameter affect the images in any way, as in they become blurry or something?</span>
<span class="comment-copy">and i have another doubt that I just used *.jpg in glob, but the png images are getting automatically converted to jpg and are being selected. is that normal and ok or should i manually convert them to png and then select?</span>
<span class="comment-copy">Regarding your first question, it depends on your use case. Quoting from Wiki, the <i>alpha</i> channel defines <b>how opaque each pixel is and allows an image to be combined over others using alpha compositing, with transparent areas and anti-aliasing of the edges of opaque regions.</b> If you are going to use the images for classification or something, it can be safely removed without much consequence, if using trained models.</span>
<span class="comment-copy">And regarding your second query, that is not the expected behavior of <b>glob.glob</b>. Documentation can be found over here: <a href="https://docs.python.org/3/library/glob.html#glob.glob" rel="nofollow noreferrer">docs.python.org/3/library/glob.html#glob.glob</a> It will recursively select files only that match the format <code>*.jpg</code>. The mistake is occurring somewhere else I feel.</span>
