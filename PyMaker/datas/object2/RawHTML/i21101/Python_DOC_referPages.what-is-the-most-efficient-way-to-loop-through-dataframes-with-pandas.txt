<div class="post-text" itemprop="text">
<p>I want to perform my own complex operations on financial data in dataframes in a sequential manner.</p>
<p>For example I am using the following MSFT CSV file taken from <a href="http://finance.yahoo.com/q/hp?s=MSFT">Yahoo Finance</a>:</p>
<pre><code>Date,Open,High,Low,Close,Volume,Adj Close
2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13
2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31
2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98
2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27

....
</code></pre>
<p>I then do the following:</p>
<pre><code>#!/usr/bin/env python
from pandas import *

df = read_csv('table.csv')

for i, row in enumerate(df.values):
    date = df.index[i]
    open, high, low, close, adjclose = row
    #now perform analysis on open/close based on date, etc..
</code></pre>
<p>Is that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? <code>df.iteritems</code> unfortunately only iterates column by column.</p>
</div>
<div class="post-text" itemprop="text">
<p>The newest versions of pandas now include a built-in function for iterating over rows. </p>
<pre><code>for index, row in df.iterrows():

    # do some logic here
</code></pre>
<p>Or, if you want it faster use <code>itertuples()</code></p>
<p>But, unutbu's suggestion to use numpy functions to avoid iterating over rows will produce the fastest code. </p>
</div>
<div class="post-text" itemprop="text">
<p>Pandas is based on NumPy arrays.
The key to speed with NumPy arrays is to perform your operations on the whole array at once, never row-by-row or item-by-item.</p>
<p>For example, if <code>close</code> is a 1-d array, and you want the day-over-day percent change,</p>
<pre><code>pct_change = close[1:]/close[:-1]
</code></pre>
<p>This computes the entire array of percent changes as one statement, instead of </p>
<pre><code>pct_change = []
for row in close:
    pct_change.append(...)
</code></pre>
<p>So try to avoid the Python loop <code>for i, row in enumerate(...)</code> entirely, and
think about how to perform your calculations with operations on the entire array (or dataframe) as a whole, rather than row-by-row.</p>
</div>
<div class="post-text" itemprop="text">
<p>Like what has been mentioned before, pandas object is most efficient when process the whole array at once. However for those who really need to loop through a pandas DataFrame to perform something, like me, I found at least three ways to do it. I have done a short test to see which one of the three is the least time consuming.</p>
<pre><code>t = pd.DataFrame({'a': range(0, 10000), 'b': range(10000, 20000)})
B = []
C = []
A = time.time()
for i,r in t.iterrows():
    C.append((r['a'], r['b']))
B.append(time.time()-A)

C = []
A = time.time()
for ir in t.itertuples():
    C.append((ir[1], ir[2]))    
B.append(time.time()-A)

C = []
A = time.time()
for r in zip(t['a'], t['b']):
    C.append((r[0], r[1]))
B.append(time.time()-A)

print B
</code></pre>
<p>Result:</p>
<pre><code>[0.5639059543609619, 0.017839908599853516, 0.005645036697387695]
</code></pre>
<p>This is probably not the best way to measure the time consumption but it's quick for me.</p>
<p>Here are some pros and cons IMHO:</p>
<ul>
<li>.iterrows(): return index and row items in separate variables, but significantly slower</li>
<li>.itertuples(): faster than .iterrows(), but return index together with row items, ir[0] is the index</li>
<li>zip: quickest, but no access to index of the row</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>You can loop through the rows by transposing and then calling iteritems:</p>
<pre><code>for date, row in df.T.iteritems():
   # do some logic here
</code></pre>
<p>I am not certain about efficiency in that case. To get the best possible performance in an iterative algorithm, you might want to explore writing it in <a href="http://cython.org">Cython</a>, so you could do something like:</p>
<pre><code>def my_algo(ndarray[object] dates, ndarray[float64_t] open,
            ndarray[float64_t] low, ndarray[float64_t] high,
            ndarray[float64_t] close, ndarray[float64_t] volume):
    cdef:
        Py_ssize_t i, n
        float64_t foo
    n = len(dates)

    for i from 0 &lt;= i &lt; n:
        foo = close[i] - open[i] # will be extremely fast
</code></pre>
<p>I would recommend writing the algorithm in pure Python first, make sure it works and see how fast it is-- if it's not fast enough, convert things to Cython like this with minimal work to get something that's about as fast as hand-coded C/C++.</p>
</div>
<div class="post-text" itemprop="text">
<p>I checked out <code>iterrows</code> after noticing <a href="https://stackoverflow.com/users/475872/nick-crawford">Nick Crawford's</a> answer, but found that it yields (index, Series) tuples. Not sure which would work best for you, but I ended up using the <code>itertuples</code> method for my problem, which yields (index, row_value1...) tuples.</p>
<p>There's also <code>iterkv</code>, which iterates through (column, series) tuples.</p>
</div>
<div class="post-text" itemprop="text">
<p>Just as a small addition, you can also do an apply if you have a complex function that you apply to a single column:</p>
<p><a href="http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.apply.html</a></p>
<pre><code>df[b] = df[a].apply(lambda col: do stuff with col here)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You have three options:</p>
<p>By <a href="https://www.neural-networks.io/en/python/dataframes.php#iterate-over-rows-by-index" rel="noreferrer">index</a> (simplest):</p>
<pre><code>&gt;&gt;&gt; for index in df.index:
...     print ("df[" + str(index) + "]['B']=" + str(df['B'][index]))
</code></pre>
<p>With <a href="https://www.neural-networks.io/en/python/dataframes.php#iterate-over-rows-with-iterrows" rel="noreferrer">iterrows</a> (most used):</p>
<pre><code>&gt;&gt;&gt; for index, row in df.iterrows():
...     print ("df[" + str(index) + "]['B']=" + str(row['B']))
</code></pre>
<p>With <a href="https://www.neural-networks.io/en/python/dataframes.php#iterate-over-rows-with-itertuples" rel="noreferrer">itertuples</a> (fastest):</p>
<pre><code>&gt;&gt;&gt; for row in df.itertuples():
...     print ("df[" + str(row.Index) + "]['B']=" + str(row.B))
</code></pre>
<p>Three options display something like:</p>
<pre><code>df[0]['B']=125
df[1]['B']=415
df[2]['B']=23
df[3]['B']=456
df[4]['B']=189
df[5]['B']=456
df[6]['B']=12
</code></pre>
<p>Source: <a href="https://www.neural-networks.io/en/python/dataframes.php" rel="noreferrer">neural-networks.io</a></p>
</div>
<div class="post-text" itemprop="text">
<p>As <a href="https://stackoverflow.com/users/653364/joris">@joris</a> pointed out, <code>iterrows</code> is much slower than <code>itertuples</code> and <code>itertuples</code> is approximately 100 times fater than <code>iterrows</code>, and I tested speed of both methods in a DataFrame with 5027505 records the result is for <code>iterrows</code>, it is 1200it/s, and  <code>itertuples</code> is 120000it/s.</p>
<p>If you use <code>itertuples</code>, note that every element in the for loop is a namedtuple, so to get the value in each column, you can refer to the following example code</p>
<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]},
                      index=['a', 'b'])
&gt;&gt;&gt; df
   col1  col2
a     1   0.1
b     2   0.2
&gt;&gt;&gt; for row in df.itertuples():
...     print(row.col1, row.col2)
...
1, 0.1
2, 0.2
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>For sure, the fastest way to iterate over a dataframe is to access the underlying numpy ndarray either via <code>df.values</code> (as you do) or by accessing each column separately <code>df.column_name.values</code>. Since you want to have access to the index too, you can use <code>df.index.values</code> for that.</p>
<pre><code>index = df.index.values
column_of_interest1 = df.column_name1.values
...
column_of_interestk = df.column_namek.values

for i in range(df.shape[0]):
   index_value = index[i]
   ...
   column_value_k = column_of_interest_k[i]
</code></pre>
<p>Not pythonic? Sure. But fast.</p>
<p>If you want to squeeze more juice out of the loop you will want to look into <a href="http://cython.org/" rel="nofollow noreferrer">cython</a>. Cython will let you gain huge speedups (think 10x-100x). For maximum performance check <a href="http://cython.readthedocs.io/en/latest/src/userguide/memoryviews.html" rel="nofollow noreferrer">memory views for cython</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>Another suggestion would be to combine groupby with vectorized calculations if subsets of the rows shared characteristics which allowed you to do so. </p>
</div>
<span class="comment-copy">have you tried writing a function and passing it to <code>df.apply()</code>?</span>
<span class="comment-copy">If you want memory efficieny you should consider using vectorized operations (using matrices and vectors). But I don't know pandas, so I can't tell you, whether such operations are possible there.</span>
<span class="comment-copy">Citing <code>unutbu</code>, NumPy seems to support vectorized operations (<code>The key to speed with NumPy arrays is to perform your operations on the whole array at once</code>).</span>
<span class="comment-copy">Most numeric operations with pandas can be vectorized - this means they are much faster than conventional iteration. OTOH, some operations (such as string and regex) are inherently hard to vectorize. This this case, it is important to understand how to loop over your data. For more information on when and how looping over your data is to be done, please read <a href="https://stackoverflow.com/questions/54028199/for-loops-with-pandas-when-should-i-care/54028200#54028200">For loops with Pandas - When should I care?</a>.</span>
<span class="comment-copy">Note that <code>iterrows</code> is very slow (it converts every row to a series, potentially messing with your data types). When you need an iterator, better to use <code>itertuples</code></span>
<span class="comment-copy">BTW itertuples returns named tuples ( <a href="https://docs.python.org/3/library/collections.html#collections.namedtuple" rel="nofollow noreferrer">docs.python.org/3/library/â€¦</a>) so you can access each column by name with row.high or getattr(row,'high')</span>
<span class="comment-copy">Be aware, according to current <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html" rel="nofollow noreferrer">docs</a>: "You should <b>never modify</b> something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect."</span>
<span class="comment-copy">Hey, how to use unutbu's solution if I want to iterate efficiently over a Pandas Series ?</span>
<span class="comment-copy">@joris. I can't agree you more, <code>itertuples</code> is approximately 100 times fater than <code>iterrows</code>.</span>
<span class="comment-copy">I agree that this is the best way and that is what I usually do for simple operations. However, in this case, this is not possible, since the resulting operations can get very complex. Specifically I am trying to backtest trading strategies.  E.g. if the price is at a new low over a 30d period, then we might want to buy the stock and get out whenever a certain condition is met and this needs to be simulated in-place. This simple example could still be done by vectorization, however, the more complex a trading-strategy gets, the less possible it becomes to use vectorization.</span>
<span class="comment-copy">You'll have to explain in more detail the exact calculation you are trying to perform. It helps to write the code any way you can first, then profile and optimize it.</span>
<span class="comment-copy">By the way, for some calculations (especially those that can not be expressed as operations on whole arrays) code using Python lists can be faster than equivalent code using numpy arrays.</span>
<span class="comment-copy">I agree vectorization is the right solution where possible-- sometimes an iterative algorithm is the only way though.</span>
<span class="comment-copy">late comment, but i have found that trying to do full calculation for a column is sometimes difficult to write and debug.   Consider intermediary calculation columns,  makes it easier to debug and understand the calculations.   have found that even the most complex logic can be implemented like this, while still avoiding looping.</span>
<span class="comment-copy">NB in Python 3 <code>zip()</code> returns an iterator, so use <code>list(zip())</code></span>
<span class="comment-copy">Could you not use <code>t.index</code> to loop through the index?</span>
<span class="comment-copy">this is amazing! zip converted my process from taking 2 hours to 2 minutes!</span>
<span class="comment-copy">I also recommend Cython; I was working on a similar problem for building my backtesting engine, and I got a 1,000x speedup. I then combined that with the multiprocessing library, which is a very nice combination.</span>
<span class="comment-copy">This answer needs updating to include the new <code>df.iterrows()</code> as per @NickCrawford's answer.</span>
<span class="comment-copy"><code>df.T.iteritems()</code> is a great solution rather than using <code>df.iterrows()</code> if you want to iterate over a specific column +1</span>
<span class="comment-copy">I've updated the answer @LondonRob</span>
<span class="comment-copy">Gives error:   <code>def my_algo(ndarray[object] dates, ndarray[float64_t] opn,                        ^ SyntaxError: invalid syntax</code></span>
<span class="comment-copy">you can do something like dict(row) to make a set out of the row with searchable columns</span>
<span class="comment-copy">I also found itertuples to be much faster (10x) in my use case as Series objects are not being created.</span>
<span class="comment-copy">FYI: <code>iterkv</code> deprecated since 0.13.1</span>
<span class="comment-copy"><code>iterrows(): Iterate over the rows of a DataFrame as (index, Series) pairs.... itertuples(): Iterate over the rows of a DataFrame as tuples of the values. This is a lot faster as iterrows(), and is in most cases preferable to use to iterate over the values of a DataFrame.</code></span>
<span class="comment-copy">probably x is a confusing name for the column name and the row variable, though I agree apply is easiest way to do it :)</span>
<span class="comment-copy">Good comment, I've edited it so that the "x" confusion has gone!</span>
<span class="comment-copy">just to add, <code>apply</code> can also be applied to multiple columns: <code>df['c'] = df[['a','b']].apply(lambda x: do stuff with x[0] and x[1] here, axis=1)</code></span>
<span class="comment-copy">Can apply take in a function defined elsewhere in code? this is  so that we can introduce a more complicated function</span>
<span class="comment-copy">I renamed <code>x</code> -&gt; <code>col</code>. Better name</span>
