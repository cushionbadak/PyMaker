<div class="post-text" itemprop="text">
<p>I have large datasets from 2 sources, one is a huge csv file and the other coming from a database query. I am writing a validation script to compare the data from both sources and log/print the differences. One thing I think is worth mentioning is that the data from the two sources is not in the exact same format or the order. For example:</p>
<p>Source 1 (CSV files):</p>
<pre><code>email1@gmail.com,key1,1
email2@gmail.com,key1,3
email1@gmail.com,key2,1
email1@gmail.com,key3,5
email2@gmail.com,key3,2
email2@gmail.com,key3,2
email3@gmail.com,key2,3
email3@gmail.com,key3,1
</code></pre>
<p>Source 2 (Database):</p>
<pre><code>email                 key1     key2    key3
email1@gmail.com      1        1       5
email2@gmail.com      3        2       &lt;null&gt;
email4@gmail.com      1        1       5
</code></pre>
<p>The output of the script I want is something like:</p>
<pre><code>source1 - source2 (or csv - db):  2 rows total with differences
email2@gmail.com      3        2       2
email3@gmail.com      &lt;null&gt;   3       1

source2 - source1 (or db-csv):  2 rows total with differences
email2@gmail.com      3        2       &lt;null&gt;
email4@gmail.com      1        1       5
</code></pre>
<p>The output format could be a little different to show more differences, more clearly (from thousands/millions of records).</p>
<p>I started writing the script to save the data from both sources into two dictionaries, and loop through the dictionaries or create sets from the dictionaries, but it seems like a very inefficient process. I considered using pandas, but pandas doesn't seem to have a way to do this type of comparison of dataframes.</p>
<p>Please tell me if theres a better/more efficient way. Thanks in advance!</p>
</div>
<div class="post-text" itemprop="text">
<p>You were in the right path. What do you want is to quickly match the 2 tables. Pandas is probably overkill. </p>
<p>You probably want to iterate through you first table and create a dictionary. What you <strong>don't</strong> want to do, is interact the two lists for each elements. Even little lists will demand a large searches.</p>
<p>The <a href="https://docs.python.org/2/library/csv.html" rel="nofollow noreferrer">ReadCsv</a> module is a good one to read your data from disk. For each row, you will put it in a dictionary where the key is the email and the value is the complete row. In a common desktop computer you can iterate 10 millions rows in a second. </p>
<p>Now you will iterate throw the second row and for each row you'll use the email to get the data from the dictionary. See that this way, since the dict is a data structure that you can get the key value in O(1), you'll interact through N + M rows. In a couple of seconds you should be able to compare both tables. It is really simple. Here is a sample code:</p>
<pre><code>import csv
firstTable = {}
with open('firstTable.csv', 'r') as csvfile:
     reader = csv.reader(csvfile, delimiter=',')
        for row in reader:
            firstTable[row[0]] = row #email is in row[0]

for row2 in get_db_table2():
    email = row2[0]
    row1 = firstTable[email] #this is a hash. The access is very quick
    my_complex_comparison_func(row1, row2)
</code></pre>
<p>If you don't have enough RAM memory to fit all the keys of the first dictionary in memory, you can use the <a href="https://docs.python.org/3/library/shelve.html" rel="nofollow noreferrer">Shelve module</a> for the firstTable variable. That'll create a index in disk with very quick access. </p>
<p>Since one of your tables is already in a database, maybe what I'd do first is to use your database to load the data in disk to a temporary table. Create an index, and make a inner join on the tables (or outer join if need to know which rows don't have data in the other table). Databases are optimized for this kind of operation. You can then make a select from python to get the joined rows and use python for your complex comparison logic. </p>
</div>
<div class="post-text" itemprop="text">
<p>You can using <code>pivot</code> convert the df , the using <code>drop_duplicates</code> after <code>concat</code></p>
<pre><code>df2=df2.applymap(lambda x : pd.to_numeric(x,errors='ignore')
pd.concat([df.pivot(*df.columns).reset_index(),df2)],keys=['db','csv']).\
  drop_duplicates(keep=False).\
     reset_index(level=0).\
       rename(columns={'level_0':'source'})
Out[261]: 
key source             email  key1  key2    key3
1       db  email2@gmail.com     3     2       2
1      csv  email2@gmail.com     3     2  &lt;null&gt;
</code></pre>
<p>Notice , here I am using the <code>to_numeric</code> to convert to numeric for your df2</p>
</div>
<span class="comment-copy">Thanks @neves. I had also considered loading it into a temporary table, but I don't want to do that in the actual database. So I thought of doing it in a local sqlite table, but for the joins to work, I would have to load the data from the actual postgres database into the sqlite db, so I thought it would be too much back-and-forth data transfer. Or do you think its still a good idea to load everything into a local sqlite db?</span>
<span class="comment-copy">@siddardha, it isn't really necessary. The approach I described is probably good enough. After loading the data to a dictionary using the email as the key, you will match it very quickly. Write me if you need more help.</span>
