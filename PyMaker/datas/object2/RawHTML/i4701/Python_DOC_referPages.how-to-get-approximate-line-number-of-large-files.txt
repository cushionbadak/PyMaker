<div class="post-text" itemprop="text">
<p>I have CSV files with up to 10M+ rows.  I am attempting to get the total line numbers of a file so I can split the processing of each file into a multiprocessing approach.  To do this, I will set a start and end line for each sub-process to handle.  This cuts down my processing time from 180s to 110s for a file size of 2GB.  However, in order to do this, It requires to know the line number count.  If I attempt to get the exact line number count, it will take ~30seconds.  I feel like this time is wasted as an approximate with the final thread possibly having to read an extra hundred thousand lines or so, would only add a couple seconds as apposed to the 30 seconds it takes to get the exact line count.  </p>
<p>How would I go about getting an approximate line count for files? I would like this estimate to be within 1 million lines (Preferably within a couple hundred thousand lines).  Would something like this be possible?</p>
</div>
<div class="post-text" itemprop="text">
<p>This will be horribly inaccurate but it will get the size of a row and divide it against the size of the file.</p>
<pre><code>import sys
import csv
import os

with open("example.csv", newline="") as f:
    reader = csv.reader(f)
    row1   = next(reader)

    _Size = sys.getsizeof(len("".join(row1)))

print("Size of Line 1 &gt; ",_Size)
print("Size of File   &gt;",str(os.path.getsize("example.csv")))
print("Approx Lines   &gt;",(os.path.getsize("example.csv") / _Size))
</code></pre>
<blockquote>
<p>(Edit) If you change the last line to
  <code>math.floor(os.path.getsize("example.csv") / _Size)</code> It's actually
  quite accurate</p>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<p>I'd suggest you split the file into chunks of similar size, before even parsing.</p>
<p>The example code below will split <code>data.csv</code> into 4 chunks of approximately equal size, by seeking and searching for the next line break. It'll then call <code>launch_worker()</code> for each chunk, indicating the start offset and length of the data that worker should handle.</p>
<p>Ideally you'd use a <a href="https://docs.python.org/3/library/subprocess.html" rel="nofollow noreferrer"><code>subprocess</code></a> for each worker.</p>
<pre class="lang-py prettyprint-override"><code>import os

n_workers = 4

# open the log file, and find out how long it is
f = open('data.csv', 'rb')
length_total = f.seek(0, os.SEEK_END)

# split the file evenly among n workers
length_worker = int(length_total / n_workers)

prev_worker_end = 0

for i in range(n_workers):
    # seek to the next worker's approximate start
    file_pos = f.seek(prev_worker_end + length_worker, os.SEEK_SET)

    # see if we tried to seek past the end of the file... the last worker probably will
    if file_pos &gt;= length_total:                                            # &lt;-- (3)
        # ... if so, this worker's chunk extends to the end of the file
        this_worker_end = length_total

    else:
        # ... otherwise, look for the next line break
        buf = f.read(256)                                                   # &lt;-- (1)
        next_line_end = buf.index(b'\n')                                    # &lt;-- (2)

        this_worker_end = file_pos + next_line_end

    # calculate how long this worker's chunk is
    this_worker_length = this_worker_end - prev_worker_end
    if this_worker_length &gt; 0:
        # if there is any data in the chunk, then try to launch a worker
        launch_worker(prev_worker_end, this_worker_length)

    # remember where the last worker got to in the file
    prev_worker_end = this_worker_end + 1
</code></pre>
<p>Some expansion on markers in the code:</p>
<ol>
<li>You'll need to make sure that the <code>read()</code> will consume at least an entire line. Alternatively you could loop to perform multiple <code>read()</code>s if you don't know how long a line could be upfront.</li>
<li>This assumes <code>\n</code> line endings... you may need to modify for your data.</li>
<li>The last worker will get slightly less data to handle that the others... this is because we always search-forwards for the next line break. The more workers you have, the less data the final worker gets. It's not very significant (~200-500 bytes in my testing).</li>
</ol>
<p>Make sure you always use binary-mode, as text-mode can give you wonky <code>seek()</code>s / <code>read()</code>s.</p>
<p>An example <code>launch_worker()</code> would look like this:</p>
<pre class="lang-py prettyprint-override"><code>def launch_worker(offset, length):
    print('Starting a worker... using chunk %d - %d (%d bytes)...' 
           % ( offset, offset + length, length ))

    with open('log.txt', 'rb') as f:
        f.seek(offset, os.SEEK_SET)
        worker_buf = f.read(length)

    lines = worker_buf.split(b'\n')

    print('First Line:')
    print('\t' + str(lines[0]))
    print('Last Line:')
    print('\t' + str(lines[-1]))
</code></pre>
</div>
<span class="comment-copy">Are all the lines approximately the same length?</span>
<span class="comment-copy">@khelwood They should be, as it's all table data.</span>
<span class="comment-copy">Then dividing the total file length by the length of any one line will give you an approximate line count</span>
<span class="comment-copy">Have you attempted to code this? You can start with the pandas library.</span>
<span class="comment-copy">try a mapreduce job, it will take care of dividing data into equal splits on its own.</span>
<span class="comment-copy">Thanks, I think I will use an approach like this but take the average of the first 100 rows, seems like the best approach I can come up with.</span>
