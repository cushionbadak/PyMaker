<div class="post-text" itemprop="text">
<p>When it comes to IO read/write files from disks, it is almost blocking operations by default. I have been working on a project that uses such operations (read/write from disks) and uses the default <code>numpy</code> blocking IO <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.io.html" rel="nofollow noreferrer">here</a>. This has been a great pick until I found out that I am dealing with really big datasets!!</p>
<p>I have been trying to improve the execution time of my project. After doing a benchmark, I found out that IO operations are the bottleneck. Thus, I should now think of something else other than <code>numpy</code> default blocking IO. After reading for few days, I found out that I have three approaches to choose from that could reduce IO time:</p>
<ol>
<li>Non-blocking approach</li>
<li>Multi-threading approach</li>
<li>Multi-processing approach</li>
</ol>
<p>I would like to know which one of these approaches will be suitable to reduce the IO time, knowing that my IO operations are always performed on a disk (local disk). Plenty of libraries I came across such as <a href="https://github.com/twisted/twisted" rel="nofollow noreferrer">twisted</a>, <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow noreferrer">asyncio</a>, <a href="https://github.com/Tinche/aiofiles" rel="nofollow noreferrer">aiofiles</a>, <a href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing" rel="nofollow noreferrer">multiprocessing</a>, and <a href="https://docs.python.org/2/library/threading.html" rel="nofollow noreferrer">multithreading</a>. Because I have never worked with IO asynchronous or events-driven networking before, I am not sure what to choose from the three approaches above!! </p>
<p>Suggestions and thoughts from you guys are valuable to me. Thank you in advance </p>
<p><strong>EDIT:</strong></p>
<p>Special thanks to <code>mobiusklein</code> who brought the following points:</p>
<ul>
<li><strong>Does your program need all its data loaded before it can begin?</strong> 
Yes, but sometimes the program needs only to load a portion of the data from a file.</li>
<li><strong>Can it start writing some data to disk before all of the work is complete?</strong>
Yes, this is in fact what I am looking for.</li>
<li><strong>Does the "work" function ever release the GIL?</strong>
I don't get this question but my program makes use of multiprocessors via the library <a href="http://mpi4py.readthedocs.io/en/stable/" rel="nofollow noreferrer">mpi4py</a>. However, IO operations are always done by a single processor.</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>If all of the information needed to start writing output is located in one node of your MPI hierarchy, <em>and</em> you want that node to still be able to contribute to other computation, <em>and</em> your other computation calls C functions that release the GIL, you could spin up a thread on each node to carry out the I/O operation within the worker node. This avoids the overhead of transmitting data to a dedicated writer at the cost of increasing the unpredictability of the per-node workload.</p>
<p>If you need to aggregate the results of more than one worker before you can start writing, you would create a dedicated writer node in your MPI hierarchy, and funnel all data to it specifically, and make it the responsibility of that node to figure out when it is ready to write data out. This could be done with one thread to receive messages and one thread to do the actual writing as well. This may not be doable with plain MPI if the writer is not also the master, in which case you might need to mix in another flavor of IPC like <code>multiprocessing</code>.</p>
</div>
<span class="comment-copy">To answer this question, we'd need to know how data flows through your program. Does your program need all its data loaded before it can begin? Can it start writing some data to disk before all of the work is complete? Does the "work" function ever release the GIL?</span>
<span class="comment-copy">@mobiusklein Great points you mentioned. I will modify my post.</span>
