<div class="post-text" itemprop="text">
<p>I use the library <em>multiprocessing</em> in a <em>flask</em>-based web application to start long-running processes. The function that does it is the following: </p>
<pre><code>def execute(self, process_id):
    self.__process_id = process_id
    process_dir = self.__dependencies["process_dir"]
    self.fit_dependencies()
    process = Process(target=self.function_wrapper, name=process_id, args=(self.__parameters, self.__config, process_dir,))
    process.start()
</code></pre>
<p>When I want to deploy some code on this web application, I restart a service  that restarts <em>gunicorn</em>, served by <em>nginx</em>. My problem is that this restart kills all children processes started by this application as if a <em>SIGINT</em> signal were sent to all children. How could I avoid that ?</p>
<p><strong>EDIT:</strong> After reading <a href="https://stackoverflow.com/questions/21665341/python-multiprocessing-and-independence-of-children-processes">this post</a>, it appears that this behavior is normal. The answer suggests to use the <em>subprocess</em> library instead. So I reformulate my question: how should I proceed if I want to start long-running tasks (which are python functions) in a python script and make sure they would survive the parent process <strong>OR</strong> make sure the parent process (which is a gunicorn instance) would survive a deployement ?  </p>
<p><strong>FINAL EDIT:</strong> I chose @noxdafox answer since it is the more complete one. First, using process queuing systems might be the best practice here. Then as a workaround, I can still use <em>multiprocessing</em> but using the <em>python-daemon</em> context (see <a href="https://dpbl.wordpress.com/2017/02/12/a-tutorial-on-python-daemon/" rel="nofollow noreferrer">here</a> ans <a href="https://www.python.org/dev/peps/pep-3143/" rel="nofollow noreferrer">here</a>) inside the function wrapper. Last, @Rippr suggests using <em>subprocess</em> with a different process group, which is cleaner than forking with <em>multiprocessing</em> but involves having standalone functions to launch (in my case I start specific functions from imported libraries).     </p>
</div>
<div class="post-text" itemprop="text">
<p>I would recommend against your design as it's quite error prone. Better solutions would de-couple the workers from the server using some sort of queuing system (<code>RabbitMQ</code>, <code>Celery</code>, <code>Redis</code>, ...).</p>
<p>Nevertheless, here's a couple of "hacks" you could try out.</p>
<ol>
<li>Turn your child processes into UNIX daemons. The <a href="https://pypi.python.org/pypi/python-daemon/" rel="nofollow noreferrer">python daemon</a> module could be a starting point.</li>
<li><p>Instruct your child processes to ignore the <code>SIGINT</code> signal. The service orchestrator might work around that by issuing a <code>SIGTERM</code> or <code>SIGKILL</code> signal if child processes refuse to die. You might need to disable such feature. </p>
<p>To do so, just add the following line at the beginning of the <code>function_wrapper</code> function:</p>
<pre><code>signal.signal(signal.SIGINT, signal.SIG_IGN)
</code></pre></li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p>Ultimately, this problem comes down to misunderstanding of what it means to do the deployment.  In non-enterprise languages like Python (compared to enterprise ones like Erlang), it is generally understood that deployment wipes out any of the preceding artefacts of running the process.  As such, it would clearly be a bug if your old children/functions don't actually terminate once a new deployment is performed.</p>
<p>To play the devil's advocate, it is even unclear from your question/spec of what your actual expectation for the deployment is â€” do you simply expect your old "functions" to run forever?  How do those functions get started in the first place?  Who's supposed to know whether or not those "functions" were modified in a given deployment, and whether or not they're supposed to be restarted and in what fashion?  A lot of consideration to these very questions is given in Erlang/OTP (which are unrelated to Python), and, as such, you can't simply expect the machine to read your mind when you use a language like Python that's not even designed for such a use-case.</p>
<p>As such, it may be a better option to separate the long-running logic from the rest of the code, and perform the deployment appropriately.  As the other answer mentions, this may involve spawning a separate <a href="http://mdoc.su/-/daemon.3" rel="nofollow noreferrer">UNIX <code>daemon</code></a> directly from within Python, or maybe even using an entirely separate logic to handle the situation.</p>
</div>
<div class="post-text" itemprop="text">
<p>Adding on to @noxdafox's excellent <a href="https://stackoverflow.com/a/49406652/6740515">answer</a>, I think you can consider this alternative:</p>
<pre><code>subprocess.Popen(['nohup', 'my_child_process'], preexec_fn=os.setpgrp)
</code></pre>
<p>Basically, the child processes are killed because they belong to the same process group as the parent. By adding the <code>preexec_fn=os.setpgrp</code> parameter, you are simply requesting the child processes to spawn in their own process groups which means they will not receive the terminate signal.</p>
<p>Explanation taken from <a href="https://stackoverflow.com/a/16928558/6740515">here</a>.</p>
</div>
<span class="comment-copy">At a glance, I would try to decouple this method from the flask application to an entirely separate process. That way, the child processes won't die with the flask app. For instance, you might be able to delegate this work to a celery app.</span>
<span class="comment-copy">Thx. Celery seems to be the main recommended option for that. But it involves adding more dependencies to my project, which I would like to avoid. There is really no way to start a completely independent process from within a python script ?</span>
<span class="comment-copy">Still I don't understand why this is happening. The children should become child of init a keep going until their terminate their code.</span>
<span class="comment-copy">Depends what your needs are for communicating with that process, and the implementation may differ based on whether you're using Python2 or Python3 and what platform you're using. Check out <a href="https://stackoverflow.com/questions/27624850/launch-a-completely-independent-process">this question</a> which may have some answers for you.</span>
<span class="comment-copy">I handle the communication with files. I'm just trying to understand why the children are shut down instead of becoming orphans..</span>
<span class="comment-copy">Thank you, this is a good sum up. Could you just elaborate about the " it's quite error prone" part ?</span>
<span class="comment-copy">And what do you think of the use of subprocess ?</span>
<span class="comment-copy">As I said, several frameworks try to prevent child processes from leaking. Therefore you might see the child processes being hard killed even ignoring <code>SIGINT</code>. There are better solution for doing what you are trying to achieve: you can orchestrate the micro services via <code>docker-compose</code> or <code>supervisor</code> for example. <code>systemd</code> could also fit your purpose. These systems were designed to do what you are trying to achieve with bare python.</span>
<span class="comment-copy">To run a python function with <code>subprocess</code> you would need to write it in a separate file and call it via <code>Popen</code> for example. This would add quite a lot of overhead as you are basically starting a brand new Python interpreter. <code>multiprocessing</code> instead (on UNIX), smartly forks the parent process which is much faster.</span>
<span class="comment-copy">Deployment here refers to the fact that the web application sometimes needs to be updated (new code replaces old codes). But I want the processes launched by the app to keep running since they are atomic computations, i.e. unrelated to the web app itself. Turning them to UNIX deamon seems a good option to me.</span>
<span class="comment-copy">The fact that python isn't specifically designed for that isn't much a problem. There are plenty of ways to design such a process management, but as my question suggests, it s a tricky thing because its highly related to the inner UNIX structure.</span>
<span class="comment-copy">Thank you for the explanation about the process groups. To fit to my situation, it would be perfect to be able to do something like: multiprocessing.Process(function_wrapper, args, preexec_fn=os.setpgrp).</span>
<span class="comment-copy">Unfortunately, process groups are not yet supported in <code>multiprocessing</code> even though there is a stub for its implementation, <a href="https://docs.python.org/3/library/multiprocessing.html?highlight=process#multiprocessing.Process" rel="nofollow noreferrer">check this</a>.</span>
