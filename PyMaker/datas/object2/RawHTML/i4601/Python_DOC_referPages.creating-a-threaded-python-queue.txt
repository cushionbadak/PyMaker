<div class="post-text" itemprop="text">
<p>How would I go and create a queue to run tasks in the background in Python?</p>
<p>I have tried via asyncio.Queue() but whenever I use Queue.put(task) it immediately starts the task.</p>
<p>It is for an application which receives an unknown amount of entries (filenames) from a database on a specified time interval. What I wish to accomplish with this backgroundqueue would be that the python application keeps running and keeps returning new filenames. Everytime the application finds new filenames it should handle them by creating a task, which would contain (method(variables)). These tasks should all be thrown into an ever expanding queue which runs the tasks on its own. Here's the code.</p>
<pre><code>class DatabaseHandler:
def __init__(self):
    try:
        self.cnx = mysql.connector.connect(user='root', password='', host='127.0.0.1', database='mydb')
        self.cnx.autocommit = True
        self.q = asyncio.Queue()
    except mysql.connector.Error as err:
        if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:
            print("Something is wrong with your user name or password")
        elif err.errno == errorcode.ER_BAD_DB_ERROR:
            print("Database does not exist")
        else:
            print(err)
    self.get_new_entries(30.0)

def get_new_entries(self, delay):
    start_time = t.time()
    while True:
        current_time = datetime.datetime.now() - datetime.timedelta(seconds=delay)
        current_time = current_time.strftime("%Y-%m-%d %H:%M:%S")
        data = current_time
        print(current_time)
        self.select_latest_entries(data)
        print("###################")
        t.sleep(delay - ((t.time() - start_time) % delay))

def select_latest_entries(self, input_data):
    query = """SELECT FILE_NAME FROM `added_files` WHERE CREATION_TIME &gt; %s"""
    cursor = self.cnx.cursor()
    cursor.execute(query, (input_data,))
    for file_name in cursor.fetchall():
        file_name_string = ''.join(file_name)
        self.q.put(self.handle_new_file_names(file_name_string))
    cursor.close()

def handle_new_file_names(self, filename):
    create_new_npy_files(filename)
    self.update_entry(filename)

def update_entry(self, filename):
    print(filename)
    query = """UPDATE `added_files` SET NPY_CREATED_AT=NOW(), DELETED=1 WHERE FILE_NAME=%s"""
    update_cursor = self.cnx.cursor()
    self.cnx.commit()
    update_cursor.execute(query, (filename,))
    update_cursor.close()
</code></pre>
<p>As I said, this will instantly run the task. </p>
<p><code>create_new_npy_files</code> is a pretty time consuming method in a static class.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are two problems with this expression:</p>
<pre><code>self.q.put(self.handle_new_file_names(file_name_string))
</code></pre>
<p>First, it is actually <em>calling</em> the <code>handle_new_file_names</code> method and is enqueueing its <em>result</em>. This is not specific to <code>asyncio.Queue</code>, it is how function calls work in Python (and most mainstream languages). The above is equivalent to:</p>
<pre><code>_tmp = self.handle_new_file_names(file_name_string)
self.q.put(_tmp)
</code></pre>
<p>The second problem is that <code>asyncio.Queue</code> operations like <code>get</code> and <code>put</code> are <a href="https://docs.python.org/3/library/asyncio-queue.html#asyncio.Queue.put" rel="nofollow noreferrer">coroutines</a>, so you must await them.</p>
<p>If you want to enqueue a callable, you can use a <code>lambda</code>:</p>
<pre><code>await self.q.put(lambda: self.handle_new_file_names(file_name_string))
</code></pre>
<p>But since the consumer of the queue is under your control, you can simply enqueue the file names, as suggested by @dirn:</p>
<pre><code>await self.q.put(file_name_string)
</code></pre>
<p>The consumer of the queue would use <code>await self.q.get()</code> to read the file names and call <code>self.handle_new_file_names()</code> on each.</p>
<p>If you plan to use asyncio, consider <a href="http://stackabuse.com/python-async-await-tutorial/" rel="nofollow noreferrer">reading a tutorial</a> that covers the basics, and switching to an <a href="https://github.com/aio-libs/aiomysql" rel="nofollow noreferrer">asyncio compliant</a> database connector, so that the database queries play along with the asyncio event loop.</p>
</div>
<div class="post-text" itemprop="text">
<p>For people who see this in the future. The answer I marked as accepted is the explanation of how to solve the problem. I'll write down some code which I used to create what I wanted. That is, tasks that should run in the background. Here you go.</p>
<pre><code>from multiprocessing import Queue
import threading

class ThisClass
    def __init__(self):
        self.q = Queue()
        self.worker = threading.Thread(target=self._consume_queue)
        self.worker.start()
        self.run()
</code></pre>
<p>The queue created is not a queue for tasks, but for the variables you want to handle.</p>
<pre><code>def run(self):
    for i in range(100):
        self.q.put(i)
</code></pre>
<p>Then for the <code>_consume_queue()</code>, which consumes the items in the queue when there are items:</p>
<pre><code>def _consume_queue(self):
    while True:
        number = self.q.get()
        # the logic you want to use per number.
</code></pre>
<p>It seems the <code>self.q.get()</code> waits for new entries, even when there are none. </p>
<p>The -simplified- code above works for me, I hope it will also work for others.</p>
</div>
<span class="comment-copy">asynio runs in one thread. It is well suited to tasks with a lot of I/O and not so well suited for CPU intensive tasks. But you still have the option to use a queue and just spawn off one or more workers (which are fed by the queue) as threads.</span>
<span class="comment-copy">How would I go and do that? This whole async and threaded stuff is still very new to me</span>
<span class="comment-copy">Just as you would run any other thread. There are <a href="https://www.tutorialspoint.com/python/python_multithreading.htm" rel="nofollow noreferrer">some examples</a>.</span>
<span class="comment-copy">Can't seem to understand how that would work for me honestly. I get that I should use workers, but how can I assign a worker to a asyncio.queue?</span>
<span class="comment-copy">Why not just put the file names on the queue? Then you have a pool of asyncio tasks or threads (depending on which you decide to use) that read from the queue.</span>
<span class="comment-copy">This looks incorrect, but fixable. <code>q.put</code> will have no effect unless actually awaited, use <code>q.put_nowait</code> for a non-coroutine entry point. Second, <code>asyncio.Queue</code> is not thread-safe, and it is a serious programming error to invoke any of its methods from any thread except the one that runs the event loop. Even if that approach appears to work now, it will fail in even subtly different circumstances. To safely enqueue something from a different thread, use <code>asyncio.run_coroutine_threadsafe(loop, q.put(...))</code>.</span>
<span class="comment-copy">I didn't create an <code>asyncio.Queue</code>, I created an <code>multiprocessing.Queue</code>. Wouldn't that make a difference?</span>
<span class="comment-copy">In that case you don't need an asyncio event loop (or indeed any part of asyncio) at all - you can just use <code>Thread(target=self._consume_queue)</code>.</span>
<span class="comment-copy">You are right, just tested that out. I will edit it in.</span>
<span class="comment-copy">This solution is indeed much simpler. The question mentioned asyncio and was tagged <code>python-asyncio</code>, which led me to believe that you were interested to solve the problem with asyncio. asyncio is somewhat specific in that it requires a bit of learning, and basing the application on the async execution model. If you have no interest in using asyncio, please mention that in the question (or don't tag it with <code>python-asyncio</code>), to avoid confusion.</span>
