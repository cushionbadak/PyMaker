<div class="post-text" itemprop="text">
<p>I'm kind of new to Python. I am indexing through a website and scraping values off of it, but since there's like 100k pages to index through, it takes a lot of time. I was wondering how I could speed it up. I read that multithreading could be conflicting / not work for this and that multiprocessing would be the best way to start. </p>
<p>Here is an example of my code:</p>
<pre><code>def main():
    for ID in range(1, 100000):
        requests.get("example.com/?id=" + str(ID))
        #do stuff/print html elements off of url.  
</code></pre>
<p>If I do something like this:</p>
<pre><code>if __name__ == '__main__':
    for i in range(50):
        p = multiprocessing.Process(target=main)
        p.start()
</code></pre>
<p>It does run the function in parallel but I only want each process to scrape an ID that is not already being scraped by another process. if I do p.join() it doesn't seem to increase the speed at all than without multiprocessing so I'm not sure what to do.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here's an example based on the <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">concurrent.futures module.</a></p>
<pre><code>import concurrent.futures

# Retrieve a single page and report the URL and contents
def load_url(page_id, timeout):
   requests.get("example.com/?id=" + str(page_id))
   return do_stuff(request)  #do stuff on html elements off of url.  


# We can use a with statement to ensure threads are cleaned up promptly
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    # Start the load operations and mark each future with its URL
    future_to_url = {executor.submit(load_url, page_id, 60): page_id for page_id in range(1,100000)}
    for future in concurrent.futures.as_completed(future_to_url):
        url = future_to_url[future]
        try:
            data = future.result()
        except Exception as exc:
            print('%r generated an exception: %s' % (url, exc))
        else:
            print('%r page is %d bytes' % (url, len(data)))
</code></pre>
</div>
<span class="comment-copy">It looks like you probably want to use a multiprocessing <code>Pool</code> and <code>map</code> that function. The same approach as the first example <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">in the docs</a> to start off.</span>
<span class="comment-copy">If you're just doing requests, <a href="https://github.com/ross/requests-futures" rel="nofollow noreferrer"><code>requests-futures</code></a> is probably much easier for aynchronous requests with a thread pool than trying to use <code>multiprocessing</code></span>
<span class="comment-copy">Both thought of the same thing :) <a href="https://github.com/ross/requests-futures" rel="nofollow noreferrer">requests-futures</a> is a really simple wrapper for this approach, similar to how <code>requests</code> is to <code>urllib</code></span>
