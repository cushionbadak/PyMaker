<div class="post-text" itemprop="text">
<p>I have an I/O trace file with the following fields ('asu', 'block_address','size', 'opcode','time_stamp').
The Data looks like this. (over 5 million rows)</p>
<pre><code>0,20941264,8192,W,0.551706
0,20939840,8192,W,0.554041
0,20939808,8192,W,0.556202
1,3436288,15872,W,1.250720
1,3435888,512,W,1.609859
1,3435889,512,W,1.634761
0,7695360,4096,R,2.346628
1,10274472,4096,R,2.436645
2,30862016,4096,W,2 448003
2,30845544,4096,W,2.449733
1,10356592,4096,W,2.449733 
</code></pre>
<p>I am trying to add a cache layer in my project and want to calculate the misses and hits.
I am using <code>@functools.lru_cache(maxsize = None)</code>
to find cache hits and misses for the block_address.
Following the <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache" rel="nofollow noreferrer">tutorial</a> I tried calculating the miss/hits. <code>blk_trace</code> is the trace array for block_address.</p>
<pre><code>@functools.lru_cache(maxsize = None)
def blk_iter():
    blk_len = len(blk_trace)
    for i in range(0,blk_len):
        print(blk_trace[i])
</code></pre>
<p>On looking at the cache info <code>blk_iter.cache_info()</code> , I get <code>CacheInfo(hits=0, misses=1, maxsize=None, currsize=1)</code> . Which is not right.
I am fairly new to python and caching concepts. I don't know what I am doing wrong.
How do I find the miss/hits for the block address? </p>
</div>
<div class="post-text" itemprop="text">
<p>The cache is for the function <code>blk_iter</code> -- you only called <code>blk_iter</code> once, therefore your cache size is one, and it has one miss.</p>
<p>Consider the following function with <code>lru_cache</code></p>
<pre><code>@lru_cache(maxsize=None)
def myfunc(x):
    print('Cache miss: ', x)
    return x + 1
</code></pre>
<p>When called with a certain value for <code>x</code> the function will run and the result will be stored in the cache. If called again with the same parameter, the function will not run at all and the cached value will be returned.</p>
<pre><code>&gt;&gt;&gt; for i in range(3):
...     print(myfunc(i))
...
Cache miss:  0
1
Cache miss:  1
2
Cache miss:  2
3
&gt;&gt;&gt; myfunc(0) # this will be a cache hit
1
&gt;&gt;&gt; myfunc(3) # this will be another miss
Cache miss:  3
4
&gt;&gt;&gt; myfunc.cache_info()
CacheInfo(hits=1, misses=4, maxsize=None, currsize=4)   
</code></pre>
<p>In your example, even if the cache was setup correctly, you would have all misses and no hits anyhow <code>for i in range(0,blk_len):</code> will call with a new argument each iteration, therefore the cache will never hit.</p>
</div>
<span class="comment-copy">ASU is the application specific unit.</span>
<span class="comment-copy">The cache is for the function <code>blk_iter</code> -- you only called <code>blk_iter</code> once, therefore your cache size is one, and has one miss. Further, the return value of the function is what is cached. Not what it prints.</span>
<span class="comment-copy">Is there a way I can design cache for the blk_trace? Like append the address to a list and pass the list to my function.  <code>l = [1,2,3] #block_address list @lru_cache(maxsize=None) def stuff(a):    for x in a:       print a  stuff(l)</code></span>
<span class="comment-copy">You can, but like I mentioned, that would only result in 100% of the results being cache misses if all you're going to do is <code>for i in range(blk_len): func(i)</code> since <code>i</code> will be different every time. Also, if you did have a cache hit, the function will not be run (and therefore prints inside wont print anything) What are you hoping to accomplish with the cache?</span>
<span class="comment-copy">I am trying to predict Block IO sequences using seq2seq tensorflow model. The cache is a layer to monitor and keep track of the addresses. The idea is to speed up application performance with prefetching.</span>
<span class="comment-copy">Unfortunately I'm not that familiar with tensorflow. If you could make a function like <code>get_data(address):</code> that does the expensive operation of fetching data by address, maybe that would work with <code>lru_cache</code>; any address previously fetched can be gotten a second time without the expensive work, since the result would be cached. But each address would need to be fetched at least once before. I'm thinking that <code>lru_cache</code> may not be the best option for your use case. Consider raising a separate question relating specifically to tensorflow with the related code.</span>
<span class="comment-copy">Thank you so much for the explanation and direction @sytech .I appreciate that :)</span>
