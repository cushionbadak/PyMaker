<div class="post-text" itemprop="text">
<p>I am learning TensorFlow by implementing a simple logisitic regression classifier that outputs whether a digit is 7 or not when fed an MNIST image. I am using Stochastic gradient descent. The crux of the Tensorflow code is</p>
<pre><code># Maximum number of epochs
MaxEpochs = 1
# Learning rate
eta = 1e-2

ops.reset_default_graph()                                       
n_x = 784
n_y = 1

x_tf = tf.placeholder(tf.float32, shape = [n_x, 1], name = 'x_tf')
y_tf = tf.placeholder(tf.float32, shape = [n_y, 1], name = 'y_tf')    

w_tf = tf.get_variable(name = "w_tf", shape = [n_x, 1], initializer = tf.initializers.random_uniform());
b_tf = tf.get_variable(name = "b_tf", shape = [n_y, 1], initializer = tf.initializers.random_uniform());

z_tf = tf.add(tf.matmul(w_tf, x_tf, transpose_a = True), b_tf, name = 'z_tf')
yPred_tf = tf.sigmoid(z_tf, name = 'yPred_tf')

Loss_tf = tf.nn.sigmoid_cross_entropy_with_logits(logits = yPred_tf, labels = y_tf, name = 'Loss_tf')
with tf.name_scope('Training'):
    optimizer_tf = tf.train.GradientDescentOptimizer(learning_rate = eta)
    train_step = optimizer_tf.minimize(Loss_tf)

init = tf.global_variables_initializer()                                                 

with tf.Session() as sess:
    sess.run(init)
    for Epoch in range(MaxEpochs):
        for Sample in range(len(XTrain)):
            x = XTrain[Sample]
            y = YTrain[Sample].reshape([-1,1])
            Train_sample = {x_tf: x, y_tf: y}
            sess.run(train_step, feed_dict = Train_sample)

toc = time.time()
print('\nElapsed time is: ', toc-tic,'s');    
</code></pre>
<p>It builds the following graph (tensorboard related code has been removed for convenience):
<a href="https://i.stack.imgur.com/bLv9Q.png" rel="nofollow noreferrer"><img alt="Computational Graph of a single neuron" src="https://i.stack.imgur.com/bLv9Q.png"/></a></p>
<p>The problem is even though the weights and biases are initialised randomly (non-zero), the neuron isn't being trained. The weight histogram is as follows.</p>
<p><a href="https://i.stack.imgur.com/YJjiW.png" rel="nofollow noreferrer"><img alt="Weights" src="https://i.stack.imgur.com/YJjiW.png"/></a></p>
<p>I didnt want to post something so trivial, but I am at my wit's end. Sorry for the long post. Thank you very much in advance for any guidance. A little side note, it is taking 93.35s to run, it only took 10 or so seconds when I did this with numpy (same stochastic implementation), why would this be so?</p>
<p>EDIT:
The bias plot over the course of the training is as follows.
<a href="https://i.stack.imgur.com/qEqVv.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/qEqVv.png"/></a></p>
<p>EDIT: The entire code, if the issue is cropping up on something outside what I previously thought.</p>
<pre><code>import tensorflow as tf
import numpy as np
import h5py
from tensorflow.python.framework import ops
import time

mnist = tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data()

def Flatten(Im):
    FlatImArray = Im.reshape([Im.shape[0],-1,1])
    return FlatImArray

DigitTested = 7

# Sperating the images with 7s from the rest
TrainIdxs = [];
for i in range(len(y_train)):
    if(y_train[i] == DigitTested):
        TrainIdxs.append(i)

TestIdxs = [];
for i in range(len(y_test)):
    if(y_test[i] == DigitTested):
        TestIdxs.append(i)

# Preparing the Datasets for training and testing
XTrain = Flatten(x_train);
YTrain = np.zeros([len(x_train),1]);
YTrain[TrainIdxs] = 1;

XTest = Flatten(x_test);
YTest = np.zeros([len(x_test),1]);
YTest[TestIdxs] = 1;

tic = time.time()
# Maximum number of epochs
MaxEpochs = 1
# Learning rate
eta = 1e-2
# Number of Epochs after which the neuron is validated 
ValidationInterval = 1

ops.reset_default_graph()                                       # to be able to rerun the model without overwriting tf variables
n_x = 784
n_y = 1

x_tf = tf.placeholder(tf.float32, shape = [n_x, 1], name = 'x_tf')
y_tf = tf.placeholder(tf.float32, shape = [n_y, 1], name = 'y_tf')    

w_tf = tf.get_variable(name = "w_tf", shape = [n_x, 1], initializer = tf.initializers.random_uniform());
b_tf = tf.get_variable(name = "b_tf", shape = [n_y, 1], initializer = tf.initializers.random_uniform());

z_tf = tf.add(tf.matmul(w_tf, x_tf, transpose_a = True), b_tf, name = 'z_tf')
yPred_tf = tf.sigmoid(z_tf, name = 'yPred_tf')

Loss_tf = tf.nn.sigmoid_cross_entropy_with_logits(logits = yPred_tf, labels = y_tf, name = 'Loss_tf')
with tf.name_scope('Training'):
    optimizer_tf = tf.train.GradientDescentOptimizer(learning_rate = eta)
    train_step = optimizer_tf.minimize(Loss_tf)


writer = tf.summary.FileWriter(r"C:\Users\braja\Documents\TBSummaries\MNIST1NTF\2")             
tf.summary.histogram('Weights', w_tf)
tf.summary.scalar('Loss', tf.reshape(Loss_tf, []))
tf.summary.scalar('Bias', tf.reshape(b_tf, []))
merged_summary = tf.summary.merge_all()

init = tf.global_variables_initializer()                                                       

with tf.Session() as sess:
    sess.run(init)
    for Epoch in range(MaxEpochs):
        for Sample in range(len(XTrain)):
            x = XTrain[Sample]
            y = YTrain[Sample].reshape([-1,1])
            Train_sample = {x_tf: x, y_tf: y}
            MergedSumm, _ = sess.run([merged_summary, train_step], feed_dict = Train_sample)
            writer.add_summary(summary = MergedSumm, global_step = Sample)
        if((Epoch+1) %ValidationInterval == 0):
            ValidationError = 0
            for Sample in range(len(XTest)):
                x = XTest[Sample]
                y = YTest[Sample].reshape([-1,1])
                Test_sample = {x_tf: x, y_tf: y}
                yPred = sess.run(yPred_tf, feed_dict = Test_sample)
                ValidationError += abs(yPred - YTest[Sample])
            print('Validation Error at', Epoch+1,'Epoch:', ValidationError);

writer.add_graph(tf.Session().graph)
writer.close()
toc = time.time()
print('\nElapsed time is: ', toc-tic,'s');    
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Looking at the bias value it looks like you are seeing saturation of the sigmoid function.</p>
<p>This happens when you push your sigmoid input(<code>z_tf</code>) to the extreme ends of the sigmoid function. When this happens, the gradient returned is so low that the training stagnates. The probable cause of this is that it seems you have doubled up on sigmoid functions; <code>sigmoid_cross_entropy_with_logits</code> applies a sigmoid to its input, but you have implemented one yourself already. Try removing one of these. </p>
<p>In addition, by default <code>tf.initializers.random_uniform())</code> produces random values between 0:1. You probably want to initialise your Weights and biases symmetrically about 0 and at really small values to start with. This can be done by passing arguments <code>minval</code> and <code>maxval</code> to tf.initializers.random_uniform().</p>
<p>They should grow during training and again this prevents sigmoid saturation.</p>
</div>
<span class="comment-copy">It looks like the weights <i>are</i> being manipulated by the <code>train_step</code> operation (the values are changing), but not in a helpful way. The first thing I would do would be to try a lower learning rate. The way your weights oscilate back and forth, particularly in the early stages is symptomatic of overshooting in the learning process. See <a href="http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#the-gradient-descent-rule-in-action" rel="nofollow noreferrer">here</a> for a detailed description. If this is the issue, results can be drastically improved using learning rate decay.</span>
<span class="comment-copy">As for the time question. Tensorflow takes time to do things like instantiate the session and graph. It initially has to compute the back propagation differentiation mechanically, which is slow, but a one off calculation, meaning that in more complex problems this becomes a less significant. You should see that doubling the number of epochs only produces a marginal increase in train time.</span>
<span class="comment-copy">@FinleyGibson Than you so much for your comments. I initially tried with a very small value of learning rate, 1e-4. I only increased to see if my weights change at all. The weights are not changing. This is a histogram plot of the weights (784 in number), i.e, the jagged plot you see is just a histogram of 784 random values generated from a uniform distribution. The time axis is the axis that goes into the page. So you can notice that the individual weights themselves dont change over the steps. I am adding the bias plot over steps to clarify that. Regarding the speed, it mught be the case.</span>
<span class="comment-copy">okay, I'm with you. So time is along the Z axis.</span>
<span class="comment-copy">I am actually trying to classify a given image as '7' or 'not 7'. So it is a binary classification with just one neuron (trying to learn how to achieve it with one beofre moving to the whole problem). As for the bias jump at step one, that just the bias being assigned a random value during initialization (it is randomly initialized to 0.9392). It doesnt change during training. I didnt want to include the code where I sift through the MNIST to bunch the '7's together in the interest of brevity. But its prolly a good idea to add that. Thanks for pointing that.</span>
