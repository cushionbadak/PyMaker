<div class="post-text" itemprop="text">
<p>I am using python 2.7 on Windows machine. I have an array of urls accompanied by data and headers, so POST method is required. 
In simple execution it works well:</p>
<pre><code>    rescodeinvalid =[]
    success = []
    for i in range(0,len(HostArray)):
       data = urllib.urlencode(post_data)
       req = urllib2.Request(HostArray[i], data)
       response = urllib2.urlopen(req)
       rescode=response.getcode()

       if responsecode == 400:
            rescodeinvalid.append(HostArray[i])

       if responsecode == 200:
           success.append(HostArray[i])
</code></pre>
<p>My question is if HostArray length is very large, then it is taking much time in loop.
<strong>So, how to check each url of HostArray in a multithread.</strong> If response code of each url is 200, then I am doing different operation. I have arrays to store 200 and 400 responses.
So, how to do this in multithread in python</p>
</div>
<div class="post-text" itemprop="text">
<p>If you want to do each one in a separate thread you could do something like:</p>
<pre><code>  rescodeinvalid =[]
  success = []

  def post_and_handle(url,post_data)
       data = urllib.urlencode(post_data)
       req = urllib2.Request(url, data)
       response = urllib2.urlopen(req)
       rescode=response.getcode()

       if responsecode == 400:
              rescodeinvalid.append(url) # Append is thread safe
       elif responsecode == 200:
              success.append(url)  # Append is thread safe

  workers = []
  for i in range(0,len(HostArray)):
         t = threading.Thread(target=post_and_handle,args=(HostArray[i],post_data))
         t.start()
         workers.append(t)

  # Wait for all of the requests to complete
  for t in workers:
       t.join()
</code></pre>
<p>I'd also suggest using requests: <a href="http://docs.python-requests.org/en/latest/" rel="nofollow noreferrer">http://docs.python-requests.org/en/latest/</a> </p>
<p>as well as a thread pool:
<a href="https://stackoverflow.com/questions/3033952/python-thread-pool-similar-to-the-multiprocessing-pool">Threading pool similar to the multiprocessing Pool?</a> </p>
<p>Thread pool usage:</p>
<pre><code>from multiprocessing.pool import ThreadPool

# Done here because this must be done in the main thread
pool = ThreadPool(processes=50) # use a max of 50 threads

# do this instead of Thread(target=func,args=args,kwargs=kwargs))
pool.apply_async(func,args,kwargs)

pool.close() # I think
pool.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><a href="http://doc.scrapy.org/en/latest/intro/tutorial.html" rel="nofollow">scrapy</a> uses twisted library to call multiple urls in parallel without the overhead of opening a new thread per request, it also manage internal queue to accumulate and even prioritize them as a bonus you can also restrict number of parallel requests by settings <a href="http://doc.scrapy.org/en/latest/topics/settings.html#concurrent-requests" rel="nofollow">maximum concurrent requests</a>, you can either launch a scrapy spider as an external process or from your code, just set spider <code>start_urls = HostArray</code></p>
</div>
<div class="post-text" itemprop="text">
<p>Your case (basically processing a list into another list) looks like an ideal candidate for <code>concurrent.futures</code> (see for example <a href="https://stackoverflow.com/a/4963934/164233">this answer</a>) or you may go all the way to <a href="http://docs.python.org/3/library/concurrent.futures.html?highlight=concurrent.future#executor-objects" rel="nofollow noreferrer"><code>Executor.map</code></a>. And of course use <code>ThreadPoolExecutor</code> to limit the number of concurrently running threads to something reasonable.</p>
</div>
<span class="comment-copy">Possible duplicate of <a href="http://stackoverflow.com/questions/13481276/threading-in-python-using-queue" title="threading in python using queue">stackoverflow.com/questions/13481276/…</a> ? And be careful not to open too many sockets at once, see <a href="http://stackoverflow.com/questions/9487569/windows-limitation-on-number-of-simultaneously-opened-sockets-connections-per-ma" title="windows limitation on number of simultaneously opened sockets connections per ma">stackoverflow.com/questions/9487569/…</a></span>
<span class="comment-copy">possible duplicate of <a href="http://stackoverflow.com/questions/4119680/multiple-asynchronous-connections-with-urllib2-or-other-http-library">Multiple (asynchronous) connections with urllib2 or other http library?</a></span>
<span class="comment-copy">Thanks for answer. One question, if len(HostArray) is large, then how many thread will start. Is there any limitation with number of threads in windows can start.</span>
<span class="comment-copy">It'll start a thread for each, so it's best to use a thread pool (see the link).</span>
<span class="comment-copy">I looked into from multiprocessing.pool import ThreadPool, but i am not getting how to add this in our main code. Can u please suggest. Thanks</span>
<span class="comment-copy">Added to answer</span>
