<div class="post-text" itemprop="text">
<p>This is the function to read Url link and convert into Beautifulsoup</p>
<pre><code>multithreadding=[]
    def scraper_worker(url):
        r=requests.get(url)
        soup = BeautifulSoup(r.text,"html.parser")
        data=soup.find("div",{"class":"main-container"})
        multithreadding.append(data) 

threadding=[]
 for u in split_link:
     t=Thread(target=scraper_worker,args=(u, ))
     t.start()
     threadding.append(t)
</code></pre>
<p>split_link is the list where 50 odd links are stored.i am facing problem running the multithreadding part</p>
</div>
<div class="post-text" itemprop="text">
<p>It is example how to use <code>queue</code> to send results from thread to main thread.</p>
<pre><code>import requests
from bs4 import BeautifulSoup
from threading import Thread
import queue

# --- functions ---

def worker(url, queue): # get queue as argument
    r = requests.get(url)

    soup = BeautifulSoup(r.text, "html.parser")
    data = soup.find("span", {"class": "text"}).get_text()

    # send result to main thread using queue
    queue.put(data)

# --- main ---

all_links = [
    'http://quotes.toscrape.com/page/' + str(i) for i in range(1, 11)
]

all_threads = []
all_results = []
my_queue = queue.Queue()

# run threads
for url in all_links:
    t = Thread(target=worker, args=(url, my_queue))
    t.start()
    all_threads.append(t)

# get results from queue    
while len(all_results) &lt; len(all_links):
    # get result from queue
    data = my_queue.get()
    all_results.append(data)

    # or with queue.empty if loop has to do something more
    # because queue.get() wait for data if queue is empty and blocks loop

    #if not my_queue.empty():
    #    data = my_queue.get()
    #    all_results.append(data)

# display results        
for item in all_results:        
    print(item[:50], '...')        
</code></pre>
</div>
<span class="comment-copy">multithreadding=[]  def scraper_worker(url):     r=requests.get(url)     soup = BeautifulSoup(r.text,"html.parser")     data=soup.find("div",{"class":"main-container"})     multithreadding.append(data)</span>
<span class="comment-copy">put this in question, it will be more readable.</span>
<span class="comment-copy">what problem ? do you get error message ? always put full error message (Traceback) in question (as text, not screenshot). There are other useful informations.</span>
<span class="comment-copy">Maybe you should use queue to send result to main thread which will add result to list. You could also use queue to send next url to thread so you could run 10 threads instead of 50.</span>
<span class="comment-copy">see also <a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example" rel="nofollow noreferrer">ThreadPoolExecutor</a> to use less threads at the same time.</span>
<span class="comment-copy">thanks for the support.@furas</span>
