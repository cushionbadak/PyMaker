<div class="post-text" itemprop="text">
<p>Is it possible to train in parallel multiple different sklearn models?</p>
<p>For example, I'd like to train one SVM, one RandomForest and one Linear Regression model at the same time. The desired output would be a list of objects returned by the .fit method.</p>
</div>
<div class="post-text" itemprop="text">
<h2>Is it possible to train in parallel multiple different <code>sklearn</code> models?</h2>
<p>Training multiple models?<br/>
<strong>YES.</strong> </p>
<p>Training multiple models in <a href="https://stackoverflow.com/revisions/8337936/4">true-<strong><code>[PARALLEL]</code></strong> scheduling</a> fashion?<br/>
<strong>NO.</strong> </p>
<p>Training one particular model, using some sort of low-level, fine-grain ( if not directly silicon-wired ) sorts of vectorisation / ILP-parallelism and improved temporal-locality and effects of cache-coherence?<br/>
<strong>YES</strong>,<br/>
already deployed, if resources and low-level code permit, yet these levels are principally constrained by low ratio of work-package payload v/s overheads - ref. <a href="https://stackoverflow.com/revisions/18374629/3">re-formulated <strong>Amdahl's Law</strong></a> so as to respect both the overheads, resources ( on lower end of the time-scale ) and indivisible atomicity of some sorts of processing-sprint(s) ( on the upper end of the time-scale .. exactly due to the indivisible implementation of the atomic processing-segments so common in the <code>sklearn</code> ML-processing pipelines ).</p>
<p>Training different models in a <a href="https://stackoverflow.com/revisions/8337936/4">"just"-<strong><code>[CONCURRENT]</code></strong> scheduling</a> fashion?<br/>
<strong>YES.</strong><br/>
Using a smart <a class="post-tag" href="/questions/tagged/distributed-system" rel="tag" title="show questions tagged 'distributed-system'">distributed-system</a> infrastructure, not a few SLOCs :o)</p>
</div>
<span class="comment-copy">Sure - <a href="https://docs.python.org/3/library/threading.html" rel="nofollow noreferrer"><code>threading</code></a> / <a href="https://docs.python.org/3.6/library/multiprocessing.html" rel="nofollow noreferrer"><code>multiprocessing</code></a>!</span>
<span class="comment-copy">Note that the Random Forest code fits trees in parallel, so you can inspect that source to get an idea of how to implement your code.</span>
