<div class="post-text" itemprop="text">
<p>I have a python array of shape (19, 73984) - this represents 19 gray flatten images of 272 x 272px size. I want to be able to process this and feed it into a feed forward neural network but I want to be able to feed it in batches.
I expect to have some kind of a function that will be ran in a for loop. That function should receive the dataset array, batch size and also the iteration's index value in order to know how many items should return and from which position. 
ex: 
<code>def get_batch_data(i, dataset, batch_size):</code></p>
<p>where <code>i</code> is a for loop iteration index that will be used to return a chunk of data starting for a certain position until the <code>dataset</code> is looped over.
Is there a better way to do this or can you help me with this?
Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>Testdata:</p>
<pre><code>bigArr = [[x,x+1,x+2,x+3] for x in range(1,1000,4) ] # 250 subLists
</code></pre>
<hr/>
<p>Easiest would probably be <code>islice()</code> from <code>itertools</code>:</p>
<pre><code>print(list(itertools.islice(bigArr,5,10)))) # start 5, stop 10, implicit 1 step
</code></pre>
<p>Doku: <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow noreferrer">islice()</a> which takes your list, a <code>start</code> value , an <code>stop</code> value and a <code>stepper</code> - and does what you want as one-liner.</p>
<hr/>
<p>You could also leverage <code>itertools.compress</code> with a sliding <code>True</code> window for the elements you want: </p>
<pre><code># only show 5 to 10th (excluded) element:
varParts = itertools.compress(bigArr, # first list 
           [1 if x in range(5,10) else 0 for x in range(len(bigArr))]) # second list

# consume iterator:
print(list(varParts)) 
</code></pre>
<p><code>Compress</code> only returns values from the first list that evaluate to <code>True</code> in the second list - the second list is build in a way that only the wanted elements evaluate to <code>True</code></p>
<p>Doku: <a href="https://docs.python.org/3/library/itertools.html#itertools.compress" rel="nofollow noreferrer">compress</a></p>
<hr/>
<p>Or do all by hand using slicing for the big array like this:</p>
<pre><code>def get_batch_data(i, arr, batchSize): 
    return arr[i:min(len(arr),i+batchSize)] 
</code></pre>
<p>Use like this:</p>
<pre><code>for i in range(0,len(bigArr),5): 
    print(get_batch_data(i,bigArr,5)) # creates sub-slices - wich take memory
</code></pre>
</div>
<span class="comment-copy">Why the batch-file tag?  If your data is already in a Python array, why not just stay in the Python environment?</span>
<span class="comment-copy">I`m afraid I don't understand what you're implying</span>
<span class="comment-copy">The question is - what constitutes a <i>useful chunk</i>... It looks like you've already got a convenient chucked array of 19 items of images there... Does that not work for you/what problem are you trying to address?</span>
<span class="comment-copy">my apologies for not explaining too well. For now, they are 19 which seems a nice chunk of data to be fed into NN. But I`m trying to find a good mechanism assuming those 19 would be a bigger number (eg: thousands). In that case, the chunk functionality would come handy.</span>
<span class="comment-copy">What I mean though is you can chunk anything anyway you want but in this case it needs to make sense for your NN. If you had a thousand image files, then you'd loop over the images files and do those one at a time. If you had a multiple gigabyte file you might want to read it in blocks of 123456 bytes and use those. Or you may want to take a base array and perform N mutations on it and send each mutated array. Sure - people can give you best guess answers but whether they'll be any good to you at all without some specific criteria is uncertain.</span>
<span class="comment-copy">Awesome and complete answer. Thank you sir!</span>
