<div class="post-text" itemprop="text">
<p>I have a csv file which is ~40gb and 1800000 lines. </p>
<p>I want to randomly sample 10,000 lines and print them to a new file.</p>
<p>Right now, my approach is to use sed as:</p>
<pre><code>(sed -n '$vars' &lt; input.txt) &gt; output.txt
</code></pre>
<p>Where <code>$vars</code> is a randomly generated list of lines. (Eg: 1p;14p;1700p;...;10203p)</p>
<p>While this works, it takes about 5 minutes per execution. It's not a huge time, but I was wondering if anybody had ideas on how to make it quicker?</p>
</div>
<div class="post-text" itemprop="text">
<p>The biggest advantage to having lines of the same length is that you don't need to find newlines to know where each line starts. With a file size of ~40GB containing ~1.8M lines, you have a line length of ~20KB/line. If you want to sample 10K lines, you have ~40MB between lines. This is almost certainly around three orders of magnitude larger than the size of a block on your disk. Therefore, seeking to the next read location is much much more efficient than reading every byte in the file.</p>
<p>Seeking will work with files that have unequal line lenghs (e.g., non-ascii characters in UTF-8 encoding), but will require minor modifications to the method. If you have unequal lines, you can seek to an estimated location, then scan to the start of the next line. This is still quite efficient because you will be skipping ~40MB for every ~20KB you need to read. Your sampling uniformity will be compromised slightly since you will select byte locations instead of line locations, and you won't know which line number you are reading for sure.</p>
<p>You can implement your solution directly with the Python code that generates your line numbers. Here is a sample of how to deal with lines that all have the same number of bytes (usually ascii encoding):</p>
<pre><code>import random
from os.path import getsize

# Input file path
file_name = 'file.csv'
# How many lines you want to select
selection_count = 10000

file_size = getsize(file_name)
with open(file_name) as file:
    # Read the first line to get the length
    file.readline()
    line_size = file.tell()
    # You don't have to seek(0) here: if line #0 is selected,
    # the seek will happen regardless later.

    # Assuming you are 100% sure all lines are equal, this might
    # discard the last line if it doesn't have a trailing newline.
    # If that bothers you, use `math.round(file_size / line_size)`
    line_count = file_size // line_size
    # This is just a trivial example of how to generate the line numbers.
    # If it doesn't work for you, just use the method you already have.
    # By the way, this will just error out (ValueError) if you try to
    # select more lines than there are in the file, which is ideal
    selection_indices = random.sample(range(line_count), selection_count)
    selection_indices.sort()

    # Now skip to each line before reading it:
    prev_index = 0
    for line_index in selection_indices:
        # Conveniently, the default seek offset is the start of the file,
        # not from current position
        if line_index != prev_index + 1:
            file.seek(line_index * line_size)
        print('Line #{}: {}'.format(line_index, file.readline()), end='')
        # Small optimization to avoid seeking consecutive lines.
        # Might be unnecessary since seek probably already does
        # something like that for you
        prev_index = line_index
</code></pre>
<p>If you are willing to sacrifice a (very) small amount of uniformity in the distribution of line numbers, you can easily apply a similar technique to files with unequal line lengths. You just generate random byte offsets, and skip to the next full line after the offset. In the following implementation, it is assumed that you know for a fact that no line is longer than 40KB in length. You would have to do something like this if your CSV had non-ascii unicode characters encoded in UTF-8, because even if the lines all contained the same number of characters, they would contain different numbers of bytes. In this case, you would have to open the file in binary mode, since otherwise you might run into decoding errors when you skip to a random byte, if that byte happens to be mid-character:</p>
<pre><code>import random
from os.path import getsize

# Input file path
file_name = 'file.csv'
# How many lines you want to select
selection_count = 10000
# An upper bound on the line size in bytes, not chars
# This serves two purposes:
#   1. It determines the margin to use from the end of the file
#   2. It determines the closest two offsets are allowed to be and
#      still be 100% guaranteed to be in different lines
max_line_bytes = 40000

file_size = getsize(file_name)
# make_offset is a function that returns `selection_count` monotonically
# increasing unique samples, at least `max_line_bytes` apart from each
# other, in the range [0, file_size - margin). Implementation not provided.
selection_offsets = make_offsets(selection_count, file_size, max_line_bytes)
with open(file_name, 'rb') as file:
    for offset in selection_offsets:
        # Skip to each offset
        file.seek(offset)
        # Readout to the next full line
        file.readline()
        # Print the next line. You don't know the number.
        # You also have to decode it yourself.
        print(file.readline().decode('utf-8'), end='')
</code></pre>
<p>All code here is Python 3.</p>
</div>
<div class="post-text" itemprop="text">
<p>In case all lines have the same length, you could do it without the need to parse the whole file or load it to memory, using <code>dd</code>.</p>
<p>You have to know the lines number, having already executed <code>wc -l</code>, and the precise byte length of each line, and of course to have test and ensure all lines really have the same length. Even <code>wc</code> will be slow as it will read the whole file.</p>
<p>For example, if every line is 20000 bytes</p>
<pre><code>#!/bin/bash

for i in `shuf -n 10000 -i 0-1799999 | sort -n`
do
    dd if=file bs=20000 skip="$i" count=1 of=output status=none \
        oflag=append conv=notrunc
done
</code></pre>
<p>This way we loop and run 10K processes, I am not sure if it could be done at once, so although dd is faster, using a language, like Python and <code>seek()</code> method, (as @tripleee says and @Mad Physicist hinted at with comments) will have the advantage of one process.</p>
<pre><code>#!/usr/bin/python3
import random

randoms = random.sample(range(0, 1800000), 10000)
randoms.sort()

lsize = 20000

with open("file", "rb") as infile, open('output', 'wb') as outfile:
    for n in randoms:
        infile.seek(lsize * n)
        outfile.write(infile.read(lsize))
</code></pre>
<p>save some more seconds, if output is small enough, you can keep it in a bytearray and write it at once at the end.</p>
<pre><code>with open("file", "rb") as infile, open('output', 'wb') as outfile:
    buf = bytearray()
    for n in randoms:
        infile.seek(lsize * n)
        buf.extend(infile.read(lsize))
    outfile.write(buf)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If indeed your lines are all the same length, your Python script can randomly <a href="https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects" rel="nofollow noreferrer"><code>seek()</code></a> ahead in the file, and you know which index exactly to seek to in order to land exactly on the character after a newline.</p>
<p>The Python script which generates the random indices for your <code>sed</code> script should be easy to adapt to this approach. Basically, when you generate <code>123p</code> to feed into <code>sed</code>, instead seek to 122*line length and read the line you land at.</p>
<p>A complication is that Python 3 prohibits random seeks in files which are opened in text mode (because it needs to know where enooded characters start and end). For a quick and dirty script, simply reading and writing bytes should be fine (in general the recommendation is to decode bytes into Unicode, then encode again before writing; but since you aren't processing the lines in Python at all, this is unnecessary).</p>
</div>
<div class="post-text" itemprop="text">
<p>For testing purposes, let's create a file of 1,800,000 lines:</p>
<pre><code>$ awk 'BEGIN {for (i=1; i&lt;=1800000; i++) print "line " i}' &gt;file
$ ls -l file
-rw-r--r--  1 dawg  wheel  22288896 Jan  1 09:41 file
</code></pre>
<p>Assuming you don't know the number of lines in that file, the fastest way to get the total number of lines is with the POSIX utility <code>wc</code>:</p>
<pre><code>$ time wc -l file
 1800000 file

real    0m0.018s
user    0m0.012s
sys 0m0.004s
</code></pre>
<p>So to get the total line count of a text file with 1,800,000 lines is pretty fast.</p>
<p>Now that you know the total number of lines, you can use <code>awk</code> to print a random sample of those lines:</p>
<pre><code>#!/bin/bash

lc=($(wc -l file))
awk -v lc="$lc" -v c=10000 '
BEGIN{srand()}
int(lc*rand())&lt;=c{print; i++}
i&gt;=c{exit}
' file &gt;rand_lines
</code></pre>
<p>That runs in about 200 ms on my older iMac. Note that the total is <em>close</em> to 10,000 but likely less since you will hit the end of the file often before you hit 10,000 lines. </p>
<p>If you want exactly 10,000 at a penalty of true randomness, you can do:</p>
<pre><code>awk -v lc="$lc" -v c=10000 '
BEGIN{srand()}
int(lc*rand())&lt;c * (1.01 or a factor to make sure that 10,000 is hit before EOF) {print; i++}
i&gt;=c{exit}
' file &gt;rand_lines
</code></pre>
<p>Or, alternatively, generate 10,000 unique numbers between 1 and the number of lines:</p>
<pre><code>awk -v lc="$lc" -v c=10000 '
BEGIN{srand()
      while (i&lt;c) {
        x=int(lc*rand())
        if (x in rl) continue  # careful if c is larger than or close to lc
        else {
        rl[x]
        i++} 
        }
     }
NR in rl' file &gt;rand_lines
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You will want to insert the data into a database (such as sqlite or mysql), and then repeat your idea in SQL</p>
<pre><code>select * from your_table where id in (1, 14, 1700, ...)
</code></pre>
<p>You can also read up how to select a random sample from this excellent tutorial <a href="http://jan.kneschke.de/projects/mysql/order-by-rand/" rel="nofollow noreferrer">http://jan.kneschke.de/projects/mysql/order-by-rand/</a> and </p>
<p>There is no way devise a shell script that would run significantly faster, as your code ultimately relies on the way filesystems fundamentally work. That is, for good performance you want to access the disk sequentially and in chunks. Databases are designed to solve this issue by storing how the data is laid out in the harddrive in a separate file called the <em>index</em>. It works much in the same way as an index of a book. </p>
<p>This is a rich topic and requires some learning. If you're new to database programming, 40 gb dataset is a good starting point, though.</p>
</div>
<div class="post-text" itemprop="text">
<p>Yet another idea, borrowed from the world of Monte Carlo simulations, would be to loop over the lines and generate a random number in each iteration. Now, if you want 10k lines from a set of 180k lines, you'd reason as follows. There is a 10/180 change that you want to include the row in question. If the random number is less than or equal to 10/180, you accept the row. Otherwise you reject it or break the loop if the desired amount of rows has been collected.</p>
<p>The downside of this approach that there is no guarantee that exactly 10k lines are sampled. I am also suspicious that there are biases in this approach and that it will not be random enough.</p>
</div>
<span class="comment-copy">How are you generating the list?  Is it the <code>sed</code> that is taking time, or the generation of the list?  Probably faster to do the whole thing in perl.</span>
<span class="comment-copy">While other tools may give slightly faster results, keep in mind that disk  I/O will always be the bottleneck here. To count lines, you have to find newlines, which means scanning every byte in the file.</span>
<span class="comment-copy">Also, if you're already in Python, you can use itertools to get the lines. No need for a subprocess</span>
<span class="comment-copy">I haven't understood the question I think, why not use <code>shuf -n10000 input.txt &gt; output.txt</code> ? (note that shuf will shuffle the lines first, so if order is important, mention in question)</span>
<span class="comment-copy">I really like this question because, A) it is just interesting, and B) it got a lot of interesting answers that taught me lots of things, even if I have the feeling that many of the posters did not realize that you are talking about 20KB lines.</span>
<span class="comment-copy">That's great, thank you so much!!! :)</span>
<span class="comment-copy">@MrD. You should also give @ thanasisp some credit. He had the same idea around the time I was writing this answer.</span>
<span class="comment-copy">I upvoted his answer too. I'll keep yours as the accepted answer as you've been following through with me since yesterday. :)</span>
<span class="comment-copy">@MrD. I appreciate that.</span>
<span class="comment-copy">Besides the questionable part where you want to start 10K dd processes, this is exactly what I had in mind. I finally posted an answer, and +1 to you.</span>
<span class="comment-copy">same for the answer you posted. I tested that <code>dd</code> is faster for few selections but I think it can't be done without the loop.</span>
<span class="comment-copy">Upvoted this too, thanks!</span>
<span class="comment-copy">@MrD This is an interesting question, with a lot of information around, useful for me too, thanks.</span>
<span class="comment-copy">I'll further comment that calling <code>sed</code> from Python seems like a poor approach. Performing the same logic entirely in Python would not be hard at all, and might well be more efficient because you don't need to keep the entire list of selected lines in memory at once, as well as of course because you avoid the overhead - and portability issues - of an external process.</span>
<span class="comment-copy">I believe this approach is what @MadPhysicist alluded to in his comments.</span>
<span class="comment-copy">By my calculation, OP's lines are ~20KB each. I think that will affect your timing by a few orders of magnitude. Your sample file is on the order of a few megabytes, OP's is 40GB. That does make a difference.</span>
<span class="comment-copy">@MadPhysicist: Fair point. However, the methodology is likely one of the faster ways to do this.</span>
<span class="comment-copy">I'm writing up an answer that I think will be much faster, but it will require a fair amount of coding unfortunately.</span>
<span class="comment-copy">If you process a new data set every time you do this, the cost of creating an index etc may well outweigh the benefit of the speed when you've done that. If you process the same data set multiple times, this is probably a good solution; and the more times you process it, the smaller the relative amortized cost of setting up the database for this will be.</span>
<span class="comment-copy">"There is no way devise a shell script that would run significantly faster". There is though. You can seek in a file, skipping about 90% of the reads you would normally have to go through.</span>
<span class="comment-copy">This has nothing to do with OP's problem</span>
<span class="comment-copy">Also, the probability wouldn't be fixed at 10/180. The probability of choosing line <code>i</code>, given that <code>k</code> lines have already been chosen, will be <code>p = (10000 - k)/(180000 - i)</code>.</span>
