<div class="post-text" itemprop="text">
<p>when i run my program (web crawler) in parallel it takes unusual amount of ram or memory through my system, i also tested with other web crawlers and my web crawler is taking twice as much as ram as they do , so my question is 
               <strong>how can i manually manage memory or ram in python, (if possible) ?</strong></p>
<p>here is my code:-</p>
<pre><code>from bs4 import BeautifulSoup
import requests
import MySQLdb as sql
import time
import warnings

print("starting")

warnings.filterwarnings('ignore')

db = sql.connect("localhost", "root", "arpit", "website")
cursor = db.cursor()
db.autocommit(True)

print("connected to database")

url = "http://www.example.com"
extension = ".com"
print("scrapping url -",url)

r = requests.head(url)
cursor.execute("insert ignore into urls(urls,status,status_code)     
values(%s,'pending',%s)", [url, r.status_code])

cursor.execute("select status from urls where status ='pending' limit 1")
result = str(cursor.fetchone())

while (result != "None"):

cursor.execute("select urls from urls where status ='pending' limit 1")
result = str(cursor.fetchone())

s_url = result[2:-3]

cursor.execute("update urls set status = 'done' where urls= %s ", [s_url])

if "https" in url:
    url1 = url[12:]
else:
    url1 = url[11:]
zone = 0
while True:

    try:
        r = requests.get(s_url,timeout=60)
        break

    except:
        if s_url == "":

            print("done")
            break
        elif zone &gt;= 4:
            print("this url is not valid -",s_url)
            break
        else:
            print("Oops!  may be connection was refused.  Try again...",s_url)
            time.sleep(0.2)
            zone = zone + 1

soup = BeautifulSoup(r.content.lower(), 'lxml')

links = soup.find_all("a")

for x in links:
    a = x.get('href')
    if a is not None and a != "":

        if a != "" and a.find("\n") != -1:
            a = a[0:a.find("\n")]

        if a != "" and a[-1] == "/":
            a = a[0:-1]

        if a != "":
            common_extension = [',',' ',"#",'"','.mp3',"jpg",'.wav','.wma','.7z','.deb','.pkg','.rar','.rpm','.tar','.zip','.bin','.dmg','.iso','.toast','.vcd','.csv','.dat','.log','.mdb','.sav','.sql','.apk','.bat','.exe','.jar','.py','.wsf','.fon','.ttf','.bmp','.gif','.ico','.jpeg','.png','.part','.ppt','.pptx','.class','.cpp','.java','.swift','.ods','.xlr','.xls','.xlsx','.bak','.cab','.cfg','.cpl','.dll','.dmp','.icns','.ini','.lnk','.msi','.sys','.tmp','.3g2','.3gp','.avi','.flv','.h264','.m4v','.mkv','.mov','.mp4','.mpg','.vob','.wmv','.doc','.pdf','.txt']
            for ext in common_extension:
                if ext in a:
                    a = ""
                    break

        if a != "":
            if a[0:5] == '/http':
                a = a[1:]
            if a[0:6] == '//http':
                a = a[2:]

            if a[0:len(url1) + 12] == "https://www." + url1:
                cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                               [a, r.status_code])
            elif a[0:len(url1) + 11] == "http://www." + url1:
                cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                               [a, r.status_code])
            elif a[0:len(url1) + 8] == "https://" + url1:
                cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                               [url + (a[(a.find(extension + "/")) + 4:]), r.status_code])
            elif a[0:len(url1) + 7] == "http://" + url1:
                cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                               [url + (a[(a.find(extension + "/")) + 4:]), r.status_code])
            elif a[0:2] == "//" and a[0:3] != "///" and "." not in a and "http" not in a and "www." not in a:
                cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                               [url + a[1:], r.status_code])
            elif a[0:1] == "/" and a[0:2] != "//" and "." not in a and "http" not in a and "www." not in a:
                cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                               [url + a[0:], r.status_code])
            elif 'http' not in a and 'www.' not in a and "." not in a and a[0] != "/":
                cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                               [url + '/' + a, r.status_code])

cursor.execute("alter table urls drop id")
cursor.execute("alter table urls add id int primary key not null  
auto_increment first")
print("new id is created")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Your code is very memory inefficient because you're doing <em>a lot</em> of slicing - and because strings are immutable, each slice allocates a new object.</p>
<p>So for example:</p>
<pre><code>if a[0:5] == '/http'
   a = a[1:]
</code></pre>
<p>Allocates a new string, copies <code>a</code> from <code>0</code> to <code>5</code> unto it, compares it to <code>'/http'</code>, and throws it away; furthermore, if it tested equal, it allocates a new string, copies <code>a</code> from <code>1</code> on unto it, and throws <code>a</code> away. And if <code>a</code> is long, or if this happens a lot, this can become quite a problem.</p>
<p>Check out <a href="https://docs.python.org/3/library/stdtypes.html#memoryview" rel="nofollow noreferrer"><code>memoryview</code>s</a> - it's a way to slice strings (well, <code>bytes</code> in Python 3) without copying them.</p>
<p>There are a lot of other ways you can optimize your code:</p>
<ol>
<li><p>Instead of re-defining <code>common_extension</code> for every link, define it once before the loop.</p></li>
<li><p>Instead of <code>a[0:5] == '/http'</code>, use <code>a.startswith('/http')</code>.</p></li>
<li><p>Instead of first 4 <code>url1</code> comparisons, use a regular expression like <code>re.match('https?://(www\.)?' + re.escape(url1), a)</code>.</p>
<p>And if you're doing that, instead of concatenating <code>'https?://(www\.)?'</code> and <code>re.escape(url1)</code> for for every link, do it once before the loop, and even <code>re.compile</code> the regular expression there.</p></li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p>You can't <em>manage</em> memory directly in Python in the sense that you can't allocate and /or free memory chunks otherwise than creating and deleting <em>objects</em>. What you can do is use tools to understand which pieces of your code use how much memory. For details, see, for example, <a href="https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python" rel="nofollow noreferrer">https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python</a></p>
<p>In python, the way to limiting memory usage then is obviously don't create large lists of objects, or other data structures. Use callbacks, yields, and /or other coding practices to limit the amount of time your structures spend in memory. </p>
<p>Also  I would recommend to submit your code to code review SE, I think they might be able to help as well. </p>
</div>
<div class="post-text" itemprop="text">
<p>The short answer is, you can't manually manage memory. However, your question really should be, how do I <em>reduce</em> the amount of memory Python uses?</p>
<p>First, know that there is a difference between the amount of memory your program <em>allocates</em> vs how much it <em>uses</em>.</p>
<p>Second, avoid allocating memory you don't need to. Each slice operation creates a new <code>str</code> object. Instead, use the <code>startswith</code> method:</p>
<pre><code>if not a:
    if a.startswith('/http'):
        a = a[1:]
    if a.startswith('//http'):
        a = a[2:]

    if a[.startswith("https://www." + url1):
        cursor.execute("insert ignore into urls(urls,status,status_code) values(%s,'pending',%s)",
                       [a, r.status_code])
</code></pre>
<p>etc.</p>
</div>
<span class="comment-copy">This isn't valid Python. Your indentation is incorrect, which means it's unclear what's happening. Can you please fix your indentation? To what level have you profiled RAM usage? Are you checking individual threads or just overall usage? Are you including your database in that?</span>
<span class="comment-copy">BTW if you want efficient crawling, try looking as <code>scrapy</code>.</span>
