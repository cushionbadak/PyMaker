<div class="post-text" itemprop="text">
<p>I need to iterate over the words in a file. The file could be very big (over 1TB), the lines could be very long (maybe just one line). Words are English, so reasonable in size. So I don't want to load in the whole file or even a whole line.</p>
<p>I have some code that works, but may explode if lines are to long (longer than ~3GB on my machine).</p>
<pre><code>def words(file):
    for line in file:
        words=re.split("\W+", line)
        for w in words:
            word=w.lower()
            if word != '': yield word
</code></pre>
<p>Can you tell be how I can, simply, rewrite this iterator function so that it does not hold more than needed in memory?</p>
</div>
<div class="post-text" itemprop="text">
<p>Don't read line by line, read in buffered chunks instead:</p>
<pre><code>import re

def words(file, buffersize=2048):
    buffer = ''
    for chunk in iter(lambda: file.read(buffersize), ''):
        words = re.split("\W+", buffer + chunk)
        buffer = words.pop()  # partial word at end of chunk or empty
        for word in (w.lower() for w in words if w):
            yield word

    if buffer:
        yield buffer.lower()            
</code></pre>
<p>I'm using the callable-and-sentinel version of the <a href="http://docs.python.org/2/library/functions.html#iter" rel="nofollow"><code>iter()</code> function</a> to handle reading from the file until <code>file.read()</code> returns an empty string; I prefer this form over a <code>while</code> loop.</p>
<p>If you are using Python 3.3 or newer, you can use <a href="http://docs.python.org/3/whatsnew/3.3.html#pep-380" rel="nofollow">generator delegation</a> here:</p>
<pre><code>def words(file, buffersize=2048):
    buffer = ''
    for chunk in iter(lambda: file.read(buffersize), ''):
        words = re.split("\W+", buffer + chunk)
        buffer = words.pop()  # partial word at end of chunk or empty
        yield from (w.lower() for w in words if w)

    if buffer:
        yield buffer.lower()            
</code></pre>
<p>Demo using a <em>small</em> chunk size to demonstrate this all works as expected:</p>
<pre><code>&gt;&gt;&gt; demo = StringIO('''\
... Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque in nulla nec mi laoreet tempus non id nisl. Aliquam dictum justo ut volutpat cursus. Proin dictum nunc eu dictum pulvinar. Vestibulum elementum urna sapien, non commodo felis faucibus id. Curabitur
... ''')
&gt;&gt;&gt; for word in words(demo, 32):
...     print word
... 
lorem
ipsum
dolor
sit
amet
consectetur
adipiscing
elit
pellentesque
in
nulla
nec
mi
laoreet
tempus
non
id
nisl
aliquam
dictum
justo
ut
volutpat
cursus
proin
dictum
nunc
eu
dictum
pulvinar
vestibulum
elementum
urna
sapien
non
commodo
felis
faucibus
id
curabitur
</code></pre>
</div>
<span class="comment-copy">See python how to read a file character by character <a href="http://stackoverflow.com/questions/2988211/how-to-read-a-single-character-at-a-time-from-a-file-in-python" title="how to read a single character at a time from a file in python">stackoverflow.com/questions/2988211/â€¦</a>  If the word is the entire line you may still have a problem but maybe in the that case you can drop it in advance</span>
<span class="comment-copy">Related: <a href="http://stackoverflow.com/q/19600475/222914">How to read records terminated by custom separator from file in python?</a></span>
<span class="comment-copy">After code reviewing, I ran it, but changed chunk size to <code>3</code>. No problems found.</span>
<span class="comment-copy">The only think I don't like is it is not super elegant. Can someone else produce something more simple?</span>
<span class="comment-copy">@richard: In Python 3.3 and up, you can use <code>yield from map(str.lower, filter(None, words))</code> instead of the <code>for word in words:</code> loop, and you could use a <code>while chunk:</code> loop with two <code>chunk = file.read(buffersize)</code> calls (one to prime the <code>while</code> loop, one in the loop) instead of the <code>iter()</code>-with-callable-and-sentinel construct I used, but unless you go for the <code>mmap</code> solution, this is pretty much it.</span>
<span class="comment-copy">Note: the mmap answer will not work, as it tries to map a potentially huge file (&gt;1TB) into a potentially small process (~3Gb). (The answer has now been deleted)</span>
