<div class="post-text" itemprop="text">
<p>I have a python function that has to run 12 times in total. I have this set up currently to use Pool from the multiprocessing library to run up to all of them in parallel. Typically I run 6 at a time because the function is CPU intensive and running 12 in parallel often causes the program to crash. When we do 6 at a time, the second set of 6 will not begin until all of the first 6 processes are finished. Ideally, we would like another one (e.g. the 7th) to kick off as soon as one from the initial batch of 6 is finished- So that 6 are running at once while there are more to start. Right now the code looks like this (it would be called twice, passing the first 6 elements in one list and then the second 6 in another:</p>
<pre><code>from multiprocessing import Pool

def start_pool(project_list):

    pool = Pool(processes=6)
    pool.map(run_assignments_parallel,project_list[0:6])
</code></pre>
<p>So i have been trying to implement a worker/queue solution and have run into some issues. I have a worker function that looks like this:</p>
<pre><code>def worker(work_queue, done_queue):
    try:
        for proj in iter(work_queue.get, 'STOP'):
            print proj
            run_assignments_parallel(proj)
            done_queue.put('finished ' + proj )
    except Exception, e:        
        done_queue.put("%s failed on %s with: %s" % (current_process().name, proj,        e.message))
    return True
</code></pre>
<p>And the code to call the worker function is as follows:</p>
<pre><code>workers = 6
work_queue = Queue()
done_queue = Queue()  
processes = []
for project in project_list:
    print project
    work_queue.put(project)
for w in xrange(workers):        
    p = Process(target=worker, args=(work_queue, done_queue))
    p.start()
    processes.append(p)
    work_queue.put('STOP')
for p in processes:
     p.join()    
     done_queue.put('STOP')
for status in iter(done_queue.get, 'STOP'):        
    print status
</code></pre>
<p>project_list is just a list of paths for the 12 projects that need to be run in the function 'run_assignments_parallel.' </p>
<p>The way this is written now, the function is getting called more than once for the same process (project) and I cant really tell what is going on. This code is based on an example i found and I am pretty sure the looping structure is messed up. Any help would be great and I aplogize for my ignorance on the matter. Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>Ideally, we would like another one (e.g. the 7th) to kick off as soon as one from the initial batch of 6 is finished- So that 6 are running at once while there are more to start. </p>
</blockquote>
<p>All you need to change is to pass all 12 input parameters instead of 6:</p>
<pre><code>from multiprocessing import Pool
pool = Pool(processes=6) # run no more than 6 at a time
pool.map(run_assignments_parallel, project_list) # pass full list (12 items)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can use the <a href="http://vmlaker.github.io/mpipe" rel="noreferrer">MPipe</a> module. </p>
<p>Create a 6-worker, single-stage pipeline and feed in all your projects as <em>tasks</em>. Then just read the <em>results</em> (in your case, statuses) off the end.</p>
<pre><code>from mpipe import Pipeline, OrderedStage

...    

pipe = Pipeline(OrderedStage(run_assignments_parallel), 6)    

for project in project_list:
   pipe.put(project)

pipe.put(None)  # Signal end of input.

for status in pipe.results():
   print(status)
</code></pre>
</div>
<span class="comment-copy">Well with the indetation you have you will get errors please check it all</span>
<span class="comment-copy">pxl fixed the obvious format but given that are all the other lines correct - the put('SOP') don't agree with the Python docs example</span>
<span class="comment-copy">Thanks! This is indeed correct and something I had tried before and now I realize (again) why my issue is a little more complicated- the function uses an API from some proprietary software that invokes an object that cannot be destroyed and recreated. In other words, each project has to be run on a separate process, which is why the start_pool function gets called again for the second set of 6. Any ideas on how to get around that?</span>
<span class="comment-copy">@user2503169: a) check that the object is already created e.g., make it a global and don't try to recreate it if it is not <code>None</code> b) Or if you want a new process for each task: <a href="http://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer"><code>maxtasksperchild=1</code></a> c) Create 12 <code>Process</code> and use <code>Semaphore</code> to avoid more than 6 running.</span>
<span class="comment-copy">Looks like maxtaskperchild would be an easy solution but I am running 2.6 (because of the proprietary software). Can you tell me how to use Semaphore? I really appreciate the help!</span>
<span class="comment-copy">@user2503169: Create <code>Semaphore</code> in the main process with value <code>6</code>. Pass it two all child processes (12), wrap your code <code>with sem: ..your child code here..</code>. Every time the processing is entered in <code>with sem</code> statement, <code>sem.acquire()</code> is called automatically that decrements the semaphore value. If it hits zero then <code>sem.acquire()</code> blocks until <code>sem.release()</code> is called (automatically on the exit from <code>with sem:</code>). For example, see how <a href="http://stackoverflow.com/a/16686329/4279"><code>RateSemaphore()</code> is used</a> (your case is simpler).</span>
<span class="comment-copy">Like this?: 'maxconnections = 6 sem = BoundedSemaphore(value=maxconnections) with sem: pool = Pool(processes=12) pool.map(run_assignments_parallel,project_list)'</span>
