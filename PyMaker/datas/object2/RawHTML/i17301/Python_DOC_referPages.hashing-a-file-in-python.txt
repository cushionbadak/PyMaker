<div class="post-text" itemprop="text">
<p>I want python to read to the EOF so I can get an appropriate hash, whether it is sha1 or md5. Please help. Here is what I have so far:</p>
<pre><code>import hashlib

inputFile = raw_input("Enter the name of the file:")
openedFile = open(inputFile)
readFile = openedFile.read()

md5Hash = hashlib.md5(readFile)
md5Hashed = md5Hash.hexdigest()

sha1Hash = hashlib.sha1(readFile)
sha1Hashed = sha1Hash.hexdigest()

print "File Name: %s" % inputFile
print "MD5: %r" % md5Hashed
print "SHA1: %r" % sha1Hashed
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>TL;DR use buffers to not use tons of memory.</strong></p>
<p>We get to the crux of your problem, I believe, when we consider the memory implications of working with <strong>very large files</strong>. We don't want this bad boy to churn through 2 gigs of ram for a 2 gigabyte file so, as <a href="https://stackoverflow.com/users/3059438/pasztorpisti">pasztorpisti</a> points out, we gotta deal with those bigger files in chunks!</p>
<pre><code>import sys
import hashlib

# BUF_SIZE is totally arbitrary, change for your app!
BUF_SIZE = 65536  # lets read stuff in 64kb chunks!

md5 = hashlib.md5()
sha1 = hashlib.sha1()

with open(sys.argv[1], 'rb') as f:
    while True:
        data = f.read(BUF_SIZE)
        if not data:
            break
        md5.update(data)
        sha1.update(data)

print("MD5: {0}".format(md5.hexdigest()))
print("SHA1: {0}".format(sha1.hexdigest()))
</code></pre>
<p>What we've done is we're updating our hashes of this bad boy in 64kb chunks as we go along with hashlib's handy dandy <a href="http://docs.python.org/2/library/hashlib.html#hashlib.hash.update" rel="noreferrer">update method</a>. This way we use a lot less memory than the 2gb it would take to hash the guy all at once!</p>
<p>You can test this with:</p>
<pre><code>$ mkfile 2g bigfile
$ python hashes.py bigfile
MD5: a981130cf2b7e09f4686dc273cf7187e
SHA1: 91d50642dd930e9542c39d36f0516d45f4e1af0d
$ md5 bigfile
MD5 (bigfile) = a981130cf2b7e09f4686dc273cf7187e
$ shasum bigfile
91d50642dd930e9542c39d36f0516d45f4e1af0d  bigfile
</code></pre>
<p>Hope that helps!</p>
<p>Also all of this is outlined in the linked question on the right hand side: <a href="https://stackoverflow.com/questions/1131220/get-md5-hash-of-big-files-in-python">Get MD5 hash of big files in Python</a></p>
<hr/>
<h3>Addendum!</h3>
<p>In general when writing python it helps to get into the habit of following <a href="http://legacy.python.org/dev/peps/pep-0008/" rel="noreferrer">pep-8</a>. For example, in python variables are typically underscore separated not camelCased. But that's just style and no one really cares about those things except people who have to read bad style... which might be you reading this code years from now.</p>
</div>
<div class="post-text" itemprop="text">
<p>For the correct and efficient computation of the hash value of a file (in Python 3):</p>
<ul>
<li>Open the file in binary mode (i.e. add <code>'b'</code> to the filemode) to avoid character encoding and line-ending conversion issues.</li>
<li>Don't read the complete file into memory, since that is a waste of memory. Instead, sequentially read it block by block and update the hash for each block.</li>
<li>Eliminate double buffering, i.e. don't use buffered IO, because we already use an optimal block size.</li>
<li>Use <code>readinto()</code> to avoid buffer churning.</li>
</ul>
<p>Example:</p>
<pre><code>import hashlib

def sha256sum(filename):
    h  = hashlib.sha256()
    b  = bytearray(128*1024)
    mv = memoryview(b)
    with open(filename, 'rb', buffering=0) as f:
        for n in iter(lambda : f.readinto(mv), 0):
            h.update(mv[:n])
    return h.hexdigest()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Open the file in binary mode, the default mode of <code>open()</code> is <code>'r'</code> which is "open for reading in text mode". In text mode newline conversion is performed on your data, this can cause platform specific bugs too but a possible problem that may happen as a result of text mode is that '\r\n' sequences are replaced to '\n' sequences in the string you get to your hands. Not all files contain '\r\n' sequences especially in case of binary files so the bug wouldn't come all the time and it would be hard to catch it.</p>
<pre><code>openedFile = open(inputFile, 'rb')
</code></pre>
<p>There is another little problem here, You read the file in one big chunk, by reading it in smaller few kilobyte chunks you would be able to hash very large files even if they wouldn't fit into your available memory.</p>
</div>
<div class="post-text" itemprop="text">
<p>I have programmed a module wich is able to hash big files with different algorithms.</p>
<pre><code>pip3 install py_essentials
</code></pre>
<p>Use the module like this:</p>
<pre><code>from py_essentials import hashing as hs
hash = hs.fileChecksum("path/to/the/file.txt", "sha256")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>import hashlib
user = input("Enter ")
h = hashlib.md5(user.encode())
h2 = h.hexdigest()
with open("encrypted.txt","w") as e:
    print(h2,file=e)


with open("encrypted.txt","r") as e:
    p = e.readline().strip()
    print(p)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I would propose simply:</p>
<pre><code>def get_digest(file_path):
    h = hashlib.sha256()

    with open(file_path, 'rb') as file:
        while True:
            # Reading is buffered, so we can read smaller chunks.
            chunk = file.read(h.block_size)
            if not chunk:
                break
            h.update(chunk)

    return h.hexdigest()
</code></pre>
<p>All other answers here seem to complicate too much. Python is already buffering when reading (in ideal manner, or you configure that buffering if you have more information about underlying storage) and so it is better to read in chunks the hash function finds ideal which makes it faster or at lest less CPU intensive to compute the hash function. So instead of disabling buffering and trying to emulate it yourself, you use Python buffering and control what you should be controlling: what the consumer of your data finds ideal, hash block size.</p>
</div>
<span class="comment-copy">and what is the problem?</span>
<span class="comment-copy">I want it to be able to hash a file. I need it to read until the EOF, whatever the file size may be.</span>
<span class="comment-copy">that is exactly what <code>file.read()</code> does - read the entire file.</span>
<span class="comment-copy">The documentation for the <code>read()</code> method says?</span>
<span class="comment-copy">You should go through "what is hashing?".</span>
<span class="comment-copy">@ranman Hello, I couldn't get the {0}".format(sha1.hexdigest()) part. Why do we use it instead of just using sha1.hexdigest() ?</span>
<span class="comment-copy">@Belial What wasn't working? I was mainly just using that to differentiate between the two hashes...</span>
<span class="comment-copy">@ranman Everything is working, I just never used this and haven't seen it in the literature. "{0}".format() ... unknown to me. :)</span>
<span class="comment-copy">How should I choose <code>BUF_SIZE</code>?</span>
<span class="comment-copy">This does doesn't generate the same results as the <code>shasum</code> binaries. The other answer listed below (the one using memoryview) is compatible with other hashing tools.</span>
<span class="comment-copy">How do you know what is an optimal block size?</span>
<span class="comment-copy">@Mitar, a lower bound is the maximum of the physical block (traditionally 512 bytes or 4KiB with newer disks) and the systems page size (4KiB on many system, other common choices: 8KiB and 64 KiB). Then you basically do some benchmarking and/or look at published <a href="http://git.savannah.gnu.org/gitweb/?p=coreutils.git;a=blob;f=src/ioblksize.h;h=ed2f4a9c4d77462f357353eb73ee4306c28b37f1;hb=HEAD#l23" rel="nofollow noreferrer">benchmark results and related work</a> (e.g. check what current rsync/GNU cp/... use).</span>
<span class="comment-copy">Would <a href="https://docs.python.org/2/library/resource.html#resource.getpagesize" rel="nofollow noreferrer"><code>resource.getpagesize</code></a> be of any use here, if we wanted to try to optimize it somewhat dynamically? And what about <a href="https://docs.python.org/3/library/mmap.html" rel="nofollow noreferrer"><code>mmap</code></a>?</span>
<span class="comment-copy">@jpmc26, getpagesize() isn't that useful here - common values are 4 KiB or 8 KiB, something in that range, i.e. something much smaller than the 128 KiB - 128 KiB is generally a good choice. mmap doesn't help much in our use case as we sequentially read the complete file from front to back. mmap has advantages when the access pattern is more random-access like, if pages are accessed more than once and/or if it the mmap simplifies read buffer management.</span>
<span class="comment-copy">Unlike the "top voted" answer this answer actually provides the same results as the <code>shasum</code> function.</span>
