<div class="post-text" itemprop="text">
<h2>Task:</h2>
<ul>
<li>Generate a pair of random numbers <code>(i,j)</code> (order doesn't matter: <code>(i,j)</code> is equivalent to <code>(j,i)</code>).</li>
<li>The pair must consist of two distinct values: <code>i != j</code></li>
<li>Pairs must be uniformly distributed. In other words, the probability is the same for all the possible pairs.</li>
<li>Do this at a constant time.</li>
</ul>
<h2>1st attempt</h2>
<p>Constant time? <strong>YES</strong>. Uniformly distributed? <strong>NO</strong></p>
<pre><code>x = np.random.randint(low=0, high=10 - 1)
y = np.random.randint(low=x + 1, high=10)
</code></pre>
<p>Visualizing the samples (by disregarding the order) :</p>
<p><a href="https://i.stack.imgur.com/jR7Be.png" rel="nofollow noreferrer">Samples visualization (not uniformly distributed)</a></p>
<p>You can easily the effect of restricting the <code>y</code> to be larger than <code>x</code>, meaning higher pairs have higher probability (opacity here expresses density).</p>
<h1>2nd attempt</h1>
<p>Constant time? <strong>NO</strong>. Uniformly distributed? <strong>YES</strong></p>
<pre><code>x = np.random.randint(low=0, high=nbr_values)
y = np.random.randint(low=0, high=nbr_values)

while x == y:
  y = np.random.randint(low=0, high=nbr_values)
</code></pre>
<p>Visualizing the samples:</p>
<p><a href="https://i.stack.imgur.com/8JC7J.png" rel="nofollow noreferrer">Samples visualization (uniformly distributed)</a></p>
<p><em>PS: it's not a homework, I'm experimenting with stochastic optimization techniques that use random neighbor generation using a swap operation.</em></p>
</div>
<div class="post-text" itemprop="text">
<p>How about this?</p>
<pre><code>x = np.random.randint(low=0, high=nbr_values)
y = np.random.randint(low=0, high=nbr_values - 1)
if y == x:
    y = nbr_values
</code></pre>
<p>The value for <code>x</code> is equally distributed among all the possible values, and the value for <code>y</code> is equally distributed among all the <em>remaining</em> values, with the current value for <code>x</code> standing in as the maximum value (could also be the minimum value, just use <code>low=1</code> in this case).</p>
<p>Graphical approximation:</p>
<pre><code>range                 0 - - - - - - - - - - - - - MAX
distribution for x    + + + + + + + + + + + + + + +
random value for x                x
distribution for y    + + + + + +   + + + + + + + +
                                  \--------------&gt;
</code></pre>
<p>Random distribution of 1,000,000 pairs in range <code>0..5</code></p>
<pre><code>0       33425   33147   33411   33340   33365
33206   0       33537   33568   33679   33317
33307   33284   0       33423   33121   33189
33235   33303   32970   0       33347   33316
33233   33946   33257   33272   0       33504
33517   33203   33394   33221   32963   0
</code></pre>
<hr/>
<p>Instead of swapping <code>x</code> with <code>max</code>, we could also shift all the values for <code>y</code> if <code>y &gt;= x</code>, i.e. <code>if y &gt;= x: y += 1</code>, yielding the same distribution. This way, the above could also be generalized to more than two values, by comparing the current value to all the previous values and shifting it up accordingly. This requires sorting the drawn values, though, so complexity is a bit higher, about O(k²logk).</p>
<pre><code>def draw(low, high, k):
    drawn = []
    for i in range(k):
        y = random.randint(low, high - i)
        for x in sorted(drawn):
            if y &gt;= x:
                y += 1
        drawn.append(y)
    return drawn
</code></pre>
<p>Tested this again with small values for <code>low</code> and <code>high</code> and 1,000,000 iterations, and the results look correct.</p>
<p>Or, you could just use <a href="https://docs.python.org/3/library/random.html#random.sample" rel="nofollow noreferrer"><code>random.sample(range(low, high+1), k)</code></a>. I don't know how exactly this is implemented, but it is very fast, even for large values for upper bound and <code>k</code> close to the maximum number of values.</p>
</div>
<div class="post-text" itemprop="text">
<p>I extended the method of @tobias_k above to <code>k</code> values.</p>
<pre><code>def uni_repl(n, k, s = 1000):
    x = np.empty((k, s), dtype = int)
    for i in range(k):
        x[i] = np.random.randint(low = 0, high = n - k + i + 1, size = s)
    for i in range(1, k):
        x[:i][x[:i] == x[i]] = n - k + i
    return x
</code></pre>
<p>testing:</p>
<pre><code>test = np.zeros((5,5,5))

np.add.at(test, list(uni_repl(5, 3)), 1)

test
Out[85]: 
array([[[     0.,      0.,      0.,      0.,      0.],
        [     0.,      0.,  16304.,  16767.,  16622.],
        [     0.,  16418.,      0.,  16631.,  16517.],
        [     0.,  16688.,  16607.,      0.,  16495.],
        [     0.,  16663.,  16544.,  16877.,      0.]],

       [[     0.,      0.,  16736.,  16767.,  16668.],
        [     0.,      0.,      0.,      0.,      0.],
        [ 16862.,      0.,      0.,  16634.,  16632.],
        [ 16791.,      0.,  16689.,      0.,  16557.],
        [ 16566.,      0.,  16843.,  16864.,      0.]],

       [[     0.,  16737.,      0.,  16638.,  16437.],
        [ 16741.,      0.,      0.,  16545.,  16617.],
        [     0.,      0.,      0.,      0.,      0.],
        [ 16804.,  16923.,      0.,      0.,  16598.],
        [ 16756.,  16850.,      0.,  16778.,      0.]],

       [[     0.,  16777.,  16675.,      0.,  16760.],
        [ 16454.,      0.,  16792.,      0.,  16669.],
        [ 16476.,  16709.,      0.,      0.,  16677.],
        [     0.,      0.,      0.,      0.,      0.],
        [ 16680.,  16863.,  16640.,      0.,      0.]],

       [[     0.,  16459.,  16446.,  16756.,      0.],
        [ 16637.,      0.,  16626.,  16756.,      0.],
        [ 16481.,  16773.,      0.,  16762.,      0.],
        [ 16754.,  16531.,  16681.,      0.,      0.],
        [     0.,      0.,      0.,      0.,      0.]]])
</code></pre>
<p>Compared to a method using <code>np.argpartition</code>, this seems to be faster as long as <code>k &lt; 0.4 * n</code></p>
<pre><code>def uni_part(n, k, s = 1000):
    x = np.random.rand(n, s)
    return np.argpartition(x, k, axis = 0)[:k]

%timeit uni_part(100, 40)
100 loops, best of 3: 3.93 ms per loop

%timeit uni_repl(100, 40)
100 loops, best of 3: 3.76 ms per loop

%timeit uni_part(100, 10)
100 loops, best of 3: 3.55 ms per loop

%timeit uni_repl(100, 10)
1000 loops, best of 3: 425 µs per loop

%timeit uni_part(100, 50)
100 loops, best of 3: 4.08 ms per loop

%timeit uni_repl(100, 50)
100 loops, best of 3: 4.89 ms per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Starting from Numpy 1.7.0 you can use <code>np.choice</code> with <code>replace=False</code>:</p>
<pre><code>np.random.choice(10, 2, replace=False)
array([2, 6])
</code></pre>
<p>Here is the distribution of 10000 samples:</p>
<p><a href="https://i.stack.imgur.com/hEK5d.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/hEK5d.png"/></a></p>
</div>
<span class="comment-copy">Well you would have constant time in the first example if you choose randomly to define the boundary to be less than, or greater than, the initially picked value. Rather than assume it's greater than.</span>
<span class="comment-copy">What would happen if you generate both of them again when they are equal?</span>
<span class="comment-copy">Consider the 0-9 range, as shown in the posted data.  wlog, assume that the first number is 5.  For the second number, we roll a D9, numbered 0-8 ... but we map <code>5</code> (the number already chosen) to <code>9</code>.  The second numbers are evenly balanced.</span>
<span class="comment-copy">I don't think it's biased to the last item, and it is virtually rebuilding the index list with <code>x</code> removed.</span>
<span class="comment-copy">@roganjosh Yes, and for all pairs where <code>x != y</code>, <code>y</code> is guaranteed <i>not</i> to be <code>9</code>. The <i>probability</i> for <code>y</code> being <code>9</code> is <i>exactly</i> the same as for any other allowed value.</span>
<span class="comment-copy">I see it now and hang my head in shame :P</span>
<span class="comment-copy">@PaulPanzer I now also added a generalization for my approach, but instead you could also just use <code>random.sample</code> with a <code>range</code>...</span>
<span class="comment-copy">Can't find fault with that (unless k is quite large).</span>
<span class="comment-copy">@PaulPanzer, yeah I checked speed and it seems to be faster than <code>argpartition</code> as long as k &lt; .4n</span>
