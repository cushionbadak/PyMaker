<div class="post-text" itemprop="text">
<p>Here is my code:</p>
<pre><code>import urllib
import webbrowser
from bs4 import BeautifulSoup
import requests
import re

address = 'https://google.com/search?q='
# Default Google search address start
file = open( "OCR.txt", "rt" )
# Open text document that contains the question
word = file.read()
file.close()

myList = [item for item in word.split('\n')]
newString = ' '.join(myList)
# The question is on multiple lines so this joins them together with proper spacing

qstr = urllib.parse.quote_plus(newString)
# Encode the string

newWord = address + qstr
# Combine the base and the encoded query

response = requests.get(newWord)

#with open('output.html', 'wb') as f:
#    f.write(response.content)
#webbrowser.open('output.html')

answers = open("ocr2.txt", "rt")

ansTable = answers.read()
answers.close()

ans = ansTable.splitlines()

ans1 = str(ans[0])
ans2 = str(ans[2])
ans3 = str(ans[4])

ans1Score = 0
ans2Score = 0
ans3Score = 0

links = []

soup = BeautifulSoup(response.text, 'lxml')

for r in soup.find_all(class_='r'):

    linkRaw = str(r)

    link = re.search("(?P&lt;url&gt;https?://[^\s]+)", linkRaw).group("url")

    if '&amp;' in link:

        finalLink = link.split('&amp;')
        link = str(finalLink[0])

    links.append(link)

#print(links)
#print(' ')

for g in soup.find_all(class_='g'):

    webBlock = str(g)

    ans1Tally = webBlock.count(ans1)
    ans2Tally = webBlock.count(ans2)
    ans3Tally = webBlock.count(ans3)

    if  ans1 in webBlock:

        ans1Score += ans1Tally

    else:

        ans1Found = False

    if ans2 in webBlock:

        ans2Score += ans2Tally

    else:

        ans2Found = False

    if ans3 in webBlock:

        ans3Score += ans3Tally

    else:

        ans3Found = False

    if ans1Found and ans2Found and ans3Found is False:

        searchLink = str(links[0])

        if searchLink.endswith('pdf'):
            pass

        else:

            response2 = requests.get(searchLink)
            soup2 = BeautifulSoup(response2.text, 'lxml')

            for p in soup2.find_all('p'):

                extraBlock = str(p)

                extraAns1Tally = extraBlock.count(ans1)
                extraAns2tally = extraBlock.count(ans2)
                extraAns3Tally = extraBlock.count(ans3)

                if ans1 in extraBlock:

                    ans1Score += extraAns1Tally

                if ans2 in extraBlock:

                    ans2Score += extraAns2Tally

                if ans3 in extraBlock:

                    ans3Score += extraAns3Tally

                with open("Results.txt", "w") as results:
                    results.write(newString + '\n\n')    
                    results.write(ans1+": "+str(ans1Score)+'\n')
                    results.write(ans2+": "+str(ans2Score)+'\n')
                    results.write(ans3+": "+str(ans3Score))

    links.pop(0)

    print(' ')
    print('-----')
    print(ans1+": "+str(ans1Score))
    print(ans2+": "+str(ans2Score))
    print(ans3+": "+str(ans3Score))
    print('-----')
</code></pre>
<p>Basically right now it is scraping each "g" one at a time, when this program can benefit massively from scraping each link all at the same time. For example, I want it to have them all scraping at the same time instead of waiting until the one before it is done. Sorry if this is a simple kind of question but I have little experience with asyncio so if anyone could help that would be massively appreciated. Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>To write async program you need:</p>
<ul>
<li>define functions with <code>async def</code></li>
<li>call it with <code>await</code></li>
<li>create event loop and run some function in it</li>
<li>run requests concurrently using <code>asyncio.gather</code></li>
</ul>
<p>All other is almost same as usual. Instead of using blocking <code>request</code> module you should use some async one. For example, <a href="https://docs.aiohttp.org/en/stable/client.html" rel="nofollow noreferrer">aiohttp</a>:</p>
<pre><code>python -m pip install aiohttp
</code></pre>
<p>And use it like this:</p>
<pre><code>async def get(url):
    async with aiohttp.ClientSession() as session:
        async with session.get('https://api.github.com/events') as resp:
            return await resp.text()
</code></pre>
<hr/>
<p>Here's code with some changes I statrted. I didn't check if it's actually works since I don't have files you use. You should also move logic inside for <code>g in soup.find_all(class_='g'):</code> to seperate function and run multiple of these functions with <code>asyncio.gather</code> to benefit of asyncio.</p>
<pre><code>import asyncio
import aiohttp
import urllib
import webbrowser
from bs4 import BeautifulSoup
import re


async def get(url):
    async with aiohttp.ClientSession() as session:
        async with session.get('https://api.github.com/events') as resp:
            return await resp.text()


async def main():
    address = 'https://google.com/search?q='
    # Default Google search address start
    file = open( "OCR.txt", "rt" )
    # Open text document that contains the question
    word = file.read()
    file.close()

    myList = [item for item in word.split('\n')]
    newString = ' '.join(myList)
    # The question is on multiple lines so this joins them together with proper spacing

    qstr = urllib.parse.quote_plus(newString)
    # Encode the string

    newWord = address + qstr
    # Combine the base and the encoded query

    text = await get(newWord)

    #with open('output.html', 'wb') as f:
    #    f.write(response.content)
    #webbrowser.open('output.html')

    answers = open("ocr2.txt", "rt")

    ansTable = answers.read()
    answers.close()

    ans = ansTable.splitlines()

    ans1 = str(ans[0])
    ans2 = str(ans[2])
    ans3 = str(ans[4])

    ans1Score = 0
    ans2Score = 0
    ans3Score = 0

    links = []

    soup = BeautifulSoup(text, 'lxml')

    for r in soup.find_all(class_='r'):

        linkRaw = str(r)

        link = re.search("(?P&lt;url&gt;https?://[^\s]+)", linkRaw).group("url")

        if '&amp;' in link:

            finalLink = link.split('&amp;')
            link = str(finalLink[0])

        links.append(link)

    #print(links)
    #print(' ')

    for g in soup.find_all(class_='g'):

        webBlock = str(g)

        ans1Tally = webBlock.count(ans1)
        ans2Tally = webBlock.count(ans2)
        ans3Tally = webBlock.count(ans3)

        if  ans1 in webBlock:

            ans1Score += ans1Tally

        else:

            ans1Found = False

        if ans2 in webBlock:

            ans2Score += ans2Tally

        else:

            ans2Found = False

        if ans3 in webBlock:

            ans3Score += ans3Tally

        else:

            ans3Found = False

        if ans1Found and ans2Found and ans3Found is False:

            searchLink = str(links[0])

            if searchLink.endswith('pdf'):
                pass

            else:

                text2 = await get(searchLink)
                soup2 = BeautifulSoup(text2, 'lxml')

                for p in soup2.find_all('p'):

                    extraBlock = str(p)

                    extraAns1Tally = extraBlock.count(ans1)
                    extraAns2tally = extraBlock.count(ans2)
                    extraAns3Tally = extraBlock.count(ans3)

                    if ans1 in extraBlock:

                        ans1Score += extraAns1Tally

                    if ans2 in extraBlock:

                        ans2Score += extraAns2Tally

                    if ans3 in extraBlock:

                        ans3Score += extraAns3Tally

                    with open("Results.txt", "w") as results:
                        results.write(newString + '\n\n')    
                        results.write(ans1+": "+str(ans1Score)+'\n')
                        results.write(ans2+": "+str(ans2Score)+'\n')
                        results.write(ans3+": "+str(ans3Score))

        links.pop(0)

        print(' ')
        print('-----')
        print(ans1+": "+str(ans1Score))
        print(ans2+": "+str(ans2Score))
        print(ans3+": "+str(ans3Score))
        print('-----')


if __name__ ==  '__main__':
    loop = asyncio.get_event_loop()
    try:
        loop.run_until_complete(main())
    finally:
        loop.run_until_complete(loop.shutdown_asyncgens())
        loop.close()
</code></pre>
<hr/>
<p><strong>Upd:</strong></p>
<p>Main idea is to move logic inside loop that does request into separate coroutine and pass multiple of these coroutines to asyncio.gather. It will parallelize your requests.</p>
<pre><code>async def main():
    # Her do all that are before the loop.

    coros = [
        process_single_g(g)
        for g
        in soup.find_all(class_='g')
    ]

    results = await asyncio.gather(*coros)  # this function will run multiple tasks concurrently
                                            # and return all results together.

    for res in results:
        ans1Score, ans2Score, ans3Score = res

        print(' ')
        print('-----')
        print(ans1+": "+str(ans1Score))
        print(ans2+": "+str(ans2Score))
        print(ans3+": "+str(ans3Score))
        print('-----')



async def process_single_g(g):
    # Here do all things you inside loop for concrete g.

    text2 = await get(searchLink)

    # ...

    return ans1Score, ans2Score, ans3Score
</code></pre>
</div>
<span class="comment-copy"><a href="https://docs.python.org/3.6/library/multiprocessing.html" rel="nofollow noreferrer">docs.python.org/3.6/library/multiprocessing.html</a> Use this module</span>
<span class="comment-copy">You should consider accepting the answer @mikhail provided.</span>
<span class="comment-copy"><i>You should also move logic inside for g in soup.find_all(class_='g'): to seperate function and run multiple of these functions with asyncio.gather to benefit of asyncio.</i>   How exactly do I do that? Thank you for the reply!</span>
<span class="comment-copy">@DevinGP I won't rewrite all code for you, but I updated answer showing how usage of <code>asyncio.gather</code> will look like in your case. You may want to start with some <a href="https://docs.python.org/3/library/asyncio-task.html#example-parallel-execution-of-tasks" rel="nofollow noreferrer">simple abstract example</a> of executing multiple coroutines parallely to get the idea.</span>
