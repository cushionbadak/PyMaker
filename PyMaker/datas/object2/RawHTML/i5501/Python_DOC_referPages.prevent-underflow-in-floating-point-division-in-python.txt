<div class="post-text" itemprop="text">
<p>Suppose both <code>x</code> and <code>y</code> are very small numbers, but I know that the true value of <code>x / y</code> is reasonable. </p>
<p>What is the best way to compute <code>x/y</code>?
In particular, I have been doing <code>np.exp(np.log(x) - np.log(y)</code> instead, but I'm not sure if that would make a difference at all?</p>
</div>
<div class="post-text" itemprop="text">
<p>Python uses the floating-point features of the hardware it runs on, according to <a href="https://docs.python.org/3/tutorial/floatingpoint.html" rel="nofollow noreferrer">Python documentation</a>. On most common machines today, that is IEEE-754 arithmetic or something near it. That Python documentation is not explicit about rounding mode but mentions in passing that the result of a sample division is the nearest representable value, so presumably Python uses round-to-nearest-ties-to-even mode. (“Round-to-nearest” for short. If two representable values are equally close in binary floating-point, the one with a zero in the low bits of its significand is produced.)</p>
<p>In IEEE-754 arithmetic in round-to-nearest mode, the result of a division is the representable value nearest to the exact mathematical value. Since you say the mathematical value of <code>x/y</code> is reasonable, it is in the normal range of representable values (not below it, in the subnormal range, where precision suffers, and not above it, where results are rounded to infinity). In the normal range, results of elementary operations will be accurate within the normal precision of the format.</p>
<p>However, since <code>x</code> and <code>y</code> are “very small numbers,” we may be concerned that they are subnormal and have a loss of precision already in them, before division is performed. In the IEEE-754 basic 64-bit binary format, numbers below 2<sup>-1022</sup> (about 2.22507•10<sup>-308</sup>) are subnormal. If <code>x</code> and <code>y</code> are smaller than that, then they have already suffered a loss of precision, and no method can produce a correct quotient from them except by happenstance. Taking the logarithms to calculate the quotient will not help.</p>
<p>If the machine you are running on happens not to be using IEEE-754, it is still likely that computing <code>x/y</code> directly will produce a better result than <code>np.exp(np.log(x)-np.log(y))</code>. The former is a single operation computing a basic function in hardware that was likely reasonably designed. The latter is several operations computing complicated functions in software that is difficult to make accurate using common hardware operations.</p>
<p>There is a fair amount of unease and distrust of floating-point operations. Lack of knowledge seems to lead to people being afraid of them. But what should be understood here is that elementary floating-point operations are very well defined and are accurate in normal ranges. The actual problems with floating-point computing arise from accumulating rounding errors over sequences of operations, from the inherent mathematics that compounds errors, and from incorrect expectations about results. What this means is that there is no need to worry about the accuracy of a single division. Rather, it is the overall use of floating-point that should be kept in mind. (Your question could be better answered if it presented more context, illuminating why this division is important, how <code>x</code> and <code>y</code> have been produced from prior data, and what the overall goal is.)</p>
<h3>Note</h3>
<p>A not uncommon deviation from IEEE-754 is to flush subnormal values to zero. If you have some <code>x</code> and some <code>y</code> that are subnormal, some implementations might flush them to zero before performing operations on them. However, this is more common in SIMD code than in normal scalar programming. And, if it were occurring, it would prevent you from evaluating <code>np.log(x)</code> and <code>np.log(y)</code> anyway, as subnormal values would be flushed to zero in those as well. So we can likely dismiss this possibility.</p>
</div>
<div class="post-text" itemprop="text">
<p>Division, like other IEEE-754-specified operations, is computed at infinite precision and then (with ordinary rounding rules) rounded to the closest representable float. The result of calculating <code>x/y</code> will almost certainly be a lot more accurate than the result of calculating <code>np.exp(np.log(x) - np.log(y)</code> (and is guaranteed not to be <em>less</em> accurate).</p>
</div>
<span class="comment-copy">Can you give an example of two numbers where this would fail? IEEE floating point is generally very robust. Are you saying that <code>x</code> and <code>y</code> are both too small to be represented in IEEE floating point?</span>
<span class="comment-copy">Actually, I don't have a specific failing example---I've just been afraid that <code>x/y</code> would fail and have been doing <code>np.exp(np.log(x) - np.log(y)</code> but then I wondered if that makes a difference at all. Let me edit the question slightly.</span>
<span class="comment-copy">I can't imagine a scenario where using <code>log</code> and <code>exp</code> would give a better result than the straight division. My imagination has been known to be lacking sometimes though.</span>
<span class="comment-copy">@MarkRansom You might certainly be correct, but I'm not sure either...</span>
<span class="comment-copy">Are you sure that taking the logarithms won't help? While x and y are denormal, their logarithms are not, so using them may perhaps give a better result, despite the more complex computation.</span>
<span class="comment-copy">@RudyVelthuis: How could it help? The result of the straight division is already the best possible (the closest representable number to the exact quotient), assuming IEEE 754 semantics are being followed.</span>
<span class="comment-copy">Why would you think the logarithms produce a better result? In IEEE-754, the result of dividing x and y is the representable value closest to the mathematically exact value. <b>No closer representable value exists.</b> Therefore it is <b>impossible</b> to return a better result in the floating-point format.</span>
<span class="comment-copy">@MarkDickinson: I assume the x and y values are denormal and thus pretty imprecise. But if they were computed using logarithms, the logarithms are not denormal. Subtracting should still have a good precision, and exponentiation should still be better than simply division of <i>denormals</i>.</span>
<span class="comment-copy">@RudyVelthuis: If x is the result of pow(10, -300) and y is the result of pow(10, -308), then dividing x by y will produce the representable result that is nearest the exact mathematical result of x/y. <b>No better result exists.</b> If your complaint is that floating-point <code>x/y</code> will have some error that differs from <code>pow(10, 8)</code>, that is because <b>there are already errors in x and y when you divide them</b>. If there are already errors in x and y, then taking the logarithm, subtracting, and exponentiating <b>will not remove those errors</b>.</span>
<span class="comment-copy">Python uses the underlying hardware floating-point. It may be IEEE-754 on most common machines but is not guaranteed.</span>
<span class="comment-copy">I have a bit of a problem with "infinite precision". The precision is usually higher than double, but not "infinite".</span>
<span class="comment-copy">@RudyVelthuis No, it is <i>infinite</i>. Obviously when you're calculating something like <code>1/3</code> it is not possible to actually have an infinite sequence of <code>10101010101...</code> sitting around before rounding, but the processor must find a way of calculating the final value as though it did. All error must come from rounding, not from computation. FWIW, The "infinite precision" verbiage is directly from the IEEE-754 standard.</span>
<span class="comment-copy">The processor simply does this by calculating with a number of extra bits in precision (e.g. 64 bits significand instead of 53) and then rounding those down to the precision required. I don't know if IEEE-745 describes it like that, but reality is that this simply doesn't happen.</span>
<span class="comment-copy">@RudyVelthuis: IEEE-754 requires an implementation to compute a result <b>as if</b> the exact mathematical result were computed with infinite precision and then rounded to the nearest representable value. To implement this in hardware, designers calculate what number of bits and what data they need to use to get the required result. If they use a fixed amount of extended precision, they have derived proofs that the precision they use is enough to get the required result.</span>
