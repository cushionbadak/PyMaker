<div class="post-text" itemprop="text">
<p>Please update the title if you can think of a good one !</p>
<p>I have a data of the following structure:</p>
<pre><code>chr    pos    A_block    A_val
  2     05       7       A,T,C
  2     11       7       T,C,G
  2     15       7       AT,C,G
  2     21       7       C,A,GT
  2     31       7       T,C,CA
  2     42       9       T,C,G
  2     55       9       C,G,GC
  2     61       9       A,GC,T
  2     05       12       AC,TG,G
  2     11       12       A,TC,TG
</code></pre>
<p><strong>Expected output: For the sake of learning I just want to rewrite the output file, same as the input file, but using the process I propose below.</strong></p>
<p><strong>I want to:</strong> <code>step 01:</code> read the values only for two consecutive blocks at a time (first 7 &amp; 9) -&gt; <code>step 02:</code> store that data in a dictionary with <code>block numbers</code> as main unique key -&gt; <code>step 03:</code> return that dictionary to the predefined function for parsing. -&gt; Now, read blocks (9 &amp; 12) -&gt; repeat the same process until end.</p>
<p>I am thinking something like:</p>
<pre><code>import req_packages
from collections import defaultdict

''' make a function that takes data from two blocks at a time '''
def parse_two_blocks(someData):
    for key, vals in someData:
        do ... something 
        write the obtained output
        clear memory  # to prevent memory buildup


''' Now, read the input file'''
with open('HaploBlock_toy.txt') as HaploBlocks:
    header = HaploBlocks.readline()  
    # only reads the first line as header

    ''' create a empty dict or default dict. Which ever is better?'''
    Hap_Dict = {}
    Hap_Dict = defaultdict(list)


    ''' for rest of the lines '''
    for lines in HaploBlocks:
        values = lines.strip('\n').split('\t')

        ''' append the data to the dict for unique keys on the for loop, until the number of unique keys is 2 '''
        Block = values[2]
        Hap_Dict[Block].append(values[3])

        do something to count the number of keys - how?
        if keys_count &gt; 2:
           return parse_two_blocks(Hap_Dict)

        elif keys_count &lt; 2 or no new keys: # This one is odd and won't work I know.
           end the program
</code></pre>
<p>So, when the code is executed it will read data from block 7 and 9, until dictionary is filled and returned to the pre-defined function. When parsing is done, it now can just retain the data from latter block of the previous parse. That way it will only have to read the remaining block.</p>
<p>Expected output: 
<strong>The main problem for me now is to be able to read two blocks at a time.</strong> I don't want to add intrinsic details of how I want to parse information in `parse_two_blocks(someData)' - this is just another problem. But, lets try to rewrite the output same as input.</p>
</div>
<div class="post-text" itemprop="text">
<p>Parse the input into an on-the-fly list (a generator) of blocks. Iterate over the pairs. It should all be done as you evaluate the pairs. That is, none of those lines should ever read or store the whole csv file at once.</p>
<pre><code>#!/usr/bin/env python3

data = """chr   pos A_block A_val
2   05  7   A,T,C
2   11  7   T,C,G
2   15  7   AT,C,G
2   21  7   C,A,GT
2   31  7   T,C,CA
2   42  9   T,C,G
2   55  9   C,G,GC
2   61  9   A,GC,T
2   05  12  AC,TG,G
2   11  12  A,TC,TG"""

import csv
import io
import itertools
import collections
import operator
from pprint import pprint

def pairwise(iterable):
    "s -&gt; (s0,s1), (s1,s2), (s2, s3), ..."
    a, b = itertools.tee(iterable)
    next(b, None)
    return zip(a, b)

def one():
    # read rows as tuples of values
    c = csv.reader(io.StringIO(data), dialect=csv.excel_tab)
    # read header row
    keys = next(c)
    block_index = keys.index('A_block')
    # group rows by block numbers
    blocks = itertools.groupby(c, key=operator.itemgetter(block_index))
    # extract just the row values for each block
    row_values = (tuple(v) for k, v in blocks)
    # rearrange the values by column
    unzipped_values = (zip(*v) for v in row_values)
    # create a dictionary for each block
    dict_blocks = (dict(zip(keys, v)) for v in unzipped_values)
    yield from pairwise(dict_blocks)


def two():
    c = csv.DictReader(io.StringIO(data), dialect=csv.excel_tab)
    blocks = itertools.groupby(c, key=lambda x: x['A_block'])
    yield from pairwise((k, list(v)) for k, v in blocks)


for a, b in one():
        pprint(a)
        pprint(b)
        print()
</code></pre>
<p>Output (of <code>one</code>):</p>
<pre><code>{'A_block': ('7', '7', '7', '7', '7'),
 'A_val': ('A,T,C', 'T,C,G', 'AT,C,G', 'C,A,GT', 'T,C,CA'),
 'chr': ('2', '2', '2', '2', '2'),
 'pos': ('05', '11', '15', '21', '31')}
{'A_block': ('9', '9', '9'),
 'A_val': ('T,C,G', 'C,G,GC', 'A,GC,T'),
 'chr': ('2', '2', '2'),
 'pos': ('42', '55', '61')}

{'A_block': ('9', '9', '9'),
 'A_val': ('T,C,G', 'C,G,GC', 'A,GC,T'),
 'chr': ('2', '2', '2'),
 'pos': ('42', '55', '61')}
{'A_block': ('12', '12'),
 'A_val': ('AC,TG,G', 'A,TC,TG'),
 'chr': ('2', '2'),
 'pos': ('05', '11')}
</code></pre>
<p><a href="https://docs.python.org/3/library/io.html#io.StringIO" rel="nofollow noreferrer"><code>io.StringIO(string)</code></a></p>
<blockquote>
<p>Take a string and return a file-like object that contains the contents of string.</p>
</blockquote>
<p><a href="https://docs.python.org/3/library/csv.html#csv.DictReader" rel="nofollow noreferrer"><code>csv.DictReader(file_object, dialect)</code></a> from the <a href="https://docs.python.org/3/library/csv.html" rel="nofollow noreferrer"><code>csv module</code></a></p>
<blockquote>
<p>Returns an ordered dict for each row where the field names taken from the very first row are used as dictionary keys for the field values.</p>
</blockquote>
<p><a href="https://docs.python.org/3/library/itertools.html#itertools.groupby" rel="nofollow noreferrer"><code>groupby(iterable, key_function)</code></a></p>
<blockquote>
<p>Make an iterator that returns consecutive keys and groups from the
  iterable. The key is a function computing a key value for each element.</p>
</blockquote>
<p><code>lambda x: x['A_block']</code></p>
<blockquote>
<p>A temporary function that takes an input named <code>x</code> and returns the value for the key <code>'A_block'</code></p>
</blockquote>
<p><code>(k, list(v)) for k, v in blocks</code></p>
<blockquote>
<p><code>groupby()</code> returns an iterator (that can only be used once) for the values. This converts that iterator to a list.</p>
</blockquote>
<p><a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow noreferrer"><code>pairwise(iterable)</code> recipe</a></p>
<blockquote>
<p>"s -&gt; (s0,s1), (s1,s2), (s2, s3), ..."</p>
</blockquote>
</div>
<span class="comment-copy">Can you show the example of what you want to get as the result?</span>
<span class="comment-copy">@ComradeAndrew: I am trying to parse the data in this manner because I have to read two blocks at a time -&gt; then, do appropriate calculations and write the values? For the sake of my learning and to keep problems to minimal for now, I just want to write the output file same as input. I can do the manipulation by myself. The main problem is reading two blocks at a time.</span>
<span class="comment-copy">Does every block contain multiple of 4 entries? I would suggest you to try to create a list of all rows and walk for them with double indexing where the first index goes in first block and the second with shift of 4 in the second block.</span>
<span class="comment-copy">No, it doesn't. It will vary. That was just a co-incidence. Sorry. I just updated the data</span>
<span class="comment-copy">Write a function that returns a list of blocks. Then write another function that iterates over that list in pairs. Then write a function that creates a database from the list using the block number as a key. Some of the "functions" could be one-line list comprehensions.</span>
<span class="comment-copy">Thank you so much Harvey. I am going to try this out, Can you please add some explanation?. But, don't you think this is heavy on memory if there are millions of lines and some hundred columns.</span>
<span class="comment-copy">Maybe I misunderstood, but I thought that you wanted all of the data in a dictionary (so no memory constraint). The memory storage can be avoided with a little bit of extra work.</span>
<span class="comment-copy">The reason I want read just two blocks at time - process it - write the output is just to save the memory buildup. Is for-loop too bad for this problem?</span>
<span class="comment-copy">two blocks that are parsed should have <code>block values</code> as main index to parse information between blocks. like <code>{'7': defaultdict(&lt;class 'list'&gt;, {'chr': ['7', '7', '7', '7', '7'], 'pos': ['05', '11', '15', '21', '31'], 'A_val': ['ATC', 'TCG', .... ]}), '9': defaultdict(&lt;class 'list'&gt;, {'chr': ['2', '2', .... ], 'pos': ['42', '55', '61'], 'A_val': ['T,C,G', 'C,G,GC', .... ]}))}</code>. Also, we read only when until we have two unique keys.</span>
<span class="comment-copy">I don't understand your comment. So do you group all of the lines for a block together or not? If you do group them, and you just want them in a different format, you can do that. So, you stop reading a file as soon you have two distinct block numbers? i.e. block 12 would never be read in the example?</span>
