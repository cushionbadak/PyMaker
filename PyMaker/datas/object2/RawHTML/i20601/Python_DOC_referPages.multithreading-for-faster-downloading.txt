<div class="post-text" itemprop="text">
<p>How can I download multiple links simultaneously? My script below works but only downloads one at a time and it is extremely slow. I can't  figure out how to incorporate multithreading in my script.</p>
<p>The Python script:</p>
<pre><code>from BeautifulSoup import BeautifulSoup
import lxml.html as html
import urlparse
import os, sys
import urllib2
import re

print ("downloading and parsing Bibles...")
root = html.parse(open('links.html'))
for link in root.findall('//a'):
  url = link.get('href')
  name = urlparse.urlparse(url).path.split('/')[-1]
  dirname = urlparse.urlparse(url).path.split('.')[-1]
  f = urllib2.urlopen(url)
  s = f.read()
  if (os.path.isdir(dirname) == 0): 
    os.mkdir(dirname)
  soup = BeautifulSoup(s)
  articleTag = soup.html.body.article
  converted = str(articleTag)
  full_path = os.path.join(dirname, name)
  open(full_path, 'w').write(converted)
  print(name)
</code></pre>
<p>The HTML file called <code>links.html</code>:</p>
<pre><code>&lt;a href="http://www.youversion.com/bible/gen.1.nmv-fas"&gt;http://www.youversion.com/bible/gen.1.nmv-fas&lt;/a&gt;

&lt;a href="http://www.youversion.com/bible/gen.2.nmv-fas"&gt;http://www.youversion.com/bible/gen.2.nmv-fas&lt;/a&gt;

&lt;a href="http://www.youversion.com/bible/gen.3.nmv-fas"&gt;http://www.youversion.com/bible/gen.3.nmv-fas&lt;/a&gt;

&lt;a href="http://www.youversion.com/bible/gen.4.nmv-fas"&gt;http://www.youversion.com/bible/gen.4.nmv-fas&lt;/a&gt;
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>It looks to me like the consumer - producer problem - see wikipedia</p>
<p>You may use</p>
<pre><code>import Queue, thread

# create a Queue.Queue here
queue = Queue.Queue()

print ("downloading and parsing Bibles...")
root = html.parse(open('links.html'))
for link in root.findall('//a'):
  url = link.get('href')
  queue.put(url) # produce




def thrad():
  url = queue.get() # consume
  name = urlparse.urlparse(url).path.split('/')[-1]
  dirname = urlparse.urlparse(url).path.split('.')[-1]
  f = urllib2.urlopen(url)
  s = f.read()
  if (os.path.isdir(dirname) == 0): 
    os.mkdir(dirname)
  soup = BeautifulSoup(s)
  articleTag = soup.html.body.article
  converted = str(articleTag)
  full_path = os.path.join(dirname, name)
  open(full_path, 'wb').write(converted)
  print(name)

thread.start_new(thrad, ()) # start 1 threads
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I use <code>multiprocessing</code> for parallelizing things  -- for some reason I like it better than <code>threading</code></p>
<pre><code>from BeautifulSoup import BeautifulSoup
import lxml.html as html
import urlparse
import os, sys
import urllib2
import re
import multiprocessing


print ("downloading and parsing Bibles...")
def download_stuff(link):
  url = link.get('href')
  name = urlparse.urlparse(url).path.split('/')[-1]
  dirname = urlparse.urlparse(url).path.split('.')[-1]
  f = urllib2.urlopen(url)
  s = f.read()
  if (os.path.isdir(dirname) == 0): 
    os.mkdir(dirname)
  soup = BeautifulSoup(s)
  articleTag = soup.html.body.article
  converted = str(articleTag)
  full_path = os.path.join(dirname, name)
  open(full_path, 'w').write(converted)
  print(name)

root = html.parse(open('links.html'))
links = root.findall('//a')
pool = multiprocessing.Pool(processes=5) #use 5 processes to download the data
output = pool.map(download_stuff,links)  #output is a list of [None,None,...] since download_stuff doesn't return anything
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In 2017 there are some other options now,  like asyncio and ThreadPoolExecutor.</p>
<p>Here is an example of ThreadPoolExecutor (included in Python futures) </p>
<pre><code>from concurrent.futures import ThreadPoolExecutor

def download(url, filename):
    ... your dowload function...
    pass

with ThreadPoolExecutor(max_workers=12) as executor:
    future = executor.submit(download, url, filename)
    print(future.result())
</code></pre>
<p>submit() function will submit the task to a queue. (queue management is done for you)</p>
<pre><code>Python version 3.5 and above:
if max_workers is None or not given, it will default to the number of processors on the 
machine, multiplied by 5.
</code></pre>
<p>You can set max_workers, a few times the number of CPU cores in practice, do some tests to see how faw you can go up, depending on context-switching overhead.  </p>
<p>For more info:
<a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">https://docs.python.org/3/library/concurrent.futures.html</a></p>
</div>
<span class="comment-copy">You haven't even tried anything yet, so you don't actually have a problem we can help with.</span>
<span class="comment-copy">open(full_path, 'wb').write(converted) !!! you want to download binary files</span>
<span class="comment-copy">I am getting this error <code>NameError: name 'queue' is not defined </code> and if I capitalize "Queue" I get this error <code>AttributeError: 'module' object has no attribute 'put'</code></span>
<span class="comment-copy">Do you really want to have a function named <code>thread</code> when you've imported a module of the same name?</span>
<span class="comment-copy">right. this was only a suggestion of structure and not a solution since he knows about multithreading.</span>
<span class="comment-copy">@Blainer, The first line <i>should</i> probably read <code>from Queue import Queue</code> (for python 2.X) and <code>from threading import Thread</code>, then the last line should be <code>Thread.start_new(thread,())</code> ... I think.  (I don't really use <code>threading</code> so I don't know for sure.)</span>
<span class="comment-copy">@user1320237, we're here to solve problems.  Handwaving and theoretical solutions are not going to help</span>
<span class="comment-copy">I am getting this error <code>AssertionError: invalid Element proxy at 163319020</code></span>
<span class="comment-copy">@Blainer : That's a little strange (although I don't know how lxml.html works, so maybe it isn't...).  You could try passing the url's instead of the links.  It's possible that some information lxml.html keeps some sort of proxy/handle on the position in the file which isn't being pickled/unpickled properly by <code>multiprocessing</code>.  If link.get returns a string, that should play a little more nicely...</span>
<span class="comment-copy">I dont know man, this is really frustrating though haha</span>
