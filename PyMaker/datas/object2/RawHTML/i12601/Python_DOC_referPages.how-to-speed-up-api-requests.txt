<div class="post-text" itemprop="text">
<p>I've constructed the following little program for getting phone numbers using google's place api but it's pretty slow. When I'm testing with 6 items it takes anywhere from 4.86s to 1.99s and I'm not sure why the significant change in time.  I'm very new to API's so I'm not even sure what sort of things can/cannot be sped up, which sort of things are left to the webserver servicing the API and what I can change myself.</p>
<pre><code>import requests,json,time
searchTerms = input("input places separated by comma")

start_time = time.time() #timer
searchTerms = searchTerms.split(',')
for i in searchTerms:
    r1 = requests.get('https://maps.googleapis.com/maps/api/place/textsearch/json?query='+ i +'&amp;key=MY_KEY')
    a = r1.json()
    pid = a['results'][0]['place_id']
    r2 = requests.get('https://maps.googleapis.com/maps/api/place/details/json?placeid='+pid+'&amp;key=MY_KEY')
    b = r2.json()
    phone = b['result']['formatted_phone_number']
    name = b['result']['name']
    website = b['result']['website']
    print(phone+' '+name+' '+website)

print("--- %s seconds ---" % (time.time() - start_time))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You may want to send requests in parallel. Python provides <a href="https://docs.python.org/3/library/multiprocessing.html" rel="noreferrer"><code>multiprocessing</code></a> module which is suitable for task like this.</p>
<p>Sample code:</p>
<pre><code>from multiprocessing import Pool

def get_data(i):
    r1 = requests.get('https://maps.googleapis.com/maps/api/place/textsearch/json?query='+ i +'&amp;key=MY_KEY')
    a = r1.json()
    pid = a['results'][0]['place_id']
    r2 = requests.get('https://maps.googleapis.com/maps/api/place/details/json?placeid='+pid+'&amp;key=MY_KEY')
    b = r2.json()
    phone = b['result']['formatted_phone_number']
    name = b['result']['name']
    website = b['result']['website']
    return ' '.join((phone, name, website))

if __name__ == '__main__':
    terms = input("input places separated by comma").split(",")
    with Pool(5) as p:
        print(p.map(get_data, terms))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Use sessions to enable persistent HTTP connections (so you don't have to establish a new connection every time)</p>
<p>Docs: <a href="http://docs.python-requests.org/en/master/user/advanced/#session-objects" rel="nofollow noreferrer">Requests Advanced Usage - Session Objects</a></p>
</div>
<div class="post-text" itemprop="text">
<p>its a matter of latency between client and servers , you can't change anything in this way unless you use multiple server location ( the near server to the client are getting the request ) .</p>
<p>in term of performance you can build a multithreding system that can handel multiple requests at once . </p>
</div>
<div class="post-text" itemprop="text">
<p>Most of the time isn't spent computing your request. The time is spent in communication with the server. That is a thing you cannot control.</p>
<p>However, you may be able to speed it along using parallelization. Create a separate thread for each request as a start.</p>
<pre><code>from threading import Thread

def request_search_terms(*args):
    #your logic for a request goes here
    pass

#...

threads = []
for st in searchTerms:
    threads.append (Thread (target=request_search_terms, args=(st,)))
    threads[-1].start()

for t in threads:
    t.join();
</code></pre>
<p>Then use a thread pool as the number of request grows, this will avoid the overhead of repeated thread creation.</p>
</div>
<div class="post-text" itemprop="text">
<p>There is no need to do multithreading yourself. <a href="https://pypi.python.org/pypi/grequests" rel="nofollow noreferrer">grequests</a> provides a quick drop-in replacement for requests. </p>
</div>
<span class="comment-copy">I think you have to consider various time factors here. First the amount of time taken by your programme to retrieve the info from the mentioned URL(this will be affected by the internet speed and the time taken by the web server to send the response) + time taken by the python to analyse that information. I would suggest to compute these two times separately and see which time is taking longer and how much variation is there..</span>
<span class="comment-copy">keep in mind that at some point you will hit Google maps' API rate limits ;)</span>
<span class="comment-copy">This is fantastic! Can you please help me understand the <code>if __name__ == '__main__':</code> portion so I can use this knowledge in the future?</span>
<span class="comment-copy"><a href="http://stackoverflow.com/questions/419163/what-does-if-name-main-do">What does <code>if __name__ == “__main__”:</code> do?</a>.</span>
<span class="comment-copy">I mean't to ask, what does everything contained within the if.   Like Pool(5) and p.map</span>
<span class="comment-copy">I'll provide some explanation although it probably won't help you seeing as it is 2.5 years too late: <code>with Pool..</code> creates the <code>Pool</code> object under the control of a context manager, meaning that the object will be destroyed and cleanup code called when the program exits the scope of the <code>with</code> statement. <code>Pool(5)</code> creates a thread pool with 5 threads that are all capable of running independently. This means that the second HTTP request you make does not have to wait for the first HTTP request to be returned - so instead of 5 200ms operations in series, you do 5 200ms waits all at once.</span>
