<div class="post-text" itemprop="text">
<p>I have a situation to call multiple requests in a scheduler job to check live user status for 1000 users at a time. But server limits maximum up to 50 users in each hit of an API request. So using following approach with <code>for</code> loop its taking around 66 seconds for 1000 users (i.e for 20 API calls).</p>
<pre><code>from apscheduler.schedulers.blocking import BlockingScheduler
sched = BlockingScheduler()

def shcdulerjob():
    """
    """
    uidlist = todays_userslist() #Get around 1000 users  from table 
    #-- DIVIDE LIST BY GIVEN SIZE (here 50) 
    split_list = lambda lst, sz: [lst[i:i+sz] for i in range(0, len(lst), sz)] 
    idlists = split_list(uidlist, 50) # SERVER MAX LIMIT - 50 ids/request 
    for idlist in idlists:
      apiurl = some_server_url + "&amp;ids="+str(idlist) 
      resp =  requests.get(apiurl)
      save_status(resp.json()) #-- Save status to db
if __name__ == "__main__":
  sched.add_job(shcdulerjob, 'interval', minutes=10)
  sched.start()
</code></pre>
<p>So, </p>
<ul>
<li>Is there any workaround so that it should optimize the time required to fetch API?</li>
<li>Does <code>Python- APScheduler</code> provide any multiprocessing option to process such api requests in a single job?</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>You could try to apply python's Thread pool from the <code>concurrent.futures</code> module, if the server allows concurrent requests. That way you would parallelise the processing, instead of the scheduling itself</p>
<p>There are some good examples provided in the documentation <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow">here</a> (If you're using python 2, there is a sort of an <a href="https://docs.python.org/2.7/library/multiprocessing.html?highlight=process%20pool#module-multiprocessing.pool" rel="nofollow">equivalent module</a></p>
<p>e.g.</p>
<pre><code>import concurrent.futures
import multiprocessing
import requests
import time
import json

cpu_start_time = time.process_time()
clock_start_time = time.time()

queue = multiprocessing.Queue()
uri = "http://localhost:5000/data.json"
users = [str(user) for user in range(1, 50)]

with concurrent.futures.ThreadPoolExecutor(multiprocessing.cpu_count()) as executor:
    for user_id, result in zip(
            [str(user) for user in range(1, 50)]
            , executor.map(lambda x: requests.get(uri, params={id: x}).content, users)
    ):
        queue.put((user_id, result))

while not queue.empty():

    user_id, rs = queue.get()
    print("User ", user_id, json.loads(rs.decode()))

cpu_end_time = time.process_time()
clock_end_time = time.time()

print("Took {0:.03}s [{1:.03}s]".format(cpu_end_time-cpu_start_time,   clock_end_time-clock_start_time))
</code></pre>
<p>If you want to use a Process pool, just make sure you don't use shared resources, e.g. queue, and write your data our independently</p>
</div>
<span class="comment-copy">Let me try this. Thanks</span>
