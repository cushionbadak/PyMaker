<div class="post-text" itemprop="text">
<p>In a list of tuples, I want to have just one copy of a tuple where it may be (x, y) or (y, x).</p>
<p>So, in:</p>
<pre><code># pairs = list(itertools.product(range(3), range(3)))
pairs = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]
</code></pre>
<p>the result should be:</p>
<pre><code>result = [(0, 0), (0, 1), (0, 2), (1, 1), (1, 2), (2, 2)] # updated pairs
</code></pre>
<p>This list of tuples is generated using <a href="https://docs.python.org/3/library/itertools.html#itertools.product" rel="nofollow noreferrer"><code>itertools.product()</code></a> but I want to remove the duplicates.</p>
<p><em>My working solution:</em></p>
<pre><code>pairs = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]

result = []
for pair in pairs:
    a, b = pair
    # reordering in increasing order
    temp = (a, b) if a &lt; b else (b, a)
    result.append(temp)
print(list(set(result))) # I could use sorted() but the order doesn't matter
</code></pre>
<p><strong>How can this be improved?</strong></p>
</div>
<div class="post-text" itemprop="text">
<p>You could use <a href="https://docs.python.org/2/library/itertools.html#itertools.combinations_with_replacement" rel="nofollow noreferrer">combinations_with_replacement</a></p>
<blockquote>
<p>The code for combinations_with_replacement() can be also expressed as a subsequence of product() after filtering entries where the elements are not in sorted order (according to their position in the input pool)</p>
</blockquote>
<pre><code>import itertools
pairs = list(itertools.combinations_with_replacement(range(3), 2))
print(pairs)

&gt;&gt;&gt; [(0, 0), (0, 1), (0, 2), (1, 1), (1, 2), (2, 2)]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This is one solution which relies on <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix" rel="nofollow noreferrer">sparse matrices</a>. This works for the following reasons:  </p>
<p>An entry in a matrix cannot contain two values. Therefore, uniqueness is guaranteed.</p>
<p>Selecting the upper triangle ensures that (0, 1) is preferred above (1, 0), and inclusion of both is not possible.</p>
<pre><code>import numpy as np
from scipy.sparse import csr_matrix, triu

lst = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1),
       (1, 2), (2, 0), (2, 1), (2, 2)]

# get row coords &amp; col coords
d1, d2 = list(zip(*lst))

# set up sparse matrix inputs
row, col, data = np.array(d1), np.array(d2), np.array([1]*len(lst))

# get upper triangle of matrix including diagonal
m = triu(csr_matrix((data, (row, col))), 0)

# output coordinates
result = list(zip(*(m.row, m.col)))

# [(0, 0), (0, 1), (0, 2), (1, 1), (1, 2), (2, 2)]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>edit</strong> I just realized, your solution matches my solution. What you are doing is just fine. If you need to do this for a very large list, then there are some other options you may want to look into, like a key value store.</p>
<p>If you need to remove dupes more programatically, then you can use a function like this:</p>
<pre><code>def set_reduce(pairs):
    new_pairs = set([])
    for x,y in pairs:
        if x &lt; y:
            new_pairs.add((x,y))
        else:
            new_pairs.add((y,x))
    return new_pairs
</code></pre>
<p>running this results in </p>
<pre><code>&gt;&gt;&gt;set_reduce(pairs)
set([(0, 1), (1, 2), (0, 0), (0, 2), (2, 2), (1, 1)])
</code></pre>
</div>
<span class="comment-copy">What is wrong with your solution? What aspect of your code are you looking to improve?</span>
<span class="comment-copy">@ChrisMartin speed improvements &gt; in-place removals &gt; concise code</span>
<span class="comment-copy">@AjitZero, these look like coordinates. Is that where they originate? The reason I ask is that sparse matrices are good at storing and manipulating such data. I've included a solution below which takes advantage of this.</span>
<span class="comment-copy">@jp_data_analysis For my case, no. Thanks for sharing that though!</span>
