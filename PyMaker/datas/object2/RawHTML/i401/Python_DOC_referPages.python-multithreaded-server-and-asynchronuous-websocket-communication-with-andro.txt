<div class="post-text" itemprop="text">
<p>I have an Android client app that sends some data to a server in Python, where the Python server is supposed to run a long time-consuming operation/computation and return the results to the client.</p>
<p>To do so, I initially started using Flask with Python on the server side, and an <a href="http://loopj.com/android-async-http/" rel="nofollow noreferrer">asynchronous android http library</a> on the client side to send the data via http POST. However, I quickly noticed that this is not the way to go, because the computation on the server takes time which causes problems such as the client getting timeout errors ... etc.</p>
<p>Then, I started using <a href="https://www.tornadoweb.org/en/stable/websocket.html" rel="nofollow noreferrer">Tornado's Websockets</a> on the server side, and an <a href="https://square.github.io/okhttp/3.x/okhttp/okhttp3/WebSocket.html" rel="nofollow noreferrer">android library for websockets</a> on the client side. However, the first main problem is that when the server is running the time-consuming operation for a given client, the other potential clients need to wait ... and it seems a bit of a pain to make tornado work in a multi-threaded setting (as it is originally planned to be single-threaded). Another minor problem, is if the client goes off-line while the server is processing his request, then the client might never get the result when he connects back.</p>
<p>Therefore, I would like to ask if you have any solutions or recommendation on what to use if I want to have such a setting with an asynchronous multi-threaded Python server who is supposed to do heavy-cpu computations with data from a client without making the other potential clients wait for their turn; and potentially making the client able to get the result from the server when he connects back.</p>
</div>
<div class="post-text" itemprop="text">
<p>FIrst of all, if you're going to do <strong>cpu-heavy</strong> operations in your backend, you [most probably] need to run it in separate <em>process</em>. Not in thread/coro/etc. The reason is that python is limited to single thread at time (you may read more about <a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow noreferrer">GIL</a>). Doing cpu-heavy operation in multithreading gives your backend some availability, but hits performance overall.</p>
<ol>
<li><p>Simple/old solution for this — run your backend in multiple process (and threads, preferably). I.e. deploy your flask with <a href="https://gunicorn.org/" rel="nofollow noreferrer">gunicorn</a>, give it multiple worker processes. This way, you'll have system that capable of doing <code>number_of_processes - 1</code> heavy computations and still be available for handling requests. Limit for processes is usually up to <code>cpu_cores * 2</code>, depending on cpu arch.</p></li>
<li><p>Slightly more complicated:</p>
<ul>
<li>accept data</li>
<li>run heavy function in different process</li>
<li>gather result, return</li>
</ul>
<p>Great interface for this would be <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor" rel="nofollow noreferrer"><code>ProcessPoolExecutor</code></a>. The drawback is — it's harder to handle failures/process hanging over</p></li>
<li><p>Another way around is task queue + workers. One of most used is <a href="http://www.celeryproject.org/" rel="nofollow noreferrer"><code>celery</code></a>. Idea is to</p>
<ul>
<li>open WS connection</li>
<li>put task in queue</li>
<li>worker (in different process or even different physical node) eventually picks up task, compute it, put result in some DB</li>
<li>main process gets callback/result of long polling over result DB</li>
<li>main process sends result over WS</li>
</ul>
<p>This is more suited for really heavy and not real-time tasks, but gives you out-of-the-box handling for failures/restarts/etc.</p></li>
</ol>
</div>
