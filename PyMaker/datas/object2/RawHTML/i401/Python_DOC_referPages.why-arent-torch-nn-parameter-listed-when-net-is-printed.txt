<div class="post-text" itemprop="text">
<p>I recently had to construct a module that required a tensor to be included. While back propagation worked perfectly using <code>torch.nn.Parameter</code>, it did not show up when printing the net object. Why isn't this <code>parameter</code> included in contrast to other modules like <code>layer</code>? (Shouldn't it behave just like <code>layer</code>?)</p>
<pre><code>import torch
import torch.nn as nn

class MyNet(torch.nn.Module):
    def __init__(self):
        super(MyNet, self).__init__()
        self.layer = nn.Linear(10, 10)
        self.parameter = torch.nn.Parameter(torch.zeros(10,10, requires_grad=True))

net = MyNet()
print(net)
</code></pre>
<p>Output:</p>
<pre><code>MyNet(
  (layer): Linear(in_features=10, out_features=10, bias=True)
)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>When you call <code>print(net)</code>, the <code>__repr__</code> method is called. <a href="https://docs.python.org/3/reference/datamodel.html#object.__repr__" rel="nofollow noreferrer"><code>__repr__</code></a> gives the “official” string representation of an object. </p>
<p>In PyTorch's <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html" rel="nofollow noreferrer"><code>nn.Module</code></a> (base class of your <code>MyNet</code> model), the <code>__repr__</code> is implemented like this:</p>
<pre><code>def __repr__(self):
        # We treat the extra repr like the sub-module, one item per line
        extra_lines = []
        extra_repr = self.extra_repr()
        # empty string will be split into list ['']
        if extra_repr:
            extra_lines = extra_repr.split('\n')
        child_lines = []
        for key, module in self._modules.items():
            mod_str = repr(module)
            mod_str = _addindent(mod_str, 2)
            child_lines.append('(' + key + '): ' + mod_str)
        lines = extra_lines + child_lines

        main_str = self._get_name() + '('
        if lines:
            # simple one-liner info, which most builtin Modules will use
            if len(extra_lines) == 1 and not child_lines:
                main_str += extra_lines[0]
            else:
                main_str += '\n  ' + '\n  '.join(lines) + '\n'

        main_str += ')'
        return main_str
</code></pre>
<p>Note that the above method returns <code>main_str</code> which contains call to only <code>_modules</code> and <code>extra_repr</code>, thus it prints only modules by default. </p>
<hr/>
<p>PyTorch also provides <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.extra_repr" rel="nofollow noreferrer"><code>extra_repr()</code></a> method which you can implement yourself for extra representation of the module.</p>
<blockquote>
<p>To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable.</p>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<p>According to <code>nn.Parameter</code> docs:</p>
<blockquote>
<p>Parameters are :class:<code>~torch.Tensor</code> subclasses, that have a
      very special property when used with :class:<code>Module</code> s - when they're
      assigned as Module attributes they are automatically added to the list of
      its parameters, and will appear e.g. in :meth:<code>~Module.parameters</code> iterator.</p>
</blockquote>
<p>So you can find it in <code>net.parameters</code>. Let's look at the following example:</p>
<p>Code:</p>
<pre><code>import torch
import torch.nn as nn

torch.manual_seed(42)

class MyNet(nn.Module):
    def __init__(self):
        super(MyNet, self).__init__()
        self.layer = nn.Linear(4, 4)
        self.parameter = nn.Parameter(torch.zeros(4, 4, requires_grad=True))
        self.tensor = torch.ones(4, 4)
        self.module = nn.Module()

net = MyNet()
print(net)
</code></pre>
<p>Output:</p>
<pre><code>MyNet(
  (layer): Linear(in_features=4, out_features=4, bias=True)
  (module): Module()
)
</code></pre>
<p>As you can see, there is no <code>tensor</code> or 'parameter' object (because <code>parameter</code> is subclass of <code>tensor</code>), only <code>Module</code>s.</p>
<p>Now let's try to get our net parameters:</p>
<p>Code:</p>
<pre><code>for p in net.parameters():
    print(p)
</code></pre>
<p>Output:</p>
<pre><code>Parameter containing:
tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]], requires_grad=True)
Parameter containing:
tensor([[ 0.3823,  0.4150, -0.1171,  0.4593],
        [-0.1096,  0.1009, -0.2434,  0.2936],
        [ 0.4408, -0.3668,  0.4346,  0.0936],
        [ 0.3694,  0.0677,  0.2411, -0.0706]], requires_grad=True)
Parameter containing:
tensor([ 0.3854,  0.0739, -0.2334,  0.1274], requires_grad=True)
</code></pre>
<p>Ok, so the first one is your <code>net.parameter</code>. Next two is weights and bias of <code>net.layer</code>. Let's verify it:</p>
<p>Code:</p>
<pre><code>print(net.layer.weight)
print(net.layer.bias)
</code></pre>
<p>Output:</p>
<pre><code>Parameter containing:
tensor([[ 0.3823,  0.4150, -0.1171,  0.4593],
        [-0.1096,  0.1009, -0.2434,  0.2936],
        [ 0.4408, -0.3668,  0.4346,  0.0936],
        [ 0.3694,  0.0677,  0.2411, -0.0706]], requires_grad=True)
Parameter containing:
tensor([ 0.3854,  0.0739, -0.2334,  0.1274], requires_grad=True)
</code></pre>
</div>
