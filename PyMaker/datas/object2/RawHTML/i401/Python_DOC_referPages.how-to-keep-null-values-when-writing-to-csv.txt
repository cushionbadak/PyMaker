<div class="post-text" itemprop="text">
<p>I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command. The issue is that Python's csv writer automatically converts Nulls into an empty string "" and it fails my job when the column is an int or float datatype and it tries to insert this "" when it should be a None or null value.</p>
<blockquote>
<p>To make it as easy as possible to interface with modules which
  implement the DB API, the value None is written as the empty string.</p>
<p><a href="https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer" rel="nofollow noreferrer">https://docs.python.org/3.4/library/csv.html?highlight=csv#csv.writer</a></p>
</blockquote>
<p>What is the best way to keep the null value? Is there a better way to write csvs in Python? I'm open to all suggestions.</p>
<p>Example:</p>
<p>I have lat and long values:</p>
<pre><code>42.313270000    -71.116240000
42.377010000    -71.064770000
NULL    NULL
</code></pre>
<p>When writing to csv it converts nulls to "":</p>
<pre><code>with file_path.open(mode='w', newline='') as outfile:
    csv_writer = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)
    if include_headers:
        csv_writer.writerow(col[0] for col in self.cursor.description)
    for row in self.cursor:
        csv_writer.writerow(row)
</code></pre>
<p>.</p>
<pre><code>42.313270000,-71.116240000
42.377010000,-71.064770000
"",""
</code></pre>
<blockquote>
<p>NULL</p>
<p>Specifies the string that represents a null value. The default is \N
  (backslash-N) in text format, and an unquoted empty string in CSV
  format. You might prefer an empty string even in text format for cases
  where you don't want to distinguish nulls from empty strings. This
  option is not allowed when using binary format.</p>
<p><a href="https://www.postgresql.org/docs/9.2/sql-copy.html" rel="nofollow noreferrer">https://www.postgresql.org/docs/9.2/sql-copy.html</a></p>
</blockquote>
<p><strong>ANSWER:</strong></p>
<p>What solved the problem for me was changing the quoting to csv.QUOTE_MINIMAL.</p>
<blockquote>
<p>csv.QUOTE_MINIMAL Instructs writer objects to only quote those fields
  which contain special characters such as delimiter, quotechar or any
  of the characters in lineterminator.</p>
</blockquote>
<p>Related questions:
- <a href="https://stackoverflow.com/questions/45892420/postgresql-copy-empty-string-as-null-not-work">Postgresql COPY empty string as NULL not work</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You have two options here: change the <code>csv.writing</code> quoting option in Python, or tell PostgreSQL to accept quoted strings as possible NULLs (requires PostgreSQL 9.4 or newer)</p>
<h2>Python <code>csv.writer()</code> and quoting</h2>
<p>On the Python side, you are telling the <code>csv.writer()</code> object to add quotes, because you configured it to use <a href="https://docs.python.org/3/library/csv.html#csv.QUOTE_NONNUMERIC" rel="nofollow noreferrer"><code>csv.QUOTE_NONNUMERIC</code></a>:</p>
<blockquote>
<p>Instructs <code>writer</code> objects to quote all non-numeric fields.</p>
</blockquote>
<p><code>None</code> values are non-numeric, so result in <code>""</code> being written.</p>
<p>Switch to using <a href="https://docs.python.org/3/library/csv.html#csv.QUOTE_MINIMAL" rel="nofollow noreferrer"><code>csv.QUOTE_MINIMAL</code></a> or <a href="https://docs.python.org/3/library/csv.html#csv.QUOTE_NONE" rel="nofollow noreferrer"><code>csv.QUOTE_NONE</code></a>:</p>
<blockquote>
<p><code>csv.QUOTE_MINIMAL</code><br/>
  Instructs <code>writer</code> objects to only quote those fields which contain special characters such as <em>delimiter</em>, <em>quotechar</em> or any of the characters in <em>lineterminator</em>.</p>
<p><code>csv.QUOTE_NONE</code><br/>
  Instructs <code>writer</code> objects to never quote fields. When the current <em>delimiter</em> occurs in output data it is preceded by the current <em>escapechar</em> character.</p>
</blockquote>
<p>Since all you are writing is longitude and latitude values, you don't need any quoting here, there are no delimiters or quotecharacters present in your data.</p>
<p>With either option, the CSV output for <code>None</code> values is simple an empty string:</p>
<pre><code>&gt;&gt;&gt; import csv
&gt;&gt;&gt; from io import StringIO
&gt;&gt;&gt; def test_csv_writing(rows, quoting):
...     outfile = StringIO()
...     csv_writer = csv.writer(outfile, delimiter=',', quoting=quoting)
...     csv_writer.writerows(rows)
...     return outfile.getvalue()
...
&gt;&gt;&gt; rows = [
...     [42.313270000, -71.116240000],
...     [42.377010000, -71.064770000],
...     [None, None],
... ]
&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_NONNUMERIC))
42.31327,-71.11624
42.37701,-71.06477
"",""

&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_MINIMAL))
42.31327,-71.11624
42.37701,-71.06477
,

&gt;&gt;&gt; print(test_csv_writing(rows, csv.QUOTE_NONE))
42.31327,-71.11624
42.37701,-71.06477
,
</code></pre>
<h2>PostgreSQL 9.4 <code>COPY FROM</code>, <code>NULL</code> values and <code>FORCE_NULL</code></h2>
<p>As of PostgreSQL 9.4, you can also force PostgreSQL to accept quoted empty strings as <code>NULL</code>s, when you use the <code>FORCE_NULL</code> option. From the <a href="https://www.postgresql.org/docs/10/sql-copy.html" rel="nofollow noreferrer"><code>COPY FROM</code> documentation</a>:</p>
<blockquote>
<p><code>FORCE_NULL</code></p>
<p>Match the specified columns' values against the null string, even if it has been quoted, and if a match is found set the value to <code>NULL</code>. In the default case where the null string is empty, this converts a quoted empty string into <code>NULL</code>. This option is allowed only in <code>COPY FROM</code>, and only when using CSV format.</p>
</blockquote>
<p>Naming the columns in a <code>FORCE_NULL</code> option lets PostgreSQL accept both the empty column and <code>""</code> as <code>NULL</code> values for those columns, e.g.:</p>
<pre class="lang-sql prettyprint-override"><code>COPY position (
    lon, 
    lat
) 
FROM "filename"
WITH (
    FORMAT csv,
    NULL '',
    DELIMITER ',',
    FORCE_NULL(lon, lat)
);
</code></pre>
<p>at which point it doesn't matter anymore what quoting options you used on the Python side.</p>
<h2>Other options to consider</h2>
<h3>For simple data transformation tasks from other databases, don't use Python</h3>
<p>If you already querying databases to collate data to go into PostgreSQL, consider <em>directly inserting into Postgres</em>. If the data comes from other sources, using the <a href="https://www.postgresql.org/docs/9.4/postgres-fdw.html" rel="nofollow noreferrer">foreign data wrapper (<em>fdw</em>) module</a> lets you cut out the middle-man and directly pull data into PostgreSQL from other sources.</p>
<h3>Numpy data? Consider using COPY FROM as binary, directly from Python</h3>
<p>Numpy data can more efficiently be inserted via <a href="https://stackoverflow.com/questions/8144002/use-binary-copy-table-from-with-psycopg2/8150329#8150329">binary <code>COPY FROM</code></a>; the linked answer augments a numpy structured array with the required extra metadata and byte ordering, then efficiently creates a binary copy of the data and inserts it into PostgreSQL using <code>COPY FROM STDIN WITH BINARY</code> and the <a href="http://initd.org/psycopg/docs/cursor.html#cursor.copy_expert" rel="nofollow noreferrer"><code>psycopg2.copy_expert()</code> method</a>. This neatly avoids number -&gt; text -&gt; number conversions.</p>
<h3>Persisting data to handle large datasets in a pipeline?</h3>
<p>Don't re-invent the data pipeline wheels. Consider using existing projects such as <a href="http://spark.apache.org/" rel="nofollow noreferrer">Apache Spark</a>, which have already solved the efficiency problems. Spark lets you <a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model" rel="nofollow noreferrer">treat data as a structured stream</a>, and includes the infrastructure to <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="nofollow noreferrer">run data analysis steps in parallel</a>, and you can treat <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" rel="nofollow noreferrer">distributed, structured data as Pandas dataframes</a>.</p>
<p>Another option might be to look at <a href="https://distributed.readthedocs.io/en/latest/" rel="nofollow noreferrer">Dask</a> to help share datasets between distributed tasks to process large amounts of data.</p>
<p>Even if converting an already running project to Spark might be a step too far, at least consider using <a href="https://arrow.apache.org/" rel="nofollow noreferrer">Apache Arrow</a>, the data exchange platform Spark builds on top of. The <a href="https://arrow.apache.org/docs/python/" rel="nofollow noreferrer"><code>pyarrow</code> project</a> would let you exchange data via Parquet files, or <a href="https://arrow.apache.org/docs/python/ipc.html" rel="nofollow noreferrer">exchange data over IPC</a>.</p>
<p>The Pandas and Numpy teams are quite heavily invested in supporting the needs of Arrow and Dask (there is considerable overlap in core members between these projects) and are actively working to make Python data exchange as efficient as possible, including <a href="https://www.python.org/dev/peps/pep-0574/" rel="nofollow noreferrer">extending Python's <code>pickle</code> module to allow for out-of-band data streams</a> to avoid unnecessary memory copying when sharing data. </p>
</div>
<div class="post-text" itemprop="text">
<p>your code</p>
<pre><code>for row in self.cursor:
    csv_writer.writerow(row)
</code></pre>
<p>uses writer as-is, but you don't have to do that. You can filter the values to change some particular values with a generator comprehension and a ternary expression</p>
<pre><code>for row in self.cursor:
    csv_writer.writerow("null" if x is None else x for x in row)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You are asking for <code>csv.QUOTE_NONNUMERIC</code>.  This will turn everything that is not a number into a string.  You should consider using <code>csv.QUOTE_MINIMAL</code> as it might be more what you are after:</p>
<h3>Test Code:</h3>
<pre><code>import csv

test_data = (None, 0, '', 'data')
for name, quotes in (('test1.csv', csv.QUOTE_NONNUMERIC),
                     ('test2.csv', csv.QUOTE_MINIMAL)):

    with open(name, mode='w') as outfile:
        csv_writer = csv.writer(outfile, delimiter=',', quoting=quotes)
        csv_writer.writerow(test_data))
</code></pre>
<h3>Results:</h3>
<p><strong>test1.csv:</strong></p>
<pre><code>"",0,"","data"
</code></pre>
<p><strong>test2.csv:</strong></p>
<pre><code>,0,,data
</code></pre>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>I'm writing data from sql server into a csv file using Python's csv module and then uploading the csv file to a postgres database using the copy command.</p>
</blockquote>
<p>I believe your true requirement is you need to hop data rows through the filesystem, and as both the sentence above and the question title make clear, you are <strong>currently</strong> doing that with a csv file.
Trouble is that csv format offers poor support for the RDBMS notion of NULL.
Let me solve your problem for you by changing the question slightly.
I'd like to introduce you to parquet format.
Given a set of table rows in memory, it allows you to <em>very quickly</em> persist them to a compressed binary file, and recover them, with metadata and NULLs intact, no text quoting hassles.
Here is an example, using the <a href="https://pypi.org/project/pyarrow/" rel="nofollow noreferrer">pyarrow 0.12.1</a> parquet engine:</p>
<pre><code>import pandas as pd
import pyarrow


def round_trip(fspec='/tmp/locations.parquet'):
    rows = [
        dict(lat=42.313, lng=-71.116),
        dict(lat=42.377, lng=-71.065),
        dict(lat=None, lng=None),
    ]

    df = pd.DataFrame(rows)
    df.to_parquet(fspec)
    del(df)

    df2 = pd.read_parquet(fspec)
    print(df2)


if __name__ == '__main__':
    round_trip()
</code></pre>
<p>Output:</p>
<pre><code>      lat     lng
0  42.313 -71.116
1  42.377 -71.065
2     NaN     NaN
</code></pre>
<p>Once you've recovered the rows in a dataframe you're free to call <code>df2.to_sql()</code> or use some other favorite technique to put numbers and NULLs into a DB table.</p>
<p>EDIT:</p>
<p>If you're able to run <code>.to_sql()</code> on the PG server, or on same LAN, then do that.
Otherwise your favorite technique will likely involve <code>.copy_expert()</code>.
Why?
The summary is that with psycopg2, "bulk INSERT is slow".
Middle layers like sqlalchemy and pandas, and well-written apps that care about insert performance, will use <a href="http://mysql-python.sourceforge.net/MySQLdb.html#some-examples" rel="nofollow noreferrer"><code>.executemany()</code></a>.
The idea is to send lots of rows all at once, without waiting for individual result status, because we're not worried about unique index violations.
So TCP gets a giant buffer of SQL text and sends it all at once, saturating the end-to-end channel's bandwidth,
much as copy_expert sends a big buffer to TCP to achieve high bandwidth.</p>
<p>In contrast the psycopg2 driver lacks support for high performance executemany.
As of 2.7.4 it just executes items one at a time, sending a SQL command across the WAN and waiting a round trip time for the result before sending next command.
Ping your server;
if ping times suggest you could get a dozen round trips per second,
then plan on only inserting about a dozen rows per second.
Most of the time is spent waiting for a reply packet, rather than spent processing DB rows.
It would be lovely if at some future date psycopg2 would offer better support for this.</p>
</div>
<div class="post-text" itemprop="text">
<p>I would use pandas,psycopg2,and sqlalchemy. Make sure  are installed. Coming from your current workflow and avoiding writing to csv</p>
<pre><code>#no need to import psycopg2
import pandas as pd
from sqlalchemy import create_engine


#create connection to postgres
engine = create_engine('postgres://.....')

#get column names from cursor.description
columns = [col[0] for col in self.cursor.description]

#convert data into dataframe
df = pd.DataFrame(cursor.fetchall(),columns=columns)

#send dataframe to postgres
df.to_sql('name_of_table',engine,if_exists='append',index=False)

#if you still need to write to csv
df.to_csv('your_file.csv')
</code></pre>
</div>
<span class="comment-copy">can you share an example? because csv writer can write integers (as strings) and floats (as strings). What do you want to write in place of <code>None</code>  / "Null" ?</span>
<span class="comment-copy">I ended up using QUOTE_MINIMAL and it worked for most datasets but it created extra columns for others. I'm still looking into why that happens but Jean-François answer along with specifying the null value in the copy command works...albeit very slowly. This is for an intermediate step to store larger datasets that can't fit in memory in a data pipeline and I wonder if you have any suggestions outside of this question. I appreciate your answer regardless.</span>
<span class="comment-copy">@JonathanPorter: it sounds as if you are trying to build a data pipeline from scratch. Have you considered using <a href="https://spark.apache.org/" rel="nofollow noreferrer">Apache Spark</a> instead? Spark has excellent, <a href="https://pypi.org/project/pyspark/" rel="nofollow noreferrer">first class Python support</a>, and lets you stream datasets, operate on datasets in parallel or lets you access data via <a href="https://arrow.apache.org/docs/python/" rel="nofollow noreferrer">Apache Arrow</a>, a much more efficient format to exchange large amounts of columnar data.</span>
<span class="comment-copy">@JonathanPorter: Jean-François's solution is slow because it transforms each row by hand, and you can't really speed up that operation when you have a structured data source as input (numpy, database query rows, etc) because such sources balk at mixing numeric and string types. You'd be better of with sticking with <code>QUOTE_NONNUMERIC</code> and use <code>FORCE_NULL</code> on the numeric columns instead. Not that I can quite envision what you mean by 'created extra columns', that sounds like a <i>new</i> problem elsewhere, probably a bug in the way you write the CSV file.</span>
<span class="comment-copy">@JonathanPorter: I think I can think of a scenario where you might see extra columns: when your data contains delimiters or quotes in a value and you didn't configure <code>COPY FROM</code>'s <code>ESCAPE</code> or <code>QUOTE</code> options to match the settings used in your writer.</span>
<span class="comment-copy">Thank you for all of these suggestions. I indeed did not configure those options but I will double check them and also look into spark! I'll be sure to post what worked best in my case.</span>
<span class="comment-copy">Although this doesn't solve the not being able to insert varchar values into an int or float column I think it can help.</span>
<span class="comment-copy">sorry I cannot help more without more input. I don't know why int or float doesn't work as it should with csv. And I don't know what varchar is.</span>
<span class="comment-copy">Note that <code>DataFrame.to_sql()</code> is really very slow indeed. Bulk inserts into PostgreSQL is still <a href="https://stackoverflow.com/a/47984180/100297">best done with COPY FROM</a>.</span>
<span class="comment-copy">Why complicate matters with additional software? <code>DataFrame.to_csv()</code> still uses the <code>csv.writer()</code> object to do the actual writing to a file.</span>
<span class="comment-copy">From what I understand, the only reason Nulls are a problem is that OP can't insert them into Postgres. Does OP really need to save them in CSV? Or is OP just using CSV as intermediary step. Regardless, pandas is good tool to use specially if OP needs to transform the data in the future. Complicated is not always a bad thing. Simple isn't always the best solution.</span>
<span class="comment-copy">Note that <code>to_sql()</code> is quite <i>slow</i>, CSV import is a lot faster for bulk import. Neither answers the question the OP has, and the answer to that question is quite simple: switch quoting configuration.</span>
<span class="comment-copy">I concede that it doesn't answer the OP's question directly, but I stand by that it's an alternative, less performant but more flexible. Also, I'm having jitters and elated that you commented on my answer.</span>
