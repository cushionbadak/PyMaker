<div class="post-text" itemprop="text">
<p>I have this code to scrape tagged users ids from medias on twitter:</p>
<pre><code>from bs4 import BeautifulSoup
from selenium import webdriver
import time
import csv
import re

# Create a new instance of the Firefox driver
driver = webdriver.Firefox()

# go to page
driver.get("http://twitter.com/RussiaUN/media")

#You can adjust it but this works fine
SCROLL_PAUSE_TIME = 2

# Get scroll height
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    # Scroll down to bottom
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    # Wait to load page
    time.sleep(SCROLL_PAUSE_TIME)

    # Calculate new scroll height and compare with last scroll height
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height


# Now that the page is fully scrolled, grab the source code.
src = driver.page_source

#Past it into BS
soup = BeautifulSoup(src, 'html.parser')
#divs = soup.find_all('div',class_='account')
divs = soup.find_all('div', {"data-user-id" : re.compile(r".*")})


#PRINT RESULT
#print('printing results')
#for div in divs:
#    print(div['data-user-id'])


#SAVE IN FILE
print('Saving results')    
#with open('file2.csv','w') as f:
 #  for div in divs:
  #      f.write(div['data-user-id']+'\n')    

with open('file.csv','w', newline='') as f:
    writer = csv.writer(f)
    for div in divs:
        writer.writerow([div['data-user-id']])
</code></pre>
<p>-But I would like to also scrape the usernames  and then organise all these datas in a csv with a column <strong>IDS</strong> and a column <strong>USERNAMES</strong>. </p>
<p>So my guess is that I have to modify this piece of code first:</p>
<pre><code>divs = soup.find_all('div', {"data-user-id" : re.compile(r".*")})
</code></pre>
<p>But I can't find a way to achieve that...</p>
<p>-Then I also have a problem with duplicates. As you can see in the code there are two ways to scrape the data:</p>
<p>1    <code>#divs = soup.find_all('div',class_='account')</code></p>
<p>2    <code>divs = soup.find_all('div', {"data-user-id" : re.compile(r".*")})</code></p>
<p>The first phrase seemed to work but was not efficient enough. Number 2 works fine but seems to give me dupplicates at the end as it goes through all the divs and not only the <code>class_='account'</code>.</p>
<p>I'm sorry if some feel that I'm a bit spammy here as I posted 3 questions in 24h...And thanks to those who helped and will be helping. </p>
</div>
<div class="post-text" itemprop="text">
<p>Python has an inbuilt <a href="https://docs.python.org/3/library/csv.html#module-csv" rel="nofollow noreferrer">csv module</a> for writing csv files. </p>
<p>Also the scroll script that you used did not seem to work as it was not scrolling all the way down and stopped after a certain amount of time. I just got ~ 1400 records in the csv file with your script.I have replaced it with pagedown key. You may want to tweak the <code>no_of_pagedowns</code> to control the amount you want to scroll down. Even with <code>200</code> pagedowns i got ~2200 records. Note that this number is without removing the duplicates.</p>
<p>I have added some additional modifications to write only the unique data to file.</p>
<pre><code>from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import csv
driver = webdriver.Firefox()
driver.get("http://twitter.com/RussiaUN/media")
time.sleep(1)
elem = driver.find_element_by_tag_name("html")
no_of_pagedowns = 200
while no_of_pagedowns:
    elem.send_keys(Keys.PAGE_DOWN)
    time.sleep(2)
    no_of_pagedowns-=1


src = driver.page_source

soup = BeautifulSoup(src, 'html.parser')
divs = soup.find_all('div',class_='account')
all_data=[]
#get only unique data
for div in divs:
    single=[div['data-user-id'],div['data-screen-name']]
    if single not in all_data:
        all_data.append(single)
with open('file.csv','w') as f:
    writer = csv.writer(f, delimiter=",")
    #headers
    writer.writerow(["ID","USERNAME"])
    writer.writerows(all_data)
</code></pre>
<p>Output</p>
<pre><code>ID,USERNAME
255493944,MID_RF
2230446228,Rus_Emb_Sudan
1024596885661802496,ambrus_drc
2905424987,Russie_au_Congo
2174261359,RusEmbUganda
285532415,tass_agency
34200559,rianru
40807205,kpru
177502586,nezavisimaya_g
23936177,vzglyad
255471924,mfa_russia
453639812,pass_blue
...
</code></pre>
<p>If you want the duplicates just remove the if condition</p>
<pre><code>for div in divs:
    single=[div['data-user-id'],div['data-screen-name']]
    all_data.append(single)
</code></pre>
</div>
<span class="comment-copy">I noticed you are trying to scrape Twitter, and they really don't want you to do that. Have you considered using the <a href="https://developer.twitter.com/en/docs/api-reference-index" rel="nofollow noreferrer">Twitter API</a> for your program?</span>
<span class="comment-copy">Hello @Polkaguy6000. When I was thinking about collecting this data 12 months ago I did not know nothing about scraping (and I still don't know much). So, when I tried to learn to get the data, I thought that some Twitter API would do the trick but nope...you can't access the medias through the API. That's why it is used by the account I'm scraping to tag users to dogwhistle and spread propaganda, because they know that nobody can really analyze the scale of what they are doing.</span>
<span class="comment-copy">@MaxBaldwin What is the attribute of the username - is it data-screen-name or data-name</span>
<span class="comment-copy">@BittoBennichan Hello again :) I just checked again and it is data-screen-name.</span>
<span class="comment-copy">@BittoBennichan I don't know, I found this part of code elsewhere and pasted it as it seemed to work ok...</span>
<span class="comment-copy">Works perfectly concerning the csv file, Thanks a lot!! But I do want the duplicates actually. Your code was good, it's just that someone helped me and gave me a bit of code that gave me more duplicates. What I want to do is count the number of times each user was tagged by the account. Otherwise I kept my scrolling method for now as it was working for me but yours is much better as you can fine tune it.</span>
<span class="comment-copy">@MaxBaldwin See my edit.</span>
<span class="comment-copy">Dude this is perfect! Thanks a lot again!</span>
