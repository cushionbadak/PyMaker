<div class="post-text" itemprop="text">
<p>Training convolutional neural network from scratch on my own dataset with Keras and Tensorflow.</p>
<p><code>learning rate = 0.0001</code>, 
5 classes to sort, 
no Dropout used,
dataset checked twice, no wrong labels found</p>
<p>Model:</p>
<pre><code>model = models.Sequential()
model.add(layers.Conv2D(16,(2,2),activation='relu',input_shape=(75,75,3)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(16,(2,2),activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(32,(2,2),activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Flatten())
model.add(layers.Dense(128,activation='relu'))
model.add(layers.Dense(5,activation='sigmoid'))

model.compile(optimizer=optimizers.adam(lr=0.0001),
             loss='categorical_crossentropy',
             metrics=['acc'])

history = model.fit_generator(train_generator,
                              steps_per_epoch=100,
                              epochs=50,
                              validation_data=val_generator,
                              validation_steps=25)
</code></pre>
<p>Everytime when model achieves 25-35 epochs (80-90% accuracy) this happens:</p>
<pre><code>Epoch 31/50
100/100 [==============================] - 3s 34ms/step - loss: 0.3524 - acc: 0.8558 - val_loss: 0.4151 - val_acc: 0.7992
Epoch 32/50
100/100 [==============================] - 3s 34ms/step - loss: 0.3393 - acc: 0.8700 - val_loss: 0.4384 - val_acc: 0.7951
Epoch 33/50
100/100 [==============================] - 3s 34ms/step - loss: 0.3321 - acc: 0.8702 - val_loss: 0.4993 - val_acc: 0.7620
Epoch 34/50
100/100 [==============================] - 3s 33ms/step - loss: 1.5444 - acc: 0.3302 - val_loss: 1.6062 - val_acc: 0.1704
Epoch 35/50
100/100 [==============================] - 3s 34ms/step - loss: 1.6094 - acc: 0.2935 - val_loss: 1.6062 - val_acc: 0.1724
</code></pre>
<p>There is some similar problems with answers, but mostly they recommend to lower learning rate, but it doesnt help at all.</p>
<p><a href="https://i.stack.imgur.com/KHhEx.png" rel="nofollow noreferrer"><img alt="Accuracy Drop" src="https://i.stack.imgur.com/KHhEx.png"/></a></p>
<p>UPD: almost all weights and biases in network became <code>nan</code>. Network somehow died inside</p>
</div>
<div class="post-text" itemprop="text">
<p>Solution in this case:</p>
<p>I changed <code>sigmoid</code> function in last layer to <code>softmax</code> function and drops are gone</p>
<p>Why this worked out?</p>
<p><code>sigmoid</code> activation function is used for binary (two-class) classifications.
In multiclassification problems we should use <code>softmax</code> function - special extension of <code>sigmoid</code> function for multiclassification problems.</p>
<p>More information: <a href="https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier">Sigmoid vs Softmax</a></p>
<p>Special thanks to @desertnaut and @Shubham Panchal for error indication</p>
</div>
<span class="comment-copy">Any <code>nan</code> value in your datasets?</span>
<span class="comment-copy">What loss/cost function do you use? Is there any specific reason to use <code>sigmoid</code> activation for the last dense layer?</span>
<span class="comment-copy">What is the number of train and validation samples? Do you use the whole validation data everytime you calculate the accuracy?</span>
<span class="comment-copy">Is this a <i>multi-label</i> problem (samples can belong to more than one class at the same time) or simply multi-class (5 non-overlapping classes)? If it is the second, you should definitely change your last activation to <code>softmax)</code>.</span>
<span class="comment-copy">The downfall seems to be because of sigmoid function the last layer. You should use a softmax activation function instead.</span>
