<div class="post-text" itemprop="text">
<p>I have a dask dataframe with 100 partitions and 57 columns. I want to save it then as avro on Google Cloud Storage. I have checked the <a href="https://intake-avro.readthedocs.io/en/latest/" rel="nofollow noreferrer"><code>intake-avro</code> lib</a>, but it only can convert avro to dask dataframes and not viceversa. </p>
<p>Is there any lib out there to do this or should I write my own? I want to make sure I don't reinvent wheel.</p>
</div>
<div class="post-text" itemprop="text">
<p>Indeed, intake-avro (and other intake drivers) are concerned with format/service -&gt; familiar container, and not with output.</p>
<p>Avro is not, generally speaking, a columnar format, and so to write it with Dask, you need a <code>bag</code>, not a <code>dataframe</code>. You will want to do:</p>
<pre><code>df.to_bag().to_avro(...)
</code></pre>
<p>(see the <a href="http://docs.dask.org/en/latest/bag-api.html#dask.bag.Bag.to_avro" rel="nofollow noreferrer">docs</a>)</p>
<p>Unfortunately, you will need to construct your own schema JSON object, which should be fairly easy from the original dataframe's dtypes.</p>
<p>The library that intake-avro and Dask use for fast reading of avro to dataframes, <a href="https://github.com/martindurant/uavro" rel="nofollow noreferrer">uavro</a>, may at some point be extended to write too.</p>
</div>
<span class="comment-copy">thanks that's really helpful. do you know if dask hhas a function to create json schema?</span>
<span class="comment-copy">It does not. I don't know if there is some other place you could get this, but would not be hard to write.</span>
