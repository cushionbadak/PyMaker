<div class="post-text" itemprop="text">
<p>Hopefully this one will be an easy one. I am trying to do some webscraping where I download all the pdf files from a page. Currently I am scraping files from a sports page for practice. I used Automatetheboringstuff + a post from another user (<a href="https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup">retrieve links from web page using python and BeautifulSoup</a>)  to come up with this code.</p>
<pre><code>import requests
import time
from bs4 import BeautifulSoup, SoupStrainer

r = requests.get('http://secsports.go.com/media/baseball')

soup = BeautifulSoup(r.content)

for link in BeautifulSoup(r.text, parseOnlyThese=SoupStrainer('a')):
    if link.has_attr('href'):
    if 'pdf' in str(link):
        image_file = open(os.path.join('E:\\thisiswhereiwantmypdfstogo', os.path.basename(link['href'])), 'wb')
        for chunk in r.iter_content(100000):
            image_file.write(chunk)
            image_file.close()
</code></pre>
<p>The files that are output to the directory I specify are all there which is great, but the filesize is the same for all of them and when I open up AdobePro to look at them I get an error that says:</p>
<p>"Adobe Acrobat could not open "FILENAMEHERE" because it is either not a supported filetype or because the file has been damaged (for example, it was sent as an email attachment and wasn't correctly decoded)."</p>
<p>A little hint that clued me in to something going wrong with the write process was that after running image_file.write(chunk) it outputs the same numbers for each file. </p>
<p>Here is what the pdfs look like in the folder:</p>
<p><a href="https://i.stack.imgur.com/TTfzq.png" rel="nofollow noreferrer"><img alt="the_corrupted_pdfs" src="https://i.stack.imgur.com/TTfzq.png"/></a></p>
<p>I am thinking I just need to add a parameter somewhere during the writing process for it to work correctly, but I have no idea what it would be. I did some Google searching for an answer and also searched a bit on here but cannot find the answer.</p>
<p>Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>Hmmm. After doing some more research it seems like I figured out the problem. I do not understand exactly why this works, but I'll take a stab at it. I modified my code such that each link(['href']) becomes a response object. Then I wrote those to my directory and it worked. </p>
</div>
<span class="comment-copy">Maybe in the lib <code>from urllib.request import urlretrieve</code> the function <code>urlretrieve(link)</code> can help to downlaod the pdf edit : <a href="https://docs.python.org/3/library/urllib.request.html#legacy-interface" rel="nofollow noreferrer">docs.python.org/3/library/urllib.request.html#legacy-interface</a> if you need some information about the function</span>
<span class="comment-copy"><a href="http://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3" title="download file from web in python 3">stackoverflow.com/questions/7243750/â€¦</a> here there is multiple exemples.</span>
<span class="comment-copy">Could you show how you modified your code? I encountered the same problem. However, in my case, the first few links work totally fine but after about 50-ish links have been downloaded, the <code>requests.get(url, stream=True)</code> keeps corrupting whatever <code>pdf</code> files downloaded.</span>
