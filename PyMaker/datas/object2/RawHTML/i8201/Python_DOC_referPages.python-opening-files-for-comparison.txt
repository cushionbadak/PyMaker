<div class="post-text" itemprop="text">
<p>I am attempting to open two files and check if the first word in file_1 is in any line in file_2. If the first word in a line from file_1 matches the first word in a line in file_2 I'd like to print both lines out. However, with the below code I am not getting any result. I will be dealing with very large files so I'd like to avoid putting the files in to memory using a list or dictionary. I can only use the built in functions in Python3.3. Any advice would be appreciated? Also if there is a better way please also advise.</p>
<p>Steps I am trying to perform:</p>
<p>1.) Open file_1
2.) Open file_2
3.) Check if the first Word is in ANY line of file_2.
4.) If the first word in both files match print the line from both file_1 and file_2.</p>
<p>Contents of files:</p>
<pre><code>file_1:
Apples: 5 items in stock
Pears: 10 items in stock
Bananas: 15 items in stock

file_2:
Watermelon: 20 items in stock
Oranges: 30 items in stock
Pears: 25 items in stock
</code></pre>
<p>Code Attempt:</p>
<pre><code>with open('file_1', 'r') as a, open('file_2', 'r') as b:
    for x, y in zip(a, b):
        if any(x.split()[0] in item for item in b):
            print(x, y)
</code></pre>
<p>Desired Output:</p>
<pre><code>('Pears: 10 items in stock', 'Pears: 25 items in stock')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Try:</p>
<pre><code>for i in open('[Your File]'):
for x in open('[Your File 2]'):
    if i == x:
        print(i)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I would actually heavily suggest against storing data in 1GB sized text files and not in some sort of database/standard data storage file format. If your data were more complex, I'd suggest CSV or some sort of delimited format at minimum. If you can split and store the data in much smaller chunks, maybe a markup language like XML, HTML, or JSON (which would make navigation and extraction of data easy) which are far more organized and already optimized to handle what you're trying to do (locating matching keys and returning their values).</p>
<p>That said, you could use the "readline" method found in section 7.2.1 of the Python 3 docs to efficiently do what you're trying to do: <a href="https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-file" rel="nofollow noreferrer">https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-file</a>.</p>
<p>Or, you could just iterate over the file:</p>
<pre><code>def _get_key(string, delim):

    #Split key out of string
    key=string.split(delim)[0].strip()
    return key

def _clean_string(string, charToReplace):

    #Remove garbage from string
    for character in charToReplace:
        string=string.replace(character,'')

    #Strip leading and trailing whitespace
    string=string.strip()
    return string

def get_matching_key_values(file_1, file_2, delim, charToReplace):

    #Open the files to be compared
    with open(file_1, 'r') as a, open(file_2, 'r') as b:

    #Create an object to hold our matches
    matches=[]

    #Iterate over file 'a' and extract the keys, one-at-a-time
    for lineA in a:
        keyA=_get_key(lineA, delim)

        #Iterate over file 'b' and extract the keys, one-at-a-time
        for lineB in b:
            keyB=_get_key(lineB, delim)

            #Compare the keys. You might need upper, but I usually prefer 
            #to compare all uppercase to all uppercase
            if keyA.upper()==keyB.upper():
                cleanedOutput=(_clean_string(lineA, charToReplace), 
                               _clean_string(lineB, charToReplace))

                #Append the match to the 'matches' list
                matches.append(cleanedOutput)

        #Reset file 'b' pointer to start of file and try again
        b.seek(0)

    #Return our final list of matches 
    #--NOTE: this method CAN return an empty 'matches' object!
    return matches
</code></pre>
<p>This is not really the best/most efficient way to go about this: </p>
<ol>
<li>ALL matches are saved to a list object in memory</li>
<li>There is no handling of duplicates</li>
<li>No speed optimization</li>
<li>Iteration over file 'b' occurs 'n' times, where 'n' is the number of 
lines in file 'a'. Ideally, you would only iterate over each file once.</li>
</ol>
<p>Even only using base Python, I'm sure there is a better way to go about it.</p>
<p>For the Gist: <a href="https://gist.github.com/MetaJoker/a63f8596d1084b0868e1bdb5bdfb5f16" rel="nofollow noreferrer">https://gist.github.com/MetaJoker/a63f8596d1084b0868e1bdb5bdfb5f16</a></p>
<p>I think the Gist also has a link to the repl.it I used to write and test the code if you want a copy to play with in your browser.</p>
</div>
<span class="comment-copy">Two big problems with your code: (1) <code>zip</code> will match up lines from each file that have the same line number. This is not helpful for seeing if a word is <i>anywhere</i> in the file. Your <code>if</code> statement is on the right track. (2) File objects can only be iterated over once. Store the lines in a list or set first.</span>
<span class="comment-copy">Thanks for the analysis. The problem is I will be dealing with very large files so I'd like to avoid putting the files in to memory using a list or dictionary.</span>
<span class="comment-copy">How large are the actual files on disk?</span>
<span class="comment-copy">Can range from 10MB to 1GB</span>
<span class="comment-copy">OK, 1 GB might be a problem (but maybe not). If you really don't want to keep them in memory, you'll need to reset the file with <code>b.seek(0)</code> before each iteration.</span>
