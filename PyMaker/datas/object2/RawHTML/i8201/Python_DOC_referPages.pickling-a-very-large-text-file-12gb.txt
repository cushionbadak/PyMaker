<div class="post-text" itemprop="text">
<p>I'm trying to pickle a large text file, using the following code:</p>
<pre><code>import pickle

file1=open('/home/mustafa/data/wiki.en.text','r')
obj=[file1.read()]
pickle.dump(obj,open('data.pkl','w'),2)
</code></pre>
<p>I get the following error:</p>
<blockquote>
<pre><code>struct.error: 'i' format requires -2147483648 &lt;= number &lt;= 2147483647
</code></pre>
</blockquote>
<p>I think it might be a multiprocessing issue.</p>
</div>
<div class="post-text" itemprop="text">
<p>For this kind of serialization pickle is not a good option. Even for cPickle, information &gt; than 4Gb can be highly problematic. Have you think on using other alternatives like SQLite or HDF5?</p>
</div>
<span class="comment-copy">(1) It's obviously integer-limit related (but i don't know the exact source) (2) I don't see some good reason to do this (3) I think you should open the target-file in binary-mode (old python 2 docs: use binary if mode &gt;= 1).</span>
<span class="comment-copy">For data of 12GB, is pickle the right tool for the job? Or do you have to use it?</span>
<span class="comment-copy">What version of the pickle protocol are you using? Note the <a href="https://docs.python.org/3/library/pickle.html#data-stream-format" rel="nofollow noreferrer">description of protocol 4</a> (introduced in 3.4) includes "adds support for very large objects".</span>
<span class="comment-copy">Why would you want to do this? It is certainly slower and more resource intensive than just re reading the file.</span>
<span class="comment-copy">Do you actually have the need to read the entire English Wikipedia into memory at once? It might be a good idea to figure out what you want to do with it then come up with a less-stressful, likely-faster solution, e.g. put individual articles into your favorite database.</span>
<span class="comment-copy">Definitely considering SQlite now or HDF5 using hickle</span>
