<div class="post-text" itemprop="text">
<p>I was trying to make my code parallel and I run into a strange thing that I am not able to explain. </p>
<p>Let me define the context. I have a really heavy computation to do, reading multiple files, performing machine learning analysis on it, a lot of math is involved. My code runs normally on Windows and Linux when is sequential, but when I try to use multiprocessing everything breaks.
Below there is an example, that I developed first on Windows:</p>
<pre><code>from multiprocessing.dummy import Pool as ThreadPool 

def ppp(element):
    window,day = element
    print(window,day)
    time.sleep(5)
    return

if __name__ == '__main__'    
    #%% Reading datasets
    print('START')
    start_time = current_milli_time()
    tree = pd.read_csv('datan\\days.csv')
    days = list(tree.columns)
    # to be able to run this code uncomment the following line and comment the previous two
    # days = ['0808', '0810', '0812', '0813', '0814', '0817', '0818', '0827', '0828', '0829']
    windows = [1000]
    processes_args = list(itertools.product(windows, days))

    pool = ThreadPool(8) 
    results = pool.map_async(ppp, processes_args)
    pool.close() 
    pool.join() 
    print('END', current_milli_time()-start_time, 'ms')
</code></pre>
<p>When I run this code on Windows the output looks like that:</p>
<pre><code>START
100010001000 1000 1000100010001000      081008120808
08130814
0818
082708171000
1000    
  08290828

END 5036 ms
</code></pre>
<p>A messy set of prints in 125 ms. Same behavior on Linux too. However, I noticed that if I apply this method on Linux, and I look into 'htop', what I am seeing is a set of threads that are randomly picked for execution, but they never execute in parallel. Thus, after some google searches I came up with this new code:</p>
<pre><code>from multiprocessing import Pool as ProcessPool

def ppp(element):
    window,day = element
    print(window,day)
    time.sleep(5)
    return

if __name__ == '__main__':
    #%% Reading datasets
    print('START')
    start_time = current_milli_time()
    tree = pd.read_csv('datan\\days.csv')
    days = list(tree.columns)
    # to be able to run this code uncomment the following line and comment the previous two
    # days = ['0808', '0810', '0812', '0813', '0814', '0817', '0818', '0827', '0828', '0829']
    windows = [1000]
    processes_args = list(itertools.product(windows, days))

    pool = ProcessPool(8) 
    results = pool.map_async(ppp, processes_args)
    pool.close() 
    pool.join() 
    print('END', current_milli_time()-start_time, 'ms')
</code></pre>
<p>As you can see, I changed the import statement, which basically creates a Process pool instead of a Thread pool. That solves the problem on Linux, in fact in the real scenario, I have 8 processors running at 100% with 8 processes running in the system. The output looks like the one before. However, when I use this code on windows, more than 10 seconds are needed for the entire running, moreover, I am not getting any of the prints of <code>ppp</code>, just the ones of the main.</p>
<p>I really tried to search for an explanation, but I am not understanding why that happens. For example here: <a href="https://stackoverflow.com/questions/41919581/python-multiprocessing-pool-strange-behavior-in-windows">Python multiprocessing Pool strange behavior in Windows</a>, they talk about safe code on windows and the answer suggests to move to Threading, that, as a side effect, will make the code not parallel, but concurrent. Here another example: <a href="https://stackoverflow.com/questions/42148344/python-multiprocessing-linux-windows-difference">Python multiprocessing linux windows difference</a>. All these questions describe <code>fork()</code> and <code>spawn</code> processes, but I personally think that the point of my question is not that. Python documentation still explains that windows does not have a <code>fork()</code> method (<a href="https://docs.python.org/2/library/multiprocessing.html#programming-guidelines" rel="nofollow noreferrer">https://docs.python.org/2/library/multiprocessing.html#programming-guidelines</a>). </p>
<p>In conclusion, right now I am convincing that I cannot do parallel processing in Windows, but I think that what I am entailing from all these discussions is wrong. Thus, my question should be: is it possible to run processes or threads in parallel (on different CPUs) in Windows?</p>
<p>EDIT: add name == main in both the examples</p>
<p>EDIT2: to be able to run the code this function and these imports are needed:</p>
<pre><code>import time
import itertools    
current_milli_time = lambda: int(round(time.time() * 1000))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You <em>can</em> do parallel processing under Windows (I have a script running now doing heavy computations and using 100% of all 8 cores) but the way it works is by creating parallel <em>processes</em>, not threads (which won't work because of the GIL except for I/O operations). A few important points:</p>
<ul>
<li>you need to use <code>concurrent.futures.ProcessPoolExecutor()</code> (note it's the process pool not the thread pool). See <a href="https://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer">https://docs.python.org/3/library/concurrent.futures.html</a>. In a nutshell, the way it works is that you put the code you want to parallelize in a function and then you call <code>executor.map()</code> which will done the split.</li>
<li>note that on Windows each parallel process will start from scratch, so you'll probably need to use <code>if __name__ == '__main__:'</code> in a few places to distinguish between what you do in the main process vs the others. The data that you load in the main script will be replicated to the child processes so it has to be serializable (pickl'able in Python lingo).</li>
<li>in order to efficiently use the core, avoid writing data to objects shared across all processes (eg. a progress counter or a common data structure). Otherwise the synchronization between the processes will kill performance. So monitor the execution from the task manager.</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>under windows, python use <code>pickle</code>/<code>unpickle</code> to mimic <code>fork</code> in multiprocessing module, when doing <code>unpickle</code>, the module get reimported, any code in global scope execute again, <a href="https://docs.python.org/2/library/multiprocessing.html#windows" rel="nofollow noreferrer">the docs</a> stated:</p>
<blockquote>
<p>Instead one should protect the “entry point” of the program by using if <strong>name</strong> == '<strong>main</strong>'</p>
</blockquote>
<p>besides, you should cosume the <code>AsyncResult</code> returned by <code>pool.map_async</code>, or simply use <code>pool.map</code>.</p>
</div>
<span class="comment-copy">python will not utilize multi-core with multi-thread because of GIL.</span>
<span class="comment-copy">My question remains if I add name == 'main', the output is the following one: START END 13070 ms. Why is slower than Threads, why there are no prints?</span>
<span class="comment-copy">Same thing, nothing change. The real code has some progressbars that tell me the state of the computation everything is fine on Linux. Still also with <code>map</code> slower than Threads.</span>
<span class="comment-copy">The example in the question, let me just modify the days variable</span>
<span class="comment-copy">the example worker function didn't do any real computation, you are basically timing the overhead of <code>multiprossing.Pool</code>.</span>
<span class="comment-copy">That can be a reason, still, I am not getting the print during the execution.</span>
