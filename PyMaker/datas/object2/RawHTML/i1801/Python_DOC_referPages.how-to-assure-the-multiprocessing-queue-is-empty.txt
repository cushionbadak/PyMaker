<div class="post-text" itemprop="text">
<p>The code below first starts multiple processes. Then it runs a <code>while True</code> loop checking the <code>queue</code> objects. Lastly, it iterates the processes to check if any alive. After all the processes are completed it <code>breaks</code> the <code>while</code> loop.
Unfortunately, it happens while the <code>queue</code> object is not empty. Breaking the loop without getting a data stored in <code>queue</code> could be an easy to oversee data loss. How to modify the code logic so it assures the <code>queue</code> object is empty before breaking the loop?</p>
<pre><code>import time, multiprocessing, os
logger = multiprocessing.log_to_stderr()

def foo(*args):
    for i in range(3):
        queue = args[0]
        queue.put(os.getpid())

items = dict()
for i in range(5):
    queue = multiprocessing.Queue()
    proc = multiprocessing.Process(target=foo, args=(queue,))
    items[proc] = queue
    proc.start()
    time.sleep(0.1)

while True:
    time.sleep(1)

    for proc, queue in items.items():
        if not queue.empty():
            print(queue.get()) 

    if not True in [proc.is_alive() for proc in items]:
        if not queue.empty():
            logger.warning('...not empty: %s' % queue.get()) 
        break 
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>synchronization issue, again. when you check a queue find it is empty, there is no guarantee that no new item would come in the future.</p>
<p>you could put a sentinel to the queue when a subprocess finishes its job, to notify there will be no more items in the queue. parent process could drain the queue until got the sentinel. this is also the method used by <code>multiprocessing.Pool</code>. you could use <code>None</code> as sentinel here:</p>
<pre><code>def foo(*args):
    for i in range(3):
        queue = args[0]
        queue.put(os.getpid())
    queue.put(None)

...

while items:
    for proc in tuple(items.keys()):
        queue = items[proc]
        if not queue.empty():
            r = queue.get()
            print(r)
            if r is None:
                proc.join()
                del items[proc]
    time.sleep(0.1)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>A working solution is posted below. Instead of running the procs with the <code>Process.run</code> this approach uses <code>multiprocessing.pool.ThreadPool.map_async</code> method that starts the processes without blocking. <code>multiprocessing.Queue</code> object is then used to store data which is accessible by <code>foo</code> function running by the MainProcess.</p>
<pre><code>import time, multiprocessing, Queue
from multiprocessing.pool import ThreadPool
logger = multiprocessing.log_to_stderr()

def foo(args):
    queue = args[0]
    arg = args[1]
    for i in range(3):
        time.sleep(2)
        queue.put([arg, time.time()])

pool = ThreadPool(processes=4)
queue = multiprocessing.Queue()
map_result = pool.map_async(foo, [(queue, arg) for arg in range(3)])

logger.warning("map_result: %s" % map_result) 

map_result.wait(timeout = 10) 
if not map_result.ready():
    message = '%s is timed out and terminated.' % pool 
    log.error(message)
    pool.terminate()
    raise Exception(message)

while not queue.empty():
    if queue.empty():
        break
    logger.warning("queue_data: %r" % queue.get(True, 0.1))  

pool.close()
pool.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>#encoding:utf-8

from multiprocessing import Pool, Manager

def tiny(q, j):
    if len(j) &lt; 100:
        q.put(j+j[-1])
    print " Done!", j
    q.put(-1)
    return

queue = Manager().Queue()
pool = Pool(processes=10)
pool.apply_async(tiny, (queue, "A"))
pool.apply_async(tiny, (queue, "B"))
pool.apply_async(tiny, (queue, "C"))

created = 3
fininshed = 0

while created &gt; fininshed:
        i = queue.get(True, None)
        if isinstance(i, int):
            fininshed += 1
        else:
            created += 1
            pool.apply_async(tiny, (queue, i))

pool.close()
pool.join()
print [worker.is_alive() for worker in pool._pool]
</code></pre>
</div>
<span class="comment-copy">Don't start processes in loops like this. Use a <a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.pool" rel="nofollow noreferrer">pool</a> or a <a href="https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor" rel="nofollow noreferrer">process pool executor</a>; these provide purpose-built facilities for spawning and managing multiple processes.</span>
<span class="comment-copy">Would be a <code>pool</code> object  consistent from one OS to another? Please post it as an answer.</span>
<span class="comment-copy">The <code>multiprocessing</code> library should work consistently across all platforms, with the exception in certain instances of OpenBSD, as noted in the gray comment following <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue.cancel_join_thread" rel="nofollow noreferrer">this function's documentation</a>, which references <a href="https://bugs.python.org/issue3770" rel="nofollow noreferrer">this issue report</a>. The actual problems with your code are pretty numerous, though; you should start over and base your implementation on the examples in the documentation I've linked.</span>
<span class="comment-copy"><code>Pool</code> has managed queues, you dont need to do it .</span>
<span class="comment-copy">While this may solve the posted problem, it's better to include a description of your code so that others can more easily understand your answer</span>
