<div class="post-text" itemprop="text">
<p>I'm implementing a web API using <a href="https://aiohttp.readthedocs.io/en/stable/" rel="nofollow noreferrer">aiohttp</a>, deployed using <a href="https://gunicorn.org/" rel="nofollow noreferrer">gunicorn</a> with UVloop enabled <code>--worker-class aiohttp.GunicornUVLoopWebWorker</code>. Therefore, my code always runs in an asynchronous context. I had the ideia of implementing parallel jobs in the handling of requests for better performance.</p>
<p>I'm not using <code>asyncio</code> because i want <a href="https://en.wikipedia.org/wiki/Parallel_computing" rel="nofollow noreferrer">Parallelism</a>, not <a href="https://en.wikipedia.org/wiki/Concurrency_(computer_science)" rel="nofollow noreferrer">Concurrency</a>.</p>
<p>I'm aware of <a href="https://docs.python.org/3.4/library/multiprocessing.html?highlight=process" rel="nofollow noreferrer">multiprocessing</a> and the <a href="https://en.wikipedia.org/wiki/Global_interpreter_lock" rel="nofollow noreferrer">GIL problem</a> in python. But joining a process also applies to my question.</p>
<p>Here is an example:</p>
<pre><code>from aiohttp.web import middleware

@middleware
async def context_init(request, handler):
    request.context = {}
    request.context['threads'] = []
    ret = await handler(request)
    for thread in request.context['threads']:
        thread.join()
    return ret
</code></pre>
<p>Taking into account that <code>thread.join()</code> or <code>process.join()</code> blocks the current thread, this will block the event loop (As far as my knowledge goes). How can I join asynchronously? What I want can be represented figuratively as this: <code>await thread.join()</code> or <code>await process.join()</code>. </p>
<p><strong>Update:</strong></p>
<p>Thanks to @user4815162342 I was able to write proper code for my project:</p>
<p>Middleware:</p>
<pre><code>from aiohttp.web import middleware
from util.process_session import ProcessSession

@middleware
async def context_init(request, handler):
    request.context = {}
    request.context['process_session'] = ProcessSession()
    request.context['processes'] = {}
    ret = await handler(request)
    await request.context['process_session'].wait_for_all()
    return ret
</code></pre>
<p>Util:</p>
<pre><code>import asyncio
import concurrent.futures
from functools import partial

class ProcessSession():

    def __init__(self):
        self.loop = asyncio.get_running_loop()
        self.pool = concurrent.futures.ProcessPoolExecutor()
        self.futures = []

    async def wait_for_all(self):
        await asyncio.wait(self.futures)

    def add_process(self, f, *args, **kwargs):
        ret = self.loop.run_in_executor(self.pool, partial(f, *args, **kwargs))
        self.futures.append(ret)
        return ret

class ProcessBase():

    def __init__(self, process_session, f, *args, **kwargs):
        self.future = process_session.add_process(f, *args, **kwargs)

    async def wait(self):
        await asyncio.wait([self.future])
        return self.future.result()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I found a solution using <a href="https://docs.python.org/3.4/library/multiprocessing.html?highlight=process#using-a-pool-of-workers" rel="nofollow noreferrer">multiprocesses</a>. It can be done using a <code>Pool</code>. The standard lib provides some "async" methods (It's not really async, it just separates the initialization of the process from the process' output): <code>apply_async</code></p>
<p>Using a simple <a href="https://github.com/django/asgiref/blob/master/asgiref/sync.py" rel="nofollow noreferrer">async wrapper</a>, I managed to deliver what I wanted:</p>
<pre><code>from multiprocessing import Pool
from async_converter import sync_to_async
import asyncio

def f(x):
    i = 0
    while i &lt; 10000000 * x:
       i = i + 1
    print("Finished: " + str(x))
    return i


async def run():
    print("Started with run")
    with Pool(processes=4) as pool:         # start 4 worker processes
        result1 = pool.apply_async(f, (10,)) # evaluate "f(10)" asynchronously
        result2 = pool.apply_async(f, (2,))
        res1 = await sync_to_async(result1.get)()
        print(res1)
        res2 = await sync_to_async(result2.get)()
        print(res2)

async def dummy(output):
    print(output)

async def main():
    # Schedule three calls *concurrently*:
    await asyncio.gather(
        run(),
        dummy("Nice"),
        dummy("Async"),
        dummy("Loop"),
        dummy("Perfect"),
        dummy("Dummy1"),
        dummy("Dummy2"),
        dummy("Dummy3"),
        dummy("Dummy4"),
        dummy("Dummy5"),
        dummy("Dummy6"),
        dummy("Dummy7"),
        dummy("Dummy8"),
        dummy("Dummy9"),
        dummy("Dummy10"),
    )

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
loop.close()
</code></pre>
<p>outputs:</p>
<pre><code>Perfect
Dummy6
Nice
Dummy1
Dummy7
Started with run
Dummy2
Dummy8
Dummy3
Dummy9
Async
Dummy4
Dummy10
Loop
Dummy5
Finished: 2
Finished: 10
100000000
20000000
</code></pre>
<p>Parallelism with asyncio :)</p>
</div>
<span class="comment-copy">Asyncio defines <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor" rel="nofollow noreferrer"><code>run_in_executor</code></a> for this purpose. You can give it a <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor" rel="nofollow noreferrer"><code>ProcessPoolExecutor</code></a> as executor, which achieves the parallelism you're looking for. Your solution works, but uses the generic <code>sync_to_async</code> decorator which runs each <code>result.get()</code> in a separate thread.</span>
<span class="comment-copy">@user4815162342 Thanks for your insight! I will absolutely use it. Way better than my make-shifted solution</span>
