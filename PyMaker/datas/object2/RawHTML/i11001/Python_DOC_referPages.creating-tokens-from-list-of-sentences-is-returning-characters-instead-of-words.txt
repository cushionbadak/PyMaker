<div class="post-text" itemprop="text">
<pre><code>from nltk.tokenize import sent_tokenize

text = open(path).read().lower().decode("utf8")
sent_tokenize_list = sent_tokenize(text)

tokens = [w for w in itertools.chain(*[sent for sent in sent_tokenize_list])]
</code></pre>
<p>The last line, "tokens", returns characters instead of words. </p>
<p>Why is this and how do I get it to return words instead? Especially considering doing it based on a list of sentences. </p>
</div>
<div class="post-text" itemprop="text">
<p>Because <a href="http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize" rel="nofollow"><code>sent_tokenize</code></a> returns a list of string sentences and <a href="https://docs.python.org/3/library/itertools.html#itertools.chain" rel="nofollow"><code>itertools.chain</code></a> chains iterables to a single iterable returning items one at a time from each until they're exhausted. In effect you've recombined the sentences to a single string and iterate over it in the list comprehension.</p>
<p>To create a single list of words from a list of sentences you can for example split and flatten:</p>
<pre><code>tokens = [word for sent in sent_tokenize_list for word in sent.split()]
</code></pre>
<p>This does not handle punctuation, but your original attempt wouldn't either. Your original would work also with split:</p>
<pre><code>tokens = [w for w in itertools.chain(*(sent.split()
                                       for sent in sent_tokenize_list))]
</code></pre>
<p>Note that you can use a generator expression instead of a list comprehension as arguments to unpack. Even better, use <code>chain.from_iterable</code>:</p>
<pre><code>tokens = [w for w in itertools.chain.from_iterable(
    sent.split() for sent in sent_tokenize_list)]
</code></pre>
<p>For punctuation handling use <a href="http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize" rel="nofollow"><code>nltk.tokenize.word_tokenize</code></a> instead of <code>str.split</code>. It'll return words and punctuation as separate items, and splits for example <code>I's</code> to <code>I</code> and <code>'s</code> (which of course is a good thing since they're in fact separate words, just contracted).</p>
</div>
<div class="post-text" itemprop="text">
<p>Firstly, if the file is in 'utf8' and you're using Python2, it'll be better if you use the <code>encoding='utf8'</code> parameter in <a href="https://docs.python.org/2/library/io.html" rel="nofollow"><code>io.open()</code></a>:</p>
<pre><code>import io

from nltk import word_tokenize, sent_tokenize

with io.open('file.txt', 'r', encoding='utf8') as fin:
    document = []
    for line in fin:
        tokens += [word_tokenize(sent) for sent in sent_tokenize(line)]
</code></pre>
<p>If it's Python3, simply do:</p>
<pre><code>from nltk import word_tokenize 

with open('file.txt', 'r') as fin:
    document = []
    for line in fin:
        tokens += [word_tokenize(sent) for sent in sent_tokenize(line)]
</code></pre>
<p>Do take a look at <a href="http://nedbatchelder.com/text/unipain.html" rel="nofollow">http://nedbatchelder.com/text/unipain.html</a></p>
<hr/>
<p>As for the tokenization, if we assume that each line contains some sort of paragraph that might be made up of one or more sentences, we would like to first initial a list to store the whole document:</p>
<pre><code>document = []
</code></pre>
<p>Then we iterate through the lines and split the line up into sentences:</p>
<pre><code>for line in fin:
    sentences = sent_tokenize(line)
</code></pre>
<p>Then we split the sentences up into the tokens:</p>
<pre><code>token = [word_tokenize(sent) for sent in sent_tokenize(line)]
</code></pre>
<p>Since we want to update our document list to store the tokenized sentences, we use:</p>
<pre><code>document = []
for line in fin:
    tokens += [word_tokenize(sent) for sent in sent_tokenize(line)]
</code></pre>
<hr/>
<p><strong>Not recommended!!!</strong> (but still possible in one line):</p>
<pre><code>alvas@ubi:~$ cat file.txt
this is a paragph. with many sentences.
yes, hahaah.. wahahha... 
alvas@ubi:~$ python
Python 2.7.11+ (default, Apr 17 2016, 14:00:29) 
[GCC 5.3.1 20160413] on linux2
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import io
&gt;&gt;&gt; from itertools import chain
&gt;&gt;&gt; from nltk import sent_tokenize, word_tokenize
&gt;&gt;&gt; list(chain(*[[word_tokenize(sent) for sent in sent_tokenize(line)] for line in io.open('file.txt', 'r', encoding='utf8')]))
[[u'this', u'is', u'a', u'paragph', u'.'], [u'with', u'many', u'sentences', u'.'], [u'yes', u',', u'hahaah..', u'wahahha', u'...']]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>May be you should to use <code>word_tokenize</code> instead of <code>sent_tokenize</code>?</p>
<pre><code>from nltk.tokenize import word_tokenize

text = open(path).read().lower().decode("utf8")
tokens = word_tokenize(text)
</code></pre>
<p><a href="http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize" rel="nofollow noreferrer">http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize</a></p>
</div>
<span class="comment-copy">First decode, then lowercase. Otherwise you'll get incorrect behavior with non-ascii characters.</span>
<span class="comment-copy">Use <code>nltk.word_tokenize()</code>, not <code>split()</code>. Punctuation and words are different tokens.</span>
<span class="comment-copy">If all you want is a flat list of tokens regardless of sentence boundaries, this is the best approach.</span>
