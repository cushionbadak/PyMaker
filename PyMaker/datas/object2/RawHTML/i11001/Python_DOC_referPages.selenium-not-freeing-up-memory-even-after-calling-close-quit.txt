<div class="post-text" itemprop="text">
<p>So I've been working on scraper that goes on 10k+pages and scrapes data from it.</p>
<p>Issue is that over time memory consumption raises drastically. So to overcome this instead of closing driver instance only at the end of scrape scraper is updated so it closes instance after every page is loaded and data extracted.</p>
<p>But ram memory still get's populated for some reason.</p>
<p>I tried using PhantomJS but it doesn't load data properly for some reason.
I also tried with initial version of scraper to limit cache in Firefox to a 100mb, that didn't work also.</p>
<p>Note: I run tests with both chromedriver and firefox instance, and unfortunately I can't use libraries such as requests, mechanize, etc... instead of selenium.</p>
<p>Any help is appreciated since I've been trying to figure this out for a week now. Thanks.</p>
</div>
<div class="post-text" itemprop="text">
<p>Are you trying to say that your drivers are what's filling up your memory? How are you closing them? If you're extracting your data, do you still have references to some collection that's storing them in memory?   </p>
<p>You mentioned that you were already running out of memory when you closed the driver instance at the end of scraping, which makes it seem like you're keeping extra references.</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://stackoverflow.com/questions/1316767/how-can-i-explicitly-free-memory-in-python/1316799#1316799">The only way</a> to force the Python interpreter to release memory to the OS is to terminate the process. Therefore, use <code>multiprocessing</code> to spawn the selenium Firefox instance; the memory will be freed when the spawned process is terminated:</p>
<pre><code>import multiprocessing as mp
import selenium.webdriver as webdriver

def worker()
    driver = webdriver.Firefox()
    # do memory-intensive work
    # closing and quitting is not what ultimately frees the memory, but it
    # is good to close the WebDriver session gracefully anyway.
    driver.close()
    driver.quit()

if __name__ == '__main__':
    p = mp.Process(target=worker)
    # run `worker` in a subprocess
    p.start()
    # make the main process wait for `worker` to end
    p.join()
    # all memory used by the subprocess will be freed to the OS
</code></pre>
<p>See also <a href="http://effbot.org/pyfaq/why-doesnt-python-release-the-memory-when-i-delete-a-large-object.htm" rel="nofollow noreferrer">Why doesn't Python release the memory when I delete a large object?</a></p>
</div>
<div class="post-text" itemprop="text">
<p>I have experienced similar issue and destroying that driver my self (i.e setting driver to None) prevent those memory leaks for me</p>
</div>
<div class="post-text" itemprop="text">
<p>I was having the same problem until putting the <code>webdriver.get(url)</code> statements inside a try/except/finally statement, and making sure <code>webdriver.quit()</code> was in the finally statement, this way, it always execute. Like:</p>
<pre><code>webdriver = webdriver.Firefox()
try:
        webdriver.get(url)
        source_body = webdriver.page_source
except Exception as e:
        print(e)
finally:
        webdriver.quit()
</code></pre>
<hr/>
<p>From the <a href="https://docs.python.org/3/reference/executionmodel.html?highlight=finally" rel="nofollow noreferrer">docs</a>:</p>
<blockquote>
<p>The finally clause of such a statement can be used to specify cleanup
  code which does not handle the exception, but is executed whether an
  exception occurred or not in the preceding code.</p>
</blockquote>
</div>
<span class="comment-copy">Yes, it seems like driver is filling memory up. I have 5 functions where Selenium is used. I use selenium alongside Scrapy. So in those functions I just instantiate new driver instance, then at the near end of function I call driver.quit() or driver.close(). As for keeping extra references, I'm not sure that I do. I use selenium for loading page, and once it loads I put page_source into Scrapy selector. I don't have any memory leaks in Scrapy.</span>
<span class="comment-copy">You can check for line-by-line memory usage (in your program not the websites) using <a href="https://pypi.python.org/pypi/memory_profiler" rel="nofollow noreferrer">memory_profiler</a>. This should help in getting a better idea of what section is consuming your memory. If you can't find anything there, posting an example function here may be helpful.</span>
<span class="comment-copy">@ScrapyNoob also check top to see if there are multiple instances of whatever browser you are using.</span>
