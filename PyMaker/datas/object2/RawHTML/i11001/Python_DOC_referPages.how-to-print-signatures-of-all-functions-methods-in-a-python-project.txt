<div class="post-text" itemprop="text">
<p>I want to extract all Python functions/methods with their signatures from a Python project. I've tried:</p>
<pre><code>$ grep -r ^def *
</code></pre>
<p>but this doesn't show full signatures when parameters span several lines. Any suggestions?</p>
</div>
<div class="post-text" itemprop="text">
<p>You can <a href="https://docs.python.org/2/library/tokenize.html" rel="nofollow">tokenize the file</a> and use that to print function definitions:</p>
<pre><code>import token
from tokenize import generate_tokens

def find_definitions(filename):
    with open(filename) as f:
        gen = generate_tokens(f.readline)
        for tok in gen:
            if tok[0] == token.NAME and tok[1] == 'def':
                # function definition, read until next colon.
                definition, last_line = [tok[-1]], tok[3][0]
                while not (tok[0] == token.OP and tok[1] == ':'):
                    if last_line != tok[3][0]:
                        # more than one line, append, track line number
                        definition.append(tok[-1])
                        last_line = tok[3][0]
                    tok = next(gen)
                if last_line != tok[3][0]:
                    definition.append(tok[-1])
                yield ''.join(definition)
</code></pre>
<p>This works regardless of how many lines a function definition uses.</p>
<p>Demo:</p>
<pre><code>&gt;&gt;&gt; import textwrap
&gt;&gt;&gt; gen = find_definitions(textwrap.__file__.rstrip('c'))
&gt;&gt;&gt; for definition in gen:
...     print(definition.rstrip())
...
    def __init__(self,
                 width=70,
                 initial_indent="",
                 subsequent_indent="",
                 expand_tabs=True,
                 replace_whitespace=True,
                 fix_sentence_endings=False,
                 break_long_words=True,
                 drop_whitespace=True,
                 break_on_hyphens=True):
    def _munge_whitespace(self, text):
    def _split(self, text):
    def _fix_sentence_endings(self, chunks):
    def _handle_long_word(self, reversed_chunks, cur_line, cur_len, width):
    def _wrap_chunks(self, chunks):
    def wrap(self, text):
    def fill(self, text):
def wrap(text, width=70, **kwargs):
def fill(text, width=70, **kwargs):
def dedent(text):
</code></pre>
<p>The above uses the <code>textwrap</code> module to demonstrate how it can handle multi-line definitions.</p>
<p>If you need to support Python 3 code with annotations, you'll need to be a little bit cleverer and track open and closing parens too; a colon within the parentheses doesn't count. On the other hand, Python 3 <a href="https://docs.python.org/3/library/tokenize.html#tokenize.tokenize" rel="nofollow"><code>tokenize.tokenize()</code></a> produces named tuples which make the function below a little easier to read:</p>
<pre><code>import token
from tokenize import tokenize

def find_definitions(filename):
    with open(filename, 'rb') as f:
        gen = tokenize(f.readline)
        for tok in gen:               
            if tok.type == token.NAME and tok.string == 'def':
                # function definition, read until next colon outside
                # parentheses.
                definition, last_line = [tok.line], tok.end[0]
                parens = 0
                while tok.exact_type != token.COLON or parens &gt; 0:
                    if last_line != tok.end[0]:
                        definition.append(tok.line)
                        last_line = tok.end[0]
                    if tok.exact_type == token.LPAR:
                        parens += 1
                    elif tok.exact_type == token.RPAR:
                        parens -= 1
                    tok = next(gen)
                if last_line != tok.end[0]:
                    definition.append(tok.line)
                yield ''.join(definition)
</code></pre>
<p>In Python 3 you'd preferably open source files in binary mode and let the tokenizer figure out the right encoding. Also, the above Python 3 version can tokenize Python 2 code without issue.</p>
</div>
<div class="post-text" itemprop="text">
<p>This isn't a place to use regex in my opinion, unless you accept the fact that you'll potentially miss many edge cases.</p>
<p>Instead I'd suggest you use <code>inspect</code> and <code>funcsigs</code> (<code>funcsigs</code> is a backport of changes made in Python 3's <code>inspect</code> module. It includes the signature parsing functions).</p>
<p>Here's the file we'll parse (<code>inspect_me.py</code>):</p>
<pre><code>import sys


def my_func(a, b=None):
    print a, b


def another_func(c):
    """
    doc comment
    """
    return c + 1
</code></pre>
<p>And here's the code that will parse it for us:</p>
<pre><code>import inspect
from funcsigs import signature

import inspect_me


if __name__ == "__main__":
    # get all the "members" of our module:
    members = inspect.getmembers(inspect_me)
    for k, v in members:
        # we're only interested in functions for now (classes, vars, etc... may come later in a very similar fashion):
        if inspect.isfunction(v):
            # the name of our function:
            print k

            # the function signature as a string
            sig = signature(v)
            print str(sig)

            # let's strip out the doc string too:
            if inspect.getdoc(v):
                print "FOUND DOC COMMENT: %s" % (inspect.getdoc(v))
</code></pre>
<p>Inspect is <code>the way</code> to go about introspection in python. <code>token</code> and <code>ast</code> could both do the job but they're much more low level/complex than what you actually need here.</p>
<p>The output of running the above:</p>
<pre><code>another_func
(c)
FOUND DOC COMMENT: doc comment
my_func
(a, b=None)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can parse the source using the <a href="https://docs.python.org/2/library/ast.html" rel="nofollow"><code>ast</code> module</a>. It allows you to see exactly the same code structure the interpreter sees. You just need to traverse it and dump out any function definitions you find.</p>
<p>If you want to handle edge cases like multi-line declarations, bash/grep is not enough.</p>
</div>
<span class="comment-copy">You could write another python code snippet with the <code>re</code> module. <code>import re; print re.findall('def.*?\)', open('file.py').read(), re.M)</code></span>
<span class="comment-copy">Possible duplicate of <a href="http://stackoverflow.com/questions/152708/how-can-i-search-for-a-multiline-pattern-in-a-file">How can I search for a multiline pattern in a file?</a></span>
<span class="comment-copy">find . -iname '*.py' | xargs pcregrep -M '^def .*\n.*\(.*\)'</span>
<span class="comment-copy">@Shiva It just introduces more edge cases which are not handled. See for example <code>def f(abc=(), this_is_skipped=0)</code> and <code>definition=...</code></span>
<span class="comment-copy">Regex are not enough. That's because default arguments can have arbitrary nesting of parenthesis, and that is not a regular language. So a regex solution will <i>not</i> be robust no matter what and there will be cases where it fails. I would simply import the module in python and use the <a href="https://docs.python.org/3/library/inspect.html" rel="nofollow noreferrer"><code>inspect</code></a> module to obtain <a href="https://docs.python.org/3/library/inspect.html#introspecting-callables-with-the-signature-object" rel="nofollow noreferrer">the signatures of the definitions</a>.  If for some reason you don't want to actually import the module, then <code>ast</code> is the solution.</span>
<span class="comment-copy">This looks almost magic to me. Thanks.</span>
<span class="comment-copy">This requires that the code is imported, which <i>can</i> have side effects and is rather heavy in terms of processing (as all the function and class definitions are loaded). This also requires that the Python code is on <code>sys.path</code>; you can't just point this script to a directory and have it print out all the function definitions.</span>
<span class="comment-copy">@MartijnPieters for sure. You could call that a disadvantage, I'd call it an advantage: you can now inspect dynamically created classes that don't physically exist in your file but are created on the fly. Meta fun stuff :) If performance is an issue.. that's not the fault of the parsing: the problem was obviously there long before you ran your inspector :P</span>
<span class="comment-copy">Well, unless you are using <code>types.FunctionType</code> to create function objects from scratch, tokenizing can handle dynamic definitions too.</span>
<span class="comment-copy">I was thinking in terms of future requirements: show classes, obj constructors, module comments, any and everything created via <code>type()</code> etc. Tokens are great if you know exactly what you're looking for in advance and don't expect things to change. IMHO they'll become a huge PITA to maintain once you start adding new requirements. Not to mention: custom token parsers will need to change with the changes to the language where some abstracted introspection doesn't.</span>
<span class="comment-copy">Actually, tokenizing is quite flexible; the tokenizer in Python 3 can handle Python 2 syntax just fine, while you can't <i>import</i> Python 2 code into 3. Handling docstrings and classes isn't too hard; just track <code>INDENT</code> and <code>DEDENT</code> tokens.</span>
<span class="comment-copy"><code>ast</code> is actually the wrong approach as you then have to map back into the source code for the lines. Go one step back in the process, only <i>tokenize</i>, as you then still have direct access to the line data.</span>
<span class="comment-copy">@MartijnPieters I disagree, AST provides <code>lineno</code> and <code>coloffset</code> on all the nodes (<a href="https://docs.python.org/2/library/ast.html#ast.AST.lineno" rel="nofollow noreferrer">docs.python.org/2/library/ast.html#ast.AST.lineno</a>), so the "map back" step is just <code>node.lineno</code>. Not that the tokenize solution is bad though :) But AST is not that different.</span>
<span class="comment-copy">Right, but that then requires you to add in the <code>linecache</code> module or read all of the source file into memory to get back to the source. And the <code>ast</code> module is far less flexible when it comes to handling older Python code; you can tokenize Python 2 code in Python 3, you can't produce an AST for it.</span>
