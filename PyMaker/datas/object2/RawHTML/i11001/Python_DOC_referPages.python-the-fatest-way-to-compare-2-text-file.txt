<div class="post-text" itemprop="text">
<p>I have 2 file: File A contains 200 000 lines; File B contains 4 000 000 lines. So, I want to compare these files and print the lines which aren't in File B.</p>
<p>For example:
File A:</p>
<pre><code>   1
   2
   3
</code></pre>
<p>File B:</p>
<pre><code>   1
   4
   5
   6
   7
</code></pre>
<p>The output:</p>
<pre><code>   2
   3
</code></pre>
<p>And the below is my code:</p>
<pre><code>for line in open ( 'C:/A.txt' ):
    if line not in open ( 'C:/B.txt' ):
        print ( line )
</code></pre>
<p>This code works but it takes a very long time to complete. So, how to speed up the code process ? </p>
<p>Any help will be extremely appreciated ! :)</p>
</div>
<div class="post-text" itemprop="text">
<p>Create a set just of the hashes of lines in file B - and compare the lines in A with those in this set - </p>
<p>Such a set will take about about one hundred megabytes of memory, therefore should fit in memory in a notebook or workstation:</p>
<pre><code>linesB = {hash(line) for line in open("fileB"))}
for line in open("fileA"):
    if hash(line) not in linesB:
         print (line)
</code></pre>
<p>The main speed up here is that unlike searching for a line linearly inside fileB, it is read only once - and each line is made available in a set, which has constant look-up time. Therefore you come down from ~200,000 X 4,000,000 comparisons (O(m X n)) to just ~200.000 Comparisons (O(m X 1)). </p>
<p>That not to mention not needing to move data rom the filsystem into the program memory 200.000 times around.</p>
<p>By keeping only the <code>hash</code> of lines in B you avoid having to keep all the text information of fileB in memory - just 24 bytes for each hash (in a 64bit system) - insteadof the textual information itself (which depends on each's lines lenght) + its hash.</p>
</div>
<div class="post-text" itemprop="text">
<p>A faster way would be to open the file once and use a set:</p>
<pre><code>with open('C:/A.txt') as a:
    with open('C:/B.txt') as b:
        lines = set(b)
    for line in a:
        if line not in lines:
            print(line)
</code></pre>
<p>Maybe a better way would be something like this:</p>
<pre><code>with open('C:/A.txt') as a, open('C:/B.txt') as b:
    lines = set()
    for line in a:
        if line not in lines:
            for line_b in b:
                lines.add(line_b)
                if line_b == line:
                    break
            else:
                print(line)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You could use the <code>set difference</code> operation to get all the lines that do not match in these files.</p>
<pre><code>with open('A.txt') as a:
    contentA = set(a)

with open('B.txt') as b:
    contentB = set(b)

print(contentA - contentB)
</code></pre>
<p>Edit: 
The reverse operation, to print contents of the file B which are not in A is now just </p>
<p><code>print(contentB - contentA)</code></p>
</div>
<span class="comment-copy">Have you looked into the <a href="https://docs.python.org/3/library/filecmp.html" rel="nofollow noreferrer"><code>filecmp</code> module</a> ?</span>
<span class="comment-copy">This is a classic example of <a href="https://en.wikipedia.org/wiki/Asymptotic_computational_complexity" rel="nofollow noreferrer">asymptotic complexity</a>, commonly referred to as <a href="https://en.wikipedia.org/wiki/Big_O_notation" rel="nofollow noreferrer">Big O notation</a>. The <code>not in</code> statement has to read the whole file every time, which is O(n) (linear-time - the amount of work is proportional to the length of the input). Since you call this once for each line in the first file you're doing that linear amount of work a linear ( O(n) ) number of times. As a result your algorithm takes O(n) x O(n), or O(n^2) time to run - also known as quadratic time.</span>
<span class="comment-copy">Thank you for giving me the code. After using hash, the code runs very fast :)</span>
<span class="comment-copy">What makes it goes fast is nltthe <code>hash</code> - is reading the second file just once and put it in a <code>set</code>, which has a constant lookup time.</span>
<span class="comment-copy">There's no guarantee that two different lines won't hash to the same value, especially as the line length increases. Given the default hash randomization of strings, this won't even be deterministic between runs. For a guarantee you'd need to add the strings themselves to the set (probed into the hash table when there's a hash collision), which greatly increases the memory requirement.</span>
<span class="comment-copy">Indeed. Still, the chances of a collision are low - (collisions happen with dicts because there are much fewer buckets, and part of the hash information is simply discarded). But if memory is an issue (and for lines of a certain size it certain will be), a double hash could be used - (hash(line), hash(line + " "))  would rule out collisions (and take ~50bytes/line in 64bit builds)</span>
