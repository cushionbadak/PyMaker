<div class="post-text" itemprop="text">
<p>Ok, so I have multiple textfiles, each containing well over 500.000 or even 1.000.000 lines.</p>
<p>Currently I do something like this:</p>
<pre><code>import random

def line_function(line):
    # Do something with given line

def random_itteration(filepath):
    with open(filepath) as f:
        lines = f.readlines()
        random.shuffle(lines)
        for line in lines:
            result = line_function(line)
</code></pre>
<p>The thing is that the <a href="https://docs.python.org/3/library/random.html" rel="nofollow noreferrer">Python Docs</a> on <code>random.shuffle()</code> clearly state (emphasis added by me):</p>
<blockquote>
<p>Note that even for small len(x), the total number of permutations of x
  can quickly grow larger than the period of most random number
  generators. <strong>This implies that most permutations of a long sequence can
  never be generated</strong>. For example, a sequence of length 2080 is the
  largest that can fit within the period of the Mersenne Twister random
  number generator.</p>
</blockquote>
<p><strong>So the question is:</strong></p>
<p>What would be the fastest and most efficient way to make my setup work as intended?</p>
<p><strong>Further info:</strong></p>
<p>There is a reason why I want to apply line_function() to a random line and not simply iterate over them in the sequence they are in. Also note that <strong>I highly prefer to only process each line once</strong>.</p>
<p>Finally, shuffling the textfile up front, or dividing it into smaller files unfortunately isn't an option. And isn't what I am asking.</p>
<hr/>
<p>Any insights are more then welcome! Thnx in advance guys. </p>
</div>
<div class="post-text" itemprop="text">
<p>As Mark Dickinson says, the doc line you are quoting has essentially no practical implications for real-world code. It definitely doesn't have any relevance for your code.</p>
<p>It doesn't matter whether the shuffle produces a truly uniform random distribution over all possible permutations. What matters is whether the shuffle is <em>distinguishable</em> from such a distribution, up to some standard of distinguishability. <code>random.shuffle</code> is statistically indistinguishable from a perfectly random shuffle up to the quality of the underlying Mersenne Twister algorithm, and the ways in which it is distinguishable have nothing to do with the period.</p>
<p>You don't need to do anything special to make your setup "work as intended". <code>random.shuffle</code> already works.</p>
</div>
<div class="post-text" itemprop="text">
<p>I'd rather do a shuffle on a list of integers than the huge lines.<br/>
(Integers being the index/position of the line in the list of lines)<br/>
Something like this:<br/><br/></p>
<pre><code>import random
from random import randint

def line_function(line):
    # Do something with given line

def random_itteration(filepath):
    with open(filepath) as f:
        lines = f.readlines()
        count = len(lines)
        #random_index_list = random.shuffle(list(xrange(count)))
        random_index_list = random.sample(range(count+1),count)
        for index in random_index_list:
            result = line_function(lines[index])

        #shuffled_lines = random.shuffle(lines)
        #for line in shuffled_lines:
        #    result = line_function(line)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You're going to have trouble doing this "quickly and efficiently" in Python, but if you must, the place to start is going to be a shuffling algorithm like the <a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle" rel="nofollow noreferrer">Fisher-Yates</a> algorithm.</p>
<p>Once you implement that, load your files, and record at which byte offset each line starts at.  Shuffle that array, open your files, then iterate over your array, and read from the offset to the next newline.</p>
<p>With datasets as large as you're proposing, it's reasonable to expect that <code>lines = f.readlines()</code> will simply be too much memory pressure, demanding a more complex, but more scalable solution, using offsets.</p>
<p>For more efficient reruns, perhaps also consider saving out the offset metadata once it's generated, so you don't need to walk over the whole file (or the whole files) every time.</p>
</div>
<span class="comment-copy">You only need one random permutation, not all of them. If you only need a "random" permutation, the quoted bit is not a problem. If you need a truly random permutation, that's a bigger struggle.</span>
<span class="comment-copy">The note is true about any way that you select things randomly from such a large set of inputs.</span>
<span class="comment-copy">Is the file static or does it change frequently. If it's static, you can create another file holding the positions of the start of each line in the file. Read that file, shuffle it, then you can seek in the main file to those indexes.</span>
<span class="comment-copy">Just ignore the "This implies that most permutations of a long sequence can never be generated." sentence. It has essentially <i>no</i> practical implication for real-world code.</span>
<span class="comment-copy">I don't understand your question, why doesn't loading the file into a list then using <code>random.shuffle</code> on the list work for you? Aside from the fact that you are doing <code>shuffled_lines = random.shuffle(lines)</code> which will return <code>None</code>...</span>
<span class="comment-copy">Ok, I have to admit that in hindsight my question was a bit of a non-question. I sensed my program slowed down just a bit when having to shuffle the big file. Went into the Python docs, discovered the quoted doc line, misinterpreted it and thought that I wasn't taking the right approach. Your answer did clear things up, so thnx!</span>
<span class="comment-copy">He wants to process <i>all</i> lines in a random order, not just one random line.</span>
<span class="comment-copy">Tried this one. Problem here is that it is hard to avoid having to process the same line twice, which is not what I want. I tried avoiding it by keeping a list of the used index numbers and then checking against that list, but it didn't seem that efficient, nor fast.</span>
<span class="comment-copy">@Montmons <b>why does this look promising</b>? What advantage does this approach give you? Not to mention, this is incorrect, since <code>random_index_list = random.shuffle(list(xrange(count)))</code> will return <code>None</code></span>
<span class="comment-copy">@StevenRumbalski As I mentioned in a comment below the question, since the files are static you can create another file containing line indexes. This is a one-time cost that doesn't have to be incurred each time you shuffle.</span>
<span class="comment-copy">@Montmons dude, <b>what</b> is it about <code>random.shuffle</code> that doesn't work for you? Is it too slow?</span>
<span class="comment-copy"><code>random.shuffle</code> already uses Fisher-Yates, see <a href="https://stackoverflow.com/questions/9371079/python-shuffle-algorithm-performance" title="python shuffle algorithm performance">stackoverflow.com/questions/9371079/â€¦</a></span>
<span class="comment-copy">Huh, so it does.  Thank you.</span>
