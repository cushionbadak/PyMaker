<div class="post-text" itemprop="text">
<p>I am writing a python script using BeautifulSoup. I need to scrape a website and count unique links ignoring the links starting with '#'.</p>
<p>Example if the following links exist on a webpage:</p>
<p><a href="https://www.stackoverflow.com/questions">https://www.stackoverflow.com/questions</a></p>
<p><a href="https://www.stackoverflow.com/foo">https://www.stackoverflow.com/foo</a></p>
<p><a href="https://www.cnn.com/" rel="nofollow noreferrer">https://www.cnn.com/</a></p>
<p>For this example, the only two unique links will be (The link information after the main domain name is removed):</p>
<pre><code>https://stackoverflow.com/    Count 2
https://cnn.com/              Count 1
</code></pre>
<p>Note: this is my first time using python and web scraping tools.</p>
<p>I appreciate all the help in advance. </p>
<p>This is what I have tried so far:</p>
<pre><code>from bs4 import BeautifulSoup
import requests


url = 'https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)'

r = requests.get(url)

soup = BeautifulSoup(r.text, "html.parser")


count = 0

for link in soup.find_all('a'):
    print(link.get('href'))
    count += 1
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>There is a function named <a href="https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse" rel="nofollow noreferrer"><code>urlparse</code></a> from <code>urllib.parse</code> which you can get <code>netloc</code> of urls. And there is a new awesome HTTP library named <a href="https://github.com/kennethreitz/requests-html" rel="nofollow noreferrer"><code>requests_html</code></a> which can help you get all links in source file.</p>
<pre><code>from requests_html import HTMLSession
from collections import Counter
from urllib.parse import urlparse

session = HTMLSession()
r = session.get("the link you want to crawl")
unique_netlocs = Counter(urlparse(link).netloc for link in r.html.absolute_links)
for link in unique_netlocs:
    print(link, unique_netlocs[link])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You could also do this:</p>
<pre><code>from bs4 import BeautifulSoup
from collections import Counter
import requests

soup = BeautifulSoup(requests.get("https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)").text, "html.parser")

foundUrls = Counter([link["href"] for link in soup.find_all("a", href=lambda href: href and not href.startswith("#"))])
foundUrls = foundUrls.most_common()

for item in foundUrls:
    print ("%s: %d" % (item[0], item[1]))
</code></pre>
<p>The <code>soup.find_all</code> line checks if every <code>a</code>tag has an <code>href</code> set and if it doesn't start with the # character.
The Counter method counts the occurrences of each list entry and the <code>most_common</code> orders by the value.</p>
<p>The <code>for</code> loop just prints the results.</p>
</div>
<div class="post-text" itemprop="text">
<p>My way to do this is to find all links using beautiful soup and then determine which link redirects to which location:</p>
<pre><code>def get_count_url(url): # get the umber of links having the same domain and suffix
r = requests.get(url)
soup = BeautifulSoup(r.text, "html.parser")
count = 0
urls={} #dictionary for the domains
# input_domain=url.split('//')[1].split('/')[0]
#library to extract the exact domain( ex.- blog.bbc.com and bbc.com have the same domains )
input_domain=tldextract.extract(url).domain+"."+tldextract.extract(url).suffix 
for link in soup.find_all('a'):
    word =link.get('href')
    # print(word)
    if word:
        # Same website or domain calls
        if "#" in word or word[0]=="/": #div call or same domain call
            if not input_domain in urls:
                # print(input_domain)
                urls[input_domain]=1 #if first encounter with the domain
            else:
                urls[input_domain]+=1 #multiple encounters
        elif "javascript" in word:
            # javascript function calls (for domains that use modern JS frameworks to display information)
            if not "JavascriptRenderingFunctionCall" in urls:
                urls["JavascriptRenderingFunctionCall"]=1
            else:
                urls["JavascriptRenderingFunctionCall"]+=1
        else:
            # main_domain=word.split('//')[1].split('/')[0]
            main_domain=tldextract.extract(word).domain+"." +tldextract.extract(word).suffix
            # print(main_domain)
            if main_domain.split('.')[0]=='www':
                main_domain = main_domain.replace("www.","") # removing the www
            if not main_domain in urls: # maintaining the dictionary
                urls[main_domain]=1
            else:
                urls[main_domain]+=1
        count += 1

for key, value in urls.items(): # printing the dictionary in a paragraph format for better readability
    print(key,value)
return count    
</code></pre>
<p>tld extract finds the correct url name and soup.find_all('a') finds a tags. The if statements check for same domain redirect, javascript redirect or other domain redirects.</p>
</div>
<span class="comment-copy">This worked but I just need to display the domain and the count for that domain. So for example if the links on the webpage our www.foo.com/bar, www.foo.com/bash, www.foo.com/bar. The output will be www.foo.com 3</span>
<span class="comment-copy">This worked but I just need to display the domain and the count for that domain. So for example if the links on the webpage our www.foo.com/bar, www.foo.com/bash, www.foo.com/bar. The output will be www.foo.com 3</span>
