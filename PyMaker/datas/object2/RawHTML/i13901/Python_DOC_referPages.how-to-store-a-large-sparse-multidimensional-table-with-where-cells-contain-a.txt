<div class="post-text" itemprop="text">
<p>I have a large, sparse, multidimensional lookup table, where cells contain arrays varying in size from 34 kB to circa 10 MB (essentially one or more elements stored in this bin/bucket/cell).  My prototype has dimensions of 30**5=24,300,000, of which only 4,568 cells are non-empty (so it's sparse).  Prototype non-empty cells contain structured arrays with sizes between 34 kB and 7.5 MB.  At 556 MB, the prototype is easily small enough to fit in memory, but the production version will be a lot larger; maybe 100â€“1000 times (it is hard to estimate).  This growth will be mostly due to increased dimensions, rather than due to the data contained in individual cells.  My typical use case is write once (or rarely), read often.</p>
<ul>
<li>I'm currently using a Python dictionary, where the keys are tuples, i.e. <code>db[(29,27,29,29,16)]</code> is a structured <code>numpy.ndarray</code> of around 1 MB.  However, as it grows, this won't fit in memory.</li>
<li>A natural and easy to implement extension would be the Python <a href="https://docs.python.org/3/library/shelve.html" rel="nofollow"><code>shelve</code></a> module.  </li>
<li>I think <a href="http://www.pytables.org/index.html" rel="nofollow"><code>tables</code></a> is fast, in particular for the write once, read often use case, but I don't think it fits my data structure.</li>
<li>Considering that I will always need access only by the tuple index, a very simple way to store it would be to have a directory with some thousands of files with names like <code>entry-29-27-29-29-16</code>, which then stores the <code>numpy.ndarray</code> object in some format (NetCDF, HDF5, npy...).</li>
<li>I'm not sure if a classical database would work, considering that the size of the entries varies considerably.</li>
</ul>
<p>What is a way to store a data structure as described above, that has efficient storage and a fast retrieval of data?</p>
</div>
<div class="post-text" itemprop="text">
<p>From what I understand, you might want to look at the amazing <a href="http://pandas.pydata.org/" rel="nofollow noreferrer">pandas</a> package, as it has a specific facility for the <a href="http://pandas.pydata.org/pandas-docs/stable/sparse.html" rel="nofollow noreferrer">sparse data structure</a> you've described.</p>
<p>Also, while this <a href="https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas">stackoverflow post</a> doesn't specifically address sparse data, it's a great description of using <code>pandas</code> for BIG data, which may be of interest.</p>
<p>Best of luck!</p>
</div>
<span class="comment-copy">I guess your suggestion of storing one small file for each tuple would work well enough.</span>
<span class="comment-copy">I will check out if that does what I seek.</span>
