<div class="post-text" itemprop="text">
<p>Spark MLLIb has a HashingTF() function that computes document term frequencies based on a hashed value of each of the terms. </p>
<p>1) what function does it use to do the hashing? </p>
<p>2) How can I achieve the same hashed value from Python? </p>
<p>3) If I want to compute the hashed output for a given single input, without computing the term frequency, how can I do this?</p>
</div>
<div class="post-text" itemprop="text">
<p>If you're in doubt is it usually good to check <a href="https://github.com/apache/spark/blob/488bad319a70975733e83c83490240a70beb0c90/python/pyspark/mllib/feature.py#L300" rel="noreferrer">the source</a>. The bucket for a given term is determined as follows:</p>
<pre><code>def indexOf(self, term):
    """ Returns the index of the input term. """
    return hash(term) % self.numFeatures
</code></pre>
<p>As you can see it is just a plain old <code>hash</code> module number of buckets.  </p>
<p>Final hash is just a vector of counts per bucket (I've omitted docstring and RDD case for brevity):</p>
<pre><code>def transform(self, document):
    freq = {}
    for term in document:
        i = self.indexOf(term)
        freq[i] = freq.get(i, 0) + 1.0
    return Vectors.sparse(self.numFeatures, freq.items())
</code></pre>
<p>If you want to ignore frequencies then you can use <code>set(document)</code> as an input, but I doubt there is much to gain here. To create <code>set</code> you'll have to compute <code>hash</code> for each element anyway.</p>
</div>
<div class="post-text" itemprop="text">
<p>It seems to me that there is something else going on under the hood other than what the source that zero323 linked. I found that hashing and then doing the modulus as the source code did wouldn't give me the same indices as hashingTF generates. At least for single characters, what I had to do was convert the char to the ascii code, like so: (Python 2.7)</p>
<pre><code>index = ord('a') # 97
</code></pre>
<p>Which corresponds to what hashingtf outputs for the index. If I did the same thing as hashingtf appears to do, which is:</p>
<pre><code>index = hash('a') % 1&lt;&lt;20 # 897504
</code></pre>
<p>I would get very clearly the wrong index.</p>
</div>
<span class="comment-copy">I am not sure if I understand what you mean by "hashed output for a given single input, without computing the term frequency" here. Do you mean something like computing hash for <code>set(document)</code>?</span>
<span class="comment-copy">Yes, given a string S, I'd like a quick way to find the hashed(S) value without having to instantiate and use the HashingTF() function in Spark.</span>
<span class="comment-copy">Thanks Zero323. I guess i was under the impression that the HashingTF was implemented in Java. Thanks!</span>
<span class="comment-copy">Most of the functions in MLlib are implemented natively and operate on native data structures. For example <code>Vectors</code> are just the wrappers for <code>numpu.ndarray</code>.</span>
<span class="comment-copy">Interesting anwser. I would like to find which term correspond to a given hash (I'm running TF-IDF then wan't to find out most important terms). It returns (hash, tfidf) tuple, any idea how i could get (term, tfidf) ?</span>
<span class="comment-copy">@pltrdy You cannot, or at least not in a general case. If you want a reversible transformation take a look at the count vectorizer. See my answer to <a href="http://stackoverflow.com/a/32286619/1560062">stackoverflow.com/a/32286619/1560062</a></span>
<span class="comment-copy">Very interesting anwser, thx</span>
<span class="comment-copy"><a href="https://docs.python.org/3/reference/expressions.html#operator-precedence" rel="nofollow noreferrer">Operator precedence</a>: <code>assert HashingTF(numFeatures=1 &lt;&lt; 20).indexOf("a") == (hash("a") % (1 &lt;&lt; 20))</code>.</span>
