<div class="post-text" itemprop="text">
<p>I need to check whether all the url's are responding or not.If some url(s) is not responding I need to display that.Here I don't want to wait
for the one by one checking and display.For this reason I want to use Multi threading concept.Here's how to use Multi-threading to make use of my code in an efficient way.</p>
<pre><code>import threading,urllib2
import time,pymongo,smtplib
from urllib2 import urlopen,URLError
from socket import socket
from threading import Thread
res = {"ftp":'ftp://ftp.funet.fi/pub/standards/RFC/rfc959.txt',"tcp":'devio.us:22',"smtp":'http://smtp.gmail.com',"http":"http://www.amazon.com"}
def allUrls():
    try:
        if 'http' in res.keys():
            http_test(res["http"])
            get_threads(res["http"])
        if 'tcp' in res.keys():
            tcp_test(res["tcp"])
        if 'ftp' in res.keys():
            ftp_test(res["ftp"])
        if 'smtp' in res.keys():
            smtp_test(res["smtp"])
    except pymongo.errors.ConnectionFailure, e:
        print "Could not connect to MongoDB: %s" % e

def tcp_test(server_info):
    cpos = server_info.find(':')
    try:
        sock = socket()
        sock.connect((server_info[:cpos], int(server_info[cpos+1:])))
        sock.close
        print (server_info + " \t\tResponding ")
    except Exception as e:
        print str(e)
def http_test(server_info):
    try:
        data = urlopen(server_info)
        print (server_info + " \t\tResponding "),data.code
        FetchUrl(server_info).start()
    except Exception as e:
        print str(e)
def ftp_test(server_info):
    try:
        data = urlopen(server_info)
        print (server_info+"  -  Responding "),data.code
    except Exception as e:
        print str(e)
def smtp_test(server_info):
   try:
      conn = smtplib.SMTP("smtp.gmail.com",587, timeout=10)
      try:
         status = conn.noop()[0]
      except:
         status = -1
      if status == 250:
         print server_info+ " \t\t Responding "
      else:
         print "Not Responding"
   except:
      print "something wrong in the URL"

start = time.time()
class FetchUrl(threading.Thread):
    def __init__(self, url):
        threading.Thread.__init__(self)
        self.setDaemon = True
        self.url = url
    def run(self):
        urlHandler = urllib2.urlopen(self.url)
        html = urlHandler.read()
        finished_fetch_url(self.url)
def finished_fetch_url(url):
    print "\"%s\" \tfetched in %ss" % (url,(time.time() - start))

def crawl(url):
    data = urllib2.urlopen(url).read()
    print (url +" \t\tReading")
def get_threads(url):
    # 5 threads. Need to pass urls from here.
    thread = threading.Thread(target=crawl(url))
    thread.start()
    thread.join()
    print "Threads Elapsed time: \t\t%s " % (time.time() - start)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Python is not designed to be multithreaded. In fact, there is a <a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow">Global Interpreter Lock (GIL)</a> baked into Python which makes true multithreading difficult with the vanilla libraries.</p>
<p>That is not to say it is completely impossible though; you can use other libraries that work around the GIL. The easiest (and most applicable) for your situation would be <a href="http://www.gevent.org/" rel="nofollow">Gevent</a>. I don't know what your exact performance requirements are and I don't have any benchmarks at hand to recommend a Gevent approach to follow but you can check them out on your own:</p>
<ul>
<li>You can <a href="http://www.gevent.org/gevent.monkey.html" rel="nofollow">monkey patch</a> your script. Monkey patching makes the vanilla libraries work with Gevent. Takes the least effort.</li>
<li>You can rewrite your script using Gevent-based networking/<a href="https://github.com/gwik/geventhttpclient" rel="nofollow">HTTP libraries</a>.</li>
</ul>
<p>Again, I've no data to tell which is better but this is what I'd do given your situation.</p>
</div>
<div class="post-text" itemprop="text">
<p>The tool which I'm using to unshorten URLs is concurrent.futures.
Take a look right here: <a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example" rel="nofollow">concurrent</a>, maybe it can help. Unfortunately, similarly to the answer skytreader gave you, I can't tell you which is better or faster.</p>
</div>
<div class="post-text" itemprop="text">
<p>You should take a look at the <a href="https://docs.python.org/3.4/library/multiprocessing.html" rel="nofollow">multiprocessing library</a> if you want to do real multi-threading to get around the Global Interpreter Lock. Also, I'd recommend looking at the <a href="http://requests.readthedocs.org/en/latest/" rel="nofollow">requests library</a> to see if that does a lot of what you're trying to implement already.</p>
</div>
