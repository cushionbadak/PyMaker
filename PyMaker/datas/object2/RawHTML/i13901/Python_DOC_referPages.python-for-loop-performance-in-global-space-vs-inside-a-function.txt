<div class="post-text" itemprop="text">
<pre><code>def main():
    for i in xrange(10**8):
        pass
main()
</code></pre>
<p>This piece of code in Python runs in  (Note: The timing is done with the time function in BASH in Linux.)</p>
<pre><code>real    0m1.841s
user    0m1.828s
sys     0m0.012s
</code></pre>
<p>However, if the for loop isn't placed within a function, </p>
<pre><code>for i in xrange(10**8):
    pass
</code></pre>
<p>then it runs for a much longer time:</p>
<pre><code>real    0m4.543s
user    0m4.524s
sys     0m0.012s
</code></pre>
<p>Why is this?</p>
</div>
<div class="post-text" itemprop="text">
<p>You might ask <em>why</em> it is faster to store local variables than globals. This is a CPython implementation detail.</p>
<p>Remember that CPython is compiled to bytecode, which the interpreter runs. When a function is compiled, the local variables are stored in a fixed-size array (<em>not</em> a <code>dict</code>) and variable names are assigned to indexes. This is possible because you can't dynamically add local variables to a function. Then retrieving a local variable is literally a pointer lookup into the list and a refcount increase on the <code>PyObject</code> which is trivial.</p>
<p>Contrast this to a global lookup (<code>LOAD_GLOBAL</code>), which is a true <code>dict</code> search involving a hash and so on. Incidentally, this is why you need to specify <code>global i</code> if you want it to be global: if you ever assign to a variable inside a scope, the compiler will issue <code>STORE_FAST</code>s for its access unless you tell it not to.</p>
<p>By the way, global lookups are still pretty optimised. Attribute lookups <code>foo.bar</code> are the <em>really</em> slow ones!</p>
<p>Here is small <a href="https://wiki.python.org/moin/PythonSpeed/PerformanceTips#Local_Variables" rel="noreferrer">illustration</a> on local variable efficiency.</p>
</div>
<div class="post-text" itemprop="text">
<p>Inside a function, the bytecode is</p>
<pre><code>  2           0 SETUP_LOOP              20 (to 23)
              3 LOAD_GLOBAL              0 (xrange)
              6 LOAD_CONST               3 (100000000)
              9 CALL_FUNCTION            1
             12 GET_ITER            
        &gt;&gt;   13 FOR_ITER                 6 (to 22)
             16 STORE_FAST               0 (i)

  3          19 JUMP_ABSOLUTE           13
        &gt;&gt;   22 POP_BLOCK           
        &gt;&gt;   23 LOAD_CONST               0 (None)
             26 RETURN_VALUE        
</code></pre>
<p>At top level, the bytecode is</p>
<pre><code>  1           0 SETUP_LOOP              20 (to 23)
              3 LOAD_NAME                0 (xrange)
              6 LOAD_CONST               3 (100000000)
              9 CALL_FUNCTION            1
             12 GET_ITER            
        &gt;&gt;   13 FOR_ITER                 6 (to 22)
             16 STORE_NAME               1 (i)

  2          19 JUMP_ABSOLUTE           13
        &gt;&gt;   22 POP_BLOCK           
        &gt;&gt;   23 LOAD_CONST               2 (None)
             26 RETURN_VALUE        
</code></pre>
<p>The difference is that <a href="http://docs.python.org/library/dis.html#opcode-STORE_FAST"><code>STORE_FAST</code></a> is faster (!) than <a href="http://docs.python.org/library/dis.html#opcode-STORE_NAME"><code>STORE_NAME</code></a>.  This is because in a function, <code>i</code> is a local but at toplevel it is a global.</p>
<p>To examine bytecode, use the <a href="http://docs.python.org/library/dis.html"><code>dis</code> module</a>.  I was able to disassemble the function directly, but to disassemble the toplevel code I had to use the <a href="http://docs.python.org/library/functions.html#compile"><code>compile</code> builtin</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>Aside from local/global variable store times, <strong>opcode prediction</strong> makes the function faster.</p>
<p>As the other answers explain, the function uses the <code>STORE_FAST</code> opcode in the loop. Here's the bytecode for the function's loop:</p>
<pre><code>    &gt;&gt;   13 FOR_ITER                 6 (to 22)   # get next value from iterator
         16 STORE_FAST               0 (x)       # set local variable
         19 JUMP_ABSOLUTE           13           # back to FOR_ITER
</code></pre>
<p>Normally when a program is run, Python executes each opcode one after the other, keeping track of the a stack and preforming other checks on the stack frame after each opcode is executed. Opcode prediction means that in certain cases Python is able to jump directly to the next opcode, thus avoiding some of this overhead.</p>
<p>In this case, every time Python sees <code>FOR_ITER</code> (the top of the loop), it will "predict" that <code>STORE_FAST</code> is the next opcode it has to execute. Python then peeks at the next opcode and, if the prediction was correct, it jumps straight to <code>STORE_FAST</code>. This has the effect of squeezing the two opcodes into a single opcode.</p>
<p>On the other hand, the <code>STORE_NAME</code> opcode is used in the loop at the global level. Python does <strong>*not*</strong> make similar predictions when it sees this opcode. Instead, it must go back to the top of the evaluation-loop which has obvious implications for the speed at which the loop is executed.</p>
<p>To give some more technical detail about this optimization, here's a quote from the <a href="https://hg.python.org/cpython/file/45b1ae1ef318/Python/ceval.c#l784" rel="noreferrer"><code>ceval.c</code></a> file (the "engine" of Python's virtual machine):</p>
<blockquote>
<p>Some opcodes tend to come in pairs thus making it possible to
   predict the second code when the first is run.  For example,
   <code>GET_ITER</code> is often followed by <code>FOR_ITER</code>. And <strong><code>FOR_ITER</code> is often
   followed by <code>STORE_FAST</code></strong> or <code>UNPACK_SEQUENCE</code>.</p>
<p>Verifying the prediction costs a single high-speed test of a register
      variable against a constant.  If the pairing was good, then the
      processor's own internal branch predication has a high likelihood of
      success, resulting in a nearly zero-overhead transition to the
      next opcode.  A successful prediction saves a trip through the eval-loop
      including its two unpredictable branches, the <code>HAS_ARG</code> test and the
      switch-case.  Combined with the processor's internal branch prediction,
      a successful <code>PREDICT</code> has the effect of making the two opcodes run as if
      they were a single new opcode with the bodies combined.</p>
</blockquote>
<p>We can see in the source code for the <a href="https://hg.python.org/cpython/file/45b1ae1ef318/Python/ceval.c#l2493" rel="noreferrer"><code>FOR_ITER</code></a> opcode exactly where the prediction for <code>STORE_FAST</code> is made:</p>
<pre class="lang-C prettyprint-override"><code>case FOR_ITER:                         // the FOR_ITER opcode case
    v = TOP();
    x = (*v-&gt;ob_type-&gt;tp_iternext)(v); // x is the next value from iterator
    if (x != NULL) {                     
        PUSH(x);                       // put x on top of the stack
        PREDICT(STORE_FAST);           // predict STORE_FAST will follow - success!
        PREDICT(UNPACK_SEQUENCE);      // this and everything below is skipped
        continue;
    }
    // error-checking and more code for when the iterator ends normally                                     
</code></pre>
<p>The <code>PREDICT</code> function expands to <code>if (*next_instr == op) goto PRED_##op</code> i.e. we just jump to the start of the predicted opcode. In this case, we jump here:</p>
<pre class="lang-C prettyprint-override"><code>PREDICTED_WITH_ARG(STORE_FAST);
case STORE_FAST:
    v = POP();                     // pop x back off the stack
    SETLOCAL(oparg, v);            // set it as the new local variable
    goto fast_next_opcode;
</code></pre>
<p>The local variable is now set and the next opcode is up for execution. Python continues through the iterable until it reaches the end, making the successful prediction each time.</p>
<p>The <a href="https://wiki.python.org/moin/CPythonVmInternals" rel="noreferrer">Python wiki page</a> has more information about how CPython's virtual machine works.</p>
</div>
<span class="comment-copy">How did you actually do the timing?</span>
<span class="comment-copy">Just an intuition, not sure if it's true: I would guess it's because of scopes. In the function case, a new scope is created (i.e. kind of a hash with variable names bound to their value). Without a function, variables are in the global scope, when you can find lot of stuff, hence slowing down the loop.</span>
<span class="comment-copy">@Scharron That doesn't seem to be it.  Defined 200k dummy variables into the scope without that visibly affecting the running time.</span>
<span class="comment-copy">Alex Martelli wrote a good answer concerning this <a href="http://stackoverflow.com/a/1813167/174728">stackoverflow.com/a/1813167/174728</a></span>
<span class="comment-copy">@Scharron you're half correct. It is about scopes, but the reason it's faster in locals is that local scopes are actually implemented as arrays instead of dictionaries (since their size is known at compile-time).</span>
<span class="comment-copy">This also applies to PyPy, up to the current version (1.8 at the time of this writing.)  The test code from the OP runs about four times slower in global scope compared to inside a function.</span>
<span class="comment-copy">@Walkerneo They aren't, unless you said it backwards. According to what katrielalex and ecatmur are saying, global variable lookups are slower than local variable lookups due to the method of storage.</span>
<span class="comment-copy">@Walkerneo foo.bar is not a local access. It is an attribute of an object. (Forgive the lack of formatting)<code>def foo_func: x = 5</code>, <code>x</code> is local to a function. Accessing <code>x</code> is local. <code>foo = SomeClass()</code>, <code>foo.bar</code> is attribute access. <code>val = 5</code> global is global. As for speed local &gt; global &gt; attribute according to what I've read here. So accessing <code>x</code> in <code>foo_func</code> is fastest, followed by <code>val</code>, followed by <code>foo.bar</code>. <code>foo.attr</code> isn't a local lookup because in the context of this convo, we're talking about local lookups being a lookup of a variable that belongs to a function.</span>
<span class="comment-copy">@thedoctar: Actually, in American English, both <code>indexes</code> and <code>indices</code> are valid spellings of the plural of <code>index</code>. You can look it up in any dictionary. A couple of popular examples: <a href="http://www.merriam-webster.com/dictionary/index" rel="nofollow noreferrer">Merriam-Webster</a>, <a href="http://dictionary.reference.com/browse/index?s=t" rel="nofollow noreferrer">dictionary.com</a></span>
<span class="comment-copy">@thedoctar: I have not found any indication that <i>indexes</i> is an invalid or improper plural of <i>index</i> in any English dialect. Many people prefer <i>indices</i>, particularly in mathematical or technical contexts, but that is far from making <i>indexes</i> wrong. See <a href="http://english.stackexchange.com/questions/61080/indexes-or-indices" title="indexes or indices">english.stackexchange.com/questions/61080/indexes-or-indices</a> and many other references.</span>
<span class="comment-copy">Confirmed by experiment.  Inserting <code>global i</code> into the <code>main</code> function makes the running times equivalent.</span>
<span class="comment-copy">This answers the question without answering the question :)  In the case of local function variables, CPython actually stores these in a tuple (which is mutable from the C code) until a dictionary is requested (e.g. via <code>locals()</code>, or <code>inspect.getframe()</code> etc.). Looking up an array element by a constant integer is much faster than searching a dict.</span>
<span class="comment-copy">It is the same with C/C++ also, using global variables causes significant slowdown</span>
<span class="comment-copy">This is the first I've seen of bytecode.. How does one look at it, and is important to know?</span>
<span class="comment-copy">@gkimsey I agree. Just wanted to share two things i) This behaviour is noted in other programming languages ii) The causal agent is more the architectural side and not the language itself in true sense</span>
<span class="comment-copy">Minor update: As of CPython 3.6, the savings from prediction go down a bit; instead of two unpredictable branches, there is only one. The change is due to <a href="https://docs.python.org/3/whatsnew/3.6.html#optimizations" rel="nofollow noreferrer">the switch from bytecode to wordcode</a>; now all "wordcodes" have an argument, it's just zero-ed out when the instruction doesn't logically take an argument. Thus, the <code>HAS_ARG</code> test never occurs (except when low level tracing is enabled both at compile and runtime, which no normal build does), leaving only one unpredictable jump.</span>
<span class="comment-copy">Even that unpredictable jump doesn't happen in most builds of CPython, because of the new (<a href="https://docs.python.org/3/whatsnew/3.1.html#optimizations" rel="nofollow noreferrer">as of Python 3.1</a>, <a href="https://docs.python.org/3/whatsnew/3.2.html#build-and-c-api-changes" rel="nofollow noreferrer">enabled by default in 3.2</a>) computed gotos behavior; when used, the <code>PREDICT</code> macro is completely disabled; instead most cases end in a <code>DISPATCH</code> that branches directly. But on branch predicting CPUs, the effect is similar to that of <code>PREDICT</code>, since branching (and prediction) is per opcode, increasing the odds of successful branch prediction.</span>
