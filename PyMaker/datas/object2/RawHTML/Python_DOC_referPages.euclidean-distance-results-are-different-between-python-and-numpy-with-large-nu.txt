<div class="post-text" itemprop="text">
<p>I am trying two methods to implement the square result of euclidean distance.</p>
<p>By Numpy:</p>
<pre><code>def inference(feature_list):
    distances = np.zeros(len(feature_list))
    for idx, pair in enumerate(feature_list):
        distances[idx] = euclidean_distances(pair[0].reshape((1, -1)), pair[1].reshape((1, -1))).item()
        distances[idx] = distances[idx] * distances[idx]
    return distances
</code></pre>
<p>By python:</p>
<pre><code>def inference1(feature_list):
    distances = np.zeros(len(feature_list))
    for idx, pair in enumerate(feature_list):
        for pair_idx in range(len(pair[0])):
            tmp = pair[0][pair_idx] - pair[1][pair_idx]
            distances[idx] += tmp * tmp

    return distances
</code></pre>
<p>Code to test the result is:</p>
<pre><code>def main(args):
    d = 128
    n = 100
    array2 = [(np.random.rand(d)/4, np.random.rand(d)/3) for x in range(n)]

    result = sample.inference(array2)
    print(list(result)) # print result 1


    result = sample.inference1(array2)
    print(list(result)) # print result 2
</code></pre>
<p>The results are different when n reaches 100000, while the results stay the same when n is small.</p>
<p>Why would it happen? How can I get the same result?</p>
</div>
<div class="post-text" itemprop="text">
<p>In this minimal example, we see that the difference between the 2 results are negligible.</p>
<pre><code>import numpy as np
from sklearn.metrics.pairwise import euclidean_distances

def inference_sklearn(feature_list):
    distances = np.zeros(len(feature_list))
    for idx, pair in enumerate(feature_list):
        distances[idx] = euclidean_distances(pair[0].reshape((1, -1)), pair[1].reshape((1, -1))).item()
        distances[idx] = distances[idx] * distances[idx]
    return distances

def inference_python(feature_list):
    distances = np.zeros(len(feature_list))
    for idx, pair in enumerate(feature_list):
        for pair_idx in range(len(pair[0])):
            tmp = pair[0][pair_idx] - pair[1][pair_idx]
            distances[idx] += tmp * tmp

    return distances


d = 128
ns = [100, 1000, 10000, 100000, 200000]
for n in ns: 
    print("n =", n)
    test_array = [(np.random.rand(d)/4, np.random.rand(d)/3) for x in range(n)]
    result_sklearn = inference_sklearn(test_array)
    result_python = inference_python(test_array)
    print(euclidean_distances([result_sklearn], [result_python])[0][0])
</code></pre>
<p>Output: </p>
<pre><code>n = 100
0.0
n = 1000
0.0
n = 10000
0.0
n = 100000
0.0
n = 200000
1.52587890625e-05
</code></pre>
<p>Don't just print your results when you want to test equality. Also you can use the <a href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.set_printoptions.html" rel="nofollow noreferrer">numpy.set_printoptions</a> to deal with the printing quality of your arrays. </p>
</div>
<span class="comment-copy">The doc (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html" rel="nofollow noreferrer">scikit-learn.org/stable/modules/generated/â€¦</a>) seems to say that "this is not the most precise way of doing this computation". Not sure if that is the explanation for your observation.</span>
<span class="comment-copy">I am confused about why the number of examples will affect the result of computation.</span>
<span class="comment-copy">After generating your 100000 values, suppose you restrict yourself to only the last 1000 of them, and ignore the rest of the 100000 values, do you still see an anomaly with the 1000 values?</span>
<span class="comment-copy">How do you test the equality between the two results ? I guess you don't just read the 100000 values ?</span>
<span class="comment-copy">@Tengerye: I am still not sure if the anomaly is in the <code>print</code>, or in the <b>actual values</b> that appear towards the end last part of your 100000 values. No worries about me not getting credit, because, I'm just throwing guesses at you, and trying to learn something along the way myself.</span>
