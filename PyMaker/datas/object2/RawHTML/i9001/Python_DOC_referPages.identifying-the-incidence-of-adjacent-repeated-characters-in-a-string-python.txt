<div class="post-text" itemprop="text">
<p>I'm learning python these days; trying out various challenges to improve my concepts. </p>
<p>One mini challenge I just tried is identifying the incidence of <code>n</code> adjacent, repeated characters in a given string. </p>
<p>I attempted it as follows:</p>
<pre><code>def uniform_string(text):
    text = text.lower()
    i = 0
    while i &lt; (len(text)-3):
        if text[i] == text[i+1] and text[i+1] == text[i+2] and text[i+2] == text[i+3]:
            return True
        i += 1
    return False
</code></pre>
<p>This assumes <code>n=4</code> and I'm now trying to generalize it. </p>
<p>But on the side, it's also got me thinking: is there a more efficient way to accomplish this? My current solution makes me do 4 times the number of lookups than the length of the string (which means, for increasing values of <code>n</code>, this would increase toward <code>O(n^2)</code>). </p>
<p>What could be a better way to tackle a challenge like this?</p>
</div>
<div class="post-text" itemprop="text">
<p>Bit of a wildcard entry:</p>
<p>Advantage pretty fast: example with 4,000,000 characters is analysed instantly</p>
<p>Disadvantage relies on numpy; convoluted</p>
<p>Here goes:</p>
<pre><code>import numpy as np
a = "etr" + 1_000_000 * "zr" + "hhh" + 1_000_000 * "Ar"
np.max(np.diff(np.r_[-1, np.where(np.diff(np.frombuffer(a.encode('utf16'), dtype=np.uint16)[1:]))[0], len(a) - 1]))                                             
3
</code></pre>
<p>How it works:</p>
<ul>
<li>encode string to fixed-width-per-character bytestring</li>
<li>interpret the buffer as numpy array</li>
<li>compute the "derivative"</li>
<li>find nonzero places = places where character changes</li>
<li>the distances between these are the repeat numbers</li>
<li>compute maximum</li>
</ul>
<p>UPDATE:</p>
<p>Here's a hybrid version that does some crude short-circuitting plus some rudimentary benchmarking to find best parameters:</p>
<pre><code>import numpy as np
from timeit import timeit

occ = 4
loc = (10, 20, 40, 80, 160, 320, 1000, 2000, 4000, 8000, 16000, 32000, 64000,
       125000, 250000, 500000, 1_000_000, 2_000_000)
a = ['pafoe&lt;03' + o * 'gr' + occ * 'x' + (2_000_000 - o) * 'u1'
      + 'leto50d-fjeoa'[occ:] for o in loc]

def brute_force(a):
    np.max(np.diff(np.r_[-1, np.where(np.diff(np.frombuffer(
        a.encode('utf16'), dtype=np.uint16)[1:]))[0], len(a) - 1]))

def reverse_bisect(a, chunk, encode_all=True):
    j = 0
    i = chunk
    n = len(a)
    if encode_all:
        av = np.frombuffer(a.encode('utf16'), dtype=np.uint16)[1:]
    while j&lt;n:
        if encode_all:
            s = av[j : j + chunk]
        else:
            s = np.frombuffer(a[j:j+chunk].encode('utf16'), dtype=np.uint16)[1:]
        if np.max(np.diff(np.r_[-1, np.where(np.diff(s))[0], len(s)-1])) &gt;= occ:
            return True
        j += chunk - occ + 1
        chunk *= 2
    return False

leave_out = 2
out = []
print('first repeat at', loc[:-leave_out])
print('brute force {}'.format(
    (timeit('[f(a) for a in A]', number=100, globals={
        'f': brute_force, 'A': a[:-leave_out]}))))
print('hybrid (reverse bisect)')
for chunk in 2**np.arange(2, 18):
    out.append(timeit('[f(a,c,e) for a in A]', number=100, globals={
        'f': reverse_bisect, 'A': a[:-leave_out], 'c': chunk, 'e': True}))
    out.append(timeit('[f(a,c,e) for a in A]', number=100, globals={
        'f': reverse_bisect, 'A': a[:-leave_out], 'c': chunk, 'e': False}))
    print('chunk: {}, timings: encode all {} -- encode chunks {}'.format(
        chunk, out[-2], out[-1]))
</code></pre>
<p>Sample runs:</p>
<pre><code>first repeat at (10, 20, 40, 80, 160, 320, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 125000, 250000, 500000)
brute force 90.26514193788171
hybrid (reverse bisect)
chunk: 4, timings: encode all 5.257935176836327 -- encode chunks 2.3392367498017848
chunk: 8, timings: encode all 5.210895746946335 -- encode chunks 2.288218504982069
chunk: 16, timings: encode all 5.268893962958828 -- encode chunks 2.2223802611697465
chunk: 32, timings: encode all 5.109196993988007 -- encode chunks 2.1715646600350738
chunk: 64, timings: encode all 5.05742059298791 -- encode chunks 2.1255820950027555
chunk: 128, timings: encode all 5.110778157133609 -- encode chunks 2.100305920932442
chunk: 256, timings: encode all 5.058305847924203 -- encode chunks 2.153960411902517
chunk: 512, timings: encode all 5.108077083015814 -- encode chunks 2.056686638854444
chunk: 1024, timings: encode all 4.969490061048418 -- encode chunks 2.0368234540801495
chunk: 2048, timings: encode all 5.153041162993759 -- encode chunks 2.465495347045362
chunk: 4096, timings: encode all 5.28073402796872 -- encode chunks 2.173405918991193
chunk: 8192, timings: encode all 5.044360157102346 -- encode chunks 2.1234876308590174
chunk: 16384, timings: encode all 5.294338152976707 -- encode chunks 2.334656815044582
chunk: 32768, timings: encode all 5.7856643970590085 -- encode chunks 2.877617093967274
chunk: 65536, timings: encode all 7.04935942706652 -- encode chunks 4.1559580829925835
chunk: 131072, timings: encode all 7.516369879012927 -- encode chunks 4.553452031919733

first repeat at (10, 20, 40)
brute force 16.363576064119115
hybrid (reverse bisect)
chunk: 4, timings: encode all 0.6122389689553529 -- encode chunks 0.045893668895587325
chunk: 8, timings: encode all 0.5982049370650202 -- encode chunks 0.03538667503744364
chunk: 16, timings: encode all 0.5907809699419886 -- encode chunks 0.025738760828971863
chunk: 32, timings: encode all 0.5741697370540351 -- encode chunks 0.01634934707544744
chunk: 64, timings: encode all 0.5719085780438036 -- encode chunks 0.013115004170686007
chunk: 128, timings: encode all 0.5666680270805955 -- encode chunks 0.011037093820050359
chunk: 256, timings: encode all 0.5664500128477812 -- encode chunks 0.010536623885855079
chunk: 512, timings: encode all 0.5695593091659248 -- encode chunks 0.01133729494176805
chunk: 1024, timings: encode all 0.5688401609659195 -- encode chunks 0.012476094998419285
chunk: 2048, timings: encode all 0.5702746720053256 -- encode chunks 0.014690137933939695
chunk: 4096, timings: encode all 0.5782928131520748 -- encode chunks 0.01891179382801056
chunk: 8192, timings: encode all 0.5943365979474038 -- encode chunks 0.0272749038413167
chunk: 16384, timings: encode all 0.609349318081513 -- encode chunks 0.04354232898913324
chunk: 32768, timings: encode all 0.6489383969455957 -- encode chunks 0.07695812894962728
chunk: 65536, timings: encode all 0.7388215309474617 -- encode chunks 0.14061277196742594
chunk: 131072, timings: encode all 0.8899400909431279 -- encode chunks 0.2977339250501245
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In the following function, <code>n</code> is the number of characters you want to check for equality, and to keep your original function call the same, you can also set the default value of <code>n</code> to 4.</p>
<pre><code>def uniform_string(text, n=4):
    text = text.lower()
    i = 0
    while i &lt; (len(text)-n):
        if text[i:i + n] == text[i] * n:
            return True
        i += 1
    return False
</code></pre>
<p>Alternatively, you can also use a for loop:</p>
<pre><code>def uniform_string(text, n):
    text = text.lower()

    for i in range(len(text) - n + 1):
        if text[i:i + n] == text[i] * n:
            return True

    return False
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can use <code>slice</code>s (<code>some_list[start:stop]</code>) and <code>set</code>s to solve your problem.</p>
<pre><code>def uniform_string(text, n):
    text = text.lower()

    for i in range(len(text) - n + 1):
        if len(set(text[i:i+n])) == 1:  # all equal
            return True
    return False
</code></pre>
<p>Your code will also be a bit cleaner if you use a <code>for</code> loop, rather than a <code>while</code> loop. :)</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use <a href="https://docs.python.org/3/library/itertools.html#itertools.groupby" rel="nofollow noreferrer"><code>groupby</code></a> to group the consecutive same characters and count the number of chars within the each group. If any of the counts is greater threshold the result is <code>True</code>:</p>
<pre><code>from itertools import groupby

def uniform_string(text, n):
    return any(sum(1 for _ in g) &gt;= n for _, g in groupby(text.lower()))

print(uniform_string('fofo', 1))
print(uniform_string('fofo', 2))
print(uniform_string('fofoo', 2))
</code></pre>
<p>Output:</p>
<pre><code>True
False
True
</code></pre>
<p>Time complexity of above is <strong>O(n)</strong>.</p>
</div>
<div class="post-text" itemprop="text">
<p>The answers posted so far miss one of Python's nicer iteration functions, <code>enumerate</code>:</p>
<pre><code>def uniform_string(text, n):
    for i, c in enumerate(text):
        if text[i:i+4] == c * n:
            print( c, 'at', i, 'in', text )
</code></pre>
<p>I'm not sure that's exactly what you asked for, but it might give you something to go on.  </p>
</div>
<div class="post-text" itemprop="text">
<p>If this is intended to tell you that every character is identical, you could do the following:</p>
<pre><code>def uniform_string(text):
    text = text.lower()
    if text.count(text[0]) == len(text):
        return True
    return False
</code></pre>
</div>
<span class="comment-copy">Fascinating! Although for smaller strings (e.g. the average sentence <code>len</code> on this page), I wonder how it's going to perform versus a traditional loop. I'll benchmark it against some other answers here.</span>
<span class="comment-copy">@HassanBaig Any chance of your sharing your results? I did a tiny test myself, just mine against the <code>groupby</code> one and I see them crossing over at around 35 characters.</span>
<span class="comment-copy">Yours gives the <code>max</code> adjacent repetition value, whereas all other entries here just break upon finding a match. Meaning, depending on text input, they'd engender varied performance. Yours - in comparison - performs much more stably across various inputs. So what's the approach you used to time your algorithm against the <code>groupby</code> one? You assumed <code>n=4</code> or what? I'm running some tests right now.</span>
<span class="comment-copy">@HassanBaig Ok, I should have said tiny and careless ;-) I just used the first line I posted and changed the number of repeats (of the groups); so I didn't vary the structure of the argument. And, of course, since it has characters repeating no more than three times, it doesn't give the <code>groupby</code> (which I copy-n-pasted) the chance to short-circuit. So maybe my tests weren't entirely fair. Btw. short-circuiting is one of <code>numpy</code>s weaker suits. But in many cases it makes up for that by mere raw speed.</span>
<span class="comment-copy">@HassanBaig FYI I've added a new version that adds some poor-man's short-circuiting. Unfortunately, not exactly a one-liner. I've also posted some internal benchmarks.</span>
<span class="comment-copy">this is not working when the repeated characters are at the end of the string.</span>
<span class="comment-copy">@lmiguelvargasf Good catch! I was missing a <code>+ 1</code> - it has been edited to reflect the fix. :)</span>
<span class="comment-copy">Ah no, not that every character is identical - that solely happens in the most extreme case.</span>
