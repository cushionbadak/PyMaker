<div class="post-text" itemprop="text">
<p>The code below handles a huge amount of data and I want to ask how can I use the multiprocessing module in Python for parallel processing in order to speed things up.
Any help is appreciated</p>
<pre><code>pats = []
for chunk in code_counter(patients, codes):
    pats.append(chunk)

def code_counter(patients, codes):
    for key, group in itertools.groupby(patients, key=operator.itemgetter('ID')):
        group_codes = [item['CODE'] for item in group]
        yield [group_codes.count(code) for code in codes]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I think your problem resides in the use of yield. I think you can't yield the data from different processes. I understood, that you use the yield cuz you can't load the data "inline"  that would cause the ram to overload. </p>
<p>maybe you can take a look at the multiprocessing Queue
<a href="http://docs.python.org/2/library/multiprocessing.html#exchanging-objects-between-processes" rel="nofollow">http://docs.python.org/2/library/multiprocessing.html#exchanging-objects-between-processes</a></p>
<p>i didn't really get what you are trying to do with your code, so i can't deliver a precise excample.</p>
</div>
<span class="comment-copy">Does the order of items in <code>pats</code> matter?</span>
<span class="comment-copy">yes! do you know how to do it?</span>
<span class="comment-copy">what is <code>patients</code> and where do you get it? for this to work, it must <a href="http://docs.python.org/3/library/itertools#itertools.groupby" rel="nofollow noreferrer">alerady be sorted</a> on <code>ID</code>.  Is it? Otherwise it would be better to just use a <a href="http://docs.python.org/3/library/collections#collections.Counter" rel="nofollow noreferrer"><code>Counter</code></a>. Piping all data to subprocesses just to get a count back will probably introduce more overhead then speedgain.</span>
