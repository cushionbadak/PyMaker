<div class="post-text" itemprop="text">
<p>I need to process some data that is a few hundred times bigger than RAM. I would like to read in a large chunk, process it, save the result, free the memory and repeat.  Is there a way to make this efficient in python?</p>
</div>
<div class="post-text" itemprop="text">
<p>The general key is that you want to process the file iteratively.</p>
<p>If you're just dealing with a text file, this is trivial: <code>for line in f:</code> only reads in one line at a time. (Actually it buffers things up, but the buffers are small enough that you don't have to worry about it.)</p>
<p>If you're dealing with some other specific file type, like a numpy binary file, a CSV file, an XML document, etc., there are generally similar special-purpose solutions, but nobody can describe them to you unless you tell us what kind of data you have.</p>
<p>But what if you have a general binary file?</p>
<hr/>
<p>First, the <a href="http://docs.python.org/3/library/io.html#io.RawIOBase.read" rel="noreferrer"><code>read</code></a> method takes an optional max bytes to read. So, instead of this:</p>
<pre><code>data = f.read()
process(data)
</code></pre>
<p>You can do this:</p>
<pre><code>while True:
    data = f.read(8192)
    if not data:
        break
    process(data)
</code></pre>
<hr/>
<p>You may want to instead write a function like this:</p>
<pre><code>def chunks(f):
    while True:
        data = f.read(8192)
        if not data:
            break
        yield data
</code></pre>
<p>Then you can just do this:</p>
<pre><code>for chunk in chunks(f):
    process(chunk)
</code></pre>
<p>You could also do this with the two-argument <code>iter</code>, but many people find that a bit obscure:</p>
<pre><code>for chunk in iter(partial(f.read, 8192), b''):
    process(chunk)
</code></pre>
<p>Either way, this option applies to all of the other variants below (except for a single <code>mmap</code>, which is trivial enough that there's no point).</p>
<hr/>
<p>There's nothing magic about the number 8192 there. You generally do want a power of 2, and ideally a multiple of your system's page size. beyond that, your performance won't vary that much whether you're using 4KB or 4MB—and if it does, you'll have to test what works best for your use case.</p>
<hr/>
<p>Anyway, this assumes you can just process each 8K at a time without keeping around any context. If you're, e.g., feeding data into a progressive decoder or hasher or something, that's perfect.</p>
<p>But if you need to process one "chunk" at a time, your chunks could end up straddling an 8K boundary. How do you deal with that?</p>
<p>It depends on how your chunks are delimited in the file, but the basic idea is pretty simple. For example, let's say you use NUL bytes as a separator (not very likely, but easy to show as a toy example).</p>
<pre><code>data = b''
while True:
    buf = f.read(8192)
    if not buf:
        process(data)
        break
    data += buf
    chunks = data.split(b'\0')
    for chunk in chunks[:-1]:
        process(chunk)
    data = chunks[-1]
</code></pre>
<p>This kind of code is very common in networking (because <code>sockets</code> <em>can't</em> just "read all", so you <em>always</em> have to read into a buffer and chunk into messages), so you may find some useful examples in networking code that uses a protocol similar to your file format.</p>
<hr/>
<p>Alternatively, you can use <a href="http://docs.python.org/3/library/mmap.html" rel="noreferrer"><code>mmap</code></a>.</p>
<p>If your virtual memory size is larger than the file, this is trivial:</p>
<pre><code>with mmap.mmap(f.fileno(), access=mmap.ACCESS_READ) as m:
    process(m)
</code></pre>
<p>Now <code>m</code> acts like a giant <code>bytes</code> object, just as if you'd called <code>read()</code> to read the whole thing into memory—but the OS will automatically page bits in and out of memory as necessary.</p>
<hr/>
<p>If you're trying to read a file too big to fit into your virtual memory size (e.g., a 4GB file with 32-bit Python, or a 20EB file with 64-bit Python—which is only likely to happen in 2013 if you're reading a sparse or virtual file like, say, the VM file for another process on linux), you have to implement windowing—mmap in a piece of the file at a time. For example:</p>
<pre><code>windowsize = 8*1024*1024
size = os.fstat(f.fileno()).st_size
for start in range(0, size, window size):
    with mmap.mmap(f.fileno(), access=mmap.ACCESS_READ, 
                   length=windowsize, offset=start) as m:
        process(m)
</code></pre>
<p>Of course mapping windows has the same issue as reading chunks if you need to delimit things, and you can solve it the same way. </p>
<p>But, as an optimization, instead of buffering, you can just slide the window forward to the page containing the end of the last complete message, instead of 8MB at a time, and then you can avoid any copying. This is a bit more complicated, so if you want to do it, search for something like "sliding mmap window", and write a new question if you get stuck.</p>
</div>
<span class="comment-copy">Possible duplicate: <a href="http://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python" title="lazy method for reading big file in python">stackoverflow.com/questions/519633/…</a></span>
<span class="comment-copy">Check out pandas and pytables / hdf or hadoop streaming with python.  If your on Linux you can use dumbo facilitate the hadoop python interaction.  Python  has a strong and vibrant community for data analysis; it's hard to miss with a Google search.</span>
<span class="comment-copy">Not a dup, but also related: <a href="http://stackoverflow.com/questions/4566498/python-file-iterator-over-a-binary-file-with-newer-idiom/4566523#4566523">Python file iterator over a binary file with newer idiom</a>.</span>
<span class="comment-copy">Also see <a href="http://stackoverflow.com/questions/1661986/why-doesnt-pythons-mmap-work-with-large-files">Why doesn't Python's mmap work with large files?</a>. It's not directly related, but it has some useful discussion on sliding mmap windows, and how <code>mmap</code> is different from <code>read</code> under the covers, and so on.</span>
<span class="comment-copy">Answer: compared to C, no. You're welcome. Please accept my comment.</span>
<span class="comment-copy">I commend you for giving such a well-thought-out answer to such a broad question. Seriously, +1.</span>
<span class="comment-copy">Thanks! In my case I would like a chunk to be the size of RAM for efficiency reasons. Can you do that without trial and error?</span>
<span class="comment-copy">@marshall: You don't really want it to be the size of (physical) RAM, because some of that RAM is needed by the rest of your interpreter space, the kernel, other processes, the disk cache, etc. Also, once you get the chunk large enough, there's not much more gain to be had; if your code is as close to fully pipelined with the disk DMAs as possible, larger reads can't help. You can (and should) test it yourself, but usually the sweet spot will be somewhere between 4KB and 8MB, not anywhere near the limit of physical memory.</span>
<span class="comment-copy">@marshall: Meanwhile, if you <i>do</i> want the physical RAM size for some reason, there's no cross-platform way to do it, but you can always read from the <code>/proc</code> filesystem for linux, <code>ctypes</code> to <code>sysctlbyname</code> for most other *nix systems, <code>win2api</code> to <code>GlobalMemoryStatusEx</code> for Windows, etc.</span>
<span class="comment-copy">Any pointers on how to decide whether 8192 bytes is a good number or not?</span>
