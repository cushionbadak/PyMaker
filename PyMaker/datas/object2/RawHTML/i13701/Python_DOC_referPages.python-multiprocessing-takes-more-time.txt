<div class="post-text" itemprop="text">
<p>I have server with 12 cores and 28GB RAM. I am running two versions of Python; one with multiprocessing and another sequential.  I expect the Multiprocessing.py to finish early compared to Sequential.py but the multiprocessing code takes 5 times more (120sec) compared to sequential code (25sec) </p>
<p>Multiprocessing.py</p>
<pre><code>import os,multiprocessing,time
def cube(x):
    print(x**3)
    return
if __name__ == '__main__':
    jobs = []
    start = time.time()
    for i in range(5000):
        p = multiprocessing.Process(target=cube(i))
        jobs.append(p)
        p.start()
    end = time.time()
    print end - start
</code></pre>
<p>Sequential.py</p>
<pre><code>import os,time
def cube(x):
    print(x**3)
    return
if __name__ == '__main__':
    start = time.time()
    for i in range(5000):
        cube(i)
    end = time.time()
    print end - start
</code></pre>
<p>Can you please help? </p>
</div>
<div class="post-text" itemprop="text">
<p>The problem is that too little work is being done relative to the IPC communication overhead. </p>
<p>The <em>cube</em> function isn't a good candidate for multiprocessing speedup.  Try something "more interesting" like function that computes the sum of cube for 1 to n or somesuch:</p>
<pre><code>import os, multiprocessing, time

def sum_of_cubes(n):
    return sum(x**3 for x in range(n))

if __name__ == '__main__':

    from multiprocessing.pool import ThreadPool as Pool

    pool = Pool(25)

    start = time.time()
    print(pool.map(sum_of_cubes, range(1000, 100000, 1000)))
    end = time.time()
    print(end - start)
</code></pre>
<p>The general rules are:</p>
<ul>
<li>don't start more pools than your cores can benefit from</li>
<li>don't pass in a lot of data or return a lot of data (too much IPC load)</li>
<li>do significant work in the process relative to the IPC overhead.</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>You shouldn't be starting a process for each multiplication. Start 12 processes and pass each one the numbers or hand out the numbers at process creation.</p>
<p>If you profile that I'm fairly certain you'll find all your time spent in process creation and clean up.</p>
<p>ALSO: I've done testing on how many processes to run vs core count and the optimum depends on architecture (e.g. some intel chips have 2x threads per core) and operating system (Linux seems to handle it better than Windows). If you're on windows I'd advise trying process counts of 0.8-2.2x core count. On Linux you can do more.</p>
</div>
<div class="post-text" itemprop="text">
<p>Would you like to try pool? For example, the following should work:</p>
<p>from multiprocessing import Pool</p>
<p>p = Pool(12)</p>
<p>Results = p.map(cube, range(5000))</p>
</div>
<span class="comment-copy">With that huge difference, it might not matter but you may want to see if the <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow noreferrer"><code>timeit</code></a> module gives similar results.  Maybe comment out the print functions while timing.  In the multiprocessing example you are timing the setup as well as the execution - is that what you intended?</span>
<span class="comment-copy">Please clean up the code in your question - indentation is off in places and you've mixed up a variable name in <code>cube(I)</code>.</span>
<span class="comment-copy">You're creating 5000 processes to do a few multiplications? That can't be faster than a loop, it's too much overhead.</span>
<span class="comment-copy">Split 5000 into 12 chunks and run each chunk on one process. That's how you should be using multiprocessing.</span>
<span class="comment-copy">Don't print if you want to test performance. Printing takes a looooooooong time.</span>
<span class="comment-copy">Okay, I have to admit, Raymond's answer is better. All that good stuff wasn't there when I contributed mine :)</span>
<span class="comment-copy">This should also not be faster.</span>
