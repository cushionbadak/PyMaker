<div class="post-text" itemprop="text">
<p>Down below you see a blue print of my crawler. I thought I could speed it up with multithreading but I can't. Often times when I load a page the webserver is slow and then it would be nice to crawl another webpage that loads faster with multithreading. But it isn't faster. Why?</p>
<pre><code>def start_it():
    while(True):
        get_urls()

def get_urls():
   response = urllib2.urlopen(url)
   page_source = str(response.read())

pool = ThreadPool(10)

pool.map(start_it())
</code></pre>
<p>Ok I tested if the threads run parallel and they are not :/ What am I doing wrong?</p>
<pre><code>def start_it():

    x = random.random()
    while(True):
        get_urls(x)

def get_urls(x):
    print(x)

pool = ThreadPool(10)

pool.map(start_it())
</code></pre>
<p>I know this because the output is always the same:</p>
<pre><code>0.1771815430790964
0.1771815430790964
0.1771815430790964
0.1771815430790964
0.1771815430790964
0.1771815430790964
0.1771815430790964
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>you need to provide <code>pool.map()</code> an iterable</p>
<p>at the moment you're running <code>start_it()</code> which basically runs all your calls one after another. I don't know what implementation of <code>ThreadPool</code> you are using but you probably need to do something like:</p>
<pre><code>pool.map(get_urls, list_of_urls)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Not to digress, but Asynchronous IO is also a good candidate for your problem. You can use an amazing library called <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow">asyncio</a> which has been recently added to python 3.4. For older versions you can use <a href="http://trollius.readthedocs.org/en/latest/asyncio.html" rel="nofollow">trollius</a> or <a href="http://twistedmatrix.com/" rel="nofollow">Twisted</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>If the code you posted actualy runs, you shouldn't do <code>pool.map(start_it())</code>, as that calls <code>start_it</code> <em>before</em> passing the result to <code>pool.map</code>. Instead you must pass <code>start_it</code> without any <code>()</code>, as in <code>pool.map(start_it)</code>. You pprobably need another argument as well (values to pass to start_it).</p>
<p>You can try the example below, which seems to work for me.</p>
<pre><code>import json
import multiprocessing.pool
import time
import urllib2

def run(no):
    for n in range(3):
        f = urllib2.urlopen("http://time.jsontest.com")
        data = json.loads(f.read())
        f.close()
        print("thread %d: %s" % (no, data))
        time.sleep(1)

pool = multiprocessing.pool.ThreadPool(3)
pool.map(run, range(3))
</code></pre>
<p>You could also use <code>multiprocess.Process</code>, e.g.:</p>
<pre><code>import multiprocessing
import time
import os

def run(jobno):
    for n in range(3):
        print("job=%d pid=%d" % (jobno, os.getpid()))
        time.sleep(1)

jobs = []
for n in range(3):
    p = multiprocessing.Process(target=run, args=[n])
    jobs.append(p)

map(lambda x: x.start(), jobs)
map(lambda x: x.join(), jobs)
</code></pre>
<p>Example output:</p>
<pre><code>job=0 pid=18347
job=1 pid=18348
job=2 pid=18349
job=0 pid=18347
job=2 pid=18349
job=1 pid=18348
job=2 pid=18349
job=0 pid=18347
job=1 pid=18348
</code></pre>
<p>Everything under the <code>multiprocessing</code> module uses <em>processes</em> instead of threads, which are truly parallel. Just note that there might be some issues with that (versus running them as threads under the same process).</p>
</div>
<div class="post-text" itemprop="text">
<p>I think this way it is running truely parallel. I experienced a significant speed up of the crawling. Awesome ;)</p>
<pre><code>import multiprocessing
import random

def worker():
    """worker function"""

    x = random.random()*10
    x = round(x)
    while(True):
        print(x , ' Worker')

if __name__ == '__main__':
    jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=worker)
        jobs.append(p)
        p.start()
</code></pre>
</div>
<span class="comment-copy">are you sure your threads are actually running concurrently? put some logging into them. Are you aware of the GIL and its implications for threaded code? miltiprocessing may work better.</span>
<span class="comment-copy">what is the difference with multiprocessing?</span>
<span class="comment-copy">please read my previous comment in full</span>
<span class="comment-copy">How can I proof that they are running concurrently?</span>
<span class="comment-copy">please read my previous comment in full - especially the bit about logging</span>
<span class="comment-copy">I don't have a list of urls, because every get_urls() process takes a entry of a url database</span>
<span class="comment-copy">well adapt what I showed you so</span>
<span class="comment-copy">Hi I edited my code and it works now. you see it below.</span>
<span class="comment-copy">Lets says its running as multiple processes, truly parallel most likely not.</span>
<span class="comment-copy">It does. From the docs: <code>Process objects represent activity that is run in a separate process</code>. You can verify by printing <code>os.getpid()</code> in each worker, and you'll see it does.</span>
<span class="comment-copy">Don't forget to <code>join</code> your processes.</span>
<span class="comment-copy">why should I join?</span>
<span class="comment-copy">Because then you know all jobs are finished.</span>
