<div class="post-text" itemprop="text">
<p>I'm writing a log file viewer for a web application and for that I want to paginate through the lines of the log file.  The items in the file are line based with the newest item on the bottom.</p>
<p>So I need a <code>tail()</code> method that can read <code>n</code> lines from the bottom and supports an offset.  What I came up with looks like this:</p>
<pre><code>def tail(f, n, offset=0):
    """Reads a n lines from f with an offset of offset lines."""
    avg_line_length = 74
    to_read = n + offset
    while 1:
        try:
            f.seek(-(avg_line_length * to_read), 2)
        except IOError:
            # woops.  apparently file is smaller than what we want
            # to step back, go to the beginning instead
            f.seek(0)
        pos = f.tell()
        lines = f.read().splitlines()
        if len(lines) &gt;= to_read or pos == 0:
            return lines[-to_read:offset and -offset or None]
        avg_line_length *= 1.3
</code></pre>
<p>Is this a reasonable approach?  What is the recommended way to tail log files with offsets?</p>
</div>
<div class="post-text" itemprop="text">
<p>This may be quicker than yours.  Makes no assumptions about line length.  Backs through the file one block at a time till it's found the right number of '\n' characters.</p>
<pre><code>def tail( f, lines=20 ):
    total_lines_wanted = lines

    BLOCK_SIZE = 1024
    f.seek(0, 2)
    block_end_byte = f.tell()
    lines_to_go = total_lines_wanted
    block_number = -1
    blocks = [] # blocks of size BLOCK_SIZE, in reverse order starting
                # from the end of the file
    while lines_to_go &gt; 0 and block_end_byte &gt; 0:
        if (block_end_byte - BLOCK_SIZE &gt; 0):
            # read the last block we haven't yet read
            f.seek(block_number*BLOCK_SIZE, 2)
            blocks.append(f.read(BLOCK_SIZE))
        else:
            # file too small, start from begining
            f.seek(0,0)
            # only read what was not read
            blocks.append(f.read(block_end_byte))
        lines_found = blocks[-1].count('\n')
        lines_to_go -= lines_found
        block_end_byte -= BLOCK_SIZE
        block_number -= 1
    all_read_text = ''.join(reversed(blocks))
    return '\n'.join(all_read_text.splitlines()[-total_lines_wanted:])
</code></pre>
<p>I don't like tricky assumptions about line length when -- as a practical matter -- you can never know things like that.</p>
<p>Generally, this will locate the last 20 lines on the first or second pass through the loop.  If your 74 character thing is actually accurate, you make the block size 2048 and you'll tail 20 lines almost immediately.</p>
<p>Also, I don't burn a lot of brain calories trying to finesse alignment with physical OS blocks.  Using these high-level I/O packages, I doubt you'll see any performance consequence of trying to align on OS block boundaries.  If you use lower-level I/O, then you might see a speedup.</p>
</div>
<div class="post-text" itemprop="text">
<p>Assumes a unix-like system on Python 2 you can do:</p>
<pre><code>import os
def tail(f, n, offset=0):
  stdin,stdout = os.popen2("tail -n "+n+offset+" "+f)
  stdin.close()
  lines = stdout.readlines(); stdout.close()
  return lines[:,-offset]
</code></pre>
<p>For python 3 you may do:</p>
<pre><code>import subprocess
def tail(f, n, offset=0):
    proc = subprocess.Popen(['tail', '-n', n + offset, f], stdout=subprocess.PIPE)
    lines = proc.stdout.readlines()
    return lines[:, -offset]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If reading the whole file is acceptable then use a deque.</p>
<pre><code>from collections import deque
deque(f, maxlen=n)
</code></pre>
<p>Prior to 2.6, deques didn't have a maxlen option, but it's easy enough to implement.</p>
<pre><code>import itertools
def maxque(items, size):
    items = iter(items)
    q = deque(itertools.islice(items, size))
    for item in items:
        del q[0]
        q.append(item)
    return q
</code></pre>
<p>If it's a requirement to read the file from the end, then use a gallop (a.k.a exponential) search.</p>
<pre><code>def tail(f, n):
    assert n &gt;= 0
    pos, lines = n+1, []
    while len(lines) &lt;= n:
        try:
            f.seek(-pos, 2)
        except IOError:
            f.seek(0)
            break
        finally:
            lines = list(f)
        pos *= 2
    return lines[-n:]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>S.Lott's answer above almost works for me but ends up giving me partial lines. It turns out that it corrupts data on block boundaries because data holds the read blocks in reversed order. When ''.join(data) is called, the blocks are in the wrong order. This fixes that.</p>
<pre><code>def tail(f, window=20):
    """
    Returns the last `window` lines of file `f` as a list.
    f - a byte file-like object
    """
    if window == 0:
        return []
    BUFSIZ = 1024
    f.seek(0, 2)
    bytes = f.tell()
    size = window + 1
    block = -1
    data = []
    while size &gt; 0 and bytes &gt; 0:
        if bytes - BUFSIZ &gt; 0:
            # Seek back one whole BUFSIZ
            f.seek(block * BUFSIZ, 2)
            # read BUFFER
            data.insert(0, f.read(BUFSIZ))
        else:
            # file too small, start from begining
            f.seek(0,0)
            # only read what was not read
            data.insert(0, f.read(bytes))
        linesFound = data[0].count('\n')
        size -= linesFound
        bytes -= BUFSIZ
        block -= 1
    return ''.join(data).splitlines()[-window:]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here is my answer. Pure python. Using timeit it seems pretty fast. Tailing 100 lines of a log file that has 100,000 lines:</p>
<pre><code>&gt;&gt;&gt; timeit.timeit('tail.tail(f, 100, 4098)', 'import tail; f = open("log.txt", "r");', number=10)
0.0014600753784179688
&gt;&gt;&gt; timeit.timeit('tail.tail(f, 100, 4098)', 'import tail; f = open("log.txt", "r");', number=100)
0.00899195671081543
&gt;&gt;&gt; timeit.timeit('tail.tail(f, 100, 4098)', 'import tail; f = open("log.txt", "r");', number=1000)
0.05842900276184082
&gt;&gt;&gt; timeit.timeit('tail.tail(f, 100, 4098)', 'import tail; f = open("log.txt", "r");', number=10000)
0.5394978523254395
&gt;&gt;&gt; timeit.timeit('tail.tail(f, 100, 4098)', 'import tail; f = open("log.txt", "r");', number=100000)
5.377126932144165
</code></pre>
<p>Here is the code:</p>
<pre><code>import os


def tail(f, lines=1, _buffer=4098):
    """Tail a file and get X lines from the end"""
    # place holder for the lines found
    lines_found = []

    # block counter will be multiplied by buffer
    # to get the block size from the end
    block_counter = -1

    # loop until we find X lines
    while len(lines_found) &lt; lines:
        try:
            f.seek(block_counter * _buffer, os.SEEK_END)
        except IOError:  # either file is too small, or too many lines requested
            f.seek(0)
            lines_found = f.readlines()
            break

        lines_found = f.readlines()

        # we found enough lines, get out
        # Removed this line because it was redundant the while will catch
        # it, I left it for history
        # if len(lines_found) &gt; lines:
        #    break

        # decrement the block counter to get the
        # next X bytes
        block_counter -= 1

    return lines_found[-lines:]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The code I ended up using.  I think this is the best so far:</p>
<pre><code>def tail(f, n, offset=None):
    """Reads a n lines from f with an offset of offset lines.  The return
    value is a tuple in the form ``(lines, has_more)`` where `has_more` is
    an indicator that is `True` if there are more lines in the file.
    """
    avg_line_length = 74
    to_read = n + (offset or 0)

    while 1:
        try:
            f.seek(-(avg_line_length * to_read), 2)
        except IOError:
            # woops.  apparently file is smaller than what we want
            # to step back, go to the beginning instead
            f.seek(0)
        pos = f.tell()
        lines = f.read().splitlines()
        if len(lines) &gt;= to_read or pos == 0:
            return lines[-to_read:offset and -offset or None], \
                   len(lines) &gt; to_read or pos &gt; 0
        avg_line_length *= 1.3
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Simple and fast solution with mmap:</p>
<pre><code>import mmap
import os

def tail(filename, n):
    """Returns last n lines from the filename. No exception handling"""
    size = os.path.getsize(filename)
    with open(filename, "rb") as f:
        # for Windows the mmap parameters are different
        fm = mmap.mmap(f.fileno(), 0, mmap.MAP_SHARED, mmap.PROT_READ)
        try:
            for i in xrange(size - 1, -1, -1):
                if fm[i] == '\n':
                    n -= 1
                    if n == -1:
                        break
            return fm[i + 1 if i else 0:].splitlines()
        finally:
            fm.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>An even cleaner python3 compatible version that doesn't insert but appends &amp; reverses:</p>
<pre><code>def tail(f, window=1):
    """
    Returns the last `window` lines of file `f` as a list of bytes.
    """
    if window == 0:
        return b''
    BUFSIZE = 1024
    f.seek(0, 2)
    end = f.tell()
    nlines = window + 1
    data = []
    while nlines &gt; 0 and end &gt; 0:
        i = max(0, end - BUFSIZE)
        nread = min(end, BUFSIZE)

        f.seek(i)
        chunk = f.read(nread)
        data.append(chunk)
        nlines -= chunk.count(b'\n')
        end -= nread
    return b'\n'.join(b''.join(reversed(data)).splitlines()[-window:])
</code></pre>
<p>use it like this:</p>
<pre><code>with open(path, 'rb') as f:
    last_lines = tail(f, 3).decode('utf-8')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I found the Popen above to be the best solution. It's quick and dirty and it works
For python 2.6 on Unix machine i used the following</p>
<pre><code>    def GetLastNLines(self, n, fileName):
    """
    Name:           Get LastNLines
    Description:        Gets last n lines using Unix tail
    Output:         returns last n lines of a file
    Keyword argument:
    n -- number of last lines to return
    filename -- Name of the file you need to tail into
    """
    p=subprocess.Popen(['tail','-n',str(n),self.__fileName], stdout=subprocess.PIPE)
    soutput,sinput=p.communicate()
    return soutput
</code></pre>
<p>soutput will have will contain last n lines of the code. to iterate through soutput line by line do:</p>
<pre><code>for line in GetLastNLines(50,'myfile.log').split('\n'):
    print line
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Update @papercrane solution to python3.
Open the file with <code>open(filename, 'rb')</code> and:</p>
<pre><code>def tail(f, window=20):
    """Returns the last `window` lines of file `f` as a list.
    """
    if window == 0:
        return []

    BUFSIZ = 1024
    f.seek(0, 2)
    remaining_bytes = f.tell()
    size = window + 1
    block = -1
    data = []

    while size &gt; 0 and remaining_bytes &gt; 0:
        if remaining_bytes - BUFSIZ &gt; 0:
            # Seek back one whole BUFSIZ
            f.seek(block * BUFSIZ, 2)
            # read BUFFER
            bunch = f.read(BUFSIZ)
        else:
            # file too small, start from beginning
            f.seek(0, 0)
            # only read what was not read
            bunch = f.read(remaining_bytes)

        bunch = bunch.decode('utf-8')
        data.insert(0, bunch)
        size -= bunch.count('\n')
        remaining_bytes -= BUFSIZ
        block -= 1

    return ''.join(data).splitlines()[-window:]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Posting an answer at the behest of commenters on <a href="/a/33811809/364696">my answer to a similar question</a> where the same technique was used to mutate the last line of a file, not just get it.</p>
<p>For a file of significant size, <a href="https://docs.python.org/3/library/mmap.html" rel="nofollow noreferrer"><code>mmap</code></a> is the best way to do this. To improve on the existing <code>mmap</code> answer, this version is portable between Windows and Linux, and should run faster (though it won't work without some modifications on 32 bit Python with files in the GB range, see the <a href="/a/33811809/364696">other answer for hints on handling this, and for modifying to work on Python 2</a>).</p>
<pre><code>import io  # Gets consistent version of open for both Py2.7 and Py3.x
import itertools
import mmap

def skip_back_lines(mm, numlines, startidx):
    '''Factored out to simplify handling of n and offset'''
    for _ in itertools.repeat(None, numlines):
        startidx = mm.rfind(b'\n', 0, startidx)
        if startidx &lt; 0:
            break
    return startidx

def tail(f, n, offset=0):
    # Reopen file in binary mode
    with io.open(f.name, 'rb') as binf, mmap.mmap(binf.fileno(), 0, access=mmap.ACCESS_READ) as mm:
        # len(mm) - 1 handles files ending w/newline by getting the prior line
        startofline = skip_back_lines(mm, offset, len(mm) - 1)
        if startofline &lt; 0:
            return []  # Offset lines consumed whole file, nothing to return
            # If using a generator function (yield-ing, see below),
            # this should be a plain return, no empty list

        endoflines = startofline + 1  # Slice end to omit offset lines

        # Find start of lines to capture (add 1 to move from newline to beginning of following line)
        startofline = skip_back_lines(mm, n, startofline) + 1

        # Passing True to splitlines makes it return the list of lines without
        # removing the trailing newline (if any), so list mimics f.readlines()
        return mm[startofline:endoflines].splitlines(True)
        # If Windows style \r\n newlines need to be normalized to \n, and input
        # is ASCII compatible, can normalize newlines with:
        # return mm[startofline:endoflines].replace(os.linesep.encode('ascii'), b'\n').splitlines(True)
</code></pre>
<p>This assumes the number of lines tailed is small enough you can safely read them all into memory at once; you could also make this a generator function and manually read a line at a time by replacing the final line with:</p>
<pre><code>        mm.seek(startofline)
        # Call mm.readline n times, or until EOF, whichever comes first
        # Python 3.2 and earlier:
        for line in itertools.islice(iter(mm.readline, b''), n):
            yield line

        # 3.3+:
        yield from itertools.islice(iter(mm.readline, b''), n)
</code></pre>
<p>Lastly, this read in binary mode (necessary to use <code>mmap</code>) so it gives <code>str</code> lines (Py2) and <code>bytes</code> lines (Py3); if you want <code>unicode</code> (Py2) or <code>str</code> (Py3), the iterative approach could be tweaked to decode for you and/or fix newlines:</p>
<pre><code>        lines = itertools.islice(iter(mm.readline, b''), n)
        if f.encoding:  # Decode if the passed file was opened with a specific encoding
            lines = (line.decode(f.encoding) for line in lines)
        if 'b' not in f.mode:  # Fix line breaks if passed file opened in text mode
            lines = (line.replace(os.linesep, '\n') for line in lines)
        # Python 3.2 and earlier:
        for line in lines:
            yield line
        # 3.3+:
        yield from lines
</code></pre>
<p>Note: I typed this all up on a machine where I lack access to Python to test. Please let me know if I typoed anything; this was similar enough to <a href="/a/33811809/364696">my other answer</a> that I <em>think</em> it should work, but the tweaks (e.g. handling an <code>offset</code>) could lead to subtle errors. Please let me know in the comments if there are any mistakes.</p>
</div>
<div class="post-text" itemprop="text">
<p>based on S.Lott's top voted answer (Sep 25 '08 at 21:43), but fixed for small files.</p>
<pre><code>def tail(the_file, lines_2find=20):  
    the_file.seek(0, 2)                         #go to end of file
    bytes_in_file = the_file.tell()             
    lines_found, total_bytes_scanned = 0, 0
    while lines_2find+1 &gt; lines_found and bytes_in_file &gt; total_bytes_scanned: 
        byte_block = min(1024, bytes_in_file-total_bytes_scanned)
        the_file.seek(-(byte_block+total_bytes_scanned), 2)
        total_bytes_scanned += byte_block
        lines_found += the_file.read(1024).count('\n')
    the_file.seek(-total_bytes_scanned, 2)
    line_list = list(the_file.readlines())
    return line_list[-lines_2find:]

    #we read at least 21 line breaks from the bottom, block by block for speed
    #21 to ensure we don't get a half line
</code></pre>
<p>Hope this is useful.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are some existing implementations of tail on pypi which you can install using pip: </p>
<ul>
<li>mtFileUtil</li>
<li>multitail</li>
<li>log4tailer</li>
<li>...</li>
</ul>
<p>Depending on your situation, there may be advantages to using one of these existing tools.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here is a pretty simple implementation:</p>
<pre><code>with open('/etc/passwd', 'r') as f:
  try:
    f.seek(0,2)
    s = ''
    while s.count('\n') &lt; 11:
      cur = f.tell()
      f.seek((cur - 10))
      s = f.read(10) + s
      f.seek((cur - 10))
    print s
  except Exception as e:
    f.readlines()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>For efficiency with very large files (common in logfile situations where you may want to use tail), you generally want to avoid reading the whole file (even if you do do it without reading the whole file into memory at once)  However, you do need to somehow work out the offset in lines rather than characters.  One possibility is reading backwards with seek() char by char, but this is very slow.  Instead, its better to process in larger blocks.</p>
<p>I've a utility function I wrote a while ago to read files backwards that can be used here.</p>
<pre><code>import os, itertools

def rblocks(f, blocksize=4096):
    """Read file as series of blocks from end of file to start.

    The data itself is in normal order, only the order of the blocks is reversed.
    ie. "hello world" -&gt; ["ld","wor", "lo ", "hel"]
    Note that the file must be opened in binary mode.
    """
    if 'b' not in f.mode.lower():
        raise Exception("File must be opened using binary mode.")
    size = os.stat(f.name).st_size
    fullblocks, lastblock = divmod(size, blocksize)

    # The first(end of file) block will be short, since this leaves 
    # the rest aligned on a blocksize boundary.  This may be more 
    # efficient than having the last (first in file) block be short
    f.seek(-lastblock,2)
    yield f.read(lastblock)

    for i in range(fullblocks-1,-1, -1):
        f.seek(i * blocksize)
        yield f.read(blocksize)

def tail(f, nlines):
    buf = ''
    result = []
    for block in rblocks(f):
        buf = block + buf
        lines = buf.splitlines()

        # Return all lines except the first (since may be partial)
        if lines:
            result.extend(lines[1:]) # First line may not be complete
            if(len(result) &gt;= nlines):
                return result[-nlines:]

            buf = lines[0]

    return ([buf]+result)[-nlines:]


f=open('file_to_tail.txt','rb')
for line in tail(f, 20):
    print line
</code></pre>
<p>[Edit] Added more specific version (avoids need to reverse twice)</p>
</div>
<div class="post-text" itemprop="text">
<p>you can go to the end of your file with f.seek(0, 2) and then read off lines one by one with the following replacement for readline():</p>
<pre><code>def readline_backwards(self, f):
    backline = ''
    last = ''
    while not last == '\n':
        backline = last + backline
        if f.tell() &lt;= 0:
            return backline
        f.seek(-1, 1)
        last = f.read(1)
        f.seek(-1, 1)
    backline = last
    last = ''
    while not last == '\n':
        backline = last + backline
        if f.tell() &lt;= 0:
            return backline
        f.seek(-1, 1)
        last = f.read(1)
        f.seek(-1, 1)
    f.seek(1, 1)
    return backline
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Based on Eyecue answer (Jun 10 '10 at 21:28): this class add head() and tail() method to file object.</p>
<pre><code>class File(file):
    def head(self, lines_2find=1):
        self.seek(0)                            #Rewind file
        return [self.next() for x in xrange(lines_2find)]

    def tail(self, lines_2find=1):  
        self.seek(0, 2)                         #go to end of file
        bytes_in_file = self.tell()             
        lines_found, total_bytes_scanned = 0, 0
        while (lines_2find+1 &gt; lines_found and
               bytes_in_file &gt; total_bytes_scanned): 
            byte_block = min(1024, bytes_in_file-total_bytes_scanned)
            self.seek(-(byte_block+total_bytes_scanned), 2)
            total_bytes_scanned += byte_block
            lines_found += self.read(1024).count('\n')
        self.seek(-total_bytes_scanned, 2)
        line_list = list(self.readlines())
        return line_list[-lines_2find:]
</code></pre>
<p>Usage:</p>
<pre><code>f = File('path/to/file', 'r')
f.head(3)
f.tail(3)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Several of these solutions have issues if the file doesn't end in \n or in ensuring the complete first line is read.</p>
<pre><code>def tail(file, n=1, bs=1024):
    f = open(file)
    f.seek(-1,2)
    l = 1-f.read(1).count('\n') # If file doesn't end in \n, count it anyway.
    B = f.tell()
    while n &gt;= l and B &gt; 0:
            block = min(bs, B)
            B -= block
            f.seek(B, 0)
            l += f.read(block).count('\n')
    f.seek(B, 0)
    l = min(l,n) # discard first (incomplete) line if l &gt; n
    lines = f.readlines()[-l:]
    f.close()
    return lines
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I had to read a specific value from the last line of a file, and stumbled upon this thread. Rather than reinventing the wheel in Python, I ended up with a tiny shell script, saved as
/usr/local/bin/get_last_netp:</p>
<pre><code>#! /bin/bash
tail -n1 /home/leif/projects/transfer/export.log | awk {'print $14'}
</code></pre>
<p>And in the Python program:</p>
<pre><code>from subprocess import check_output

last_netp = int(check_output("/usr/local/bin/get_last_netp"))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Not the first example using a deque, but a simpler one.  This one is general:  it works on any iterable object, not just a file.</p>
<pre><code>#!/usr/bin/env python
import sys
import collections
def tail(iterable, N):
    deq = collections.deque()
    for thing in iterable:
        if len(deq) &gt;= N:
            deq.popleft()
        deq.append(thing)
    for thing in deq:
        yield thing
if __name__ == '__main__':
    for line in tail(sys.stdin,10):
        sys.stdout.write(line)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>This is my version of tailf

import sys, time, os

filename = 'path to file'

try:
    with open(filename) as f:
        size = os.path.getsize(filename)
        if size &lt; 1024:
            s = size
        else:
            s = 999
        f.seek(-s, 2)
        l = f.read()
        print l
        while True:
            line = f.readline()
            if not line:
                time.sleep(1)
                continue
            print line
except IOError:
    pass
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>import time

attemps = 600
wait_sec = 5
fname = "YOUR_PATH"

with open(fname, "r") as f:
    where = f.tell()
    for i in range(attemps):
        line = f.readline()
        if not line:
            time.sleep(wait_sec)
            f.seek(where)
        else:
            print line, # already has newline
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>import itertools
fname = 'log.txt'
offset = 5
n = 10
with open(fname) as f:
    n_last_lines = list(reversed([x for x in itertools.islice(f, None)][-(offset+1):-(offset+n+1):-1]))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>abc = "2018-06-16 04:45:18.68"
filename = "abc.txt"
with open(filename) as myFile:
    for num, line in enumerate(myFile, 1):
        if abc in line:
            lastline = num
print "last occurance of work at file is in "+str(lastline) 
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>There is very useful <a href="https://pypi.org/project/file-read-backwards/" rel="nofollow noreferrer">module</a> that can do this:</p>
<pre><code>from file_read_backwards import FileReadBackwards

with FileReadBackwards("/tmp/file", encoding="utf-8") as frb:

# getting lines by lines starting from the last line up
for l in frb:
    print(l)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>On second thought, this is probably just as fast as anything here.</p>
<pre><code>def tail( f, window=20 ):
    lines= ['']*window
    count= 0
    for l in f:
        lines[count%window]= l
        count += 1
    print lines[count%window:], lines[:count%window]
</code></pre>
<p>It's a lot simpler.  And it does seem to rip along at a good pace.  </p>
</div>
<div class="post-text" itemprop="text">
<p>I found a probably the easiest way to find the first or last N lines of a file</p>
<p><strong>Last N lines of a file(For Ex:N=10)</strong></p>
<pre><code>file=open("xyz.txt",'r")
liner=file.readlines()
for ran in range((len(liner)-N),len(liner)):
    print liner[ran]
</code></pre>
<p><strong>First N lines of a file(For Ex:N=10)</strong></p>
<pre><code>file=open("xyz.txt",'r")
liner=file.readlines()
for ran in range(0,N+1):
    print liner[ran]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>it's so simple:</p>
<pre><code>def tail(fname,nl):
with open(fname) as f:
    data=f.readlines() #readlines return a list
    print(''.join(data[-nl:]))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Although this isn't really on the efficient side with big files, this code is pretty straight-forward:<br/></p>
<ol>
<li>It reads the file object, <code>f</code>.</li>
<li>It splits the string returned using newlines, <code>\n</code>.</li>
<li><p>It gets the array lists last indexes, using the negative sign to stand for the last indexes, and the <code>:</code> to get a subarray.<br/><br/></p>
<pre><code>def tail(f,n):
    return "\n".join(f.read().split("\n")[-n:])
</code></pre></li>
</ol>
</div>
<span class="comment-copy">On my system (linux SLES 10), seeking relative to the end raises an IOError "can't do nonzero end-relative seeks". I like this solution but have modified it to get the file length (<code>seek(0,2)</code> then <code>tell()</code>), and use that value to seek relative to the beginning.</span>
<span class="comment-copy">Congrats - this question made it into the Kippo source code</span>
<span class="comment-copy">This fails on small logfiles -- IOError: invalid argument -- f.seek( block*1024, 2 )</span>
<span class="comment-copy">Very nice approach indeed. I used a slightly modified version of the code above and came up with this recipe: <a href="http://code.activestate.com/recipes/577968-log-watcher-tail-f-log/" rel="nofollow noreferrer">code.activestate.com/recipes/577968-log-watcher-tail-f-log</a></span>
<span class="comment-copy">No longer works in python 3.2. I'm getting <code>io.UnsupportedOperation: can't do nonzero end-relative seeks</code> I can change the offset to 0, but that defeats the purpose of the function.</span>
<span class="comment-copy">@DavidEnglund Reason is <a href="http://www.velocityreviews.com/forums/t748976-python-3-2-bug-reading-the-last-line-of-a-file.html" rel="nofollow noreferrer">here</a>. In brief: seeking relative to the end of the file is not allowed in text mode, presumably because the file contents have to be decoded, and, in general, seeking to an arbitrary position within a sequence of encoded bytes can have undefined results when you attempt to decode to Unicode starting from that position. The suggestion offered at the link is to try opening the file in binary mode and do the decoding yourself, catching the DecodeError exceptions.</span>
<span class="comment-copy">DON'T USE THIS CODE. It corrupts lines in some border cases in python 2.7. The answer from @papercrane below fixes it.</span>
<span class="comment-copy">Should be platform independent.  Besides, if you read the question you will see that f is a file like object.</span>
<span class="comment-copy">the question doesn't say platform dependence is unacceptable. i fail to see why this deserves two downvotes when it provides a very unixy (may be what you're looking for... certainly was for me) way of doing exactly what the question asks.</span>
<span class="comment-copy">Thanks, I was thinking I had to solve this in pure Python but there's no reason not to use UNIX utilities when they are at hand, so I went with this. FWIW in modern Python, subprocess.check_output is likely preferable to os.popen2; it simplifies things a bit as it just returns the output as a string, and raises on a non-zero exit code.</span>
<span class="comment-copy">Although this is platform dependent, it is a <i>very</i> efficient way of doing what has been asked, as well as being an extremely fast way of doing it (You don't have to load the entire file into memory). @Shabbyrobe</span>
<span class="comment-copy">You might want to precalculate the offset like :<code>offset_total = str(n+offset)</code> and replace this line <code>stdin,stdout = os.popen2("tail -n "+offset_total+" "+f)</code> to avoid <code>TypeErrors (cannot concatenate int+str)</code></span>
<span class="comment-copy">Why does that bottom function work? <code>pos *= 2</code> seems completely arbitrary. What is its significance?</span>
<span class="comment-copy">@2mac <a href="https://en.wikipedia.org/wiki/Exponential_search" rel="nofollow noreferrer">Exponential Search</a>.  It reads from the end of file iteratively, doubling the amount read each time, until enough lines are found.</span>
<span class="comment-copy">I think that the solution to read from the end will not support files encoded with UTF-8, since the character length is variable, and you could (likely will) land at some odd offset that cannot be interpreted correctly.</span>
<span class="comment-copy">Inserting at the beginning of the list is a bad idea. Why not use deque structure?</span>
<span class="comment-copy">Elegant solution! Is the  <code>if len(lines_found) &gt; lines:</code> really necessary? Wouldn't the <code>loop</code> condition catch it as well?</span>
<span class="comment-copy">A question for my understanding: is <code>os.SEEK_END</code> used simply for clarity? As far as I have found, its value is constant (= 2). I was wondering about leaving it out to be able to leave out the <code>import os</code>. Thanks for the great solution!</span>
<span class="comment-copy">@MaximilianPeters yes. It's not necessary. I commented it out.</span>
<span class="comment-copy">@DexterMorgan you can replace <code>os.SEEK_END</code> with its integer equivalent. It was mainly there for readability.</span>
<span class="comment-copy">I upvoted, but have a small nit.  After the seek, the first line read may be incomplete, so to get N _complete_lines I changed the <code>while len(lines_found) &lt; lines</code> to <code>while len(lines_found) &lt;= lines</code> in my copy.  Thanks!</span>
<span class="comment-copy">does not exactly answer the question.</span>
<span class="comment-copy">Good starting point, thanks</span>
<span class="comment-copy">This is probably the fastest answer when the input could be huge (or it would be, if it used the <code>.rfind</code> method to scan backwards for newlines, rather than performing byte at a time checks at the Python level; in CPython, replacing Python level code with C built-in calls usually wins by a lot). For smaller inputs, the <code>deque</code> with a <code>maxlen</code> is simpler and probably similarly fast.</span>
<span class="comment-copy">Not too shabby – but I would in general advise not to add an answer to a 10-year old question with plenty of answers. But help me out: what is specific to Python 3 in your code?</span>
<span class="comment-copy">The other answers were not exactly working out well :-) py3: see <a href="https://stackoverflow.com/questions/136168/get-last-n-lines-of-a-file-with-python-similar-to-tail/48087596#comment16595577_136368" title="get last n lines of a file with python similar to tail">stackoverflow.com/questions/136168/…</a></span>
<span class="comment-copy">Are you aware of any module that works on Windows? I tried <code>tailhead</code>, <code>tailer</code> but they didn't work. Also tried <code>mtFileUtil</code>. It was initially throwing error because <code>print</code> statements were not having parenthesis (I am on Python 3.6). I added those in <code>reverse.py</code> and the error messages were gone but when my script calls the module (<code>mtFileUtil.tail(open(logfile_path), 5)</code>), it doesn't print anything.</span>
<span class="comment-copy">Great example! Could you please explain the use of try before the <code>f.seek</code>? Why not before the <code>with open</code>? Also, why in the <code>except</code> you do a <code>f.readlines()</code>??</span>
<span class="comment-copy">Honestly, the try should probably go first..  I don't remember having a reason for not catching the open() other than on a healthy standard Linux system, /etc/passwd should always be readable.  try, then with is the more common order.</span>
<span class="comment-copy">A quick tests shows that this performs a lot worse than my version from above.  Probably because of your buffering.</span>
<span class="comment-copy">I suspect it's because I'm doing multiple seeks backwards, so aren't getting as good use of the readahead buffer.  However, I think it may do better when your guess at the line length isn't accurate (eg. very large lines), as it avoids having to re-read data in this case.</span>
<span class="comment-copy">Because nearly everything here doesn't work with log files with more than 30 MB or so without loading the same amount of memory into the RAM ;)  Your first version is a lot better, but for the test files here it performs slightly worse than mine and it doesn't work with different newline characters.</span>
<span class="comment-copy">I was wrong.  Version 1 took 0.00248908996582 for 10 tails through the dictionary.  Version 2 took 1.2963051796 for 10 tails through the dictionary.   I'd almost vote myself down.</span>
<span class="comment-copy">"doesn't work with different newline characters."  Replace datacount('\n') with len(data.splitlines()) if it matters.</span>
<span class="comment-copy">Is there some point to get full log file data in memory?</span>
<span class="comment-copy">The person who downvoted my answer, could you please explain why?</span>
<span class="comment-copy">the very first moment you use <code>f.read()</code> and not seek on the file handler, you are putting ALL of your file on memory. Buffering whole file (and not seeking) is WRONG, so your anwers doesn't really add anything new to the problem, just another way to fullfit your memory. Now, try to use your code with a 10gb file then look what happens. Using itertools is another way to try out, but both <b>seek</b> and <b>tail</b> will do the trick. You don't need to put all your lines into memory to process them, you can put them in chunks. I hope you understand.</span>
<span class="comment-copy">This function wasn't meant to be a finalized function. It was merely a worst case scenario.</span>
