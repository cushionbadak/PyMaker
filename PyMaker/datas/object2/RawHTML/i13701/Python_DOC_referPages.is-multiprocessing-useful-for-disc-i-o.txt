<div class="post-text" itemprop="text">
<p>Imagine a simple <a href="https://docs.python.org/3/library/os.html#os.walk" rel="nofollow noreferrer"><code>walk</code></a> done on a storage device (HDD, SSD, DVD etc). The task is to index all filenames in to an index-file. We are doing a top-down walk and this, I believe, can be parallelized to improve performance. <a href="https://stackoverflow.com/questions/5858198/is-it-useful-to-use-multithreading-to-handle-files-on-a-hard-drive">This thread discusses the same issue</a>, but with multithreading rather than multiprocessing.</p>
<p>However, the trouble is finding how to start such a process. Since directories have a tree structure, we can't predict which branch will be more lengthy than others.</p>
<p>Take a look at this directory tree:</p>
<p><a href="https://i.stack.imgur.com/AP4qH.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/AP4qH.png"/></a> </p>
<p>If I start from the <code>project</code> folder and make two processes to walk through each subfolder, one of the process will stop after walking through the <code>sec1</code> folder. It is just an empty folder, while the other one is heavily branched out. This is not at all beneficial. Are there any ways to overcome such issues with disc I/O multiprocessing. Could you illustrate it with an testable example code?</p>
</div>
<div class="post-text" itemprop="text">
<p>Since you do not know the balance of the tree, there is no reasonable way for you to split the task between the processors.</p>
<p>The more important point is that traversing I/O is not a CPU-bound task, it is I/O bound.  Therefore, adding more CPU horsepower is not going to have that much of an effect on the end result.</p>
<p>Imagine you have the most power super-giga-peta-hertz 16-CPU computer at your disposal, and further <code>walk</code> has been enhanced for multi-processors.</p>
<p>Now you attach a 5400 RPM 1TB hard drive over USB to this device, and do a walk.</p>
<p>It is obvious that walk can only walk as fast as the disk can spin, because that's how fast the filesystem (and underlying subsystems) can read the partition table to figure out the directory structure.</p>
<p>However, if you were doing a CPU-bound task <em>on each file</em> (like image processing for example), then this portion of your program would benefit from increased CPU performance but it would be waiting to launch till the file system can provide it something to work on - and now you are back to the slow IO issue.</p>
</div>
<span class="comment-copy">(I assume you mean useful for speed, not just concurrency. And in the context of Python, I'm talking about multiprocessing, not multithreading, since multithreading in Python won't take advantage of multiple cores.) For <i>disk I/O</i>? No way. For <i>any I/O</i>? No, besides <i>maybe</i> for <i>very</i> high performance tasks (in which case, you probably shouldn't be using Python). If what you're doing with the I/O is computationally intensive, than sure, but that's not I/O, that's computation. I/O is going to be the bottleneck by far.</span>
<span class="comment-copy">@Cyphase I was somewhat motivated by that other post (linked above). It says that at a hardware level, there is this sorting algorithm (elevator algorithm) that optimizes seek times. Could you please explain why it isn't so? An answer elaborating your comment would be great!</span>
<span class="comment-copy">Is there a specific bottleneck you're running into?  Are you just curious about distributing workloads using multiprocessing?</span>
<span class="comment-copy">@Jason Yes there's a bottleneck! I have an external HDD filled with data <code>(5TB)</code>, and the process takes almost half an hour. I have tested the above case using 4 core multi process, and 1 core serial process. The multiprocessing gave me a result of <code>3.4555622 min</code> while single-process-1-core ended up with <code>5.3389001 min</code>. I didn't switch the machine, I just turned off the three cores.  The test was done on a <code>1TB</code> HDD with <code>60.34GB</code> data. I then tried it with 2 cores, and the result was <code>4.7889236 min</code> somewhere in between last two.</span>
<span class="comment-copy">@Jason So I think there might be a relation between these two variables, but not sure what controls them. The test case has different folder structure than the picture above. It is somewhat evenly branched.</span>
