<div class="post-text" itemprop="text">
<p>I currently have a bash script, script.sh, with two nested loops. The first enumerates possible values for a, and the second enumerates possible values for b, like</p>
<pre><code>#!/bin/sh
for a in {1..10}
do
    for b in {1..10}
    do
        nohup python script.py $a $b &amp;
    done
done
</code></pre>
<p>So this spawns off 100 Python processes running script.py, one for each (a,b) pair. However, my machine only has 5 cores, so I want to cap the number of processes at 5 to avoid thrashing/wasteful switching. The goal is that I am always running 5 processes until all 100 processes are done.</p>
<p>xargs seems to be one way to do this, but I don't know how to pass these arguments to xargs. I've checked other similar questions but don't understand the surrounding bash jargon well enough to know what's happening. For example, I tried</p>
<pre><code>seq 1 | xargs -i --max-procs=5 bash script.sh
</code></pre>
<p>but this doesn't seem to do anything - script.sh runs as before and still spawns off 100 processes.</p>
<p>I assume I'm misunderstanding how xargs works.</p>
<p>Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>This would actually look more like:</p>
<pre><code>#!/bin/bash
for a in {1..10}; do
  for b in {1..10}; do
    printf '%s\0' "$a" "$b"
  done
done | xargs -0 -x -n 2 -P 5 python script.py
</code></pre>
<p>Note that there's no <code>nohup</code>, nor any <code>&amp;</code> -- to track the number of concurrent invocations, <code>xargs</code> needs to be directly executing the Python script, and that process can't exit until it's complete.</p>
<p>The non-standard (but widely available) <code>-0</code> extension requires input to be in NUL-delimited form (as created with <code>printf '%s\0'</code>); this ensures correct behavior with arguments having spaces, quotes, backslashes, etc.</p>
<p>The likewise non-standard <code>-P 5</code> sets the maximum number of processes (in a way slightly more portable than <code>--max-procs=5</code>, which is supported on GNU but not modern BSD xargs).</p>
<p>The <code>-n 2</code> indicates that each instance of the Python script receives only two arguments, thus starting one per pair of inputs.</p>
<p>The <code>-x</code> (used in conjunction with <code>-n 2</code>) indicates that if a single Python instance can't be given two arguments (for instance, if the arguments are so long that both can't fit on a single command line), this should be treated as a failure, rather than invoking a Python instance with only one argument.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you use bash, then the following should work:</p>
<pre><code>#!/bin/bash

for a in {1..10}
do
    for b in {1..10}
    do
        if [ `jobs | wc -l` -lt 6 ]; then # less than 6 background jobs
            nohup python script.py $a $b &amp;
        else
            wait -n   # wait for any background job to terminate
        fi
    done
done
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>GNU Parallel is made for exactly these kinds of jobs:</p>
<pre><code>parallel python script.py ::: {1..10} ::: {1..10}
</code></pre>
<p>If you need $a and $b placed differently you can use {1} and {2} to refer to the two input sources:</p>
<pre><code>parallel python script.py --option-a {1} --option-b {2} ::: {1..10} ::: {1..10}
</code></pre>
<p>GNU Parallel is a general parallelizer and makes is easy to run jobs in parallel on the same machine or on multiple machines you have ssh access to. It can often replace a <code>for</code> loop.</p>
<p>If you have 32 different jobs you want to run on 4 CPUs, a straight forward way to parallelize is to run 8 jobs on each CPU:</p>
<p><img alt="Simple scheduling" src="https://i.stack.imgur.com/uH0Dh.png"/></p>
<p>GNU Parallel instead spawns a new process when one finishes - keeping the CPUs active and thus saving time:</p>
<p><img alt="GNU Parallel scheduling" src="https://i.stack.imgur.com/17FsG.png"/></p>
<p><strong>Installation</strong></p>
<p>If GNU Parallel is not packaged for your distribution, you can do a personal installation, which does not require root access. It can be done in 10 seconds by doing this:</p>
<pre><code>(wget -O - pi.dk/3 || curl pi.dk/3/ || fetch -o - http://pi.dk/3) | bash
</code></pre>
<p>For other installation options see <a href="http://git.savannah.gnu.org/cgit/parallel.git/tree/README" rel="nofollow noreferrer">http://git.savannah.gnu.org/cgit/parallel.git/tree/README</a></p>
<p><strong>Learn more</strong></p>
<p>See more examples: <a href="http://www.gnu.org/software/parallel/man.html" rel="nofollow noreferrer">http://www.gnu.org/software/parallel/man.html</a></p>
<p>Watch the intro videos: <a href="https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1" rel="nofollow noreferrer">https://www.youtube.com/playlist?list=PL284C9FF2488BC6D1</a></p>
<p>Walk through the tutorial: <a href="http://www.gnu.org/software/parallel/parallel_tutorial.html" rel="nofollow noreferrer">http://www.gnu.org/software/parallel/parallel_tutorial.html</a></p>
<p>Sign up for the email list to get support: <a href="https://lists.gnu.org/mailman/listinfo/parallel" rel="nofollow noreferrer">https://lists.gnu.org/mailman/listinfo/parallel</a></p>
</div>
<span class="comment-copy"><code>xargs</code> can't, and doesn't, change how its subprocesses <i>start their own children</i>; rather, it provides control over how it starts its subprocesses.</span>
<span class="comment-copy">In the case of Python I'd actually suggest to use the Python <code>multiprocessing</code> module instead of <code>xargs</code>. Or alternatively the GNU <code>parallels</code> utility. <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">docs.python.org/3/library/multiprocessing.html</a></span>
<span class="comment-copy">@Wolph, I definitely agree re: multiprocessing. As for GNU parallel -- it's a larger, more complicated and hairier codebase than <code>xargs</code>, which is a model of simplicity in comparison. Given a Pythonist's preference for simplicity, I'd consider xargs the obvious choice among the two. :)</span>
<span class="comment-copy">@CharlesDuffy that's fair. But at the same time it enables much more control over how the parallel processes are being executed and it's more portable than GNU specific xargs extenstions :)</span>
<span class="comment-copy">@Wolph, not GNU-specific; <code>--max-procs</code> is a GNUism, but <code>-P</code> works on OS X and modern FreeBSD as well.</span>
<span class="comment-copy">Fails in Cygwin with: -bash: fi: line 4: syntax error near unexpected token <code>for' -bash: fi: line 4: </code>  for b in {1..10}'</span>
<span class="comment-copy">@OleTange, I was missing some <code>; do</code>s -- that's fixed now.</span>
<span class="comment-copy">Take out the <code>nohup</code> if you want <code>wait</code> or <code>jobs</code> to work.</span>
<span class="comment-copy">@CharlesDuffy It works with <code>nohup</code> as well. Tested with <code>nohup sleep 5&amp;</code> + <code>wait</code>.</span>
<span class="comment-copy">Granted -- it's the shell-builtin cousin <code>disown</code> that would have that effect.</span>
<span class="comment-copy">...isn't <code>wait -n</code> a fairly recent addition to the shell, by the way? I want to say it's somewhere post-4.0.</span>
