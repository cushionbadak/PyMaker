<div class="post-text" itemprop="text">
<p>I have a script that uses a large chunk of text to train a model. The way it's written now I can either read from a file or stdin</p>
<pre><code>parser.add_argument('-i', help='input_file', default=sys.stdin)
... # do a bunch of other stuff
if args.i is sys.stdin:
    m.train(args.i)
else:
    m.train(open(args.i, 'r'))
</code></pre>
<p>then I can call my script as: </p>
<pre><code>python myscript.py -i trainingdata.txt
</code></pre>
<p>or </p>
<pre><code>cat trainingdata.txt | python myscript.py
</code></pre>
<p>The second version is especially useful if I want to search the filesystem, and use multiple files to train the model. However this becomes tricky, due to the pipe, if I simultaneously try to profile using <code>cProfiler</code> i.e. </p>
<pre><code>python -m cProfile myscript.py ... 
</code></pre>
<p>I know that I can send it multiple files using the <code>-i</code> option, and iterate over the files, but then I will have to change the behaviour of the <code>train()</code> method to avoid overwriting data. </p>
<p>Is there a good way to open an IO channel, for the lack of a better expression, that concatenates the input without explicitly reading and writing line by line?</p>
</div>
<div class="post-text" itemprop="text">
<p>you can <a href="https://docs.python.org/3/library/itertools.html#itertools.chain" rel="nofollow noreferrer"><code>chain</code></a> open files and use a generator to <code>yield</code> open files from the filenames:</p>
<pre><code>from itertools import chain

def yield_open(filenames):
    for filename in filenames:
        with open(filename, 'r') as file:
            yield file

def train(file):
    for line in file:
        print(line, end='')
    print()

files = chain.from_iterable(yield_open(filenames=['file1.txt', 'file2.txt']))
train(files)
</code></pre>
<p>this has the added benefit that only one of your files is open at the time.</p>
<p>you could also use that as a 'data pipeline' (may be more readable):</p>
<pre><code>file_gen = yield_open(filenames=['file1.txt', 'file2.txt'])
files = chain.from_iterable(file_gen)
train(files)
</code></pre>
</div>
<span class="comment-copy">I feel like you might want to look into Pandas.</span>
<span class="comment-copy">@mauve i am already working with pandas in this project, what specifically are you referring to?</span>
<span class="comment-copy">I thought it would help with the "not reading in line by line" part. I have some code where I open each file in a directory, read it into DataFrames, concatenate the DataFrames, sort it by time, and output as 10-hour chunks in csv form.</span>
<span class="comment-copy">The <code>fileinput</code> module does a great job at concatenating files given as parameters and optionaly the standard input. Maybe you should look at it...</span>
<span class="comment-copy">@SergeBallesta If I understand it right, it's not compatible with argparse which is necessary in my case. I have multiple different CL arguments that need handling. But nevertheless it's a good tip, I'll try to keep it in mind</span>
<span class="comment-copy">Interesting.. that should imitate the behaviour of the piped <code>cat</code> command.. Is there a more generic way to write the <code>with</code> statement for an arbitrary (potentially large) number of files?</span>
<span class="comment-copy">@posdef tried to do that. is this more or less what you mean?</span>
