<div class="post-text" itemprop="text">
<p>I transformed the existing code which was in python pasted below was in pyspark. </p>
<p>Python code:</p>
<pre><code>import json
import csv


def main():
    # create a simple JSON array
    with open('paytm_tweets_data_1495614657.json') as str:

        tweetsList = []
        # change the JSON string into a JSON object
        jsonObject = json.load(str)

        #print(jsonObject)

        # # print the keys and values
        for i in range(len(jsonObject)):
            tweetsList.insert(i,jsonObject[i]["text"])

        #print(tweetsList)
    displaySentiment(tweetsList)



def displaySentiment(tweetsList):
    aDict = {}

    from sentiment import sentiment_score

    for i in range(len(tweetsList)):
        aDict[tweetsList[i]] = sentiment_score(tweetsList[i])
    print (aDict)


    with open('PaytmtweetSentiment.csv', 'w') as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames = ["Tweets", "Sentiment Value"])
        writer.writeheader()
        writer = csv.writer(csv_file)
        for key, value in aDict.items():
            writer.writerow([key, value])


if __name__ == '__main__':
    main()
</code></pre>
<p>Converted Pyspark Code:</p>
<pre><code>import json
import csv
import os
from pyspark import SparkContext, SparkConf
from pyspark.python.pyspark.shell import spark

os.environ['PYSPARK_PYTHON'] = "/usr/local/bin/python3"


def main():
    path = "/Users/i322865/DeepInsights/bitbucket-code/ai-engine/twitter-sentiment-analysis/flipkart_tweets_data_1495601666.json"
    peopleDF = spark.read.json(path).rdd
    df = peopleDF.map(lambda row: row['text'])
    displaySentiment(df.collect())



def displaySentiment(tweetsList):
    from sentiment import sentiment_score

    aDict = sentiment_score(tweetsList)

    #
    with open('paytmtweetSentiment.csv', 'w') as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames = ["Tweets", "Sentiment Value"])
        writer.writeheader()
        writer = csv.writer(csv_file)
        for i in range(len(tweetsList)):
            writer.writerow([tweetsList[i], aDict[i]])
            print([tweetsList[i], aDict[i]])


if __name__ == '__main__':
    conf = SparkConf().setAppName("Test").setMaster("local")
    sc = SparkContext.getOrCreate(conf=conf)
    main()
</code></pre>
<p>I ran both programs but didn't see any significant performance improvement. What am I missing? Please could you shed some thoughts?</p>
<p>Also, Should I use 'reduce' as well? I am currently using only 'map'.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you want to parallely process something in PySpark, don't <code>collect()</code> back to a Python list</p>
<pre><code>def calc_sentiment(tweetsDf):  # You should pass a dataframe
    from sentiment import sentiment_score

    # Add a new column over the Tweets for the sentiment
    return tweetsDf.withColumn('sentiment_score', sentiment_score(tweetsDf.text))
</code></pre>
<p>Obviously, <code>sentiment_score</code> needs changed as well to both accept and return a PySpark <code>Column</code></p>
<p>Then, you would have something like this </p>
<pre><code>def main():
    path = "..../twitter-sentiment-analysis/flipkart_tweets_data_1495601666.json"
    twitterDf = spark.read.json(path)

    # Don't call collect, only sample the Dataframe
    sentimentDf = calc_sentiment(twitterDf)
    sentimentDf.show(5)

    # TODO: Write sentimentDf to a CSV
    sentimentDf.write.csv(....)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In addition to the collection issue others have pointed out, your PySpark implementation may be slower simply because Spark was not meant for your current use case.</p>
<p>Fundamentally, Spark aims to speed up operations on very large, distributed data sets (multiple machines), not local parallelization. To achieve this, it uses overhead structures and processes.</p>
<p>For single/small datasets, this overhead could easily become dominant and slow down your solution. <a href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html" rel="nofollow noreferrer">This article</a> discusses the use of Hadoop, which is very similar. You may have tried <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">multiprocessing</a> instead?</p>
<p>If you're sure that Spark is right for you, it maybe helpful to post a new question detailing your Spark set up, how you're measuring your performance, and your data set.</p>
</div>
<div class="post-text" itemprop="text">
<p>I think it perfectly makes sense that you don't see any speed up. You are first creating an RDD (so you distribute data) then you collect them to run your second function which is the analysis function. In effect, you destroy what your first function did by collecting all your data to the driver machine that goes on to apply your displaySentiment() function. So what you in effect do is to run the program in the driver machine which is just one machine. Hence no speed up.</p>
</div>
<span class="comment-copy">This type of questions isn't fit for the site but as a matter of fact, this is still a bad code, to be honest, no offense of course ! Pyspark isn't a programming language. Python is on the other hand.</span>
<span class="comment-copy">@eliasah Sorry, modified the question. Thanks for the quick feedback.</span>
<span class="comment-copy">Calling <code>df.collect()</code> twice is of course less performant. Calling it at all renders Spark mostly useless</span>
<span class="comment-copy">@cricket_007 Thanks. Sorry about the print statement. Say if I eliminated/removed it(check the question now). Please can you tell me what else am I missing? Should I use 'reduce' as well? I am currently using only 'map'.</span>
<span class="comment-copy">I don't know what you're expecting to happen, but <code>df.collect()</code> being passed into <code>displaySentiment</code> gives you no benefits. All you speed up here is reading the JSON data, which is bound by your disk IO anyway. You need to pass the <i>dataframe</i> into <code>displaySentiment</code>, not a <i>python list</i></span>
<span class="comment-copy">thanks for the suggestion. Just an add-on question, instead of .json file, if i pass the stream data, will there be any changes needs to be done in above suggested code</span>
<span class="comment-copy">As long as you can get your stream into a Dataframe object, then not that I am aware of</span>
