<div class="post-text" itemprop="text">
<p>I'm computing the MSE <strong><em>on the training set</em></strong> so I expect the MSE to decrease when using higher polynoms. However, from degree 4 to 5, the MSE increases significantly. What could be the cause?</p>
<pre><code>import pandas as pd, numpy as np
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

path = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv"
df = pd.read_csv(path)
r=[]
max_degrees = 10

y = df['price'].astype('float')
x = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']].astype('float')

for i in range(1,max_degrees+1):
    Input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(degree=i)), ('model', LinearRegression())]
    pipe = Pipeline(Input)
    pipe.fit(x,y)
    yhat = pipe.predict(x)
    r.append(mean_squared_error(yhat, y))
    print("MSE for MLR of degree "+str(i)+" = "+str(round(mean_squared_error(yhat, y)/1e6,1)))

plt.figure(figsize=(10,3))
plt.plot(list(range(1,max_degrees+1)),r)
plt.show()
</code></pre>
<p>Results:</p>
<p><a href="https://i.stack.imgur.com/32nVu.png" rel="nofollow noreferrer"><img alt="image" src="https://i.stack.imgur.com/32nVu.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>Originally, you have 200 observations in y and 4 features (columns) in X, which you then scale and transform as polynomial features.</p>
<p>Degree 4 thus has 120 &lt; 200 polynomial features, whereas degree 5 is the first to have 210 &gt; 200 polynomial features, namely more features than observations.</p>
<p>When there are more features than observations, linear regression is ill-defined and should not be used, as explained <a href="https://medium.com/@jennifer.zzz/more-features-than-data-points-in-linear-regression-5bcabba6883e" rel="nofollow noreferrer">here</a>. This may explain the sudden deterioration in fitting the train set when advancing from degree 4 to degree 5. For higher degrees, it appears that the LR solver was nevertheless able to overfit the train data.</p>
</div>
