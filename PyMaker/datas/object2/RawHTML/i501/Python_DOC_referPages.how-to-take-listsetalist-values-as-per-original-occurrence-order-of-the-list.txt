<div class="post-text" itemprop="text">
<p>Is there a built-in that removes duplicates from list in Python, whilst preserving order? I know that I can use a set to remove duplicates, but that destroys the original order. I also know that I can roll my own like this:</p>
<pre><code>def uniq(input):
  output = []
  for x in input:
    if x not in output:
      output.append(x)
  return output
</code></pre>
<p>(Thanks to <a href="https://stackoverflow.com/users/28169/unwind">unwind</a> for that <a href="https://stackoverflow.com/questions/479897/how-do-you-remove-duplicates-from-a-list-in-python#479921">code sample</a>.)</p>
<p>But I'd like to avail myself of a built-in or a more Pythonic idiom if possible.</p>
<p>Related question: <a href="https://stackoverflow.com/questions/89178/in-python-what-is-the-fastest-algorithm-for-removing-duplicates-from-a-list-so-t">In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique <em>while preserving order</em>?</a></p>
</div>
<div class="post-text" itemprop="text">
<p>Here you have some alternatives: <a href="http://www.peterbe.com/plog/uniqifiers-benchmark" rel="nofollow noreferrer">http://www.peterbe.com/plog/uniqifiers-benchmark</a></p>
<p>Fastest one:</p>
<pre><code>def f7(seq):
    seen = set()
    seen_add = seen.add
    return [x for x in seq if not (x in seen or seen_add(x))]
</code></pre>
<p>Why assign <code>seen.add</code> to <code>seen_add</code> instead of just calling <code>seen.add</code>? Python is a dynamic language, and resolving <code>seen.add</code> each iteration is more costly than resolving a local variable. <code>seen.add</code> could have changed between iterations, and the runtime isn't smart enough to rule that out. To play it safe, it has to check the object each time.</p>
<p>If you plan on using this function a lot on the same dataset, perhaps you would be better off with an ordered set: <a href="http://code.activestate.com/recipes/528878/" rel="nofollow noreferrer">http://code.activestate.com/recipes/528878/</a></p>
<p><em>O</em>(1) insertion, deletion and member-check per operation.</p>
<p>(Small additional note: seen.add() always returns None, so the <em>or</em> above is there only as a way to attempt a set update, and not as an integral part of the logical test.)</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Edit 2016</strong></p>
<p>As Raymond <a href="https://stackoverflow.com/a/39835527/336527">pointed out</a>, in python 3.5+ where <code>OrderedDict</code> is implemented in C, the list comprehension approach will be slower than <code>OrderedDict</code> (unless you actually need the list at the end - and even then, only if the input is very short). So the best solution for 3.5+ is <code>OrderedDict</code>.</p>
<p><strong>Important Edit 2015</strong></p>
<p>As <a href="https://stackoverflow.com/a/19279812/1219006">@abarnert</a> notes, the <a href="https://pythonhosted.org/more-itertools/api.html" rel="noreferrer"><code>more_itertools</code></a> library (<code>pip install more_itertools</code>) contains a <a href="https://pythonhosted.org/more-itertools/api.html#more_itertools.unique_everseen" rel="noreferrer"><code>unique_everseen</code></a> function that is built to solve this problem without any <strong>unreadable</strong> (<code>not seen.add</code>) <strong>mutations</strong> in list comprehensions. This is also the fastest solution too:</p>
<pre><code>&gt;&gt;&gt; from  more_itertools import unique_everseen
&gt;&gt;&gt; items = [1, 2, 0, 1, 3, 2]
&gt;&gt;&gt; list(unique_everseen(items))
[1, 2, 0, 3]
</code></pre>
<p>Just one simple library import and no hacks.
This comes from an implementation of the itertools recipe <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="noreferrer"><code>unique_everseen</code></a> which looks like:</p>
<pre><code>def unique_everseen(iterable, key=None):
    "List unique elements, preserving order. Remember all elements ever seen."
    # unique_everseen('AAAABBBCCDAABBB') --&gt; A B C D
    # unique_everseen('ABBCcAD', str.lower) --&gt; A B C D
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in filterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element
</code></pre>
<hr/>
<p>In Python <code>2.7+</code> the <strike>accepted common idiom</strike> (which works but isn't optimized for speed, I would now use <a href="https://pythonhosted.org/more-itertools/api.html#more_itertools.unique_everseen" rel="noreferrer"><code>unique_everseen</code></a>) for this uses <a href="http://docs.python.org/3/library/collections.html#collections.OrderedDict" rel="noreferrer"><code>collections.OrderedDict</code></a>:</p>
<p>Runtime: <strong>O(N)</strong></p>
<pre><code>&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt; items = [1, 2, 0, 1, 3, 2]
&gt;&gt;&gt; list(OrderedDict.fromkeys(items))
[1, 2, 0, 3]
</code></pre>
<p>This looks much nicer than:</p>
<pre><code>seen = set()
[x for x in seq if x not in seen and not seen.add(x)]
</code></pre>
<p>and doesn't utilize the <strong>ugly hack</strong>:</p>
<pre><code>not seen.add(x)
</code></pre>
<p>which relies on the fact that <code>set.add</code> is an in-place method that always returns <code>None</code> so <code>not None</code> evaluates to <code>True</code>.</p>
<p>Note however that the hack solution is faster in raw speed though it has the same runtime complexity O(N).</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>In Python 2.7</strong>, the new way of removing duplicates from an iterable while keeping it in the original order is:</p>
<pre><code>&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt; list(OrderedDict.fromkeys('abracadabra'))
['a', 'b', 'r', 'c', 'd']
</code></pre>
<p><strong>In Python 3.5</strong>, the OrderedDict has a C implementation. My timings show that this is now both the fastest and shortest of the various approaches for Python 3.5.</p>
<p><strong>In Python 3.6</strong>, the regular dict became both ordered and compact.  (This feature is holds for CPython and PyPy but may not present in other implementations).  That gives us a new fastest way of deduping while retaining order:</p>
<pre><code>&gt;&gt;&gt; list(dict.fromkeys('abracadabra'))
['a', 'b', 'r', 'c', 'd']
</code></pre>
<p><strong>In Python 3.7</strong>, the regular dict is guaranteed to both ordered across all implementations.  <strong>So, the shortest and fastest solution is:</strong></p>
<pre><code>&gt;&gt;&gt; list(dict.fromkeys('abracadabra'))
['a', 'b', 'r', 'c', 'd']
</code></pre>
<hr/>
<p>Response to @max:  Once you move to 3.6 or 3.7 and use the regular dict instead of <em>OrderedDict</em>, you can't really beat the performance in any other way.  The dictionary is dense and readily converts to a list with almost no overhead.  The target list is pre-sized to len(d) which saves all the resizes that occur in a list comprehension.  Also, since the internal key list is dense, copying the pointers is about almost fast as a list copy.</p>
</div>
<div class="post-text" itemprop="text">
<pre><code>sequence = ['1', '2', '3', '3', '6', '4', '5', '6']
unique = []
[unique.append(item) for item in sequence if item not in unique]
</code></pre>
<p>unique → <code>['1', '2', '3', '6', '4', '5']</code></p>
</div>
<div class="post-text" itemprop="text">
<pre><code>from itertools import groupby
[ key for key,_ in groupby(sortedList)]
</code></pre>
<p>The list doesn't even have to be <em>sorted</em>, the sufficient condition is that equal values are grouped together.</p>
<p><strong>Edit: I assumed that "preserving order" implies that the list is actually ordered. If this is not the case, then the solution from MizardX is the right one.</strong></p>
<p>Community edit: This is however the most elegant way to "compress duplicate consecutive elements into a single element".</p>
</div>
<div class="post-text" itemprop="text">
<p>I think if you wanna maintain the order,</p>
<h2>you can try this:</h2>
<pre><code>list1 = ['b','c','d','b','c','a','a']    
list2 = list(set(list1))    
list2.sort(key=list1.index)    
print list2
</code></pre>
<h2>OR similarly you can do this:</h2>
<pre><code>list1 = ['b','c','d','b','c','a','a']  
list2 = sorted(set(list1),key=list1.index)  
print list2 
</code></pre>
<h2>You can also do this:</h2>
<pre><code>list1 = ['b','c','d','b','c','a','a']    
list2 = []    
for i in list1:    
    if not i in list2:  
        list2.append(i)`    
print list2
</code></pre>
<h2>It can also be written as this:</h2>
<pre><code>list1 = ['b','c','d','b','c','a','a']    
list2 = []    
[list2.append(i) for i in list1 if not i in list2]    
print list2 
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Not to kick a dead horse (this question is very old and already has lots of good answers), but here is a solution using pandas that is quite fast in many circumstances and is dead simple to use.  </p>
<pre><code>import pandas as pd

my_list = [0, 1, 2, 3, 4, 1, 2, 3, 5]

&gt;&gt;&gt; pd.Series(my_list).drop_duplicates().tolist()
# Output:
# [0, 1, 2, 3, 4, 5]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>For another very late answer to another very old question:</p>
<p>The <a href="http://docs.python.org/3/library/itertools.html#itertools-recipes"><code>itertools</code> recipes</a> have a function that does this, using the <code>seen</code> set technique, but:</p>
<ul>
<li>Handles a standard <code>key</code> function.</li>
<li>Uses no unseemly hacks.</li>
<li>Optimizes the loop by pre-binding <code>seen.add</code> instead of looking it up N times. (<code>f7</code> also does this, but some versions don't.)</li>
<li>Optimizes the loop by using <code>ifilterfalse</code>, so you only have to loop over the unique elements in Python, instead of all of them. (You still iterate over all of them inside <code>ifilterfalse</code>, of course, but that's in C, and much faster.)</li>
</ul>
<p>Is it actually faster than <code>f7</code>? It depends on your data, so you'll have to test it and see. If you want a list in the end, <code>f7</code> uses a listcomp, and there's no way to do that here. (You can directly <code>append</code> instead of <code>yield</code>ing, or you can feed the generator into the <code>list</code> function, but neither one can be as fast as the LIST_APPEND inside a listcomp.) At any rate, usually, squeezing out a few microseconds is not going to be as important as having an easily-understandable, reusable, already-written function that doesn't require DSU when you want to decorate.</p>
<p>As with all of the recipes, it's also available in <a href="https://pypi.python.org/pypi/more-itertools"><code>more-iterools</code></a>.</p>
<p>If you just want the no-<code>key</code> case, you can simplify it as:</p>
<pre><code>def unique(iterable):
    seen = set()
    seen_add = seen.add
    for element in itertools.ifilterfalse(seen.__contains__, iterable):
        seen_add(element)
        yield element
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Just to add another (very performant) implementation of such a functionality from an external module<sup>1</sup>: <a href="https://iteration-utilities.readthedocs.io/en/latest/generated/unique_everseen.html" rel="nofollow noreferrer"><code>iteration_utilities.unique_everseen</code></a>:</p>
<pre><code>&gt;&gt;&gt; from iteration_utilities import unique_everseen
&gt;&gt;&gt; lst = [1,1,1,2,3,2,2,2,1,3,4]

&gt;&gt;&gt; list(unique_everseen(lst))
[1, 2, 3, 4]
</code></pre>
<h1>Timings</h1>
<p>I did some timings (Python 3.6) and these show that it's faster than all other alternatives I tested, including <code>OrderedDict.fromkeys</code>, <code>f7</code> and <code>more_itertools.unique_everseen</code>:</p>
<pre><code>%matplotlib notebook

from iteration_utilities import unique_everseen
from collections import OrderedDict
from more_itertools import unique_everseen as mi_unique_everseen

def f7(seq):
    seen = set()
    seen_add = seen.add
    return [x for x in seq if not (x in seen or seen_add(x))]

def iteration_utilities_unique_everseen(seq):
    return list(unique_everseen(seq))

def more_itertools_unique_everseen(seq):
    return list(mi_unique_everseen(seq))

def odict(seq):
    return list(OrderedDict.fromkeys(seq))

from simple_benchmark import benchmark

b = benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],
              {2**i: list(range(2**i)) for i in range(1, 20)},
              'list size (no duplicates)')
b.plot()
</code></pre>
<p><a href="https://i.stack.imgur.com/XLrov.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/XLrov.png"/></a></p>
<p>And just to make sure I also did a test with more duplicates just to check if it makes a difference:</p>
<pre><code>import random

b = benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],
              {2**i: [random.randint(0, 2**(i-1)) for _ in range(2**i)] for i in range(1, 20)},
              'list size (lots of duplicates)')
b.plot()
</code></pre>
<p><a href="https://i.stack.imgur.com/YCx2c.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/YCx2c.png"/></a></p>
<p>And one containing only one value:</p>
<pre><code>b = benchmark([f7, iteration_utilities_unique_everseen, more_itertools_unique_everseen, odict],
              {2**i: [1]*(2**i) for i in range(1, 20)},
              'list size (only duplicates)')
b.plot()
</code></pre>
<p><a href="https://i.stack.imgur.com/SPCcT.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/SPCcT.png"/></a></p>
<p>In all of these cases the <code>iteration_utilities.unique_everseen</code> function is the fastest (on my computer).</p>
<hr/>
<p>This <code>iteration_utilities.unique_everseen</code> function can also handle unhashable values in the input (however with an <code>O(n*n)</code> performance instead of the <code>O(n)</code> performance when the values are hashable).</p>
<pre><code>&gt;&gt;&gt; lst = [{1}, {1}, {2}, {1}, {3}]

&gt;&gt;&gt; list(unique_everseen(lst))
[{1}, {2}, {3}]
</code></pre>
<hr/>
<p><sup>1</sup> Disclaimer: I'm the author of that package.</p>
</div>
<div class="post-text" itemprop="text">
<p>In <strong>Python 3.7</strong> and above, dictionaries are <a href="https://mail.python.org/pipermail/python-dev/2017-December/151283.html" rel="noreferrer">guaranteed</a> to remember their key insertion order. The answer to <a href="https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6">this</a> question summarizes the current state of affairs.</p>
<p>The <code>OrderedDict</code> solution thus becomes obsolete and without any import statements we can simply issue:</p>
<pre><code>&gt;&gt;&gt; lst = [1, 2, 1, 3, 3, 2, 4]
&gt;&gt;&gt; list(dict.fromkeys(lst))
[1, 2, 3, 4]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>For no hashable types (e.g. list of lists), based on MizardX's:</p>
<pre><code>def f7_noHash(seq)
    seen = set()
    return [ x for x in seq if str( x ) not in seen and not seen.add( str( x ) )]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Borrowing the recursive idea used in definining Haskell's <code>nub</code> function for lists, this would be a recursive approach:</p>
<pre><code>def unique(lst):
    return [] if lst==[] else [lst[0]] + unique(filter(lambda x: x!= lst[0], lst[1:]))
</code></pre>
<p>e.g.:</p>
<pre><code>In [118]: unique([1,5,1,1,4,3,4])
Out[118]: [1, 5, 4, 3]
</code></pre>
<p>I tried it for growing data sizes and saw sub-linear time-complexity (not definitive, but suggests this should be fine for normal data).</p>
<pre><code>In [122]: %timeit unique(np.random.randint(5, size=(1)))
10000 loops, best of 3: 25.3 us per loop

In [123]: %timeit unique(np.random.randint(5, size=(10)))
10000 loops, best of 3: 42.9 us per loop

In [124]: %timeit unique(np.random.randint(5, size=(100)))
10000 loops, best of 3: 132 us per loop

In [125]: %timeit unique(np.random.randint(5, size=(1000)))
1000 loops, best of 3: 1.05 ms per loop

In [126]: %timeit unique(np.random.randint(5, size=(10000)))
100 loops, best of 3: 11 ms per loop
</code></pre>
<p>I also think it's interesting that this could be readily generalized to uniqueness by other operations. Like this:</p>
<pre><code>import operator
def unique(lst, cmp_op=operator.ne):
    return [] if lst==[] else [lst[0]] + unique(filter(lambda x: cmp_op(x, lst[0]), lst[1:]), cmp_op)
</code></pre>
<p>For example, you could pass in a function that uses the notion of rounding to the same integer as if it was "equality" for uniqueness purposes, like this:</p>
<pre><code>def test_round(x,y):
    return round(x) != round(y)
</code></pre>
<p>then unique(some_list, test_round) would provide the unique elements of the list where uniqueness no longer meant traditional equality (which is implied by using any sort of set-based or dict-key-based approach to this problem) but instead meant to take only the first element that rounds to K for each possible integer K that the elements might round to, e.g.:</p>
<pre><code>In [6]: unique([1.2, 5, 1.9, 1.1, 4.2, 3, 4.8], test_round)
Out[6]: [1.2, 5, 1.9, 4.2, 3]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>5 x faster reduce variant but more sophisticated</p>
<pre><code>&gt;&gt;&gt; l = [5, 6, 6, 1, 1, 2, 2, 3, 4]
&gt;&gt;&gt; reduce(lambda r, v: v in r[1] and r or (r[0].append(v) or r[1].add(v)) or r, l, ([], set()))[0]
[5, 6, 1, 2, 3, 4]
</code></pre>
<p>Explanation:</p>
<pre><code>default = (list(), set())
# use list to keep order
# use set to make lookup faster

def reducer(result, item):
    if item not in result[1]:
        result[0].append(item)
        result[1].add(item)
    return result

&gt;&gt;&gt; reduce(reducer, l, default)[0]
[5, 6, 1, 2, 3, 4]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>MizardX's answer gives a good collection of multiple approaches.</p>
<p>This is what I came up with while thinking aloud:</p>
<pre><code>mylist = [x for i,x in enumerate(mylist) if x not in mylist[i+1:]]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can reference a list comprehension as it is being built by the symbol '_[1]'. <br/>For example, the following function unique-ifies a list of elements without changing their order by referencing its list comprehension.</p>
<pre><code>def unique(my_list): 
    return [x for x in my_list if x not in locals()['_[1]']]
</code></pre>
<p>Demo:</p>
<pre><code>l1 = [1, 2, 3, 4, 1, 2, 3, 4, 5]
l2 = [x for x in l1 if x not in locals()['_[1]']]
print l2
</code></pre>
<p>Output:</p>
<pre><code>[1, 2, 3, 4, 5]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You could do a sort of ugly list comprehension hack.</p>
<pre><code>[l[i] for i in range(len(l)) if l.index(l[i]) == i]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Relatively effective approach with <code>_sorted_</code> a <code>numpy</code> arrays:</p>
<pre><code>b = np.array([1,3,3, 8, 12, 12,12])    
numpy.hstack([b[0], [x[0] for x in zip(b[1:], b[:-1]) if x[0]!=x[1]]])
</code></pre>
<p>Outputs:</p>
<pre><code>array([ 1,  3,  8, 12])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>l = [1,2,2,3,3,...]
n = []
n.extend(ele for ele in l if ele not in set(n))
</code></pre>
<p>A generator expression that uses the O(1) look up of a set to determine whether or not to include an element in the new list.</p>
</div>
<div class="post-text" itemprop="text">
<p>A simple recursive solution:</p>
<pre><code>def uniquefy_list(a):
    return uniquefy_list(a[1:]) if a[0] in a[1:] else [a[0]]+uniquefy_list(a[1:]) if len(a)&gt;1 else [a[0]]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you need one liner then maybe this would help:</p>
<pre><code>reduce(lambda x, y: x + y if y[0] not in x else x, map(lambda x: [x],lst))
</code></pre>
<p>... should work but correct me if i'm wrong</p>
</div>
<div class="post-text" itemprop="text">
<p>If you routinely use <a href="http://pandas.pydata.org/" rel="nofollow"><code>pandas</code></a>, and aesthetics is preferred over performance, then consider the built-in function <code>pandas.Series.drop_duplicates</code>:</p>
<pre><code>    import pandas as pd
    import numpy as np

    uniquifier = lambda alist: pd.Series(alist).drop_duplicates().tolist()

    # from the chosen answer 
    def f7(seq):
        seen = set()
        seen_add = seen.add
        return [ x for x in seq if not (x in seen or seen_add(x))]

    alist = np.random.randint(low=0, high=1000, size=10000).tolist()

    print uniquifier(alist) == f7(alist)  # True
</code></pre>
<p>Timing: </p>
<pre><code>    In [104]: %timeit f7(alist)
    1000 loops, best of 3: 1.3 ms per loop
    In [110]: %timeit uniquifier(alist)
    100 loops, best of 3: 4.39 ms per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>this will preserve order and run in O(n) time. basically the idea is to create a hole wherever there is a duplicate found and sink it down to the bottom. makes use of a read and write pointer. whenever a duplicate is found only the read pointer advances and write pointer stays on the duplicate entry to overwrite it.</p>
<pre><code>def deduplicate(l):
    count = {}
    (read,write) = (0,0)
    while read &lt; len(l):
        if l[read] in count:
            read += 1
            continue
        count[l[read]] = True
        l[write] = l[read]
        read += 1
        write += 1
    return l[0:write]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>A solution without using imported modules or sets:</p>
<pre><code>text = "ask not what your country can do for you ask what you can do for your country"
sentence = text.split(" ")
noduplicates = [(sentence[i]) for i in range (0,len(sentence)) if sentence[i] not in sentence[:i]]
print(noduplicates)
</code></pre>
<p>Gives output:</p>
<pre><code>['ask', 'not', 'what', 'your', 'country', 'can', 'do', 'for', 'you']
</code></pre>
</div>
<div class="post-text" itemprop="text">
<h3>An in-place method</h3>
<p>This method is quadratic, because we have a linear lookup into the list for every element of the list (to that we have to add the cost of rearranging the list because of the <code>del</code> s).</p>
<p>That said, it is possible to operate in place if we start from the end of the list and proceed toward the origin removing each term that is present in the sub-list at its left</p>
<p>This idea in code is simply</p>
<pre><code>for i in range(len(l)-1,0,-1): 
    if l[i] in l[:i]: del l[i] 
</code></pre>
<hr/>
<p><em>A simple test of the implementation</em></p>
<pre><code>In [91]: from random import randint, seed                                                                                            
In [92]: seed('20080808') ; l = [randint(1,6) for _ in range(12)] # Beijing Olympics                                                                 
In [93]: for i in range(len(l)-1,0,-1): 
    ...:     print(l) 
    ...:     print(i, l[i], l[:i], end='') 
    ...:     if l[i] in l[:i]: 
    ...:          print( ': remove', l[i]) 
    ...:          del l[i] 
    ...:     else: 
    ...:          print() 
    ...: print(l)
[6, 5, 1, 4, 6, 1, 6, 2, 2, 4, 5, 2]
11 2 [6, 5, 1, 4, 6, 1, 6, 2, 2, 4, 5]: remove 2
[6, 5, 1, 4, 6, 1, 6, 2, 2, 4, 5]
10 5 [6, 5, 1, 4, 6, 1, 6, 2, 2, 4]: remove 5
[6, 5, 1, 4, 6, 1, 6, 2, 2, 4]
9 4 [6, 5, 1, 4, 6, 1, 6, 2, 2]: remove 4
[6, 5, 1, 4, 6, 1, 6, 2, 2]
8 2 [6, 5, 1, 4, 6, 1, 6, 2]: remove 2
[6, 5, 1, 4, 6, 1, 6, 2]
7 2 [6, 5, 1, 4, 6, 1, 6]
[6, 5, 1, 4, 6, 1, 6, 2]
6 6 [6, 5, 1, 4, 6, 1]: remove 6
[6, 5, 1, 4, 6, 1, 2]
5 1 [6, 5, 1, 4, 6]: remove 1
[6, 5, 1, 4, 6, 2]
4 6 [6, 5, 1, 4]: remove 6
[6, 5, 1, 4, 2]
3 4 [6, 5, 1]
[6, 5, 1, 4, 2]
2 1 [6, 5]
[6, 5, 1, 4, 2]
1 5 [6]
[6, 5, 1, 4, 2]

In [94]:                                                                                                                             
</code></pre>
</div>
<span class="comment-copy">I think your implementation looks the cleanest..</span>
<span class="comment-copy">@JesseDhillon <code>seen.add</code> could have changed between iterations, and the runtime isn't smart enough to rule that out. To play safe, it has to check the object each time. -- If you look at the bytecode with <code>dis.dis(f)</code>, you can see that it executes <code>LOAD_ATTR</code> for the <code>add</code> member on each iteration. <a href="http://ideone.com/tz1Tll" rel="nofollow noreferrer">ideone.com/tz1Tll</a></span>
<span class="comment-copy">@SergeyOrshanskiy <a href="http://stackoverflow.com/q/1055243">Almost O(1)</a>.</span>
<span class="comment-copy">When I try this on a list of lists I get: TypeError: unhashable type: 'list'</span>
<span class="comment-copy">Your solution is not the fastest one. In Python 3 (did not test 2) this is faster (300k entries list - 0.045s (yours) vs 0.035s (this one):  seen = set(); return [x for x in lines if x not in seen and not seen.add(x)]. I could not find any speed effect of the seen_add line you did.</span>
<span class="comment-copy">@user136036 Please link to your tests. How many times did you run them?<code>seen_add</code> is an improvement but timings can be affected by system resources at the time. Would be interested to see full timings</span>
<span class="comment-copy">Converting to some custom kind of dict just to take keys? Just another crutch.</span>
<span class="comment-copy">@Nakilon Are you saying it's not clear? Or are you expecting Python to be more pure?</span>
<span class="comment-copy">@Nakilon I don't really see how it's a crutch. It doesn't expose any mutable state, so its very clean in that sense. Internally, Python sets are implemented with dict()  (<a href="http://stackoverflow.com/questions/3949310/how-is-cpythons-set-implemented" title="how is cpythons set implemented">stackoverflow.com/questions/3949310/…</a>), so basically you're just doing what the interpreter would've done anyway.</span>
<span class="comment-copy">+1 This is a good and pythonic way, and is vetted by a python core developer <a href="http://stackoverflow.com/a/7961425/674039">here</a>.</span>
<span class="comment-copy">@CommuSoft I agree, although practically it's almost always O(n) due to the super highly unlikely worst case</span>
<span class="comment-copy">It is faster than any other approach on my machine (python 3.5) <i>as long as</i> I don't convert <code>OrderedDict</code> to a list in the end. If I do need to convert it to a list, for <i>small</i> inputs the list comprehension approach is still faster by up to 1.5 times. That said, this solution is much cleaner.</span>
<span class="comment-copy">The only gotcha is that the iterable "elements" must be hashable - would be nice to have the equivalent for iterables with arbitrary elements (as a list of lists)</span>
<span class="comment-copy">Insertion order iteration over a dict provides functionality that services more use-cases than removing duplicates. For example, scientific analyses rely on <b>reproducible</b> computations which non-deterministic dict iteration doesn't support. Reproducibility is a major current objective in computational scientific modeling, so we welcome this new feature. Although I know it's trivial to build with a deterministic dict, a high-performance, deterministic <code>set()</code> would help more naive users develop reproducible codes.</span>
<span class="comment-copy">This may not be the most concise, but I like it for its clarity. Thanks</span>
<span class="comment-copy">It's worth noting that this runs in <code>n^2</code></span>
<span class="comment-copy">Ick. 2 strikes: Using a list for membership testing (slow, O(N)) and using a list comprehension for the side effects (building another list of <code>None</code>references in the process!)</span>
<span class="comment-copy">I agree with @MartijnPieters there's absolutely <i>no</i> reason for the list comprehension with side effects. Just use a <code>for</code> loop instead</span>
<span class="comment-copy">But this doesn't preserve order!</span>
<span class="comment-copy">Hrm, this is problematic, since I cannot guarantee that equal values are grouped together without looping once over the list, by which time I could have pruned the duplicates.</span>
<span class="comment-copy">I assumed that "preserving order" implied that the list is actually ordered.</span>
<span class="comment-copy">Added clarification.</span>
<span class="comment-copy">Maybe the specification of the input list is a little bit unclear. The values don't even need to be grouped together: [2, 1, 3, 1]. So which values to keep and which to delete?</span>
<span class="comment-copy">Your first two answers assume that the order of the list can be rebuilt using a sorting function, but this may not be so.</span>
<span class="comment-copy">It's also O(n^2), worst case on an already sorted list</span>
<span class="comment-copy">Most answers are focused on performance.  For lists that aren't large enough to worry about performance, the sorted(set(list1),key=list1.index) is the best thing I've seen.  No extra import, no extra function, no extra variable, and it's fairly simple and readable.</span>
<span class="comment-copy">Neat solution +1</span>
<span class="comment-copy">I completely overlooked <code>more-itertools</code> this is clearly the best answer. A simple <code>from more_itertools import unique_everseen</code> <code>list(unique_everseen(items))</code> A much faster approach than mine and much better than the accepted answer, I think the library download is worth it. I am going to community wiki my answer and add this in.</span>
<span class="comment-copy">Note that raymonds answer already mentions that approach.</span>
<span class="comment-copy">Note that performance will get bad when the number of unique elements is very large relative to the total number of elements, since each successive recursive call's use of <code>filter</code> will barely benefit from the previous call at all. But if the number of unique elements is small relative to the array size, this should perform pretty well.</span>
<span class="comment-copy">Your solution is nice, but it takes the last appearance of each element. To take the first appearance use: [x for i,x in enumerate(mylist) if x not in mylist[:i]]</span>
<span class="comment-copy">Since searching in a list is an <code>O(n)</code> operation and you perform it on each item, the resulting complexity of your solution would be <code>O(n^2)</code>. This is just unacceptable for such a trivial problem.</span>
<span class="comment-copy">Please note it won't work in Python 2.7+.</span>
<span class="comment-copy">Also note that it would make it an O(n^2) operation, where as creating a set/dict (which has constant look up time) and adding only previously unseen elements will be linear.</span>
<span class="comment-copy">This is Python 2.6 only I believe. And yes it is O(N^2)</span>
<span class="comment-copy">Prefer <code>i,e in enumerate(l)</code> to <code>l[i] for i in range(len(l))</code>.</span>
<span class="comment-copy">Clever use of <code>extend</code> with a generator expression which depends on the thing being extended (so +1), but <code>set(n)</code> is recomputed at each stage (which is linear) and this bumps the overall approach to being quadratic. In fact, this is almost certainly worse than simply using <code>ele in n</code>. Making a set for a single membership test isn't worth the expense of the set creation. Still -- it is an interesting approach.</span>
<span class="comment-copy">can lambda work with if ? I think it's not correct.</span>
<span class="comment-copy">it's a <a href="http://www.python.org/dev/peps/pep-0308/" rel="nofollow noreferrer">conditional expression</a> so it's good</span>
<span class="comment-copy">yes, it's correct. I'm sorry for my mistake. I tried it just now.</span>
<span class="comment-copy">this is O(N**2) complexity + list slicing each time.</span>
<span class="comment-copy">Before posting I  have searched the body of the answers for 'place' to no avail. If others have solved the problem in a similar way please alert me and I'll remove my answer asap.</span>
