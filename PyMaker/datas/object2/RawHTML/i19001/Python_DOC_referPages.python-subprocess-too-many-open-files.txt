<div class="post-text" itemprop="text">
<p>I am using subprocess to call another program and save its return values to a variable. This process is repeated in a loop, and after a few thousands times the program crashed with the following error:</p>
<pre><code>Traceback (most recent call last):
  File "./extract_pcgls.py", line 96, in &lt;module&gt;
    SelfE.append( CalSelfEnergy(i) )
  File "./extract_pcgls.py", line 59, in CalSelfEnergy
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)
  File "/usr/lib/python3.2/subprocess.py", line 745, in __init__
    restore_signals, start_new_session)
  File "/usr/lib/python3.2/subprocess.py", line 1166, in _execute_child
    errpipe_read, errpipe_write = _create_pipe()
OSError: [Errno 24] Too many open files
</code></pre>
<p>Any idea how to solve this issue is much appreciated!</p>
<p>Code supplied from comments:</p>
<pre><code>cmd = "enerCHARMM.pl -parram=x,xtop=topology_modified.rtf,xpar=lipid27_modified.par,nobuildall -out vdwaals {0}".format(cmtup[1])
p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)
out, err = p.communicate()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In Mac OSX (El Capitan) See current configuration:</p>
<pre><code>#ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
file size               (blocks, -f) unlimited
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) unlimited
open files                      (-n) 256
pipe size            (512 bytes, -p) 1
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 709
virtual memory          (kbytes, -v) unlimited
</code></pre>
<p>Set <strong>open files</strong> value to 10K :</p>
<pre><code>#ulimit -Sn 10000
</code></pre>
<p>Verify results:</p>
<pre><code>#ulimit -a

core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
file size               (blocks, -f) unlimited
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) unlimited
open files                      (-n) 10000
pipe size            (512 bytes, -p) 1
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 709
virtual memory          (kbytes, -v) unlimited
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I guess the problem was due to the fact that I was processing an open file with subprocess:</p>
<pre><code>cmd = "enerCHARMM.pl -par param=x,xtop=topology_modified.rtf,xpar=lipid27_modified.par,nobuildall -out vdwaals {0}".format(cmtup[1])
p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)
</code></pre>
<p>Here the cmd variable contain the name of a file that has just been created but not closed. Then the <code>subprocess.Popen</code> calls a system command on that file. After doing this for many times, the program crashed with that error message.</p>
<p>So the message I learned from this is</p>
<blockquote>
<p>Close the file you have created, then process it</p>
</blockquote>
<p>Thanks</p>
</div>
<div class="post-text" itemprop="text">
<p>You can try raising the open file limit of the OS:</p>
<p><code>ulimit -n 2048</code></p>
</div>
<div class="post-text" itemprop="text">
<p>A child process created by <code>Popen()</code> may inherit open file descriptors (a finite resource) from the parent. Use <a href="https://docs.python.org/3/library/subprocess.html#subprocess.Popen" rel="nofollow"><code>close_fds=True</code></a> on POSIX (default since Python 3.2), to avoid it. Also, <a href="https://www.python.org/dev/peps/pep-0446/#issues-fixed-in-the-subprocess-module" rel="nofollow">"PEP 0446 -- Make newly created file descriptors non-inheritable" deals with some remaining issues (since Python 3.4)</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>As others have noted, raise the limit in /etc/security/limits.conf and also file descriptors was an issue for me personally, so I did </p>
<pre><code>sudo sysctl -w fs.file-max=100000 
</code></pre>
<p>And added a line with fs.file-max = 100000 to /etc/sysctl.conf (reload with sysctl -p)</p>
<p>Also if you want to make sure that your process is not affected by anything else (which mine was), use </p>
<pre><code>cat /proc/{process id}/limits 
</code></pre>
<p>to find out what the actual limits of your process are, as for me the software running the python scripts also had its limits applied which have overridden the system wide settings.</p>
<p>Posting this answer here after resolving my particular issue with this error and hopefully it helps someone.</p>
</div>
<div class="post-text" itemprop="text">
<p>Maybe you are invoking the command multiple times. If so, each time you're doing <code>stdout=subprocess.PIPE</code>. Between each call try doing <code>p.stdout.close()</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>opens file in subprocess. It is blocking call.</p>
<pre><code>ss=subprocess.Popen(tempFileName,shell=True)
 ss.communicate()
</code></pre>
</div>
<span class="comment-copy">Communicate() closes the pipe, so that's not your problem. In the end, Popen() is just the command that happens to run when you run out of pipes... the problem could be elsewhere in your code with other files being left open. I noticed "SelfE.append" ... are you opening other files and keeping them in a list?</span>
<span class="comment-copy">Actually that command will not raise the limit above what has been set in <code>/etc/security/limits.conf</code>. To raise it, you need to place lines like these <code>* soft nofile 4096</code> / <code>* hard nofile 4096</code> in that file (replace <code>4096</code> with your own value).</span>
<span class="comment-copy">Ran into this problem yesterday, and I had to edit both <code>/etc/security/limits.conf</code> AND raise the limit via <code>ulimit -n</code> in ubuntu to overcome this error.</span>
<span class="comment-copy">I don't think this works, at least in all cases. I generated 1200 sleep spawned processes on a system with a 1024 open file limit (default on Ubuntu) and it blew up even with close_fds=True. So I think there's more to it than that. Since you still have over the limit in open processes anyways and this only works if you assume that the problem lies in finished processes that left open file descriptors.</span>
<span class="comment-copy">@Sensei it does work: open files in the parent (make sure the fds are inheritable) then  spawn subprocesses with <code>close_fds=False</code> (both are default on old Python versions, follow the links). See how much sooner you'll get the error. Obviously <code>close_fds</code> can't prevent the error in the general case: you don't even need to spawn a new process, to get it.</span>
<span class="comment-copy">Except that it doesn't. I ran a simple for loop and generated enough subprocesses to hit the OS limit. I did this with close_fds=True. It had no impact. I may be wrong about why but my guess is simply that this solution only works if you're spawning a few subprocesses and never cleaning up the descriptors. In such a case this argument makes sense but I don't see it working if you actually intend to spawn and run that many processes at once.</span>
<span class="comment-copy">@Sensei: I know that it works because there are tests in the stdlib that exercise this option (i.e., I know that it works not only for me). Now, your code may not work as you expect it. In that case, create a minimal but complete code example, describe the exact behavior that you expect and what happens instead step by step and publish it as a separate SO question (mention OS, Python version).</span>
