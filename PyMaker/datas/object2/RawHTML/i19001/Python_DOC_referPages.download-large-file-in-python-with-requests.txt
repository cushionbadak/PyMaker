<div class="post-text" itemprop="text">
<p><a href="http://docs.python-requests.org/en/latest/" rel="noreferrer">Requests</a> is a really nice library. I'd like to use it for download big files (&gt;1GB).
The problem is it's not possible to keep whole file in memory I need to read it in chunks. And this is a problem with the following code</p>
<pre><code>import requests

def DownloadFile(url)
    local_filename = url.split('/')[-1]
    r = requests.get(url)
    f = open(local_filename, 'wb')
    for chunk in r.iter_content(chunk_size=512 * 1024): 
        if chunk: # filter out keep-alive new chunks
            f.write(chunk)
    f.close()
    return 
</code></pre>
<p>By some reason it doesn't work this way. It still loads response into memory before save it to a file.</p>
<p><strong>UPDATE</strong></p>
<p>If you need a small client (Python 2.x /3.x) which can download big files from FTP, you can find it <a href="https://github.com/keepitsimple/pyFTPclient" rel="noreferrer">here</a>. It supports multithreading &amp; reconnects (it does monitor connections) also it tunes socket params for the download task. </p>
</div>
<div class="post-text" itemprop="text">
<p>With the following streaming code, the Python memory usage is restricted regardless of the size of the downloaded file:</p>
<pre><code>def download_file(url):
    local_filename = url.split('/')[-1]
    # NOTE the stream=True parameter below
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192): 
                if chunk: # filter out keep-alive new chunks
                    f.write(chunk)
                    # f.flush()
    return local_filename
</code></pre>
<p>Note that the number of bytes returned using <code>iter_content</code> is not exactly the <code>chunk_size</code>; it's expected to be a random number that is often far bigger, and is expected to be different in every iteration.</p>
<p>See <a href="http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow" rel="noreferrer">http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow</a> for further reference.</p>
</div>
<div class="post-text" itemprop="text">
<p>It's much easier if you use <a href="http://docs.python-requests.org/en/master/api/#requests.Response.raw"><code>Response.raw</code></a> and <a href="https://docs.python.org/3/library/shutil.html#shutil.copyfileobj"><code>shutil.copyfileobj()</code></a>:</p>
<pre><code>import requests
import shutil

def download_file(url):
    local_filename = url.split('/')[-1]
    r = requests.get(url, stream=True)
    with open(local_filename, 'wb') as f:
        shutil.copyfileobj(r.raw, f)

    return local_filename
</code></pre>
<p>This streams the file to disk without using excessive memory, and the code is simple.</p>
</div>
<div class="post-text" itemprop="text">
<p>Your chunk size could be too large, have you tried dropping that - maybe 1024 bytes at a time? (also, you could use <code>with</code> to tidy up the syntax)</p>
<pre><code>def DownloadFile(url):
    local_filename = url.split('/')[-1]
    r = requests.get(url)
    with open(local_filename, 'wb') as f:
        for chunk in r.iter_content(chunk_size=1024): 
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)
    return 
</code></pre>
<p>Incidentally, how are you deducing that the response has been loaded into memory?</p>
<p>It sounds as if python isn't flushing the data to file, from other <a href="https://stackoverflow.com/questions/7127075/what-exactly-the-pythons-file-flush-is-doing">SO questions</a> you could try <code>f.flush()</code> and <code>os.fsync()</code> to force the file write and free memory;</p>
<pre><code>    with open(local_filename, 'wb') as f:
        for chunk in r.iter_content(chunk_size=1024): 
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)
                f.flush()
                os.fsync(f.fileno())
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Not exactly what OP was asking, but... it's ridiculously easy to do that with <code>urllib</code>:</p>
<pre><code>from urllib.request import urlretrieve
url = 'http://mirror.pnl.gov/releases/16.04.2/ubuntu-16.04.2-desktop-amd64.iso'
dst = 'ubuntu-16.04.2-desktop-amd64.iso'
urlretrieve(url, dst)
</code></pre>
<p>Or this way, if you want to save it to a temporary file:</p>
<pre><code>from urllib.request import urlopen
from shutil import copyfileobj
from tempfile import NamedTemporaryFile
url = 'http://mirror.pnl.gov/releases/16.04.2/ubuntu-16.04.2-desktop-amd64.iso'
with urlopen(url) as fsrc, NamedTemporaryFile(delete=False) as fdst:
    copyfileobj(fsrc, fdst)
</code></pre>
<p>I watched the process:</p>
<pre><code>watch 'ps -p 18647 -o pid,ppid,pmem,rsz,vsz,comm,args; ls -al *.iso'
</code></pre>
<p>And I saw the file growing, but memory usage stayed at 17 MB. Am I missing something?</p>
</div>
<span class="comment-copy">@Shuman As I see you resolved the issue when switched from http:// to https:// (<a href="https://github.com/kennethreitz/requests/issues/2043" rel="nofollow noreferrer">github.com/kennethreitz/requests/issues/2043</a>). Can you please update or delete your comments because people may think that there are issues with the code for files bigger 1024Mb</span>
<span class="comment-copy">the <code>chunk_size</code> is crucial. by default it's 1 (1 byte). that means that for 1MB it'll make 1 milion iterations. <a href="http://docs.python-requests.org/en/latest/api/#requests.Response.iter_content" rel="nofollow noreferrer">docs.python-requests.org/en/latest/api/…</a></span>
<span class="comment-copy"><code>f.flush()</code> seems unnecessary. What are you trying to accomplish using it? (your memory usage won't be 1.5gb if you drop it). <code>f.write(b'')</code> (if <code>iter_content()</code> may return an empty string) should be harmless and therefore <code>if chunk</code> could  be dropped too.</span>
<span class="comment-copy">@RomanPodlinov: <code>f.flush()</code> doesn't flush data to physical disk. It transfers the data to OS. Usually, it is enough unless there is a power failure. <code>f.flush()</code> makes the code slower here for no reason. The flush happens when the correponding file buffer (inside app) is full. If you need more frequent writes; pass buf.size parameter to <code>open()</code>.</span>
<span class="comment-copy">Don't forget to close the connection with <code>r.close()</code></span>
<span class="comment-copy">Note that you may need to adjust when <a href="https://github.com/kennethreitz/requests/issues/2155" rel="nofollow noreferrer">streaming gzipped responses</a> per issue 2155.</span>
<span class="comment-copy">THIS should be the correct answer! The <a href="https://stackoverflow.com/a/16696317/2662454">accepted</a> answer gets you up to 2-3MB/s. Using copyfileobj gets you to ~40MB/s. Curl downloads (same machines, same url, etc) with ~50-55 MB/s.</span>
<span class="comment-copy">To make sure the Requests connection gets released, you can use a second (nested) <code>with</code> block to make the request: <code>with requests.get(url, stream=True) as r:</code></span>
<span class="comment-copy">@ChristianLong: That's true, but only very recently, as the feature to support <code>with requests.get()</code> was only merged on 2017-06-07!  Your suggestion is reasonable for people who have Requests 2.18.0 or later.  Ref: <a href="https://github.com/requests/requests/issues/4136" rel="nofollow noreferrer">github.com/requests/requests/issues/4136</a></span>
<span class="comment-copy">A small caveat for using <code>.raw</code> is that it does not handle decoding. Mentioned in the docs here: <a href="http://docs.python-requests.org/en/master/user/quickstart/#raw-response-content" rel="nofollow noreferrer">docs.python-requests.org/en/master/user/quickstart/…</a></span>
<span class="comment-copy">I use System Monitor in Kubuntu. It shows me that python process memory increases (up to 1.5gb from 25kb).</span>
<span class="comment-copy">That memory bloat sucks, maybe <code>f.flush(); os.fsync()</code> might force a write an memory free.</span>
<span class="comment-copy">it's <code>os.fsync(f.fileno())</code></span>
<span class="comment-copy">You need to use stream=True in the requests.get() call. That's what's causing the memory bloat.</span>
<span class="comment-copy">minor typo: you miss a colon (':') after <code>def DownloadFile(url)</code></span>
<span class="comment-copy">For Python 2.x, use <code>from urllib import urlretrieve</code></span>
