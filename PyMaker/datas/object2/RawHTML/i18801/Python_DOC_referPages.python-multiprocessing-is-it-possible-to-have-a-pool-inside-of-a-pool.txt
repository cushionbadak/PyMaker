<div class="post-text" itemprop="text">
<p>I have a module A that does a basic map/reduce by taking data and sending it to modules B, C, D, etc for analysis and then joining their results together.</p>
<p>But it appears that modules B, C, D, etc cannot themselves create a multiprocessing pool, or else I get</p>
<pre><code>AssertionError: daemonic processes are not allowed to have children
</code></pre>
<p>Is it possible to parallelize these jobs some other way?</p>
<p>For clarity, here's a(n admittedly bad) baby example. (I would normally try/catch but you get the gist.)</p>
<pre><code>A.py:

  import B
  from multiprocessing import Pool

  def main():
    p = Pool()
    results = p.map(B.foo,range(10))
    p.close()
    p.join()
    return results


B.py:

  from multiprocessing import Pool

  def foo(x):
    p = Pool()
    results = p.map(str,x)
    p.close()
    p.join()
    return results
</code></pre>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>is it possible to have a pool inside of a pool?</p>
</blockquote>
<p>Yes, it is possible though it might not be a good idea unless you want to raise <a href="https://mail.python.org/pipermail/python-list/2011-March/600152.html" rel="nofollow noreferrer">an army of zombies</a>. From <a href="https://stackoverflow.com/a/8963618/4279">Python Process Pool non-daemonic?</a>:</p>
<pre><code>import multiprocessing.pool
from contextlib import closing
from functools import partial

class NoDaemonProcess(multiprocessing.Process):
    # make 'daemon' attribute always return False
    def _get_daemon(self):
        return False
    def _set_daemon(self, value):
        pass
    daemon = property(_get_daemon, _set_daemon)

# We sub-class multiprocessing.pool.Pool instead of multiprocessing.Pool
# because the latter is only a wrapper function, not a proper class.
class Pool(multiprocessing.pool.Pool):
    Process = NoDaemonProcess

def foo(x, depth=0):
    if depth == 0:
        return x
    else:
        with closing(Pool()) as p:
            return p.map(partial(foo, depth=depth-1), range(x + 1))

if __name__ == "__main__":
    from pprint import pprint
    pprint(foo(10, depth=2))
</code></pre>
<h3>Output</h3>
<pre><code>[[0],
 [0, 1],
 [0, 1, 2],
 [0, 1, 2, 3],
 [0, 1, 2, 3, 4],
 [0, 1, 2, 3, 4, 5],
 [0, 1, 2, 3, 4, 5, 6],
 [0, 1, 2, 3, 4, 5, 6, 7],
 [0, 1, 2, 3, 4, 5, 6, 7, 8],
 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],
 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]
</code></pre>
<p><a href="http://docs.python.org/3/library/concurrent.futures.html" rel="nofollow noreferrer"><code>concurrent.futures</code></a> supports it by default:</p>
<pre><code># $ pip install futures # on Python 2
from concurrent.futures import ProcessPoolExecutor as Pool
from functools import partial

def foo(x, depth=0):
    if depth == 0:
        return x
    else:
        with Pool() as p:
            return list(p.map(partial(foo, depth=depth-1), range(x + 1)))

if __name__ == "__main__":
    from pprint import pprint
    pprint(foo(10, depth=2))
</code></pre>
<p>It produces the same output.</p>
<blockquote>
<p>Is it possible to parallelize these jobs some other way?</p>
</blockquote>
<p>Yes. For example, look at how <a href="http://docs.celeryproject.org/en/latest/userguide/canvas.html#the-primitives" rel="nofollow noreferrer"><code>celery</code> allows to create a complex workflow</a>.</p>
</div>
<span class="comment-copy">Thanks for pointing out that <code>futures</code> support it by default.</span>
<span class="comment-copy">Didn't even know about <code>concurrent.futures</code> module - thank you! Real life (and time) saver!</span>
<span class="comment-copy">What is the army of zombies?</span>
<span class="comment-copy">@FrankMeulenaar: <a href="https://askubuntu.com/a/427222/3712">What is a <code>&lt;defunct&gt;</code> process, and why doesn't it get killed?</a></span>
<span class="comment-copy">@jfs: Okay, your original answer linked to a python mail discussion and I didn't see the connection with zombies.</span>
