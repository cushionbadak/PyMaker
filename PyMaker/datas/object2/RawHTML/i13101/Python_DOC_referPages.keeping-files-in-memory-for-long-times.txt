<div class="post-text" itemprop="text">
<p>I am working with a relatively large file (approximately 2GB). Its content is continously needed during a while loop that runs for at least 1-2 days.</p>
<p>Having enough RAM, I load the whole file in memory before the loop, using: </p>
<pre><code>f = open(filename)
lines = f.readlines()

while ...
    #using different portions of the file (randomly picked)
</code></pre>
<ul>
<li><p>I am wondering whether in doing so I will face memory management issues, if the program is to run for long times. Will the file with its full content stay intact in memory for however long it may be needed? If not, what alternatives do I have?</p></li>
<li><p>Of course initially I did try to do things properly, by only reading the parts I need for every iteration of the loop, using islice from itertools, and setting the iterator back to 0 using seek(0) to prepare for the subsequent run of the loop. But it runs very slowly since the file is large and the while loop is long.</p></li>
</ul>
<hr/>
<p>More clarification, after comments:</p>
<p>When I wasn't loading it in memory, I was basically doing: </p>
<pre><code>from itertools import islice 
f = open(filename) 
while ...:
    for line in islice(f, start_line, end_line): 
        text += line 
    f.seek(0) 
</code></pre>
<p>And it was really slow compared to when I loaded all in memory as follows:</p>
<pre><code>lines = f.readlines() 
while...: 
    for i in range(start_line, end_line): text += lines[i]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The datatype which you are keeping in the memory is a list, not a file object, so Python will be especially careful not to garbage-collect it as you are using that list later.</p>
<p>It doesn't matter if you are not using it in close sequence. Python analyzes the code before compiling it, and he knows that you will be using this list later.</p>
<p>Anyway, if you are using seek() and tell() on the file object, I don't see why it would be slow.</p>
<p>Unless your lines are big as elephants.</p>
<p>Seek moves the read/write pointer to the block of memory where you wish (inside a file). When you afterward do f.readline(), it jumps directly there.</p>
<p>Shouldn't be slow. If you use that you will avoid the possibility that some other program crashes because Python reserved a lot of memory.</p>
<p>Further more, Python lists, aren't exactly indefinite. I think that it can hold some more over 10**7 items on 32-bit PC.</p>
<p>So it does matter how many lines you have as well.</p>
<p>Example for fast random line reading directly from HD/SSD/Flash:</p>
<pre><code>from random import randint
from time import sleep

f = open("2GB.file", "rb")
linemap = [] # Keeps the start and end position of each line
for x in f:
    linemap.append((f.tell(), len(x)))
    # It is slightly faster to have start and length than only start and then f.readline()
    # But either way will work OK for you

def getline (index):
    line = linemap[index]
    f.seek(line[0])
    return f.read(line[1])

def getslice (start=0, stop=None):
    if stop==None: stop = len(linemap)
    howmany = 0
    for x in xrange(start, stop): howmany += linemap[x][1]
    f.seek(linemap[start][0])
    return f.read(howmany).splitlines(1)

while True:
    print getline(randint(0, len(linemap)-1))
    sleep(2)
</code></pre>
<p>Of course, speed can never match direct access from RAM. Just to be clear. But this is fast as thunder compare to your solution with islice(). While you can actually use islice() to do the same thing with same speed, but you will have to seek even then and code will become a bit confusing.</p>
</div>
<div class="post-text" itemprop="text">
<p>To explain as per my comment, you may create a function to return an in-memory bytes buffer, and cache the function to have more controls over merely a variable.</p>
<p>For example (if you are on python3.2+, 3.3+ with "typed" option):</p>
<pre><code>from functools import lru_cache
import io

@lru_cache(maxsize=None, typed=True)  # typed will cache as per different arg.
def get_cached_file(filename):
    m = io.BytesIO()
    with open(filename, 'rb') as f:
        m.write(f.read())
    return m
</code></pre>
<hr/>
<p>Usage:</p>
<pre><code>a = get_cached_file('a.file')
b = get_cached_file('b.file')

# since the files are new to cache, they belong "misses"
get_cached_file.cache_info()
CacheInfo(hits=0, misses=2, maxsize=None, currsize=2)

a1 = get_cached_file('a.file')
b2 = get_cached_file('b.file')

# simply return the result from cache, ie. "hits"
get_cached_file.cache_info()
CacheInfo(hits=2, misses=2, maxsize=None, currsize=2)
</code></pre>
<p>To read the buffers, you just need to <code>seek(0)</code> or whatever you want with it.</p>
<hr/>
<p>You can also clear the cache:</p>
<pre><code>get_cached_file.cache_clear()

# now its counter reset to "0"
get_cached_file.cache_info()
CacheInfo(hits=0, misses=0, maxsize=None, currsize=0)
</code></pre>
<p>You can read more <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache" rel="nofollow">here</a></p>
<p>If you are on python2.x, look out for some existing library for caching in memory, such as memcached or redis. You can of course also implement your own caching.</p>
<p>Hope this helps.</p>
</div>
<span class="comment-copy">Unless the computer tries to load more into RAM than it has room for, no, it isn't an issue to have things in RAM for a long time. Computers don't get bored or tired.</span>
<span class="comment-copy">why not create a function to return the file, and cache the function itself?</span>
<span class="comment-copy">there are more advantages using cache over an in-memory variable, ie. control over cache duration, invalidate it etc. Although there are all sitting in memory.</span>
<span class="comment-copy">@user929304, answer added, although very roughly I hope you get the idea.</span>
<span class="comment-copy">If the file is in CSV format or similar, you may want to use the <a href="http://pandas.pydata.org/" rel="nofollow noreferrer">Pandas</a> library. Pandas has an very <i>efficient</i> and <i>fast</i> method to store large files into memory.</span>
<span class="comment-copy">Both. Seeking to zero meant that you were reading the file from the beginning over and over again. Never use anything from itertools before seeing the code of a tool you wish to use. Itertools is handy, but it is not universal for all purposes like some people would like it to be.</span>
<span class="comment-copy">No and yes. 1st, enumerate() will take time and memory. Just have another variable that you will increment at the end of loop. Second, just take a slice out of list, if you have it in memory: lst[10:50] i.e. lst[start:stop:step] No ifs and iterating needed. This is what islice() does over a file object directly, but it returns an iterator, not a list.</span>
<span class="comment-copy">That means that you have to iterate over returned object with a loop, or call the next() method to get the first item in the slice, then next() again, until the StopIteration exception is raised. If you need this kind of behavior, If you still need this feature. At the end of iteration where you get your result, call f.tell() to see where are you. Or make a memory map for all indexes (where each line begins) and keep only that in RAM. Then you can seek() around wherever you need.</span>
<span class="comment-copy">See my edit, I added an example. Hope it is what you need. You should copy your code samples from a comment to your Q, instead of posting only pseudo-code. So that it is clearer to everyone what you want to achieve.</span>
<span class="comment-copy">Exactly, but better do it like I did it in getslice() function I added. It is always best to read as much bytes as possible at once. You may add linemap as an argument if you'd like, it makes no difference, except that you may have more maps for more files at the same time. That would be cool.</span>
<span class="comment-copy">@user929304, not a problem. As I said, they are all living in the memory, but you have more controls using caching, and choose when to invalidate them.</span>
