<div class="post-text" itemprop="text">
<p>Every recipe that I've found for creating a daemon process in Python involves forking twice (for Unix) and then closing all open file descriptors. (See <a href="http://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/">http://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/</a> for an example).</p>
<p>This is all simple enough but I seem to have an issue. On the production machine that I am setting up, my daemon is aborting - silently since all open file descriptors were closed. I am having a tricky time debugging the issue currently and am wondering what the proper way to catch and log these errors are.</p>
<p>What is the right way to setup logging such that it continues to work after daemonizing? Do I just call <code>logging.basicConfig()</code> a second time after daemonizing? What's the right way to capture <code>stdout</code> and <code>stderr</code>? I am fuzzy on the details of why all the files are closed. Ideally, my main code could just call <code>daemon_start(pid_file)</code> and logging would continue to work.</p>
</div>
<div class="post-text" itemprop="text">
<p>I use the <code>python-daemon</code> library for my daemonization behavior. </p>
<p>Interface described here: </p>
<ul>
<li><a href="http://www.python.org/dev/peps/pep-3143/">http://www.python.org/dev/peps/pep-3143/</a></li>
</ul>
<p>Implementation here:</p>
<ul>
<li><a href="http://pypi.python.org/pypi/python-daemon/">http://pypi.python.org/pypi/python-daemon/</a></li>
</ul>
<p>It allows specifying a <code>files_preserve</code> argument, to indicate any file descriptors that should <em>not</em> be closed when daemonizing.</p>
<p>If you need logging via the <em>same</em> <code>Handler</code> instances before and after daemonizing, you can:</p>
<ol>
<li>First set up your logging Handlers using <code>basicConfig</code> or <code>dictConfig</code> or whatever.</li>
<li>Log stuff</li>
<li>Determine what file descriptors your <code>Handler</code>s depend on. Unfortunately this is dependent on the <code>Handler</code> subclass. If your first-installed <code>Handler</code> is a <code>StreamHandler</code>, it's the value of <code>logging.root.handlers[0].stream.fileno()</code>; if your second-installed <code>Handler</code> is a <code>SyslogHandler</code>, you want the value of <code>logging.root.handlers[1].socket.fileno()</code>; etc. This is messy :-(</li>
<li>Daemonize your process by creating a <code>DaemonContext</code> with <code>files_preserve</code> equal to a list of the file descriptors you determined in step 3.</li>
<li>Continue logging; your log files should not have been closed during the double-fork.</li>
</ol>
<p>An alternative might be, as @Exelian suggested, to actually use different <code>Handler</code> instances before and after the daemonziation. Immediately after daemonizing, destroy the existing handlers (by <code>del</code>ing them from <code>logger.root.handlers</code>?) and create identical new ones; you can't just re-call <code>basicConfig</code> because of the issue that @dave-mankoff pointed out.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can simplify the code for this if you set up your logging handler objects separately from your root logger object, and then add the handler objects as an independent step rather than doing it all at one time.  The following should work for you.</p>
<pre><code>import daemon
import logging

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)
fh = logging.FileHandler("./foo.log")
logger.addHandler(fh)

context = daemon.DaemonContext(
   files_preserve = [
      fh.stream,
   ],
)

logger.debug( "Before daemonizing." )
context.open()
logger.debug( "After daemonizing." )
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>We just had a similar issue, and due to some things beyond my control, the daemon stuff was separate from the stuff creating the logger. However, logger has a .handlers and .parent attributes that make it possible with something like:</p>
<pre><code>    self.files_preserve = self.getLogFileHandles(self.data.logger)

def getLogFileHandles(self,logger):
    """ Get a list of filehandle numbers from logger
        to be handed to DaemonContext.files_preserve
    """
    handles = []
    for handler in logger.handlers:
        handles.append(handler.stream.fileno())
    if logger.parent:
        handles += self.getLogFileHandles(logger.parent)
    return handles
</code></pre>
</div>
<span class="comment-copy">Call the logging config AFTER daemonizing is indeed the way to go.</span>
<span class="comment-copy">I noticed this comment in the logging docs: "This function does nothing if the root logger already has handlers configured for it." If I want logging before and after daemonizing, how does that affect the situation?</span>
<span class="comment-copy">If I'm correct it's possible to add handlers/filters after initializing the logger. This means you could add a FileHandler before starting the daemon context and add another after starting it. I'm not entirely sure this works though.</span>
<span class="comment-copy">This answer is life saver, thank you!</span>
<span class="comment-copy">Remember to call <a href="https://pagure.io/python-daemon/blob/master/f/daemon/daemon.py#_400" rel="nofollow noreferrer"><code>context.close()</code></a> or use <a href="https://docs.python.org/3/reference/compound_stmts.html#the-with-statement" rel="nofollow noreferrer"><code>with</code></a> statement.</span>
<span class="comment-copy">This solution worked with daemonizing a python custom management command.</span>
<span class="comment-copy">this is the cleanest way! thanks!</span>
