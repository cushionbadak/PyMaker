<div class="post-text" itemprop="text">
<p>I was working on <a href="http://projecteuler.net/problem=204" rel="nofollow">Project Euler</a> and wondered if I could speed up my solution using PyPy. However, I found results quite disappointing, as it took more time to compute.</p>
<pre><code>d:\projeuler&gt;pypy problem204.py
3462.08630405 mseconds

d:\projeuler&gt;python problem204.py
1823.91602542 mseconds
</code></pre>
<p>Since mseconds output were calculated using python's <code>time</code> modules, so I ran it again using builtin benchmark commands. </p>
<pre><code>d:\projeuler&gt;pypy -mtimeit -s "import problem204" "problem204._main()"
10 loops, best of 3: 465 msec per loop

d:\projeuler&gt;python -mtimeit -s "import problem204" "problem204._main()"
10 loops, best of 3: 1.87 sec per loop
</code></pre>
<p>PyPy reports that it took about half second to finish running. However, I tried running pypy problem204 several times and outputs were never even close to benchmarked .5 seconds. unlike pypy, python's mtimeit results are consistent with outputs. Is pypy giving me inaccurate benchmarks, or is there some magic I don't understand?</p>
</div>
<div class="post-text" itemprop="text">
<p>Note that timeit</p>
<ol>
<li>runs the statement several times (10 in your case), and</li>
<li>does that several times (3 by default) and gives the minimum of that, for reasons <a href="http://docs.python.org/3/library/timeit.html#timeit.Timer.repeat">outlined in the documentation</a>.</li>
</ol>
<p>It depends on your code, but it's entirely possible that the JIT compiler is to blame for this confusing result. The JIT warmup overhead is incurred every time you launched a new pypy process, but only once during the timeit benchmark (because that one runs <code>_main</code> several times in the same process). Moreover, if some part of your code is run so often that it's not compiled when <code>_main</code> runs once, but only when it runs, say, three times, subsequent runs will also be faster, which further removes the best result from the first one (i.e. the one for running <code>pypy problem204.py</code> once).</p>
<p>The <code>timeit</code> result is correct in that it (roughly) matches how fast the code will be in the best case - warmed-up JIT compiler, rarely losing the CPU to other programs, etc.
Your problem is that you want to know something different - the time including JIT warmup.</p>
</div>
<span class="comment-copy">+1. I would use <code>timeit</code> if you needed to compare speeds of a subroutine for a program that runs indefinitely like a web application, and the command-line <code>time</code> wrapper (not the python module) for a program that runs once like this one.</span>
<span class="comment-copy">@AndrewGorcester Agreed, though <code>time</code> is sadly not available on some platforms (<i>mumble</i> Windows <i>mumble</i>). Powershell apparently has an equivalent, though I am yet to figure out how the hell it works.</span>
<span class="comment-copy">note that timeit does few more things (like disabling GC in CPython). Also, for reasons outlined in multiple papers, the minimum is incorrect (say you have a gc collect every 3 loops, then the minimum will skip the hard ones). PyPy did not modify timeit, but it makes even less sense on an interpreter with a JIT compiler.</span>
<span class="comment-copy">Thanks. I can see my code speeding up gradually on each time of execution with pypy. It feels like cheating though.</span>
