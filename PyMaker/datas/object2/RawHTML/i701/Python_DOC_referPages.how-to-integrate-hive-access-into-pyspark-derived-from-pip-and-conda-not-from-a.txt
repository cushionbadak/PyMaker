<div class="post-text" itemprop="text">
<p>I build and programmatically use my <code>PySpark</code> environment from the ground-up via <code>conda</code> and <code>pip</code> pyspark (like I demonstrate <a href="https://jupyter.ai/pyspark-session/" rel="nofollow noreferrer">Here</a>); rather than use <code>PySpark</code> from the downloadable Spark distribution. As you can see in the first code-snippet of the URL above, I accomplish this through (among other things) k/v conf-pairs in my SparkSession startup script. (By the way, this approach enables me to do work in various REPLs, IDEs and JUPYTER).</p>
<p>However, with respect to configuring Spark support for accessing HIVE databases and metadata-stores, the manual says this:</p>
<blockquote>
<p>Configuration of <code>Hive</code> is done by placing your <code>hive-site.xml</code>, <code>core-site.xml</code> (for security configuration), and <code>hdfs-site.xml</code> (for HDFS configuration) file in <code>conf/</code>.</p>
</blockquote>
<p>By <code>conf/</code> above they mean the <code>conf/</code> directory in the Spark distribution package. But <code>pyspark</code> via <code>pip</code> and <code>conda</code> doesn't have that directory, of course, so how might HIVE database and metastore support be plugged into Spark in that case?</p>
<p>I suspect this might be accommodated by specially-prefixed SparkConf K/V pairs of the form: <code>spark.hadoop.*</code> (see <a href="https://spark.apache.org/docs/latest/configuration.html#custom-hadoophive-configuration" rel="nofollow noreferrer">here</a>); and if yes, I'd still need to determine which HADOOP / HIVE / CORE directives are needed. I guess I'll trial &amp; error that. :)</p>
<p><strong>Note</strong>: <code>.enableHiveSupport()</code> has already been included.</p>
<p>I'll tinker with <code>spark.hadoop.*</code> K/V pairs, but if anyone knows how this is done offhand, please do let me know.</p>
<p>Thank you. :)</p>
<p><strong>EDIT</strong>: After the solution was provided, I updated the content in the <a href="https://jupyter.ai/pyspark-session/" rel="nofollow noreferrer">first URL above</a>. It now integrates the <code>SPARK_CONF_DIR</code> and <code>HADOOP_CONF_DIR</code> environment variable approach discussed below.</p>
</div>
<div class="post-text" itemprop="text">
<p>In this case I'd recommend <a href="https://spark.apache.org/docs/latest/configuration.html" rel="nofollow noreferrer">the official configuration guide</a> (emphasis mine):</p>
<blockquote>
<p>If you plan to read and write from HDFS using Spark, there are two Hadoop configuration files that should be included on Spark’s classpath:</p>
<ul>
<li>hdfs-site.xml, which provides default behaviors for the HDFS client.</li>
<li>core-site.xml, which sets the default filesystem name.</li>
</ul>
<p>(...)</p>
<p><strong>To make these files visible to Spark, set <code>HADOOP_CONF_DIR</code> in <code>$SPARK_HOME/conf/spark-env.sh</code></strong> to a location containing the configuration files.</p>
</blockquote>
<p><a href="https://spark.apache.org/docs/latest/configuration.html#overriding-configuration-directory" rel="nofollow noreferrer">Additionally</a>:</p>
<blockquote>
<p><strong>To specify a different configuration directory other than the default “<code>SPARK_HOME/conf</code>”, you can set <code>SPARK_CONF_DIR</code></strong>. Spark will use the configuration files (spark-defaults.conf, spark-env.sh, log4j.properties, etc) from this directory.</p>
</blockquote>
<p>So it is possible to use arbitrary directory accessible to your Spark installation to place desired configuration files, and <code>SPARK_CONF_DIR</code> and / or <code>HADOOP_CONF_DIR</code> can be easily set directly in your script, using <a href="https://docs.python.org/3/library/os.html#os.environ" rel="nofollow noreferrer"><code>os.environ</code></a>.</p>
<p>Finally there is even no need for separate Hadoop configuration files, most of the time, as Hadoop specific properties can be set directly in Spark documentation, using <code>spark.hadoop.*</code> prefix.</p>
</div>
<span class="comment-copy">Thank you. I totally forgot about <code>SPARK_CONF_DIR</code>; and I agree with you in preferring to use <code>spark.hadoop.*</code> prefixed-keys over separate Hadoop XML configuration files. I'm working on the latter now. Thank you for confirming that in this, the marked, answer.</span>
