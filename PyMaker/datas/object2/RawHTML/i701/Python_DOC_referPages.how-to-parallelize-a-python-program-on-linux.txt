<div class="post-text" itemprop="text">
<p>I have a script that takes in <code>input a list of filenames</code> and loops over them to generate an output file per input file, so this is a case which can be easily parallelized I think.</p>
<p>I have a 8 core machine.</p>
<p>I tried on using <code>-parallel</code> flag on this command:</p>
<pre><code>python perfile_code.py list_of_files.txt
</code></pre>
<p>But I can't make it work, i.e. specific question is: how to use parallel in bash with a python command in Linux, along with the arguments for the specific case mentioned above.</p>
<p>There is a Linux parallel command (<code>sudo apt-get install parallel</code>), which I read somewhere can do this job but I don't know how to use it.</p>
<p>Most of the internet resources explain how to do it in <a href="https://stackoverflow.com/questions/31864503/python-how-to-parallel-consume-and-operate-on-files-in-a-directory">python</a> but can it be done in bash?</p>
<p>Please help, thanks.</p>
<pre><code>Based on an answer, here is a working example that is still not working, please suggest how to make it work.
</code></pre>
<p>I have a folder with 2 files, i just want to create their duplicates with a different name parallely in this example. </p>
<pre><code># filelist is the directory containing two file names, a.txt and b.txt.
# a.txt is the first file, b.xt is the second file
# i pass an .txt file with both the names to the main program

from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
import sys

def translate(filename):
    print(filename)
    f = open(filename, "r")
    g = open(filename + ".x", , "w")
    for line in f:
        g.write(line)

def main(path_to_file_with_list):
    futures = []
    with ProcessPoolExecutor(max_workers=8) as executor:
        for filename in Path(path_to_file_with_list).open():
            executor.submit(translate, "filelist/" + filename)
        for future in as_completed(futures):
            future.result()

if __name__ == "__main__":
     main(sys.argv[1])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can just use an ordinary shell <code>for</code> command, and append the <code>&amp;</code> background indicator to the <em>python</em> command inside the <code>for</code>:</p>
<pre><code>for file in `cat list_of_files.txt`;
   do python perfile_code.py $file &amp;
done
</code></pre>
<p>Of course, assuming your python code will generate separate outputs by itself.</p>
<p>It is just this simple. 
Although not usual - in general people will favor using Python itself to control the parallel execution of the loop, if you can edit the program. One nice way to do is to use <code>concurrent.futures</code> in Python to create a worker pool with 8 workers - the shell approach above will launch all instances in parallel at once.</p>
<p>Assuming your code have a <code>translate</code> function that takes in a filename, your Python code could be written as:</p>
<pre><code>from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path:

def translate(filename):
    ...

def main(path_to_file_with_list):
    futures = []
    with ProcessPoolExecutor(max_workers=8) as executor:
        for filename in Path(path_to_file_with_list).open():
            executor.submit(translate, filename)
        for future in as_completed(futures):
            future.result()

if __name__ == "__main__":
     import sys
     main(argv[1])
</code></pre>
<p>This won't depend on special shell syntax, and takes care of corner cases, and number-or-workers handling, which could be hard to do  properly from bash.</p>
</div>
<div class="post-text" itemprop="text">
<p>Based on your comment,</p>
<blockquote>
<p>@Ouroborus no, no consider this opensource.com/article/18/5/gnu-parallel i want to run a python program along with this parallel..for a very specific case..if an arbitrary convert program can be piped to parallel ..why wouldn't a python program?</p>
</blockquote>
<p>I think this might help:</p>
<p><code>convert</code> wasn't chosen arbitrarily. It was chosen because it is a better known program that (roughly) maps a single input file, provided via the command line, to a single output file, also provided via the command line.</p>
<p>The typical shell <code>for</code> loop can be used to iterate over a list. In the article you linked, they show an example</p>
<pre><code>for i in *jpeg; do convert $i $i.png ; done
</code></pre>
<p>This (again, roughly) takes a list of file names and applies them, one by one, to a command template and then runs that command.</p>
<p>The issue here is that <code>for</code> would necessarily wait until a command is finished before running the next one and so may under-utilize today's multi-core processors.</p>
<p><code>parallel</code> acts a kind of replacement for <code>for</code>. It makes the assumption that a command can be executed multiple times simultaneously, each with different arguments, without each instance interfering with the others.</p>
<p>In the article, they show a command using <code>parallel</code></p>
<pre><code>find . -name "*jpeg" | parallel -I% --max-args 1 convert % %.png
</code></pre>
<p>that is equivalent to the previous <code>for</code> command. The difference (still roughly) is that <code>parallel</code> runs several variants of the templated command simultaneously without necessarily waiting for each to complete.</p>
<hr/>
<p>For your specific situation, in order to be able to use <code>parallel</code>, you would need to:</p>
<ul>
<li>Adjust your python script so that it takes one input (such as a file name) and one output (also possibly a file name), both via the command line.</li>
<li>Figure out how to setup <code>parallel</code> so that it can receive a list of those file names for insertion into a command template to run your python script on each of those files individually.</li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>It is unclear from your question how you run your tasks in serial. But if we assume you run:</p>
<pre><code>python perfile_code.py file1
python perfile_code.py file2
python perfile_code.py file3
:
python perfile_code.py fileN
</code></pre>
<p>then the simple way to parallelize this would be:</p>
<pre><code>parallel python perfile_code.py ::: file*
</code></pre>
<p>If you have a list of files with one line per file then use:</p>
<pre><code>parallel python perfile_code.py :::: filelist.txt
</code></pre>
<p>It will run one job per cpu thread in parallel. So if <code>filelist.txt</code> contains 1000000 names, then it will not run them all at the same time, but only start a new job when one finishes.</p>
</div>
<span class="comment-copy">why vote for close? This is a very specific question, asking how to use parallel in bash with python, along with arguments. I have edited the question to make it more clear, please reconsider.</span>
<span class="comment-copy">You're showing fundamental lack of knowledge regarding the topic of parallelism. <code>-parallel</code> isn't a valid command line option for Python. Programming for parallel operations usually requires proactive development strategies on the part of the programmer. I'd suggest googling "python parallel".</span>
<span class="comment-copy">@Ouroborus no, no consider this <a href="https://opensource.com/article/18/5/gnu-parallel" rel="nofollow noreferrer">opensource.com/article/18/5/gnu-parallel</a> i want to run a python program along with this parallel..for a very specific case..if an arbitrary convert program can be piped to parallel ..why wouldn't a python program?</span>
<span class="comment-copy">That still requires that you understand how parallism works in general  and that your software is capable of operating in that environment. As you describe it, your current python script would not benefit from gnu <code>parallel</code>. Reading and understanding the article you linked would go a long way towards you understanding what you need to do.</span>
<span class="comment-copy">There's no turn key <code>--parallel</code> flag. You need to write the parallelism yourself see: <a href="https://docs.python.org/3/library/multiprocessing.html?highlight=process" rel="nofollow noreferrer">multiprocessing</a></span>
<span class="comment-copy">thanks for a working example! i tried your example, please take a look at the edited question, it still doesnt work.</span>
<span class="comment-copy">does the example you wrote work both in python 3 and 2?</span>
<span class="comment-copy">The only Python3.5+ part there is the <code>pathlib.Path</code>. In Python 2.7, just use plain old <code>open(path_to_file_wiith_list)</code> instead, and no need to <code>from pathlib import Path</code>. <code>concurrent.futures</code> works the same in Python 2.7 and newer versions.</span>
<span class="comment-copy">This risks overloading your machine. If <code>list_of_files.txt</code> contains 1000000 names, then it is likely your machine will slow to a crawl.</span>
<span class="comment-copy">The Python version creates a pool of workers. The queued future objects use a mini,um of resources - even 1_000_000 names is peanuts in a machine with 4GB main memory.  Of course, 40_000_000 would start to be something - and if one have this big a filelist, it is just a matter of taking care to create the futures as well - it would be a matter of 6 or 7 extra LoC above.  The shell version yes, launch all processes in parallel immediately - even a couple thousand filenames would overwhelm any machine.</span>
<span class="comment-copy">could you take a look at the working example i edited in the question above?</span>
<span class="comment-copy">@Rafael Aside from an obvious syntax error, it looks like it should do what you expect. Rudimentary testing shows it works.</span>
