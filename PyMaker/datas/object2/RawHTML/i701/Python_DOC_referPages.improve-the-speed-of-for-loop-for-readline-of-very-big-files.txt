<div class="post-text" itemprop="text">
<p>I am trying to improve the speed of my for loop for reading lines from very big files. I have two files, I am taking information from first file line by line in a <code>for</code> loop and matching these each to lines from a second file through an <code>if</code> statement. Since both the files have millions of lines, this is taking too long.</p>
<p>I am posting my code here; How can I improve the loop statement to increase the speed of execution?</p>
<pre><code>#!/usr/bin/python

#open file1
f1 = open("../../Reapeat_analysis/FIMO/fimo_out/fimo.gff",'r')
#open file2
f2 = open("../BS_Forward.fastq_bismark_pe.CX_report.txt",'r')

f1.seek(0)
f2.seek(0)

#open and save output
fOut = open("output_sample_CG+.txt",'w')

#Reading file1 lines in for loop
for line1 in f1:
    line1 = line1.split('\t')
    s1 = int(line1[3])
    s2 = int(line1[4])
    s0 = str(line1[0])
    count = 0
    percent = 0
    lt = []

    #Reading file2 lines for each file1 line
    for line2 in f2:
        line2 = line2.split("\t")

        #Matching desired condition
        if (s0 == str(line2[0])) and (s1 &lt;= int(line2[1]) &lt;= s2) and (str(line2[5])=="CG") and (str(line2[2])=="+"):
            lt.append(line2)
            count = count + 1

    #saving each matched conditions
    fOut.write(str(s1) + "-" + str(s2) + ":" + str(s0) + "\t" + str(count) + "\t" + str(lt))
    f2.seek(0)
fOut.close()
</code></pre>
<p>Between 0 and 100 lines of the <code>f2</code> file match the <code>(str(line2[5])=="CG") and (str(line2[2])=="+")</code> filter.</p>
</div>
<div class="post-text" itemprop="text">
<p>You have a O(N * M) loop over file I/O, that is very slow indeed. You can improve per-line processing by using the <code>csv</code> module to do parse each line into a list for you in C code, and drop the redundant <code>str()</code> calls (you already have strings), but your real problem is the nested loop.</p>
<p>You can easily avoid that loop. There may be millions of rows in your second file, but you already filter those rows to a <strong>much</strong> smaller number, between 0 and 100. That can be trivially held in memory and accessed per <code>s0</code> value in next to no time.</p>
<p>Store the information from each row in a dictionary; pre-parse the 2nd column integer, and store the whole row for output to the output file in the <code>lt</code> list:</p>
<pre><code>import csv

# dictionary mapping row[0] to a list of (int(row[1]), line) values
report_map = {}

with open("../BS_Forward.fastq_bismark_pe.CX_report.txt", 'r', newline='') as report:
    reader = csv.reader(report, delimiter='\t')
    for row in reader:
        if row[2] != "+" or row[5] != "CG":
            continue
        key, value = row[0], int(row[1])
        line = '\t'.join(row)
        report_map.setdefault(key, []).append((value, line))
</code></pre>
<p>After building that dictionary, you can look up matches against <code>s0</code> in O(1) time, so your loop over <code>f1</code> is a straightforward loop with a cheap operation for each row. When you find a match in the <code>report_map</code> dictionary, you only need to loop over the associated list to filter on the <code>row[1]</code> integer values:</p>
<pre><code>with open("../../Reapeat_analysis/FIMO/fimo_out/fimo.gff", 'r', newline='') as fimo, \
     open("output_sample_CG+.txt", 'w', newline='') as fout:
    reader = csv.reader(fimo, delimiter='\t')
    writer = csv.writer(fout, delimeter='\t')
    for row in reader:
        s0 = row[0]
        s1, s2 = map(int, row[3:5])
        if s0 not in report_map:
            continue
        lt = [r for i, r in report_map[s0] if s1 &lt;= i &lt;= s2]
        writer.writerow(["{}-{}:{}".format(s1, s2, s0), len(lt), str(lt)])
</code></pre>
<p>I strongly recommend <em>against</em> storing the whole line from the <code>BS_Forward.fastq_bismark_pe.CX_report.txt</code> file, certainly not as a Python <a href="https://docs.python.org/3/library/functions.html#repr" rel="nofollow noreferrer">printable representation</a>. I don't know how you plan to use that data, but at least consider using JSON to serialise the <code>lt</code> list to a string representation. JSON is readable by other platforms and faster to parse back into a suitable Python data structure.</p>
</div>
<span class="comment-copy">Yes, looping over both files in a nested loop will take N * M time, with disk IO being the bottleneck. Don't do that. Find a way of caching the information from at least one file in memory, or use a database.</span>
<span class="comment-copy">you have a O(n^2) complexity ... couldnt you first process one file into some kind of lookup dict?  yeah what @MartijnPieters said :P</span>
<span class="comment-copy">How many rows in f2 actually matter, so have <code>+</code> in the 3rd and <code>CG</code> in the 6th column?</span>
<span class="comment-copy">@MartijnPieters in file2, there are 8 columns but I am matching with 4 columns only, and there are approximately 2 million rows.</span>
<span class="comment-copy">@kashiff007: out of those 2 million, how many match those criteria I mentioned? You don't use all 2 million.</span>
<span class="comment-copy">I am grateful that you teach me file hadelling with dictionary. I have to play with it to get familiar.</span>
