<div class="post-text" itemprop="text">
<p>I've been working on a performance critical application which requires frequently requires making copies of a 2D list of integers and modifying the copy (I'm implementing the minimax algorithm).</p>
<p>I've noticed there is a <em>huge</em> difference in performance between a copy, and a deepcopy on lists with the same number of elements, and I'd like to understand if my thinking is correct.</p>
<p>To reproduce my problem, run the following code: </p>
<pre><code>import numpy as np

np.random.seed(0)
lst1 = np.random.randint(100, size=1000 * 1000).tolist()
lst2 = np.random.randint(100, size=(1000, 1000)).tolist()
</code></pre>
<p>Now, timing the statements below, you should see timings similar to mine.</p>
<pre><code>%timeit copy.copy(lst1)
%timeit lst1.copy()
%timeit copy.deepcopy(lst2)

5 ms ± 49.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
5.47 ms ± 551 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
1.61 s ± 112 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>Both <code>lst1</code> and <code>lst2</code> have a million elements, but reliably copying the former is 200x faster than a nested list with the same number of elements. I thought this would have to do with the fact that making deep copies of nested lists might require some recursive implementation that is slow, so I tried</p>
<pre><code>%timeit copy.deepcopy(lst1)
1.43 s ± 90.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 
</code></pre>
<p>And the timings still show a massive slowdown. I've checked the <a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" rel="nofollow noreferrer">docs</a>  but not much explanation was offered. However, from the timings, I suspect that <code>deepcopy</code> is copying each <em>int</em> as well, creating new integers. But this seems like a wasteful thing to do. </p>
<p>Am I right in my thinking here? What is deepcopy doing here that <code>list.copy</code> and shallow copy don't?</p>
<p>I've seen <a href="https://stackoverflow.com/questions/24756712/deepcopy-is-extremely-slow">deepcopy() is extremely slow</a> but it seems that question is asking for an alternative rather than an explanation (it wasn't clear to me). </p>
</div>
<div class="post-text" itemprop="text">
<p><code>deepcopy</code> isn't copying the ints. There's no way it could do that anyway.</p>
<p><code>deepcopy</code> is slow because it needs to handle the full complexity of a deep copy, even if that turns out to be unnecessary. That includes dispatching to the appropriate copier for every object it finds, even if the copier turns out to <a href="https://github.com/python/cpython/blob/v3.7.2/Lib/copy.py#L190" rel="nofollow noreferrer">basically just be <code>lambda x: x</code></a>. That includes maintaining a memo dict and keeping track of every object copied, to handle duplicate references to the same objects, even if there are none. That includes special copy handling for data structures like <a href="https://github.com/python/cpython/blob/v3.7.2/Lib/copy.py#L210" rel="nofollow noreferrer">lists</a> and <a href="https://github.com/python/cpython/blob/v3.7.2/Lib/copy.py#L236" rel="nofollow noreferrer">dicts</a>, so it doesn't go into an infinite recursion when trying to copy a data structure with recursive references.</p>
<p>All of that has to be done no matter whether it pays off. All of it is expensive.</p>
<p>Also, <code>deepcopy</code> is pure-Python. That doesn't help. Comparing <code>deepcopy</code> to <code>pickle.loads(pickle.dumps(whatever))</code>, which performs a very similar job, <code>pickle</code> wins handily due to the C implementation. (On Python 2, replace <code>pickle</code> with <code>cPickle</code>.) <code>pickle</code> still loses hard to an implementation that takes advantage of the known structure of the input, though:</p>
<pre><code>In [15]: x = [[0]*1000 for i in range(1000)]

In [16]: %timeit copy.deepcopy(x)
1.05 s ± 5.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [17]: %timeit pickle.loads(pickle.dumps(x))
78 ms ± 4.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [18]: %timeit [l[:] for l in x]
4.56 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>In programming, deep copy is equivalent to a physical copy of something. It is an actual copy of the original object. In most programming tools, you can play around with it, modify it without affecting the original object.
However, on the other hand, a shallow copy is a reference to the original object. If you change it, it will affect the original object as well.
In short, since the deep copy is the actual copy of the original object, it is heavier that the shallow copy which just points to the original object.</p>
<p>Shallow copy: You can have a picture(s) of your new furniture, and get an idea of what it really looks like. You can easily carry around the picture.</p>
<p>Deep copy: You can go to the furniture shop, and look at the real furniture. You probably can't carry around easily, and you may need some help to take it home.</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://docs.python.org/2/library/copy.html" rel="nofollow noreferrer">https://docs.python.org/2/library/copy.html</a></p>
<p>The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances):</p>
<ol>
<li>A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original.</li>
<li>A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original</li>
</ol>
<p>So effectively, the shallow copy will create a new list and populate it with a reference to every element in the original list. Because every element in the original list is itself a list, it's much faster to just store a reference to this than it is to create a new copy. 
Deepcopy does some clever stuff in how it copies each element in order to avoid errors. But in essence you don't need to understand that to know why one shallowcopy is faster than deepcopy....</p>
</div>
<span class="comment-copy">It may indeed be wasteful, but that's what <code>deepcopy</code> does: it copies <i>everything</i>. It doesn't know that you only want to copy the lists.</span>
<span class="comment-copy">It doesn't copy everything (it doesn't copy immutable built-in types), but it checks everything and maintains a cache of all seen objects</span>
<span class="comment-copy">Thank you for the comments, folks. Please consider fleshing them out as an answer. It would seem like my only option here is to switch to a 1D implementation and implement some logic to convert 2D indices to 1D indices when accessing list elements in my code.</span>
<span class="comment-copy">Note, here is the source code: <a href="https://github.com/python/cpython/blob/master/Lib/copy.py#L128" rel="nofollow noreferrer">github.com/python/cpython/blob/master/Lib/copy.py#L128</a></span>
<span class="comment-copy">If you know exactlywhat your list is shaped like you can probably implement your own naive copy operation, i.e.: <code>[sub.copy() for sub in nested_list]</code>. This will be much faster</span>
<span class="comment-copy">"However, on the other hand, a shallow copy is a reference to the original object. If you change it, it will affect the original object as well. " This is not true. <code>a = []; import copy; b = copy.copy(a); a.append('foo'); print('b:',b,'a:',a)</code>.</span>
