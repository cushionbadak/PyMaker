<div class="post-text" itemprop="text">
<p>Copying a file in Python3 using the following code takes a lot of time:</p>
<p><code>shutil.copy(self.file, self.working_dir)</code></p>
<p>However, the <code>cp</code> command of Linux is pretty fast. If I try to execute the bash command from Python3 for copying files with sizes greater than 100GB, will that be a reliable option for production servers?</p>
<p>I have seen <a href="https://stackoverflow.com/a/11584547/10834788">this</a> answer but its suggestions are not fast.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you are running on Windows, Python's copy buffer size may be too small: <a href="https://stackoverflow.com/a/28584857/679240">https://stackoverflow.com/a/28584857/679240</a></p>
<p>You would need to implement something similar to this (warning: untested):</p>
<pre><code>def copyfile_largebuffer(src, dst, length=16*1024*1024):
    with open(newfile, 'wb') as outfile, open(oldfile, 'rb') as infile:
        copyfileobj_largebuffer(infile, outfile, length=length)

def copyfileobj_largebuffer(fsrc, fdst, length=16*1024*1024):
    while 1:
        buf = fsrc.read(length)
        if not buf:
            break
        fdst.write(buf)
</code></pre>
</div>
<span class="comment-copy">One possible issue you might encounter using <code>shutil.copy()</code> is that on POSIX platforms, file owner and group, aswell as ACLs <a href="https://docs.python.org/3/library/shutil.html" rel="nofollow noreferrer">will be lost</a> during transfer. <code>shutil.copyfile()</code> should suit your needs just fine. Have you ran any benchmarks and seen actual performance degradation?</span>
<span class="comment-copy">Another thing to consider is that for a 100GB file, using the main thread may block a lot and that might not be what you want, so a new thread or a subprocess would be pretty much required to background the large copy task.</span>
