<div class="post-text" itemprop="text">
<div class="question-status question-originals-of-duplicate">
<p>This question already has an answer here:</p>
<ul>
<li>
<a dir="ltr" href="/questions/588004/is-floating-point-math-broken">Is floating point math broken?</a>
<span class="question-originals-answer-count">
                    28 answers
                </span>
</li>
</ul>
</div>
<p>I know these are float point division. But why did these two formula behave differently?</p>
<p>And I did some more investigation, the result confusing me even more:</p>
<pre><code>&gt;&gt;&gt;0.9/3
0.3

&gt;&gt;&gt;1.2/3
0.39999999999999997

&gt;&gt;&gt;1.5/3
0.5
</code></pre>
<p>What's the logic here to decide whether the result is printed with one decimal place or more?</p>
<p>PS: I used python3.4 to do the experiment above.</p>
</div>
<div class="post-text" itemprop="text">
<p>Because the exact values of the floating point results are slightly different.</p>
<pre><code>&gt;&gt;&gt; '%.56f' % 0.4
'0.40000000000000002220446049250313080847263336181640625000'
&gt;&gt;&gt; '%.56f' % (0.4/2)
'0.20000000000000001110223024625156540423631668090820312500'
&gt;&gt;&gt; '%.56f' % 0.6
'0.59999999999999997779553950749686919152736663818359375000'
&gt;&gt;&gt; '%.56f' % (0.6/3)
'0.19999999999999998334665463062265189364552497863769531250'
&gt;&gt;&gt; '%.56f' % 0.2
'0.20000000000000001110223024625156540423631668090820312500'
&gt;&gt;&gt; (0.2 - 0.6/3) == 2.0**-55
True
</code></pre>
<p>As you can see, the result that is printed as "0.2" is indeed slightly closer to 0.2. I added the bit at the end to show you what the exact value of the difference between these two numbers is. (In case you're curious, the above representations are the exact values - adding any number of digits beyond this just adds more zeroes).</p>
</div>
<div class="post-text" itemprop="text">
<p>Check out the documentation on <a href="https://docs.python.org/3.4/tutorial/floatingpoint.html">floating point numbers in python</a>.</p>
<p>Most specifically:</p>
<blockquote>
<p>Interestingly, there are many different decimal numbers that share the same nearest approximate binary fraction. For example, the numbers 0.1 and 0.10000000000000001 and 0.1000000000000000055511151231257827021181583404541015625 are all approximated by 3602879701896397 / 2 ** 55. Since all of these decimal values share the same approximation, any one of them could be displayed while still preserving the invariant eval(repr(x)) == x.</p>
<p>Historically, the Python prompt and built-in repr() function would choose the one with 17 significant digits, 0.10000000000000001. Starting with Python 3.1, Python (on most systems) is now able to choose the shortest of these and simply display 0.1.</p>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<p>Floating point numbers are implemented as <code>binary64</code> according to <a href="http://en.wikipedia.org/wiki/IEEE_floating_point" rel="nofollow noreferrer">IEEE 754</a> (as in virtually all programming languages).</p>
<p>This standard gives 52 bits to the "significand / fraction" (approximately 16 decimal digits of accuracy), 11 bits to the exponent and 1 bit to the sign (plus or minus):</p>
<p><img alt="IEEE 754 bits" src="https://i.stack.imgur.com/9cWe7.png"/></p>
<p>In particular, a number like <code>0.4</code> can not be represented as</p>
<pre><code>(1 + f) * 2**(exponent)
</code></pre>
<p>for some fraction in base 2 and an exponent that can be represented with 11 bits (-1022 through 1023).</p>
<p>Viewing <code>0.4</code> in hex for example:</p>
<pre><code>&gt;&gt;&gt; (0.4).hex()
'0x1.999999999999ap-2'
</code></pre>
<p>we see the best approximation in our set of numbers is</p>
<pre><code>+ 2**(-2) * (1 + 0x999999999999a/ float(2**52))
</code></pre>
<p>Trying to represent this in base 2, we have</p>
<pre><code>2**(-2) * (1 + 0.6)
</code></pre>
<p>but <code>0.6 = 9/15 = 1001_2/1111_2</code> written in base 2 has a repeating string of four binary digits</p>
<pre><code>0.1001100011000110001...
</code></pre>
<p>so can never be represented using a finite number of binary digits.</p>
<hr/>
<h2>A bit more in depth</h2>
<p>So we can "unpack" <code>0.4</code></p>
<pre><code>&gt;&gt;&gt; import struct
&gt;&gt;&gt; # 'd' for double, '&gt;' for Big-endian (left-to-right bits)
&gt;&gt;&gt; float_bytes = struct.pack('&gt;d', 0.4)
</code></pre>
<p>as 8 bytes (1 byte is 8 bits)</p>
<pre><code>&gt;&gt;&gt; float_bytes
'?\xd9\x99\x99\x99\x99\x99\x9a'
</code></pre>
<p>or as 16 hex digits (1 hex digit is 4 bits, since <code>2**4 == 16</code>)</p>
<pre><code>&gt;&gt;&gt; ''.join(['%2x' % (ord(digit),) for digit in float_bytes])
'3fd999999999999a'
</code></pre>
<p>or as all 64 bits in their glory</p>
<pre><code>&gt;&gt;&gt; float_bits = ''.join(['%08d' % (int(bin(ord(digit))[2:]),)
...                       for digit in float_bytes])
&gt;&gt;&gt; float_bits
'0011111111011001100110011001100110011001100110011001100110011010'
</code></pre>
<p>From there, the first bit is the sign bit:</p>
<pre><code>&gt;&gt;&gt; sign = (-1)**int(float_bits[0], 2)
&gt;&gt;&gt; sign
1
</code></pre>
<p>The next 11 bits are the exponent (but shifted by 1023, a convention
of <code>binary64</code>):</p>
<pre><code>&gt;&gt;&gt; exponent = int(float_bits[1:1 + 11], 2) - 1023
&gt;&gt;&gt; exponent
-2
</code></pre>
<p>The final 52 bits are the fractional part</p>
<pre><code>&gt;&gt;&gt; fraction = int(float_bits[1 + 11:1 + 11 + 52], 2)
&gt;&gt;&gt; fraction
2702159776422298
&gt;&gt;&gt; hex(fraction)
'0x999999999999a'
</code></pre>
<p>Putting it all together</p>
<pre><code>&gt;&gt;&gt; sign * 2**(exponent) * (1 + fraction / float(2**52))
0.4
</code></pre>
</div>
<span class="comment-copy">None of these results are integers</span>
<span class="comment-copy">You might want to see this StackOverflow question: <a href="http://stackoverflow.com/q/588004/1364007">"Is floating point math broken?"</a> - it basically covers some very similar floating point issues.</span>
<span class="comment-copy">The <a href="https://docs.python.org/3/whatsnew/3.1.html#other-language-changes" rel="nofollow noreferrer">What's new in Python 3.1 docs (scroll to end of linked section, just before "New, Improved and Deprecated Modules")</a> are a useful explanation for why/when Python 2.7/3.1+ have much shorter <code>float</code> <code>repr</code>s for some values. Straight from the horse's mouth, so to speak.</span>
<span class="comment-copy">surely you mean binary64</span>
<span class="comment-copy">D'oh! Thanks for pointing it out, edited.</span>
