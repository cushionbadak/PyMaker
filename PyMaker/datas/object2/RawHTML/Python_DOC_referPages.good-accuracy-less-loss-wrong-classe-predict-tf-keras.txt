<div class="post-text" itemprop="text">
<p>i made a program that predict characters in keras using EMNIST by class as a dataset and the training took me 10 hours in gpu i made an CNN architecture that gives me good accuracy and less loss<br/>
as shown in both images and also the predict picture i got .. 
<a href="https://i.stack.imgur.com/HnSEm.png" rel="nofollow noreferrer">enter image description here</a></p>
<p><a href="https://i.stack.imgur.com/Q11Yn.png" rel="nofollow noreferrer">enter image description here</a></p>
<p><a href="https://i.stack.imgur.com/QxVsH.png" rel="nofollow noreferrer">enter image description here</a></p>
<p>so i use in my program 1 python file for training and other for predicting word by taking each character seperated.
training file contain this architecture : </p>
<pre><code>K.set_learning_phase(1)

model = Sequential()

model.add(Reshape((28,28,1), input_shape=(784,)))
model.add(Convolution2D(32, (5,5), input_shape=(28,28,1),
                         activation='relu',padding='same',
                        kernel_constraint=maxnorm(3)))
model.add(Convolution2D(32, (5,5),activation='relu'))

model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Convolution2D(64,(5,5), activation='relu', padding='same', 
``kernel_constraint=maxnorm(3)))
model.add(Convolution2D(64, (5,5), activation='relu'))

model.add(MaxPooling2D(pool_size=(2,2)))``

model.add(Flatten())

model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))
model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))

model.add(Dropout(0.5))

model.add(Dense(62, activation='softmax'))

#opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
#opt = optimizers.Adadelta()
opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, 
`decay=0.0)
model.compile(loss='categorical_crossentropy', optimizer=opt, 
metrics=`['accuracy'])


print(model.summary())
history = model.fit(train_images,train_labels,validation_data=(test_images, 
``test_labels), 

batch_size=128, epochs=200)

    #evaluating model on test data. will take time
    scores = model.evaluate(test_images,test_labels, verbose = 0)
    print("Accuracy: %.2f%%"%(scores[1]*100))````
</code></pre>
<p>and the code for loading the model and predicting the characters </p>
<pre><code>`json_file = open('model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)


loaded_model.load_weights('model.h5')

model = loaded_model


print('Model successfully loaded')

characters = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 
'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',
          'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 
'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f',
          'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 
't', 'u', 'v', 'w', 'x', 'y', 'z']

# enter input image here
image = cv2.imread('example.png')

height, width, depth = image.shape

# resizing the image to find spaces better
image = cv2.resize(image, dsize=(width * 5, height * 4), 
interpolation=cv2.INTER_CUBIC)
# grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# binary
ret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)

# dilation
kernel = np.ones((5, 5), np.uint8)
img_dilation = cv2.dilate(thresh, kernel, iterations=1)

# adding GaussianBlur
gsblur = cv2.GaussianBlur(img_dilation, (5, 5), 0)

# find contours
ctrs, hier = cv2.findContours(gsblur.copy(), cv2.RETR_EXTERNAL, 
cv2.CHAIN_APPROX_SIMPLE)

m = list()
# sort contours
sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])
pchl = list()
dp = image.copy()
for i, ctr in enumerate(sorted_ctrs):
# Get bounding box
x, y, w, h = cv2.boundingRect(ctr)
cv2.rectangle(dp, (x - 10, y - 10), (x + w + 10, y + h + 10), (90, 0, 255), 
9)

plt.imshow(dp)
plt.show()
plt.close()

for i, ctr in enumerate(sorted_ctrs):
# Get bounding box
x, y, w, h = cv2.boundingRect(ctr)
# Getting ROI
roi = image[y - 10:y + h + 10, x - 10:x + w + 10]
roi = cv2.resize(roi, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)
roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

roi = np.array(roi)
t = np.copy(roi)
t = t / 255.0
t = 1 - t
t = t.reshape(1, 784)
m.append(roi)
pred = model.predict_classes(t)
pchl.append(pred)

pcw = list()
interp = 'bilinear'
fig, axs = plt.subplots(nrows=len(sorted_ctrs), sharex=True, figsize=(1, 
len(sorted_ctrs)))
for i in range(len(pchl)):
# print (pchl[i][0])
pcw.append(characters[pchl[i][0]])
axs[i].set_title('-------&gt; predicted letter: ' + characters[pchl[i][0]], 
x=2.5, y=0.24)
axs[i].imshow(m[i], interpolation=interp)

plt.show()

predstring = ''.join(pcw)
print('Predicted String: ' + predstring)
</code></pre>
<p>`
the code for creating the plots </p>
<pre><code># summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.grid()
plt.savefig('Accuracy')
#plt.show()
#plt.close()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.grid()
plt.savefig('Loss')
</code></pre>
<p>the output i get is in image i've put above thanks for giving advices and help.</p>
</div>
<div class="post-text" itemprop="text">
<p>Your model is clearly overfitting, which you can see as your test accuracy clearly diverges from your training accuracy.</p>
<p>You might want to look into regularization techniques to avoid this.</p>
</div>
<span class="comment-copy">What are the red and green lines in your second picture? Also, based on the first picture, it looks like you have a case of model overfit.</span>
<span class="comment-copy">thanks for responding, i modify the post  could you please recheck the the main post i did how i made them.</span>
<span class="comment-copy">@Scratch'N'Purr what do you suggest for the overfit ?</span>
<span class="comment-copy">I would simplify the structure of the neural network, either by shaving off a couple of the layers and/or reducing the number of nodes per layer. There is no correct answer here. You would have to experiment with different architectures but considering the scope of your problem, you don't need a deep NN to solve this problem. Also, since this is an iterative approach, just test with 20 epochs and tweak the params with each experiment.</span>
<span class="comment-copy">@Scratch'N'Purr okay thanks, would you estimate what would be the loss value so i can stop changing the architecture, and also do you thinks that i can get good accuracy and loss in 20 epochs ?</span>
<span class="comment-copy">thanks for responding, could you tell me sir whether its  a good architecture i made there or not.</span>
