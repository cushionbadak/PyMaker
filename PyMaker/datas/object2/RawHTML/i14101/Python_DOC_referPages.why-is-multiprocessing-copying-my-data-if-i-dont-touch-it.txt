<div class="post-text" itemprop="text">
<p>I was tracking down an out of memory bug, and was horrified to find that python's multiprocessing appears to copy large arrays, even if I have no intention of using them. </p>
<p>Why is python (on Linux) doing this, I thought copy-on-write would protect me from any extra copying? I imagine that whenever I reference the object some kind of trap is invoked and only then is the copy made. </p>
<p>Is the correct way to solve this problem for an arbitrary data type, like a 30 gigabyte custom dictionary to use a <code>Monitor</code>? Is there some way to build Python so that it doesn't have this nonsense?</p>
<pre><code>import numpy as np
import psutil
from multiprocessing import Process
mem=psutil.virtual_memory()
large_amount=int(0.75*mem.available)

def florp():
    print("florp")

def bigdata():
    return np.ones(large_amount,dtype=np.int8)

if __name__=='__main__':
    foo=bigdata()#Allocated 0.75 of the ram, no problems
    p=Process(target=florp)
    p.start()#Out of memory because bigdata is copied? 
    print("Wow")
    p.join()
</code></pre>
<p>Running:</p>
<pre><code>[ebuild   R    ] dev-lang/python-3.4.1:3.4::gentoo  USE="gdbm ipv6 ncurses readline ssl threads xml -build -examples -hardened -sqlite -tk -wininst" 0 KiB
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I'd expect this behavior -- when you pass code to Python to compile, anything that's not guarded behind a function or object is immediately <code>exec</code>ed for evaluation.</p>
<p>In your case, <code>bigdata=np.ones(large_amount,dtype=np.int8)</code> has to be evaluated -- unless your actual code has different behavior, <code>florp()</code> not being called has nothing to do with it.  </p>
<p>To see an immediate example:</p>
<pre><code>&gt;&gt;&gt; f = 0/0
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
ZeroDivisionError: integer division or modulo by zero
&gt;&gt;&gt; def f():
...     return 0/0
...
&gt;&gt;&gt;
</code></pre>
<p>To apply this to your code, put <code>bigdata=np.ones(large_amount,dtype=np.int8)</code> behind a function and call it as your need it, otherwise, Python is trying to be hepful by having that variable available to you at runtime.</p>
<p>If <code>bigdata</code> doesn't change, you could write a function that gets or sets it on an object that you keep around for the duration of the process. </p>
<p>edit: Coffee just started working.  When you make a new process, Python will need to copy all objects into that new process for access.  You can avoid this by using threads or by a mechanism that will allow you to share memory between processes such as <a href="https://docs.python.org/2/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow">shared memory maps</a> or shared <a href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.sharedctypes" rel="nofollow">ctypes</a></p>
</div>
<div class="post-text" itemprop="text">
<p>The problem was that by default Linux checks for the worst case memory usage, which can indeed exceed memory capacity. This is true even if the python language doesn't exposure the variables. You need to turn off "overcommit" system wide, to achieve the expected COW behavior.</p>
<pre><code>sysctl `vm.overcommit_memory=2'
</code></pre>
<p>See <a href="https://www.kernel.org/doc/Documentation/vm/overcommit-accounting" rel="nofollow">https://www.kernel.org/doc/Documentation/vm/overcommit-accounting</a></p>
</div>
<span class="comment-copy">Python (or, rather, CPython) uses a reference counter embedded in the object. Whenever an object is passed to a function, its reference counter is incremented, causing a modification to the object and thus a page fault in the child process. I wouldn't say it explains your particular example above though. Still, consider using multithreading instead of multiprocessing.</span>
<span class="comment-copy">@UlrichEckhardt But I didn't pass anything to <code>florp</code>!</span>
<span class="comment-copy">You're right, I hacked enter too early, see the last two sentences which I appended. BTW: I can't reproduce these issues here, using Python 3.4.2 on Linux/x86_64.</span>
<span class="comment-copy">Have you tried to do the same in C: <code>malloc()</code> large amount, call <code>fork()</code> and see what happens? Try a different <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">start method: 'forkserver' or 'spawn'</a>. Related: <a href="http://stackoverflow.com/q/20111242/4279">How to avoid [Errno 12] Cannot allocate memory errors caused by using subprocess module</a>. Look at this <a href="https://gist.github.com/zed/7637011" rel="nofollow noreferrer">code example</a> and <a href="http://stackoverflow.com/a/13329386/4279">this answer</a></span>
<span class="comment-copy">I tried wrapping the array in a function call, but no dice or did you have something else in mind? See edit.</span>
<span class="comment-copy">I'd have to see what you're actually doing then.   I assume <code>florp()</code> does something more than just print in your actual code. Oh actually, I just thought of an important distinction about processes, give me a second to add it to my answer.</span>
