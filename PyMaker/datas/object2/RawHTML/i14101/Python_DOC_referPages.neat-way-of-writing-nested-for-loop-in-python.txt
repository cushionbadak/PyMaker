<div class="post-text" itemprop="text">
<p>(Update) I need to find stationary distribution of a Markov Chain with 4.5 million states. It means I need to solve a linear system with 4.5 million equations. Each state is a vector of size 6. I am trying to store each state in a list. The following in part of my effort in creating all admissible states. </p>
<p>I am trying to loop through a big set of numbers and create a set of vectors. Here is a simplified version of my code:</p>
<pre><code>mylist=[]
for i in range(1,4):
    for j in range(1,4-i):
        for k in range(0,5-i-j):
            Temp=[i,j,k]
            mylist.extend(Temp)
            print(mylist)
            mylist=[]
            Temp=[]
</code></pre>
<p>which will give me:</p>
<pre><code>[1, 1, 0]
[1, 1, 1]
[1, 1, 2]
[1, 2, 0]
[1, 2, 1]
[2, 1, 0]
[2, 1, 1]
</code></pre>
<p>my question is: is there a neater, nicer, more efficient way of doing this in Python?</p>
<p>Thank you</p>
</div>
<div class="post-text" itemprop="text">
<p>If you are looking for a one line code that would create the same vector , you can use <a href="http://www.secnetix.de/olli/Python/list_comprehensions.hawk" rel="nofollow">list comprehension</a> in python.</p>
<p>Example -</p>
<pre><code>myList = [[i,j,k] for i in range(1,4) for j in range(1,4-i) for k in range(0,5-i-j)]
myList
&gt;&gt; [[1, 1, 0], [1, 1, 1], [1, 1, 2], [1, 2, 0], [1, 2, 1], [2, 1, 0], [2, 1, 1]]
</code></pre>
<p>Though I do not think this is in anyway neater or more efficient.</p>
<p>Though after some testing using <code>timeit</code> , we can see that list comprehension may be a little bit faster -</p>
<pre><code>In [1]: def foo1():
   ...:     l = []
   ...:     for i in range(100):
   ...:                 for j in range(100):
   ...:                         l.append([i,j])
   ...:     return l

In [3]: def foo2():
   ...:     return [[i,j] for i in range(100) for j in range(100)]
   ...: 

In [4]: %timeit foo1()
100 loops, best of 3: 3.08 ms per loop

In [5]: %timeit foo2()
100 loops, best of 3: 2.16 ms per loop

In [6]: %timeit foo2()
100 loops, best of 3: 2.18 ms per loop

In [7]: %timeit foo1()
100 loops, best of 3: 3.11 ms per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I completely don't get what you are trying to archive with your lists.
You could get the same output with this:</p>
<pre><code>for i in range(1,4):
    for j in range(1,4-i):
        for k in range(0,5-i-j):
            print([i,j,k])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>from itertools import product
mylist =[[i,j,k] for i,j,k in product(range(1,4),range(1,4),range(2))]
</code></pre>
</div>
<span class="comment-copy">I'm voting to close this question as off-topic because it belongs on <a href="http://codereview.stackexchange.com/">codereview.stackexchange.com</a></span>
<span class="comment-copy">@CoreyOgburn if it were actual code from a real project, perhaps. As it stands it's hypothetical/example code, which is explicitly off-topic on Code Review. Also, being a better fit for CR isn't a close reason; to help migrate a question you need to custom-flag for moderator attention.</span>
<span class="comment-copy">Do it once then serialize it so the next time you need it you can just <i>read it in</i>.</span>
<span class="comment-copy">@wwii: thank you; can you please explain a bit what is serialization? I am dealing with huge amount of data and can definitely use any hint for saving time ( and specially memory).</span>
<span class="comment-copy">You may need to expand on your question and describe what type of efficiency you are trying to achieve. I took it to mean time so my suggestion is to keep your code as is, create the data once (while having a cup of tea) and write it to disc to be read back in when needed (<a href="https://en.wikipedia.org/wiki/Serialization" rel="nofollow noreferrer">serialization</a>).  It could be easily written as a text file or <a href="https://docs.python.org/3/library/pickle.html#pickle-python-object-serialization" rel="nofollow noreferrer">pickled</a>.  You should also define <i>huge amount of data</i> in the question.</span>
