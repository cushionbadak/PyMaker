<div class="post-text" itemprop="text">
<p>I'm coding an API endpoint, which you can think is really an aggregator of other APIs. You send it certain parameters and what it does is translate those parameters to the underlying API's parameters and makes the calls. Now ideally I wouldn't want to make these calls in a serial manner.</p>
<p>What is the best way to do this?</p>
<p>EDIT: As I suspected, threads or processes isn't the way to go when the thing you're trying to do is getting URLs. This is because most of the time is spent waiting for the network to reply, so what you really want is a change to manage changing between tasks which are waiting into tasks which are actually doing the requests. Because of this, I think that the answers which exist to a similar question are actually bad answers.</p>
</div>
<div class="post-text" itemprop="text">
<p>After some research, as far as I can tell single-threaded asynchronous code is a much better answer than threads for the specific case of getting several URLs, and especially so for the case of many many URLs:</p>
<p>There's Twisted, the framework:
<a href="http://twistedmatrix.com/documents/14.0.1/web/howto/client.html" rel="nofollow">http://twistedmatrix.com/documents/14.0.1/web/howto/client.html</a></p>
<p>And gevent, the library:
<a href="http://sdiehl.github.io/gevent-tutorial/" rel="nofollow">http://sdiehl.github.io/gevent-tutorial/</a></p>
<p>Simple example from <a href="http://mauveweb.co.uk/posts/2014/07/gevent-asynchronous-io-made-easy.html" rel="nofollow">http://mauveweb.co.uk/posts/2014/07/gevent-asynchronous-io-made-easy.html</a>, doing 100 calls using a Pool of 20:</p>
<pre><code>from gevent import monkey
monkey.patch_all()

import urllib2
from gevent.pool import Pool


def download(url):
    return urllib2.urlopen(url).read()


if __name__ == '__main__':
    urls = ['http://httpbin.org/get'] * 100
    pool = Pool(20)
    print pool.map(download, urls)
</code></pre>
</div>
<span class="comment-copy">You can find it in this other SO question: <a href="http://stackoverflow.com/questions/983144/how-to-do-a-non-blocking-url-fetch-in-python" title="how to do a non blocking url fetch in python">stackoverflow.com/questions/983144/â€¦</a> . Please try to search for your question before you ask.</span>
<span class="comment-copy">I'm not sure spawning whole new processes or threads is the way to go about this. Also, that answer is from 2009. Please try to think through your comments.</span>
<span class="comment-copy">I did not intend to offend you. It is standard SO policy to flag duplicates in order to avoid having many (accepted) answers to basically the same question. Otherwise, if you want to rule out some concurrency models, like threading and multiprocessing, you should state this constraint in your question. This will force you to <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow noreferrer">use Python 3</a> or <a href="http://gevent.org" rel="nofollow noreferrer">install non-trivial libraries with system dependencies</a>; the answer from 2009 is completely valid as-is in 2015, though. Again, my apologies if I seemed rude.</span>
<span class="comment-copy">No problem, thank you.</span>
