<div class="post-text" itemprop="text">
<p>If I have an int that fits into 32 bits, what is the fastest way to split it up into four 8-bit values in python? My simple timing test suggests that bit masking and shifting is moderately faster than <code>divmod()</code>, but I'm pretty sure I haven't thought of everything.</p>
<pre><code>&gt;&gt;&gt; timeit.timeit("x=15774114513484005952; y1, x =divmod(x, 256);y2,x = divmod(x, 256); y3, y4 = divmod(x, 256)")
0.5113952939864248
&gt;&gt;&gt; timeit.timeit("x=15774114513484005952; y1=x&amp;255; x &gt;&gt;= 8;y2=x&amp;255; x&gt;&gt;=8; y3=x&amp;255; y4= x&gt;&gt;8")
0.41230630996869877
</code></pre>
<p>Before you ask: this operation will be used a lot. I'm using python 3.4. </p>
</div>
<div class="post-text" itemprop="text">
<p>If you're doing it a lot, the fastest approach is to create specialized a specialized <a href="https://docs.python.org/3/library/struct.html#struct.Struct" rel="nofollow"><code>Struct</code> instance</a> and pre-bind the <code>pack</code> method:</p>
<pre><code># Done once
int_to_four_bytes = struct.Struct('&lt;I').pack

# Done many times (you need to mask here, because your number is &gt;32 bits)
y1, y2, y3, y4 = int_to_four_bytes(x &amp; 0xFFFFFFFF)
</code></pre>
<p>Using <code>struct.pack</code> directly would use a cached <code>Struct</code> object after the first use, but you'd pay cache lookup costs to go from format string to cached <code>Struct</code> every time, which is suboptimal. By creating and prebinding the <code>pack</code> of a <code>Struct</code> object (which is implemented in C in CPython), you bypass all Python byte code execution beyond the actual function call, and spend no time on cache lookups. On my machine, this runs in about 205 ns, vs. 267 ns for shift and mask (without reassigning <code>x</code>).</p>
<p>An alternate approach (for more general, not <code>struct</code> compatible sizes) is using <a href="https://docs.python.org/3/library/stdtypes.html#int.to_bytes" rel="nofollow"><code>int.to_bytes</code></a>; for example, in this case:</p>
<pre><code>y1, y2, y3, y4 = (x &amp; 0xFFFFFFFF).to_bytes(4, 'big')
</code></pre>
<p>which takes about the same amount of time as the manually shifting and masking approach (it took 268 ns per loop), but scales to larger numbers of bytes better.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you need something very fast, you should look into writing a C extension (see <a href="https://docs.python.org/2/extending/extending.html" rel="nofollow">this</a>). You can do this with or without <a href="http://cython.org/" rel="nofollow">cython</a>.
If you write a lot of these, I definitely recommend looking into cython.</p>
<p>This type of things is exactly what Python is awesome for: you can write your speed critical components in C directly, and it will interact (almost) seamlessly with your python code.</p>
</div>
<span class="comment-copy">Have you tried using the <code>struct</code> package for this ?</span>
<span class="comment-copy">Are you asking for additional solutions?</span>
<span class="comment-copy">Also, <code>15774114513484005952</code> doesn't fit into 32 bits at all, did you mean 64 bits? Or do you want to ignore the higher bits?</span>
<span class="comment-copy">Side-note: <code>divmod</code> is usually a pessimization, even when you actually need both quotient and remainder. In general, <code>x, y = x // y, x % y</code> beats <code>x, y = divmod(x, y)</code> for all numbers outside the number theory/cryptographic number range, simply because the cost of looking up and calling the <code>divmod</code> function is substantially higher than using the dedicated <code>BINARY_XXX</code> op codes.</span>
<span class="comment-copy">Sorry for mixing up the example sizes; I'm primarily interested in shorter (32-bit) integers, but was also curious about "long" types (64 bits and beyond).</span>
<span class="comment-copy">I was dubious, so I profiled it: <code>&gt;&gt;&gt; timeit.timeit("x=15774114513484005952; y1, x =divmod(x, 256);y2,x = divmod(x, 256); y3, y4 = divmod(x, 256)")</code>: 0.46334090128630123 <code>&gt;&gt;&gt; timeit.timeit("x=15774114513484005952; y1=x&amp;255; x &gt;&gt;= 8;y2=x&amp;255; x&gt;&gt;=8; y3=x&amp;255; y4= x&gt;&gt;8")</code>: 0.34517306721051 <code>&gt;&gt;&gt; timeit.timeit("x=15774114513484005952; ys=P(x&amp;0xffffffff)", "import struct; P=struct.Struct('&gt;I').pack")</code>: 0.1559582769015151.  It's marginally slower than that if you include the unpacking, but still faster than the alternatives.</span>
<span class="comment-copy">@RoadieRich: Yar. Pre-built <code>struct</code>s are actually used internally in <a href="https://hg.python.org/cpython/file/ab3e4be4b539/Objects/memoryobject.c#l1897" rel="nofollow noreferrer">CPython's C code</a> because they're hyperoptimized. I was actually comparing to more minimalist shift and mask code that didn't reassign <code>x</code>, just did <code>y1 = x &amp; 0xFF</code>, <code>y2 = (x &gt;&gt; 8) &amp; 0xFF</code>, <code>y3 = (x &gt;&gt; 16) &amp; 0xFF</code>, <code>y4 = (x &gt;&gt; 24) &amp; 0xFF</code>, which doesn't fare quite as poorly, but even then, the pre-built/bound <code>Struct.pack</code> takes 23.2% less time per loop than manual shifting.</span>
<span class="comment-copy">I guess this should have the foot note that results in non-cPython implementations may vary.</span>
<span class="comment-copy">@RoadieRich: Well, that's a footnote for <i>every</i> microoptimization you might make in Python. Unless otherwise specified, the assumption is usually that you're running on CPython. The rules for optimizing Python differ dramatically in different interpreters, and usually, CPython is the most divergent. Most of the other interpreters are either pre-compiled or JIT-ing, and usually see little to no benefit from using standard library functions over writing your own code specialized to the task. Heck, for Cython, the best Cython code is basically C written with Python syntax. :-)</span>
<span class="comment-copy">Wonderful, thanks! I knew I was forgetting something :-)</span>
<span class="comment-copy">For this particular case, I'd be surprised if you improved much on a pre-bound <code>pack</code> of a pre-built <code>struct.Struct()</code> unless the code calling it was also Cythonized or otherwise running in C. If the code performing this operation was also Cythonized, then yes, you could see a gain, but if it's a simple accelerator solely for extracting the bytes, <code>struct.Struct.pack</code> is already a C function (as is <code>int.to_bytes</code>); at best you'd just specialize it to the case of extracting four bytes from arbitrary sized <code>int</code>s (avoiding the need to mask off excess bits). Lot of hassle for tiny gain.</span>
<span class="comment-copy">Thanks for the idea. Fortunately speed is not that critical in this case, but it's good to be reminded of the extension option.</span>
