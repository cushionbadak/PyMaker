<div class="post-text" itemprop="text">
<p>I'd like to parallelize my Python program so that it can make use of multiple processors on the machine that it runs on.  My parallelization is very simple, in that all the parallel "threads" of the program are independent and write their output to separate files.  I don't need the threads to exchange information but it is imperative that I know when the threads finish since some steps of my pipeline depend on their output.</p>
<p>Portability is important, in that I'd like this to run on any Python version on Mac, Linux, and Windows. Given these constraints, which is the most appropriate Python module for implementing this? I am trying to decide between thread, subprocess, and multiprocessing, which all seem to provide related functionality.</p>
<p>Any thoughts on this?  I'd like the simplest solution that's portable.</p>
</div>
<div class="post-text" itemprop="text">
<p><code>multiprocessing</code> is a great Swiss-army knife type of module.  It is more general than threads, as you can even perform remote computations.  This is therefore the module I would suggest you use.</p>
<p>The <code>subprocess</code> module would also allow you to launch multiple processes, but I found it to be less convenient to use than the new multiprocessing module.</p>
<p>Threads are notoriously subtle, and, with CPython, you are often limited to one core, with them (even though, as noted in one of the comments, the Global Interpreter Lock (GIL) can be released in C code called from Python code).</p>
<p>I believe that most of the functions of the three modules you cite can be used in a platform-independent way.  On the portability side, note that <code>multiprocessing</code> only comes in standard since Python 2.6 (a version for some older versions of Python does exist, though).  But it's a great module!</p>
</div>
<div class="post-text" itemprop="text">
<p>For me this is actually pretty simple:</p>
<h1>The <em>subprocess</em> option:</h1>
<p><code>subprocess</code> is <strong>for running other executables</strong> --- it's basically a wrapper around <code>os.fork()</code> and <code>os.execve()</code> with some support for optional plumbing (setting up PIPEs to and from the subprocesses.  (Obviously other inter-process communications (IPC) mechanisms, such as sockets, SysV shared memory and message queues could be used --- but you're going to be limited to whatever interfaces and IPC channels are supported by the programs you're calling).</p>
<p>Commonly one uses <code>subprocess</code> synchronously --- simply calling some external utility and reading back its output or awaiting its completion (perhaps reading its results from a temporary file, or after it's posted them to some database).</p>
<p>However one can spawn hundreds of subprocesses and poll them.  My own personal favorite utility <a href="https://bitbucket.org/jimd/classh" rel="noreferrer">classh</a> does exactly that.  <strong>The biggest disadvantage</strong> of the <code>subprocess</code> module is that its I/O support is generally blocking.  There is a draft <a href="https://www.python.org/dev/peps/pep-3145/" rel="noreferrer" title="PEP 3145 -- Asynchronous I/O For subprocess.Popen">PEP-3145</a> to fix that in some future version of Python 3.x and an alternative <a href="http://www.lysator.liu.se/~bellman/download/asyncproc.py" rel="noreferrer" title="asyncproc.py">asyncproc</a> (Warning that leads right to the download, not to any sort of documentation nor README).  I've also found that it's relatively easy to just import <code>fcntl</code> and manipulate your <code>Popen</code> PIPE file descriptors directly --- though I don't know if this is portable to non-UNIX platforms.</p>
<p><code>subprocess</code> <strong>has almost no event handling support</strong> ... <strong>though</strong> you can use the <code>signal</code> module and plain old-school UNIX/Linux signals --- killing your processes softly, as it were.</p>
<h1>The <em>multiprocessing</em> option:</h1>
<p><code>multiprocessing</code> is <strong>for running functions within your existing (Python) code</strong> with support for more flexible communications among this family of processes.  In particular it's best to build your <code>multiprocessing</code> IPC around the module's <code>Queue</code> objects where possible, but you can also use <code>Event</code> objects and various other features (some of which are, presumably, built around <code>mmap</code> support on the platforms where that support is sufficient).</p>
<p>Python's <code>multiprocessing</code> module is intended to provide interfaces and features which are very <strong>similar to</strong> <code>threading</code> while allowing CPython to scale your processing among multiple CPUs/cores despite the GIL (Global Interpreter Lock).  It leverages all the fine-grained SMP locking and coherency effort that was done by developers of your OS kernel.</p>
<h1>The <em>threading</em> option:</h1>
<p><code>threading</code> is <strong>for a fairly narrow range of applications which are I/O bound</strong> (don't need to scale across multiple CPU cores) and which benefit from the extremely low latency and switching overhead of thread switching (with shared core memory) vs. process/context switching.  On Linux this is almost the empty set (Linux process switch times are extremely close to its thread-switches).</p>
<p><code>threading</code> suffers from <strong>two major disadvantages in Python</strong>.</p>
<p>One, of course, is implementation specific --- mostly affecting CPython.  That's the GIL.  For the most part, most CPython programs will not benefit from the availability of more than two CPUs (cores) and often performance will <em>suffer</em> from the GIL locking contention.</p>
<p>The larger issue which is not implementation specific, is that threads share the same memory, signal handlers, file descriptors and certain other OS resources.  Thus the programmer must be extremely careful about object locking, exception handling and other aspects of their code which are both subtle and which can kill, stall, or deadlock the entire process (suite of threads).</p>
<p>By comparison the <code>multiprocessing</code> model gives each process its own memory, file descriptors, etc.  A crash or unhandled exception in any one of them will only kill that resource and robustly handling the disappearance of a child or sibling process can be considerably easier than debugging, isolating and fixing or working around similar issues in threads.</p>
<ul>
<li>(Note: use of <code>threading</code> with major Python systems, such as <a href="http://www.numpy.org/" rel="noreferrer">NumPy</a>, may suffer considerably less from GIL contention then most of your own Python code would.  That's because they've been specifically engineered to do so).</li>
</ul>
<h1>The <em>twisted</em> option:</h1>
<p>It's also worth noting that <a href="http://twistedmatrix.com/" rel="noreferrer" title="Twisted Matrix Labs">Twisted</a> offers yet another alternative which is both <strong>elegant and very challenging to understand</strong>.  Basically, at the risk of over simplifying to the point where fans of Twisted may storm my home with pitchforks and torches, Twisted provides and event-driven co-operative multi-tasking within any (single) process.</p>
<p>To understand how this is possible one should read about the features of <code>select()</code> (which can be built around the <em>select()</em> or <em>poll()</em> or similar OS system calls).  Basically it's all driven by the ability to make a request of the OS to sleep pending any activity on a list of file descriptors or some timeout.</p>
<p>Awakening from each of these calls to <code>select()</code> is an event --- either one involving input available (readable) on some number of sockets or file descriptors, or buffering space becoming available on some other (writable) descriptors or sockets, some exceptional conditions (TCP out-of-band PUSH'd packets, for example), or a TIMEOUT.</p>
<p>Thus the Twisted programming model is built around handling these events then looping on the resulting "main" handler, allowing it to dispatch the events to your handlers.</p>
<p>I personally think of the name, <strong><em>Twisted</em></strong> as evocative of the programming model ... since your approach to the problem must be, in some sense, "twisted" inside out.  Rather than conceiving of your program as a series of operations on input data and outputs or results, you're writing your program as a service or daemon and defining how it reacts to various events.  (In fact the core "main loop" of a Twisted program is (usually?  always?) a <code>reactor()</code>.</p>
<p>The <strong>major challenges to using Twisted</strong> involve twisting your mind around the event driven model and also eschewing the use of any class libraries or toolkits which are not written to co-operate within the Twisted framework.  This is why Twisted supplies its own modules for SSH protocol handling, for curses, and its own subprocess/popen functions, and many other modules and protocol handlers which, at first blush, would seem to duplicate things in the Python standard libraries.</p>
<p>I think it's useful to understand Twisted on a conceptual level even if you never intend to use it.  It may give insights into performance, contention, and event handling in your threading, multiprocessing and even subprocess handling as well as any distributed processing you undertake.</p>
<p>(<strong>Note:</strong> Newer versions of Python 3.x are including <a href="https://docs.python.org/3/library/asyncio-task.html" rel="noreferrer">asyncio</a> (asynchronous I/O) features such as <em>async def</em>, the <em>@async.coroutine</em> decorator, and the <em>await</em> keyword, and <em>yield from future</em> support.  All of these are roughly similar to <strong>Twisted</strong> from a process (co-operative multitasking) perspective).</p>
<h1>The <em>distributed</em> option:</h1>
<p>Yet another realm of processing you haven't asked about, but which is worth considering, is that of <strong><em>distributed</em></strong> processing.  There are many Python tools and frameworks for distributed processing and parallel computation.  Personally I think the easiest to use is one which is least often considered to be in that space.</p>
<p>It is almost trivial to build distributed processing around <a href="http://redis.io/" rel="noreferrer" title="Redis">Redis</a>.  The entire key store can be used to store work units and results, Redis LISTs can be used as <code>Queue()</code> like object, and the PUB/SUB support can be used for <code>Event</code>-like handling. You can hash your keys and use values, replicated across a loose cluster of Redis instances, to store the topology and hash-token mappings to provide consistent hashing and fail-over for scaling beyond the capacity of any single instance for co-ordinating your workers and marshaling data (pickled, JSON, BSON, or YAML) among them.</p>
<p>Of course as you start to build a larger scale and more sophisticated solution around Redis you are re-implementing many of the features that have already been solved using, <a href="http://www.celeryproject.org/" rel="noreferrer">Celery</a>, <a href="https://spark.apache.org/" rel="noreferrer">Apache Spark</a> and <a href="http://hadoop.apache.org/" rel="noreferrer">Hadoop</a>, <a href="https://zookeeper.apache.org/" rel="noreferrer">Zookeeper</a>, <a href="https://github.com/coreos/etcd" rel="noreferrer">etcd</a>, <a href="http://cassandra.apache.org/" rel="noreferrer">Cassandra</a> and so on.  Those alll have modules for Python access to their services.</p>
<p>[Update: A couple of resources for consideration if you're considering Python for computationally intensive across distributed systems: <a href="https://ipyparallel.readthedocs.io/en/latest/" rel="noreferrer">IPython Parallel</a> and <a href="https://spark.apache.org/docs/0.9.0/python-programming-guide.html" rel="noreferrer">PySpark</a>.  While these are general purpose distributed computing systems, they are particularly accessible and popular subsystems data science and analytics]. </p>
<h1>Conclusion</h1>
<p>There you have the gamut of processing alternatives for Python, from single threaded, with simple synchronous calls to sub-processes, pools of polled subprocesses, threaded and multiprocessing, event-driven co-operative multi-tasking, and out to distributed processing.</p>
</div>
<div class="post-text" itemprop="text">
<p>In a similar case I opted for separate processes and the little bit of necessary communication trough network socket. It is highly portable and quite simple to do using python, but probably not the simpler (in my case I had also another constraint: communication with other processes written in C++).</p>
<p>In your case I would probably go for multiprocess, as python threads, at least when using CPython, are not real threads. Well, they are native system threads but C modules called from Python may or may not release the GIL and allow other threads them to run when calling blocking code.</p>
</div>
<div class="post-text" itemprop="text">
<p>To use multiple processors in CPython your <em>only</em> choice is the <a href="http://docs.python.org/library/multiprocessing.html" rel="nofollow noreferrer"><code>multiprocessing</code></a> module. CPython keeps a lock on it's internals (the <a href="http://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow noreferrer">GIL</a>) which prevents threads on other cpus to work in parallel. The <code>multiprocessing</code> module creates new processes ( like <code>subprocess</code> ) and manages communication between them.</p>
</div>
<div class="post-text" itemprop="text">
<p>Shell out and let the unix out to do your jobs:</p>
<p>use <a href="http://pypi.python.org/pypi/iterpipes/" rel="nofollow">iterpipes</a> to wrap subprocess and then:</p>
<p><a href="http://tedziuba.com" rel="nofollow">From Ted Ziuba's site</a></p>
<p>INPUTS_FROM_YOU | xargs -n1 -0 -P NUM ./process  #NUM parallel processes</p>
<p>OR</p>
<p><a href="https://savannah.gnu.org/projects/parallel/" rel="nofollow">Gnu Parallel</a> will also serve</p>
<p>You hang out with GIL while you send the backroom boys out to do your multicore work.</p>
</div>
<span class="comment-copy">Related: <a href="http://stackoverflow.com/questions/1743293/why-does-my-python-program-average-only-33-cpu-per-process-how-can-i-make-pytho/1743312#1743312" title="why does my python program average only 33 cpu per process how can i make pytho">stackoverflow.com/questions/1743293/â€¦</a> (read my answer there to see why threads are a non-starter for pure-Python code)</span>
<span class="comment-copy">"Any Python version" is FAR too vague. Python 2.3? 1.x? 3.x? It is simply an impossible condition to satisfy.</span>
<span class="comment-copy">for an assignment, i just used the "multiprocessing" module and its pool.map() method. piece of cake !</span>
<span class="comment-copy">Why the downvote?</span>
<span class="comment-copy">Is a thing like Celery under consideration too? Why is it or is it not?</span>
<span class="comment-copy">As far as I can tell Celery is more involved (you have to install some message broker), but it's an option that should probably be considered, depending on the problem at hand.</span>
<span class="comment-copy">It is hard to use multiprocessing with classes/OOP though.</span>
<span class="comment-copy">@Tjorriemorrie: I'm going to guess that you mean that it's hard to dispatch method calls to instances of objects which might be in other processes. I'd suggest that this is the same problem you'd have with threads, but more readily visible (rather than be fragile and subject to obscure race conditions).  I'd think that the recommended approach would be to arrange for all such dispatch to occur through Queue objects, which work single threaded, multi-threaded and across processes.  (With some Redis or Celery Queue implementation, even across a cluster of nodes)</span>
<span class="comment-copy">This is a really good answer. I wish it was in the introduction to concurrency in the Python3 docs.</span>
<span class="comment-copy">@root-11 you're welcome to propose it to the document maintainers; I've published it here for free use.  You and they are welcome to use it, whole or in parts.</span>
<span class="comment-copy">That's not quite true, AFAIK you can release the GIL using the C API, and there are other implementations of Python such as IronPython or Jython which don't suffer from such limitations. I didn't downvote though.</span>
<span class="comment-copy">"Portability is important, in that I'd like this to run on any Python version on Mac, Linux and Windows."</span>
<span class="comment-copy">With this solution, can you interact repeatedly with the job? You can do this in multiprocessing, but I don't think so in subprocess.</span>
