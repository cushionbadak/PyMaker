<div class="post-text" itemprop="text">
<p>I am writing a tensorflow.Keras wrapper to perform ML experiments.</p>
<p>I need my framework to be able to perform an experiment as specified in a configuration yaml file and run in parallel in a GPU. </p>
<p>Then I need a guarantee that if I ran the experiment again I would get if not the exact same results something reasonably close.</p>
<p>To try to ensure this, my training script contains these lines at the beginning, following the guidelines in the <a href="https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development" rel="nofollow noreferrer">official documentation</a>:</p>
<pre><code># Set up random seeds
random.seed(seed)
np.random.seed(seed)
tf.set_random_seed(seed)
</code></pre>
<p>This has proven to not be enough.</p>
<p>I ran the same configuration 4 times, and plotted the results:</p>
<p><a href="https://i.stack.imgur.com/poW8V.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/poW8V.png"/></a></p>
<p>As you can see, results vary a lot between runs.</p>
<p>How can I set up a training session in Keras to ensure I get reasonably similar results when training in a GPU? Is this even possible?</p>
<p>The full training script can be found <a href="https://github.com/Jsevillamol/ctlearn/blob/master/ctalearn/train.py" rel="nofollow noreferrer">here</a>.</p>
<p>Some of my colleagues are using <a href="https://github.com/ctlearn-project/ctlearn" rel="nofollow noreferrer">just pure TF</a>, and their results seem far more consistent. What is more, they do not seem to be seeding any randomness except to ensure that the train and validation split is always the same.</p>
</div>
<div class="post-text" itemprop="text">
<p>Try adding seed parameters to weights/biases initializers. Just to add more specifics to Alexander Ejbekov's comment. </p>
<p>Tensorflow has two random seeds graph level and op level. If you're using more than one graph, you need to specify seed in every one. You can override graph level seed with op level, by setting seed parameter within function. And you can make two functions even from different graphs output same value if same seed is set. 
Consider this example:</p>
<pre><code>g1 = tf.Graph()
with g1.as_default():
    tf.set_random_seed(1)
    a = tf.get_variable('a', shape=(1,), initializer=tf.keras.initializers.glorot_normal())
    b = tf.get_variable('b', shape=(1,), initializer=tf.keras.initializers.glorot_normal(seed=2))
with tf.Session(graph=g1) as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(a)) 
    print(sess.run(b))
g2 = tf.Graph()
with g2.as_default():
    a1 = tf.get_variable('a1', shape=(1,), initializer=tf.keras.initializers.glorot_normal(seed=1))

with tf.Session(graph=g2) as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(a1))
</code></pre>
<p>In this example, output of <code>a</code> is the same as <code>a1</code>, but <code>b</code> is different.</p>
</div>
<span class="comment-copy">Personally I never seed anything other than the train/test split. I am not familiar with your data and I can't say with certainty but in my experience where models most commonly fail in the training are 1. batch sizes and 2. optimizers and learning rate. Looking at your script, you are using an Adam optimizer which I personally adore but it can overshoot significantly if you don't get your learning rate right. At a glance, I'd say explore your data more to figure out the magnitudes of variations and play with batch sizes and learning rate(assuming there aren't bugs in your model somewhere).</span>
<span class="comment-copy">Thank you for your answer! How can I adapt this if I am using the <code>tensorflow.keras</code> interface instead of bare TF sessions?</span>
<span class="comment-copy">you can set seed in your keras.layers, and you can access graph with <code>tf.get_default_graph()</code></span>
