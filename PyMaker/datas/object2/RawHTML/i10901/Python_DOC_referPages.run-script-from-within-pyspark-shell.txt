<div class="post-text" itemprop="text">
<p>I can run my python+pyspark script from the unix command line by typing</p>
<pre><code>pyspark script.py
</code></pre>
<p>But how do I run script.py from within the pyspark shell? This seems like an elementary question but I can't find the answer anywhere. I tried </p>
<pre><code>execfile('script.py')
</code></pre>
<p>But I get an error which includes:</p>
<pre><code>ValueError: Cannot run multiple SparkContexts at once
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Could the error come from <code>script.py</code> trying to create a new SparkContext variable?</p>
<p>When you launch the pyspark interactive client it usually says :
<code>SparkContext available as sc, HiveContext available as sqlContext.</code></p>
<p>If your script file contains <code>sc = SparkContext()</code>, maybe try commenting it.</p>
</div>
<span class="comment-copy">Try <code>subprocess.call</code>(<a href="https://docs.python.org/3/library/subprocess.html#subprocess.call" rel="nofollow noreferrer">docs.python.org/3/library/subprocess.html#subprocess.call</a>). My understanding is that <code>execfile</code> tries to evaluate the file in the same Python instance, whereas with the <code>subprocess</code> module you can spawn another instance of Python and PySpark, without any conflict.</span>
<span class="comment-copy">Thank you for the tip. After making my code executable and adding a chmod, I am able to run the code this way. But after it runs, I cannot access the variables in the code. So it is nearly the same as running 'pyspark script.py' in unix.</span>
<span class="comment-copy">Yes, you would not have access to the variables unless you pipe them into another variable or persist them in some data structure. <code>subprocess</code> will only help you invoke another spark program. You could try something similar to <code>subprocess.Popen</code> with <code>stdout=PIPE</code></span>
<span class="comment-copy">I'm curious as to which version of Spark you're using such that you could execute $ pyspark script.py. In my case: "Running python applications through 'pyspark' is not supported as of Spark 2.0."</span>
<span class="comment-copy">My question was pre-Spark 2.0</span>
