<div class="post-text" itemprop="text">
<p>I have a list <code>dataframe_chunk</code> which contains chunks of a very large pandas dataframe.I would like to write every single chunk into a different csv, and to do so in parallel. However, I see the files being written sequentially and I'm not sure why this is the case. Here's the code:</p>
<pre><code>import concurrent.futures as cfu

def write_chunk_to_file(chunk, fpath):  
    chunk.to_csv(fpath, sep=',', header=False, index=False)

pool = cfu.ThreadPoolExecutor(N_CORES)

futures = []
for i in range(N_CORES):
    fpath = '/path_to_files_'+str(i)+'.csv'
    futures.append(pool.submit( write_chunk_to_file(dataframe_chunk[i], fpath)))

for f in cfu.as_completed(futures):
    print("finished at ",time.time())
</code></pre>
<p>Any clues?</p>
</div>
<div class="post-text" itemprop="text">
<p>One thing that is stated in the <a href="https://docs.python.org/2.7/library/threading.html" rel="nofollow">Python 2.7.x <code>threading</code> docs</a> 
but not in the 3.x docs is that 
Python cannot achieve true parallelism using the <code>threading</code> library - only one thread will execute at a time.</p>
<p>You should try using <code>concurrent.futures</code> with the <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor" rel="nofollow"><code>ProcessPoolExecutor</code></a> which uses separate processes for each job and therefore can achieve true parallelism on a multi-core CPU.</p>
<p><strong>Update</strong></p>
<p>Here is your program adapted to use the <code>multiprocessing</code> library instead:</p>
<pre><code>#!/usr/bin/env python3

from multiprocessing import Process

import os
import time

N_CORES = 8

def write_chunk_to_file(chunk, fpath):  
    with open(fpath, "w") as f:
      for x in range(10000000):
        f.write(str(x))

futures = []

print("my pid:", os.getpid())
input("Hit return to start:")

start = time.time()
print("Started at:", start)

for i in range(N_CORES):
    fpath = './tmp/file-'+str(i)+'.csv'
    p = Process(target=write_chunk_to_file, args=(i,fpath))
    futures.append(p)

for p in futures:
  p.start()

print("All jobs started.")

for p in futures:
  p.join()

print("All jobs finished at ",time.time())
</code></pre>
<p>You can monitor the jobs with this shell command in another window:</p>
<pre><code>while true; do clear; pstree 12345; ls -l tmp; sleep 1; done
</code></pre>
<p>(Replace 12345 with the pid emitted by the script.)</p>
</div>
<span class="comment-copy">yeah tried that too, same thing :(</span>
<span class="comment-copy">Answer updated - I had better luck using <code>multiprocessing</code> directly.</span>
