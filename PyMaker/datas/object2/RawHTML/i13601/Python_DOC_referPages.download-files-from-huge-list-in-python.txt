<div class="post-text" itemprop="text">
<p>I have a list with 2M+ file links and would like to download them. My current approach is using urllib.urlretrieve, but it does it serially and really slow. 
How could I speed it up? Is it possible to do it asyncronously (and how)? </p>
</div>
<div class="post-text" itemprop="text">
<p>If you are running Python 3.4+, you can use the stdlib <a href="https://docs.python.org/3/library/asyncio.html" rel="nofollow"><code>asyncio</code></a> module to write asynchronous code. See <a href="http://aiohttp.readthedocs.org/en/stable/" rel="nofollow"><code>aiohttp</code></a> for an async web client. There are countless examples of making parallel requests using <a href="http://aiohttp.readthedocs.org/en/stable/" rel="nofollow"><code>aiohttp</code></a>, so use those as a starting point.</p>
</div>
<div class="post-text" itemprop="text">
<p>It seems you have a large list of tasks (retrieve file and store somewhere) which are independent of each other and can be run in parallel.</p>
<p>I would suggest looking at something like Celery. <a href="http://www.celeryproject.org/" rel="nofollow">http://www.celeryproject.org/</a> Read what it's about and see if you agree its a good fit for your problem domain.</p>
</div>
<span class="comment-copy">Nothing prevents you from dividing the list into N smaller lists and running the urllib based program on each list simultaneously...</span>
<span class="comment-copy">Just write it directly to the file. Are you doing any console output? Remove this and should be faster</span>
<span class="comment-copy">Check out the <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor" rel="nofollow noreferrer"><code>concurrent.futures.ThreadPoolExecutor</code></a> class.</span>
