<div class="post-text" itemprop="text">
<p>I want to write a library that manages child processes with asyncio. I don't want to force my callers to be asynchronous themselves, so I'd prefer to get a <code>new_event_loop</code>, do a <code>run_until_complete</code>, and then <code>close</code> it. Ideally I'd like to do this without conflicting with any other asyncio stuff the caller might be doing.</p>
<p>My problem is that waiting on subprocesses doesn't work unless you call <code>set_event_loop</code>, which attaches the internal watcher. But of course if I do that, I might conflict with other event loops in the caller. A workaround is to cache the caller's current loop (if any), and then call <code>set_event_loop</code> one more time when I'm done to restore the caller's state. That almost works. But if the caller is not an asyncio user, a side effect of calling <code>get_event_loop</code> is that I've now created a global loop that didn't exist before, and Python will print a scary warning if the program exits without calling <code>close</code> on that loop.</p>
<p>The only meta-workaround I can think of is to do an <code>atexit.register</code> callback that closes the global loop. That won't conflict with the caller because close is safe to call more than once, unless the caller has done something crazy like trying to start the global loop during exit. So it's still not perfect.</p>
<p>Is there a perfect solution to this?</p>
</div>
<div class="post-text" itemprop="text">
<p>What you're trying to achieve looks very much like <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor" rel="nofollow">ProcessPoolExecutor</a> (in <a href="https://docs.python.org/3/library/concurrent.futures.html#module-concurrent.futures" rel="nofollow">concurrent.futures</a>).</p>
<p>Asyncronous caller:</p>
<pre><code>@coroutine
def in_process(callback, *args, executor=ProcessPoolExecutor()):
    loop = get_event_loop()
    result = yield from loop.run_in_executor(executor, callback, *args)
    return result
</code></pre>
<p>Synchronous caller:</p>
<pre><code>with ProcessPoolExecutor() as executor:
    future = executor.submit(callback, *args)
    result = future.result()
</code></pre>
</div>
<span class="comment-copy">That's not a bad idea, and now I'm wondering whether it might just be easier for me to use threads instead of wrestling with asyncio. But at the same time, forking off a subprocess feels kind of like giving up :(</span>
<span class="comment-copy">@JackO'Connor I think it might be a good idea to stick with <code>asyncio</code>. It is about to become the new standard for asynchronous programming in python, especially with the new <a href="https://www.python.org/dev/peps/pep-0492/" rel="nofollow noreferrer">async and await keywords</a> introduced in python 3.5. I recently read <a href="https://glyph.twistedmatrix.com/2014/02/unyielding.html" rel="nofollow noreferrer">a very good article</a> that explains why threads are bad and microthreads just as bad. I feel like explicit coroutines in a single main thread, with specific handling of threads and processes through an PoolExecutor is the cleanest solution.</span>
