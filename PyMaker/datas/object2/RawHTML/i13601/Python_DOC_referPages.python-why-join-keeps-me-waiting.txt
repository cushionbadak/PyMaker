<div class="post-text" itemprop="text">
<p>I want to do clustering on 10,000 models. Before that, I have to calculate the pearson corralation coefficient associated with every two models. That's a large amount of computation, so I use multiprocessing to spawn processes, assigning the computing job to 16 cpus.My code is like this:</p>
<pre><code>import numpy as np
from multiprocessing import Process, Queue

def cc_calculator(begin, end, q):
    index=lambda i,j,n: i*n+j-i*(i+1)/2-i-1
    for i in range(begin, end):
        for j in range(i, nmodel):
            all_cc[i][j]=get_cc(i,j)
            q.put((index(i,j,nmodel),all_cc[i][j]))

def func(i):
    res=(16-i)/16
    res=res**0.5
    res=int(nmodel*(1-res))
    return res

nmodel=int(raw_input("Entering the number of models:"))
all_cc=np.zeros((nmodel,nmodel))
ncc=int(nmodel*(nmodel-1)/2)
condensed_cc=[0]*ncc
q=Queue()
mprocess=[]

for ii in range(16):
    begin=func(i)
    end=func(i+1)
    p=Process(target=cc_calculator,args=(begin,end,q))
    mprocess+=[p]
    p.start()

for x in mprocess:
    x.join()

while not q.empty():
    (ind, value)=q.get()
    ind=int(ind)
    condensed_cc[ind]=value
np.save("condensed_cc",condensed_cc)
</code></pre>
<p>where get_cc(i,j) calculates the corralation coefficient associated with model i and j. all_cc is an upper triangular matrix and all_cc[i][j] stores the cc value. condensed_cc is another version of all_cc. I'll process it to achive condensed_dist to do the clustering. The "func" function helps assign to each cpu almost the same amout of computing.</p>
<p>I run the program successfully with nmodel=20. When I try to run the program with nmodel=10,000, however, seems that it never ends.I wait about two days and use top command in another terminal window, no process with command "python" is still running. But the program is still running and there is no output file. I use Ctrl+C to force it to stop, it points to the line: x.join(). nmodel=40 ran fast but failed with the same problem.</p>
<p>Maybe this problem has something to do with q. Because if I comment the line: q.put(...), it runs successfully.Or something like this:</p>
<pre><code>q.put(...)
q.get()
</code></pre>
<p>It is also ok.But the two methods will not give a right condensed_cc. They don't change all_cc or condensed_cc.</p>
<p>Another example with only one subprocess:</p>
<pre><code>from multiprocessing import Process, Queue

def g(q):
    num=10**2
    for i in range(num):
        print '='*10
        print i
        q.put((i,i+2))
        print "qsize: ", q.qsize()

q=Queue()
p=Process(target=g,args=(q,))
p.start()
p.join()

while not q.empty():
    q.get()
</code></pre>
<p>It is ok with num= 100 but fails with num=10,000. Even with num=100**2, they did print all i and q.qsizes. I cannot figure out why. Also, Ctrl+C causes trace back to p.join().</p>
<p>I want to say more about the size problem of queue. <a href="https://docs.python.org/3/library/multiprocessing.html?highlight=join#multiprocessing.Process.join" rel="nofollow">Documentation about Queue and its put method</a> introduces Queue as Queue([maxsize]), and it says about the put method:...block if neccessary until a free slot is available. These all make one think that the subprocess is blocked because of running out of spaces of the queue. However, as I mentioned before in the second example, the result printed on the screen proves an increasing qsize, meaning that the queue is not full. I add one line:</p>
<pre><code>print q.full()
</code></pre>
<p>after the print size statement, it is always false for num=10,000 while the program still stuck somewhere. Emphasize one thing: top command in another terminal shows no process with command python. That really puzzles me.</p>
<p>I'm using python 2.7.9.</p>
</div>
<div class="post-text" itemprop="text">
<p>I believe the problem you are running into is described in the multiprocessing programming guidelines: <a href="https://docs.python.org/2/library/multiprocessing.html#multiprocessing-programming" rel="nofollow">https://docs.python.org/2/library/multiprocessing.html#multiprocessing-programming</a></p>
<p>Specifically this section:</p>
<blockquote>
<p>Joining processes that use queues</p>
<p>Bear in mind that a process that has put items in a queue will wait before terminating until all the buffered items are fed by the “feeder” thread to the underlying pipe. (The child process can call the cancel_join_thread() method of the queue to avoid this behaviour.)</p>
<p>This means that whenever you use a queue you need to make sure that all items which have been put on the queue will eventually be removed before the process is joined. Otherwise you cannot be sure that processes which have put items on the queue will terminate. Remember also that non-daemonic processes will be joined automatically.</p>
<p>An example which will deadlock is the following:</p>
<pre><code>from multiprocessing import Process, Queue

def f(q):
    q.put('X' * 1000000)

if __name__ == '__main__':
    queue = Queue()
    p = Process(target=f, args=(queue,))
    p.start()
    p.join()                    # this deadlocks
    obj = queue.get()
</code></pre>
<p>A fix here would be to swap the last two lines (or simply remove the p.join() line).</p>
</blockquote>
<p>You might also want to check out the section on "Avoid Shared State".</p>
<p>It looks like you are using <code>.join</code> to avoid the race condition of <code>q.empty()</code> returning <code>True</code> before something is added to it.  You should not rely on <code>.empty()</code> at all while using multiprocessing (or multithreading).  Instead you should handle this by <em>signaling</em> from the worker process to the main process when it is done adding items to the queue.  This is normally done by placing a <em>sentinal value</em> in the queue, but there are other options as well.</p>
</div>
<span class="comment-copy">Did you start each <code>process</code> in <code>mprocess</code> list before joining them? <code>for x in mprocess: x.start()</code></span>
<span class="comment-copy">@ozgur I did. Sorry for missing that in my question. I've edit the question again.</span>
<span class="comment-copy">sounds like your queue is running out of space, you should probably empty it as it is being filled.  you also appear to be trying to modify a global list in a process which will not work.</span>
<span class="comment-copy">@bj0 I edit the question again to detail the size problem of queue. As to the global list, I modify it through retriving elements from the queue in the parent process after calculating the values, putting them to the queue in the subprocess and joining the subprocess.</span>
