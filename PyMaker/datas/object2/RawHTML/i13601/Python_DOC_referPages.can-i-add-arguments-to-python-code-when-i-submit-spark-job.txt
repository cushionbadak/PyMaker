<div class="post-text" itemprop="text">
<p>I'm trying to use <code>spark-submit</code> to execute my python code in spark cluster. </p>
<p>Generally we run <code>spark-submit</code> with python code like below.</p>
<pre><code># Run a Python application on a cluster
./bin/spark-submit \
  --master spark://207.184.161.138:7077 \
  my_python_code.py \
  1000
</code></pre>
<p>But I wanna run <code>my_python_code.py</code>by passing several arguments Is there smart way to pass arguments? </p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Yes</strong>:  Put this in a file called args.py</p>
<pre><code>#import sys
print sys.argv
</code></pre>
<p>If you run</p>
<pre><code>spark-submit args.py a b c d e 
</code></pre>
<p>You will see:</p>
<pre><code>['/spark/args.py', 'a', 'b', 'c', 'd', 'e']
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Even <code>sys.argv</code> is a winning solution, I prefer this proper way to handle line command args in my Spark jobs:</p>
<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--ngrams", help="some useful description.")
args = parser.parse_args()
if args.ngrams:
    ngrams = args.ngrams
</code></pre>
<p>Then launch your job as following:</p>
<pre><code>spark-submit job.py --ngrams 3
</code></pre>
<p>More information about <code>argparse</code> module can be found in <a href="https://docs.python.org/3/howto/argparse.html" rel="noreferrer">Argparse Tutorial</a> </p>
</div>
<div class="post-text" itemprop="text">
<p>Ah, it's possible. <a href="http://caen.github.io/hadoop/user-spark.html" rel="nofollow">http://caen.github.io/hadoop/user-spark.html</a> </p>
<pre><code>spark-submit \
    --master yarn-client \   # Run this as a Hadoop job
    --queue &lt;your_queue&gt; \   # Run on your_queue
    --num-executors 10 \     # Run with a certain number of executors, for example 10
    --executor-memory 12g \  # Specify each executor's memory, for example 12GB
    --executor-cores 2 \     # Specify each executor's amount of CPUs, for example 2
    job.py ngrams/input ngrams/output
</code></pre>
</div>
<span class="comment-copy">Not working! Results says " [TerminalIPythonApp] CRITICAL | Unrecognized flag: '--ngrams' "</span>
<span class="comment-copy">If you have configs you want to send with your spark submit job, make sure to run with config info right after spark-submit, like: <code>spark-submit --master somemasterurl job.py --ngrams 3</code></span>
<span class="comment-copy">I think the question is not how to pass them in but rather how to access the arguments once they where passed in</span>
