<div class="post-text" itemprop="text">
<p>I am building a data pipeline using tf.data.Dataset API but got an OOM error. Assume I already have <code>features</code> and <code>labels</code> in hand, which are 4D numpy arrays in the order of [N,H,W,C]. Here is how I create my <code>dataset</code> object:</p>
<pre><code>batch_size = 100
num_samples = features.shape[0] # number of training samples

features_placeholder = tf.placeholder(tf.float32, [None, feature_size], name='features_placeholder')
labels_placeholder = tf.placeholder(tf.float32, [None, label_count], name='labels_placeholder')

dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))
dataset = dataset.batch(batch_size)
dataset = dataset.shuffle(num_samples)
dataset = dataset.prefetch(buffer_size=1)
iterator = dataset.make_initializable_iterator()
init_op = iterator.initializer
</code></pre>
<p>The reason I used <code>tf.placeholder</code> can be refered to <a href="https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays" rel="nofollow noreferrer">this guide</a>, which basically suggests defining <code>dataset</code> using <code>tf.placeholder</code> to save memory if the data are large numpy arrays(there are 54368 samples in my training dataset). The training part looks like:</p>
<pre><code>for i in range(epoch):
    sess.run([init_op, optimizer], 
             feed_dict={features_placeholder:features, labels_placeholder:labels]}
</code></pre>
<p>But I got an error that says:</p>
<p><code>OOM when allocating tensor with shape[54368,40,3,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc</code></p>
<p>which, as I trace back, occurred at a <code>tf.layers.conv2d</code> layer defined in my model. How do I solve this OOM problem?</p>
</div>
<div class="post-text" itemprop="text">
<p>In docs for <code>shuffle</code> it is written that <code>... fills a buffer with buffer_size elements ...</code> so in your case your dataset would take at least 54368*40*3*64*32*2 bits which is around 3.4GB. Just for the shuffle operation. Are you using 4GB gpu?</p>
<p>Another thing is that prefetch buffer_size should rather be bigger than 1. Why would you want to prefetch 1 element, maybe a batch or two?</p>
</div>
<span class="comment-copy">Have you tried with a smaller batch size?</span>
<span class="comment-copy">You misplaced dataset.batch and shuffle. Also there's something wrong with the tensor size. Are you constructing dataset from a single numpy array?</span>
<span class="comment-copy">@borarak By keeping the same batch size but feeding smaller arrays into <code>features_placeholder</code> and <code>labels_placeholder</code> there is no more error, so the batch size should not matter. However that means I can only read part of training data into <code>features</code> and <code>labels</code> instead of the whole dataset.</span>
<span class="comment-copy">@Sharky Can you specify what's wrong with the tensor size? I am trying to construct a dataset using <code>tf.placeholder</code> with the same shape as <code>features</code> and <code>labels</code> except for rank 0(the number of data samples)</span>
<span class="comment-copy">first dimension should be batch size,  not size of dataset</span>
