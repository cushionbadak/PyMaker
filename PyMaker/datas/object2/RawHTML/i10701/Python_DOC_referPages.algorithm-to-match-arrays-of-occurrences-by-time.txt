<div class="post-text" itemprop="text">
<p>Let's say that there's a "master" array of times with these values:</p>
<pre><code>master = [1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0]
</code></pre>
<p>I want to find the most "compatible" array among several candidates:</p>
<pre><code>candidates = [
    [0.01, 0.48, 1.03, 1.17, 1.5],
    [1.25, 1.4, 1.5, 1.9, 2.0],
    ...
]
</code></pre>
<p>In this case I consider the first candidate most compatible because after adding <code>1</code> to each value, 4 of the values are very close to values that exist in <code>master</code> (the 2nd candidate only has 3 values that match `master'), and order matters (though we can say the arrays are already sorted with no duplicate values, since they represent times).</p>
<p>A physical example could be that <code>master</code> is an array of beat onsets for a clean recording of an audio track, while the candidates are arrays of beat onsets for various audio recordings that may or may not be of the same audio track. I'd like to find the candidate that is most likely to be a recording of (at least a portion of) the same audio track.</p>
<p>I'm not sure of an algorithm to choose among these candidates. I've done some searching that led me to topics like cross-correlation, string distance, and fuzzy matching, but I'd like to know if I'm missing the forest for the trees here. I'm most familiar with data analysis in NumPy and Pandas, so I will tag the question as such.</p>
</div>
<div class="post-text" itemprop="text">
<p>One way would be to create those sliding 1D arrays as a stacked 2D array with <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" rel="nofollow"><code>broadcasting</code></a> and then get the distances against the <code>2D</code> array with <a href="http://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.spatial.distance.cdist.html" rel="nofollow"><code>Scipy's cdist</code></a>. Finally, we get the minimum distance along each row and choose the row with minimum of such distances. Thus, we would have an implementation like so -</p>
<pre><code>from scipy.spatial.distance import cdist

Na = a.shape[1]
Nb = b.size
b2D = b[np.arange(Nb-Na+1)[:,None] + np.arange(Na)]
closesetID = cdist(a,b2D).min(1).argmin()
</code></pre>
<p>Sample run -</p>
<pre><code>In [170]: a = np.random.randint(0,99,(400,500))

In [171]: b = np.random.randint(0,99,(700))

In [172]: b[100:100+a.shape[1]] = a[77,:] + np.random.randn(a.shape[1])
          # Make b starting at 100th col same as 77th row from 'a' with added noise

In [173]: Na = a.shape[1]
     ...: Nb = b.size
     ...: b2D = b[np.arange(Nb-Na+1)[:,None] + np.arange(Na)]
     ...: closesetID = cdist(a,b2D).min(1).argmin()
     ...: 

In [174]: closesetID
Out[174]: 77
</code></pre>
<p><strong>Note:</strong> To me it looked like using the default option of <code>cdist</code>, which is the euclidean distance made sense for such a problem. There are numerous other options as listed in the docs that are based on differentiation between inputs and as such could replace the default one.</p>
</div>
<span class="comment-copy">hope this can help <a href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="nofollow noreferrer">en.wikipedia.org/wiki/Cosine_similarity</a></span>
<span class="comment-copy">This might be dumb...what exactly does "shifting it over by 1" mean. Do you mean bitwise operations? Just wondering.</span>
<span class="comment-copy">@NoahHerron I think he is talkin about some offset. Just add 1 to all values.</span>
<span class="comment-copy">In regards to the python I would recommend checking out Python's <a href="https://docs.python.org/3/library/difflib.html" rel="nofollow noreferrer">difflib</a> for a simple solution or <a href="http://scikit-learn.org/stable/" rel="nofollow noreferrer">scikit-learn</a> for a more sophisticated approach. Otherwise in terms of algorithm selection for the type of data you are dealing with I wonder if <a href="http://stats.stackexchange.com/">cross-validated</a> would provide a better answer?</span>
<span class="comment-copy">A solution to this problem is very very dependent on the exact model. Shifting by x is for example a model-decision. Also there needs to be an error-metric / similarity-metric: when are two arrays equal? Let's take your example: you say, after adding 1, 4 values are near-hits. Do these 4 values need to be in order compared to the shifted input or not? Without a model of these decisions it is impossible to construct something good.</span>
<span class="comment-copy">Hi Divakar, this is great! I think this is a great starting point. I'm looking into generalizing this approach a bit though. It looks like your solution as stated will not work if the length of the candidate arrays (<code>Na</code>) is greater than the length of the master array (<code>Nb</code>).</span>
<span class="comment-copy">The quick-and-dirty solution I found is to use <code>numpy.tile</code> (<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html" rel="nofollow noreferrer">docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html</a>) to "loop" the master array.</span>
<span class="comment-copy">Then again, I checked against the sample <code>master</code> and <code>candidates</code>, and this algorithm chooses the wrong candidate because it doesn't take the potential offset into account...</span>
<span class="comment-copy">I checked all the available distance metrics other than euclidean and none of them seem suited to this task as-is. I'll investigate further.</span>
