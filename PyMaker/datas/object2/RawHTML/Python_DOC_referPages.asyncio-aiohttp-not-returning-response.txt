<div class="post-text" itemprop="text">
<p>I am trying to scrape some data from <a href="https://www.officialcharts.com/" rel="nofollow noreferrer">https://www.officialcharts.com/</a> by parallelising web requests using asyncio/aiohttp. I implemented the code given at the link <a href="https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html" rel="nofollow noreferrer">here</a>. </p>
<p>I followed two different procedures. The first one goes like this.</p>
<pre><code>from bs4 import BeautifulSoup
from urllib.request import urlopen
from selenium import webdriver
import time
import pandas as pd
import numpy as np
import re
import json

import requests
from bs4 import BeautifulSoup
from datetime import date, timedelta
from IPython.display import clear_output
import memory_profiler

import spotipy
import spotipy.util as util
import pandas as pd
from  more_itertools import unique_everseen

weeks = []
d = date(1970, 1, 1) 
d += timedelta(days = 6 - d.weekday())

for i in range(2500):    
    weeks.append(d.strftime('%Y%m%d'))
    d += timedelta(days = 7)

import asyncio
from aiohttp import ClientSession
import nest_asyncio
nest_asyncio.apply()

result = []
async def fetch(url, session):
    async with session.get(url) as response:
        return await response.read()

async def run(r):  
    tasks = []

    # Fetch all responses within one Client session,
    # keep connection alive for all requests.
    async with ClientSession() as session:
        for i in range(r):
            url = 'https://www.officialcharts.com/charts/singles-chart/' + weeks[i] + '/'
            task = asyncio.ensure_future(fetch(url, session))
            tasks.append(task)

        responses = await asyncio.gather(*tasks)
        result.append(responses)


loop = asyncio.get_event_loop()
future = asyncio.ensure_future(run(5))
loop.run_until_complete(future)

print('Done')
print(result[0][0] == None)
</code></pre>
<p>The problem with above code is, it fails when I make more than simultaneous 1000 requests.</p>
<p>The author of the <a href="https://pawelmhm.github.io/asyncio/python/aiohttp/2016/04/22/asyncio-aiohttp.html" rel="nofollow noreferrer">post</a> implemented a different procedure to address this issue and he claims we can do as many as 10K requests. I followed along his second procedure and here is my code for that.</p>
<pre><code>import random
import asyncio
from aiohttp import ClientSession
import nest_asyncio
nest_asyncio.apply()

result = []
async def fetch(url, session):
    async with session.get(url) as response:
        delay = response.headers.get("DELAY")
        date = response.headers.get("DATE")
        print("{}:{} with delay {}".format(date, response.url, delay))
        return await response.read()


async def bound_fetch(sem, url, session):
    # Getter function with semaphore.
    async with sem:
        await fetch(url, session)


async def run(r):
    tasks = []
    # create instance of Semaphore
    sem = asyncio.Semaphore(1000)

    # Create client session that will ensure we dont open new connection
    # per each request.
    async with ClientSession() as session:
        for i in range(r):         
            url = 'https://www.officialcharts.com/charts/singles-chart/' + weeks[i] + '/'
            task = asyncio.ensure_future(bound_fetch(sem, url, session))
            tasks.append(task)

        responses = await asyncio.gather(*tasks)
        result.append(responses)

number = 5

loop = asyncio.get_event_loop()
future = asyncio.ensure_future(run(number))
loop.run_until_complete(future)

print('Done')
print(result[0][0] == None)
</code></pre>
<p>For some reason, this doesn't return any responses.</p>
<p>PS:I am not from CS background and just program for fun. I have no clue what's going on inside the asyncio code.</p>
</div>
<div class="post-text" itemprop="text">
<p>Try to use the latest version.</p>
<pre><code>#!/usr/bin/python3
# -*- coding: utf-8 -*-

# python 3.7.2

from aiohttp import ClientSession, client_exceptions
from asyncio import Semaphore, ensure_future, gather, run

limit = 10
http_ok = [200]


async def scrape(url_list):

    tasks = list()

    sem = Semaphore(limit)

    async with ClientSession() as session:
        for url in url_list:
            task = ensure_future(scrape_bounded(url, sem, session))
            tasks.append(task)

        result = await gather(*tasks)

    return result


async def scrape_bounded(url, sem, session):
    async with sem:
        return await scrape_one(url, session)


async def scrape_one(url, session):

    try:
        async with session.get(url) as response:
            content = await response.read()
    except client_exceptions.ClientConnectorError:
        print('Scraping %s failed due to the connection problem', url)
        return False

    if response.status not in http_ok:
        print('Scraping%s failed due to the return code %s', url, response.status)
        return False

    return content


if __name__ == '__main__':
    urls = ['http://example.com/1', 'http://example.com/2']
    res = run(scrape(urls))

    print(res)
</code></pre>
<p>This is a template of a real <a href="https://github.com/caa06d9c/dataModels/blob/wf/specs/Weather/WeatherForecast/harvest/spain/harvester.py" rel="nofollow noreferrer">project</a> that works as predicted.</p>
<p>If u have some questions, I can extend the example. </p>
</div>
<span class="comment-copy">I get an error "asyncio.run() cannot be called from a running event loop", in Jupyter-Lab. But it works in python shell. Thanks!</span>
<span class="comment-copy">U should check carefully python version, it should be 3.7.2+ strictly because asyncio was changed since last edition</span>
