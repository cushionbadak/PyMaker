<div class="post-text" itemprop="text">
<p>Is it possible to know a "cost" in some measure(seconds, CPU ticks, logarithm scale, anything) for each instruction? Or at least for <em>some</em> instructions, skipping something like SLICE. There is a description at <a href="https://docs.python.org/3/library/dis.html" rel="nofollow">https://docs.python.org/3/library/dis.html</a>. There is source code: <a href="https://hg.python.org/cpython/file/tip/Python/ceval.c#l1199" rel="nofollow">https://hg.python.org/cpython/file/tip/Python/ceval.c#l1199</a>. I guess it is possible to estimate how much resources will eat each instruction by analyzing the source code, but I doubt that this can be done by noob like me. May be somebody already done this? Of course there are numerous high level advices about optimization, about over optimization, but may be such measure will help beginners with better bytecode understanding without digging into the C sources?</p>
<p>Edit: the actual question is not about profiling or debugging code - I know about various profiling methods - the question is particularly about bytecode. I am keeping in mind CPU instructions which has a cost measure - cycles per instruction.</p>
</div>
<div class="post-text" itemprop="text">
<p>The typical way to measure the performance of "small" pieces of code is <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow"><code>timeit</code></a>.  For larger things, we usually use <a href="https://docs.python.org/3/library/profile.html" rel="nofollow"><code>cProfile</code></a> instead.</p>
<p>These techniques will measure the code by <em>actually running it</em> and seeing how long it takes to execute, so they will not produce totally deterministic answers.  If you're looking for something more theoretical, you may need to look at the disassembly and the CPython <a href="https://hg.python.org/cpython/file/tip" rel="nofollow">source code</a>, and reason about how fast it is.  You may find the source easier to read if you consult the <a href="https://docs.python.org/3/c-api/index.html" rel="nofollow">Python/C API docs</a>, since a lot of CPython source is ultimately calling those same functions.</p>
</div>
<div class="post-text" itemprop="text">
<p>As mentioned by @kevin you can use cProfile. You can see <a href="http://lanyrd.com/2013/pycon/scdywg/" rel="nofollow">http://lanyrd.com/2013/pycon/scdywg/</a></p>
<p>For line by line profiling you can use, <a href="https://github.com/rkern/line_profiler" rel="nofollow">https://github.com/rkern/line_profiler</a>
which I found out from <a href="https://youtu.be/OLs9zPP2phk?t=7m41s" rel="nofollow">here</a>.</p>
</div>
<span class="comment-copy">Computers today are almost (really completely) deterministic. Therefore it is possible to do this. However the inclusion of other process, multithreaded/processor computers, proprietary operating systems, and paging memory out to hard disk or virtual memory make this very difficult.</span>
<span class="comment-copy">Can you give us a better idea of the actual problem you are trying to solve?  Is this simple curiosity, or do you have some script that's running too slowly and you're trying to figure out why?</span>
<span class="comment-copy">@Kevin, it is just curiosity.</span>
<span class="comment-copy">SLICE specifically will completely depend on what is being sliced, the implementation of <code>list.__getslice__</code> will have different performance from something in pandas etc.  I don't think there is a definitive way of measuring the cost of bytecodes when a bunch of them will just cause other thing to happen.</span>
