<div class="post-text" itemprop="text">
<p>I'm analyzing a cvs file data line by line. For each line, I will generate a string storing the analyzed results.</p>
<p>Since the file is huge, I must do multiprocessing. But because I have to access the results by their index,I have to store them in order (to a local file). </p>
<p>One way I tried and worked is using lock, but it's still very slow. If i didn't lock it, it runs fast, but the data will be messed.</p>
<p>What I want to do is to store those results in order to a global list. And I can write to a local file when all of the subprocesses are done.
Any hints how to do it without a lock and how to speed it up?</p>
<p>The following are my multiprocessing part code:</p>
<pre><code>def worker(dat,fileName,l):
    l.acquire()
    target = open(fileName,"a")
    for values in dat:
        # recursively apply different start mean2, find best solution
        model = MixGaussian(values)
        bf = model.findBestFit()
        # find the value where it's equally probable belongs to two gaussians 
        m1 = bf[0]
        m2 = bf[1]
        d1 = bf[2]
        d2 = bf[3]

        # calculate x
        k = math.log((d1+0.001)/(d2+0.001))* d1 * d2
        a = d1 -d2 
        b = 2 * (m1*d2 - m2 * d1)
        c = m2 * m2 * d1 - m1* m1 * d2 - k 
        delta = -1 * math.sqrt(b*b - 4 * a * c)
        if a == 0:
            a += 0.01
        x = (-1 * b + delta) / (2 * a)
        bf.append(x)
        print bf

        target.write(",".join(str(ele) for ele in bf))
        target.write("\n")

    target.close()
    l.release()

if __name__ == "__main__":
    # read from line 8000 to 8999
    data = readFile("unc_expr.tsv",8000,9000)
    target = open("modelstest9.csv","w")
    target.write("mean_1,mean_2,deviantion_1,deviation_2,cross_value")
    target.write("\n")
    target.close()

    numPrcs = 16
    d = []
    for i in range(numPrcs-1):
        d.append(data[i*len(data)/numPrcs:(i+1) *len(data)/numPrcs])
    d.append(data[(numPrcs-1)*len(data)/numPrcs:])

    start_time = time.time()
    lock = Lock()
    print("start time: %s"%start_time)

    for i in range(numPrcs):    
        Process(target=worker,args=(d[i],"modelstest9.csv",lock)).start()
</code></pre>
<p>Thanks!!</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example" rel="nofollow">ThreadPoolExecuter</a> is great for running the same method in parallel with different sets of data, and for re-assembling the results from each thread.</p>
<p>As the results come back from each thread put them in a list something like <code>[(index1, result1), (index2, result2),...]</code> as/when they return, sort the list by index when all threads have completed, then write the sorted list to a file.</p>
<p>The catch here is that this keeps all of your results in memory, but you should be able to do this as you're already keeping them all in memory in each of your processes.</p>
</div>
<div class="post-text" itemprop="text">
<p>I'd suggest using a <code>multiprocessing.Pool</code> and using its <code>imap</code> method to do the work. Have the workers <code>return</code> values instead of writing them directly, with the main process performing all the I/O. <code>imap</code> guarantees you get the results in the order the tasks were dispatched, and with only the main process performing I/O, conflicts are impossible.</p>
<p>It's also an improvement because you can split up your work into fixed chunks instead of carefully dividing the work to match the number of processes you wish to launch. <code>multiprocessing.Pool</code>, by default, spawns a number of workers equal to your CPU cores (so no need to manually specify the number of workers and risk too few, wasting cores, or too many, wasting time on context switches). And <code>map</code>/<code>imap</code> and company will seamlessly split X work items among N workers, without needing to ensure the number of work items equals the number of work items.</p>
</div>
<span class="comment-copy">Protecting your whole worker code with a lock has the same effect as not using multiprocessing at all, you're forcing all tasks to be performed serially.</span>
<span class="comment-copy">yep I realized it  and thanks for the note. I locked the writing process instead, but it will write the results of the quickest subprocess.</span>
<span class="comment-copy">If the processes are CPU bound, you'd want <code>ProcessPoolExecutor</code> to bypass GIL issues (which is also what you use <code>multiprocessing</code> for).</span>
<span class="comment-copy">how to add arguments to imap() here? I don't really understand imap() methods. Thanks for the help!</span>
<span class="comment-copy">@AlexWang: You'd define the function as <code>def worker(allargs):</code> with the first line <code>dat,fileName,l = allargs</code> to unpack them. Then you'd just create a generator that produce three-tuples, and pass that: <code>for result in pool.imap(worker, generatoroftuples):</code></span>
