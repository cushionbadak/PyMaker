<div class="post-text" itemprop="text">
<p>I am saving RandomForestClassifier model from sklearn library with code below</p>
<pre><code>with open('/tmp/rf.model', 'wb') as f:
    cPickle.dump(RF_model, f)
</code></pre>
<p>It takes a lot of space on my hard drive. There are only 50 trees in the model, however it takes over 50 MB on disk (analyzed dataset is ~ 20MB, with 21 features). Does anybody have idea why? I observe similar behavior for ExtraTreesClassifier.</p>
<p>Edit:
The RF parameters:</p>
<pre><code>"n_estimators": 50,
"max_features": 0.2,
"min_samples_split": 20,
"criterion": "gini",
"min_samples_leaf": 11
</code></pre>
<p>As suggested by @dooms I checked the sys.getsizeof and it returns 64 - I assume that this is only pointer size.</p>
<p>I tried other way to save a model:</p>
<pre><code>from sklearn.externals import joblib
joblib.dump(RF_model, 'filename.pkl') 
</code></pre>
<p>By using this way I get 1 *.pkl file and 201 *.npy files with total size 14.9 MB, so smaller than previous 53 MB. There is a pattern in these 201 npy files - there 4 files per tree in Forest:</p>
<p>The first file (231 KB) content:</p>
<pre><code>array([(1, 1062, 20, 0.2557438611984253, 0.4997574055554296, 29168, 46216.0),
       (2, 581, 12, 0.5557271242141724, 0.49938159451291675, 7506, 11971.0),
       (3, 6, 14, 0.006186043843626976, 0.4953095968671224, 4060, 6422.0),
       ...,
       (4123, 4124, 15, 0.6142271757125854, 0.4152249134948097, 31, 51.0),
       (-1, -1, -2, -2.0, 0.495, 11, 20.0),
       (-1, -1, -2, -2.0, 0.3121748178980229, 20, 31.0)], 
      dtype=[('left_child', '&lt;i8'), ('right_child', '&lt;i8'), ('feature', '&lt;i8'), ('threshold', '&lt;f8'), ('impurity', '&lt;f8'), ('n_node_samples', '&lt;i8'), ('weighted_n_node_samples', '&lt;f8')])
</code></pre>
<p>The second file (66 kB) content:</p>
<pre><code>array([[[  2.25990000e+04,   2.36170000e+04]],

       [[  6.19600000e+03,   5.77500000e+03]],

       [[  3.52200000e+03,   2.90000000e+03]],

       ..., 
       [[  3.60000000e+01,   1.50000000e+01]],

       [[  1.10000000e+01,   9.00000000e+00]],

       [[  2.50000000e+01,   6.00000000e+00]]])
</code></pre>
<p>The third file (88B):</p>
<pre><code>array([2])
</code></pre>
<p>The last file from group (96B):</p>
<pre><code>array([ 0.,  1.])
</code></pre>
<p>Any ideas what it is? I tried to look into the Tree code in sklearn, but it is hard. Any ideas how to save sklearn tree that it store less disk? (just to point that similar size ensemble of xgboost took ~200KB total size)</p>
</div>
<div class="post-text" itemprop="text">
<p>I've seen the same behavior using pickle dumps. The dump is about 10s times bigger that the <a href="https://docs.python.org/3/library/sys.html#sys.getsizeof" rel="nofollow">size in memory</a>.</p>
<pre><code>from sys import getsizeof
memory_size = getsizeof(RF_model)
</code></pre>
<p>See if there is a huge difference and if it's the case, see <a href="http://scikit-learn.org/stable/modules/model_persistence.html" rel="nofollow">another way</a> to save your model.</p>
</div>
<span class="comment-copy">what are the parameters for the classifier? number of trees and max depth / min_samples_{split,leaf} are relevant.</span>
<span class="comment-copy">This is a rather misleading answer. 1. <code>getsizeof</code> will provide wrong size estimates for a complex object (this has been discussed before, see <a href="http://stackoverflow.com/questions/19516403/why-pickle-dumpobj-has-diferent-size-with-sys-getsizeofobj-how-to-save-vari">here for example</a>.  2. Suggesting with a link "another way to save your model" really belongs to a comment and not to an answer (if the link did answer the question, the whole solution should have been put here on SO).</span>
