<div class="post-text" itemprop="text">
<p>I am trying to process a 3GB XML file, and am getting a memoryerror in the middle of a loop that reads the file and stores some data in a dictionary.</p>
<pre><code>class Node(object):
    def __init__(self, osmid, latitude, longitude):
        self.osmid = int(osmid)
        self.latitude = float(latitude)
        self.longitude = float(longitude)
        self.count = 0


context = cElementTree.iterparse(raw_osm_file, events=("start", "end"))
context = iter(context)
event, root = context.next()

for event, elem in context:
    if event == "end" and elem.tag == "node":
        lat = float(elem.get('lat'))
        lon = float(elem.get('lon'))
        osm_id = int(elem.get('id'))
        nodes[osm_id] = Node(osm_id, lat, lon)
        root.clear()
</code></pre>
<p>I'm using an iterative parsing method so the issue isn't with reading the file.  I just want to store the data in a dictionary for later processing, but it seems the dictionary is getting too large.  Later in the program I read in links and need to check if the nodes referenced by the links were in the initial batch of nodes, which is why I am storing them in a dictionary.</p>
<p>How can I either greatly reduce memory footprint (the script isn't even getting close to finishing so shaving bits and pieces off won't help much) or greatly increase the amount of memory available to python?  Monitoring the memory usage it looks like python is pooping out at about 1950 MB, and my computer still has about 6 GB available of RAM.</p>
</div>
<div class="post-text" itemprop="text">
<p>Assuming you have tons of <code>Node</code>s being created, you might consider using <a href="https://docs.python.org/3/reference/datamodel.html#slots" rel="nofollow"><code>__slots__</code></a> to predefine a fixed set of attributes for each <code>Node</code>. This removes the overhead of storing a per-instance <code>__dict__</code> (in exchange for preventing the creation of undeclared attributes) and can easily cut memory usage per <code>Node</code> by a factor of ~5x (<a href="https://docs.python.org/3/whatsnew/3.3.html#pep-412-key-sharing-dictionary" rel="nofollow">less on Python 3.3+ where shared key <code>__dict__</code> reduces the per-instance memory cost for free</a>).</p>
<p>It's easy to do, just change the declaration of <code>Node</code> to:</p>
<pre><code>class Node(object):
    __slots__ = 'osmid', 'latitude', 'longitude', 'count'

    def __init__(self, osmid, latitude, longitude):
        self.osmid = int(osmid)
        self.latitude = float(latitude)
        self.longitude = float(longitude)
        self.count = 0
</code></pre>
<p>For example, on Python 3.5 (where shared key dictionaries already save you something), the difference in object overhead can be seen with:</p>
<pre><code> &gt;&gt;&gt; import sys
 &gt;&gt;&gt; ... define Node without __slots___
 &gt;&gt;&gt; n = Node(1,2,3)
 &gt;&gt;&gt; sys.getsizeof(n) + sys.getsizeof(n.__dict__)
 248
 &gt;&gt;&gt; ... define Node with __slots__
 &gt;&gt;&gt; n = Node(1,2,3)
 &gt;&gt;&gt; sys.getsizeof(n)  # It has no __dict__ now
 72
</code></pre>
<p>And remember, this is Python 3.5 with shared key dictionaries; in Python 2, the per-instance cost with <code>__slots__</code> would be similar (one pointer sized variable larger IIRC), while the cost without <code>__slots__</code> would go up by a few hundred bytes.</p>
<p>Also, assuming you're on a 64 bit OS, make sure you've installed the 64 bit version of Python to match the 64 bit OS; otherwise, Python will be limited to ~2 GB of virtual address space, and your 6 GB of RAM counts for very little.</p>
</div>
<span class="comment-copy">Are you running 64-bit Python?</span>
<span class="comment-copy">Oh shoot, I thought I was but just checked and I am actually using 32.  It is my understanding that there is a hard cap on memory usage with 32, but not with 64, right?</span>
<span class="comment-copy">There is also a hard cap on 64-bit, but you're very unlikely to hit it, since it's in the multiples of terabytes.</span>
<span class="comment-copy">@user2913671: There is a hard cap on 64 bit too. But it's at least 256x larger (so 512 GB instead of 2 GB of address space), and I think at this point it's usually 65536 times larger (so 128 TB of address space, which I'm pretty sure should be enough). :-)</span>
<span class="comment-copy">This worked great! Cut the memory usage a ton &amp; switching to 64 bit version gave me the extra GB that I still needed.  Thanks!</span>
