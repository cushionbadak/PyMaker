<div class="post-text" itemprop="text">
<p>I am consulting the official <a href="https://docs.python.org/3/reference/grammar.html" rel="nofollow noreferrer">Python grammar specification as of Python 3.6</a>.</p>
<p>I am unable to find any syntax for comments (they appear prepended with a <code>#</code>) and docstrings (they should appear with <code>'''</code>). A quick look at <a href="https://docs.python.org/3/reference/lexical_analysis.html" rel="nofollow noreferrer">the lexical analysis</a> page didn't help either - docstrings are defined there as <code>longstrings</code> but do not appear in the grammar specifications. A type named <code>STRING</code> appears further, but no reference to its definition takes place.</p>
<p>Given this, I am curious about how the CPython compiler knows what comments and docstrings are. How is this feat accomplished? </p>
<p>I originally guessed that comments and docstrings are removed in a first pass by the CPython compiler, but then that beggars the question of how <code>help()</code> is able to render the relevant docstrings. </p>
</div>
<div class="post-text" itemprop="text">
<h2>Section 1</h2>
<h3>What happens to comments?</h3>
<p>Comments (anything preceded by a <code>#</code>) are ignored during tokenization/lexical analysis, so there is no need to write rules to parse them. They do not provide any semantic information to the interpreter/compiler, since they only serve to improve the verbosity of your program for the reader's sake, and so they are ignored.</p>
<p>Here's the lex specification for the ANSI C programming language: <a href="http://www.quut.com/c/ANSI-C-grammar-l-1998.html" rel="nofollow noreferrer">http://www.quut.com/c/ANSI-C-grammar-l-1998.html</a>. I'd like to draw your attention to the way comments are being processed here:</p>
<pre><code>"/*"            { comment(); }
"//"[^\n]*      { /* consume //-comment */ }
</code></pre>
<p>Now, take a look at the rule for <code>int</code>.</p>
<pre><code>"int"           { count(); return(INT); }
</code></pre>
<p>Here's the lex function to process <code>int</code> and other tokens:</p>
<pre><code>void count(void)
{
    int i;

    for (i = 0; yytext[i] != '\0'; i++)
        if (yytext[i] == '\n')
            column = 0;
        else if (yytext[i] == '\t')
            column += 8 - (column % 8);
        else
            column++;

    ECHO;
}
</code></pre>
<p>You see here it ends with the <code>ECHO</code> statement, meaning it is a valid token and must be parsed.</p>
<p>Now, here's the lex function to process comments:</p>
<pre><code>void comment(void)
{
    char c, prev = 0;

    while ((c = input()) != 0)      /* (EOF maps to 0) */
    {
        if (c == '/' &amp;&amp; prev == '*')
            return;
        prev = c;
    }
    error("unterminated comment");
}
</code></pre>
<p>There's no <code>ECHO</code> here. So, nothing is returned.</p>
<p>This is a representative example, but python does the exact same thing.</p>
<hr/>
<h2>Section 2</h2>
<h3>What happens to docstrings?</h3>
<p>Note: This section of my answer is meant to be a complement to @MartijnPieters' answer. It is not meant to replicate any of the information he has furnished in his post. Now, with that said,... </p>
<blockquote>
<p><em>I originally guessed that comments and docstrings are removed in a
  first pass by the CPython compiler[...]</em></p>
</blockquote>
<p>Docstrings (string literals that are not assigned to any variable name, anything within <code>'...'</code>, <code>"..."</code>, <code>'''...'''</code>, or <code>"""..."""</code>) are indeed processed. They are parsed as simple string literals (<code>STRING+</code> token), as  Martijn Pieters mentions in <a href="https://stackoverflow.com/a/44717667/4909087">his answer</a>. As of the current docs, it is only mentioned in passing that docstrings are assigned to the function/class/module's <code>__doc__</code> attribute. How it is done is not really mentioned in depth anywhere. </p>
<p>What actually happens is that they are tokenised and parsed as string literals and the resultant parse tree generated will contain them. From the parse tree the byte code is generated, with the docstrings in their rightful place in the <code>__doc__</code> attribute (they are not explicitly a part of the byte code as illustrated below). I won't go into details since the answer I linked above describes the same in very nice detail.</p>
<p>Of course, it is possible to ignore them completely. If you use <code>python -OO</code> (the <code>-OO</code> flag stands for "optimize intensely", as opposed to <code>-O</code> which stands for "optimize mildly"), with the resultant byte code stored in <code>.pyo</code> files, which exclude the docstrings. </p>
<p>An illustration can be seen below:</p>
<p>Create a file <code>test.py</code> with the following code:</p>
<pre><code>def foo():
    """ docstring """
    pass
</code></pre>
<p>Now, we'll compile this code with the normal flags set.</p>
<pre><code>&gt;&gt;&gt; code = compile(open('test.py').read(), '', 'single')
&gt;&gt;&gt; import dis
&gt;&gt;&gt; dis.dis(code)
  1           0 LOAD_CONST               0 (&lt;code object foo at 0x102b20ed0, file "", line 1&gt;)
              2 LOAD_CONST               1 ('foo')
              4 MAKE_FUNCTION            0
              6 STORE_NAME               0 (foo)
              8 LOAD_CONST               2 (None)
             10 RETURN_VALUE
</code></pre>
<p>As you can see, there is no mention of our docstring in the byte code. However, they <em>are</em> there. To get the docstring, you can do...</p>
<pre><code>&gt;&gt;&gt; code.co_consts[0].co_consts
(' docstring ', None)
</code></pre>
<p>So, as you can see, the docstring <em>does</em> remain, just not as a part of the main bytecode. Now, let's recompile this code, but with the optimisation level set to 2 (equivalent of the <code>-OO</code> switch):</p>
<pre><code>&gt;&gt;&gt; code = compile(open('test.py').read(), '', 'single', optimize=2)
&gt;&gt;&gt; dis.dis(code)
  1           0 LOAD_CONST               0 (&lt;code object foo at 0x102a95810, file "", line 1&gt;)
              2 LOAD_CONST               1 ('foo')
              4 MAKE_FUNCTION            0
              6 STORE_NAME               0 (foo)
              8 LOAD_CONST               2 (None)
             10 RETURN_VALUE
</code></pre>
<p>No, difference, but...</p>
<pre><code>&gt;&gt;&gt; code.co_consts[0].co_consts
(None,)
</code></pre>
<p>The docstrings have gone now. </p>
<p>The <code>-O</code> and <code>-OO</code> flag only remove things (optimisation of byte code is done by default... <code>-O</code> removes assert statements and <code>if __debug__:</code> suites from the generated bytecode, while <code>-OO</code> ignores docstrings in addition). The resultant compile time will decrease slightly. In addition, the speed of execution remains the same, unless you have a large amount of <code>assert</code> and <code>if __debug__:</code> statements, otherwise making no difference to performance.</p>
<p>Also, do remember that the docstrings are preserved only if they are the first thing in the function/class/module definition. All additional strings are simply dropped during compilation. If you change <code>test.py</code> to the following:</p>
<pre><code>def foo():
    """ docstring """

    """test"""
    pass
</code></pre>
<p>And then repeat the same process with <code>optimization=0</code>, this is is stored in the <code>co_consts</code> variable upon compilation:</p>
<pre><code>&gt;&gt;&gt; code.co_consts[0].co_consts
(' docstring ', None)
</code></pre>
<p>Meaning, <code>""" test """</code> has been ignored. It'll interest you to know that this removal is done as part of the base optimisation on the byte code.</p>
<hr/>
<h2>Section 3</h2>
<h3>Additional reading</h3>
<p>(You may find these references as interesting as I did.)</p>
<ol>
<li><p><a href="https://stackoverflow.com/questions/4777113/what-does-python-optimization-o-or-pythonoptimize-do">What does Python optimization (-O or PYTHONOPTIMIZE) do?</a></p></li>
<li><p><a href="https://stackoverflow.com/questions/8822335/what-do-the-python-file-extensions-pyc-pyd-pyo-stand-for">What do the python file extensions, .pyc .pyd .pyo stand for?</a></p></li>
<li><p><a href="https://stackoverflow.com/questions/1983177/are-python-docstrings-and-comments-stored-in-memory-when-a-module-is-loaded">Are Python docstrings and comments stored in memory when a module is loaded?</a></p></li>
<li><p><a href="https://www.programiz.com/python-programming/methods/built-in/compile" rel="nofollow noreferrer">Working with compile()</a></p></li>
<li><p>The <a href="https://docs.python.org/2/library/dis.html" rel="nofollow noreferrer"><code>dis</code></a> module</p></li>
<li><p><a href="https://github.com/python/cpython/blob/v3.6.1/Python/peephole.c" rel="nofollow noreferrer"><code>peephole.c</code></a> (courtesy Martijn) - The source code for all compiler optimisations. This is particularly fascinating, if you can understand it!</p></li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p>A docstring is not a separate grammar entity. It is just a regular <code>simple_stmt</code> (following that rule down all the way to <code>atom</code> and <code>STRING+</code> <sup>*</sup>. If it is the <em>first</em> statement in a function body, class or module, then it <em>used</em> as the docstring by the compiler.</p>
<p>This is documented in the reference documentation as footnotes to the <a href="https://docs.python.org/3/reference/compound_stmts.html#id8" rel="noreferrer"><code>class</code></a> and <a href="https://docs.python.org/3/reference/compound_stmts.html#id7" rel="noreferrer"><code>def</code></a> compound statements:</p>
<blockquote>
<p>[3] A string literal appearing as the first statement in the function body is transformed into the function’s <code>__doc__</code> attribute and therefore the function’s docstring.</p>
<p>[4] A string literal appearing as the first statement in the class body is transformed into the namespace’s <code>__doc__</code> item and therefore the class’s docstring.</p>
</blockquote>
<p>There currently is no reference documentation that specifies the same for modules, I regard this as a documentation bug.</p>
<p>Comments are removed by the tokenizer and never need to be parsed as grammar. Their whole <em>point</em> is to not have meaning on a grammar level. See the <a href="https://docs.python.org/3/reference/lexical_analysis.html#comments" rel="noreferrer"><em>Comments</em> section</a> of the Lexical Analysis documentation:</p>
<blockquote>
<p>A comment starts with a hash character (#) that is not part of a string literal, and ends at the end of the physical line. A comment signifies the end of the logical line unless the implicit line joining rules are invoked. <strong>Comments are ignored by the syntax; they are not tokens</strong>.</p>
</blockquote>
<p>Bold emphasis mine. So the <a href="https://github.com/python/cpython/blob/v3.6.1/Parser/tokenizer.c#L1496-L1501" rel="noreferrer">tokenizer</a> skips comments altogether:</p>
<pre class="lang-c prettyprint-override"><code>/* Skip comment */
if (c == '#') {
    while (c != EOF &amp;&amp; c != '\n') {
        c = tok_nextc(tok);
    }
}
</code></pre>
<p>Note that Python source code goes through 3 steps:</p>
<ol>
<li>Tokenizing</li>
<li>Parsing</li>
<li>Compilation</li>
</ol>
<p>The grammar only applies to the parsing stage; comments are dropped in the tokenizer, and docstrings are only special to the compiler.</p>
<p>To illustrate how the parser doesn't treat docstrings as anything other than a string literal expression, you can access any Python parse results as an <em>Abstract Syntax Tree</em>, via the <a href="https://docs.python.org/3/library/ast.html" rel="noreferrer"><code>ast</code> module</a>. This produces Python objects that <em>directly</em> reflect the parse tree that the Python grammar parser produces, from which Python bytecode is then compiled:</p>
<pre><code>&gt;&gt;&gt; import ast
&gt;&gt;&gt; function = 'def foo():\n    "docstring"\n'
&gt;&gt;&gt; parse_tree = ast.parse(function)
&gt;&gt;&gt; ast.dump(parse_tree)
"Module(body=[FunctionDef(name='foo', args=arguments(args=[], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=[Expr(value=Str(s='docstring'))], decorator_list=[], returns=None)])"
&gt;&gt;&gt; parse_tree.body[0]
&lt;_ast.FunctionDef object at 0x107b96ba8&gt;
&gt;&gt;&gt; parse_tree.body[0].body[0]
&lt;_ast.Expr object at 0x107b16a20&gt;
&gt;&gt;&gt; parse_tree.body[0].body[0].value
&lt;_ast.Str object at 0x107bb3ef0&gt;
&gt;&gt;&gt; parse_tree.body[0].body[0].value.s
'docstring'
</code></pre>
<p>So you have <code>FunctionDef</code> object, which has, as the first element in the body, an expression that is a <code>Str</code> with value <code>'docstring'</code>. It is the <em>compiler</em> that then generates a code object, storing that docstring in a separate attribute.</p>
<p>You can compile the AST into bytecode with the <a href="https://docs.python.org/3/library/functions.html#compile" rel="noreferrer"><code>compile()</code> function</a>; again, this is using the actual codepaths the Python interpreter uses. We'll use the <a href="https://docs.python.org/3/library/dis.html" rel="noreferrer"><code>dis</code> module</a> to decompile the bytecode for us:</p>
<pre><code>&gt;&gt;&gt; codeobj = compile(parse_tree, '', 'exec')
&gt;&gt;&gt; import dis
&gt;&gt;&gt; dis.dis(codeobj)
  1           0 LOAD_CONST               0 (&lt;code object foo at 0x107ac9d20, file "", line 1&gt;)
              2 LOAD_CONST               1 ('foo')
              4 MAKE_FUNCTION            0
              6 STORE_NAME               0 (foo)
              8 LOAD_CONST               2 (None)
             10 RETURN_VALUE
</code></pre>
<p>So the compiled code produced the top-level statements for a module. The <a href="https://docs.python.org/3/library/dis.html#opcode-MAKE_FUNCTION" rel="noreferrer"><code>MAKE_FUNCTION</code> opcode</a> uses a stored codeobject (part of the top-level code object constants) to build a function. So we look at that nested code object, at index 0:</p>
<pre><code>&gt;&gt;&gt; dis.dis(codeobj.co_consts[0])
  1           0 LOAD_CONST               1 (None)
              2 RETURN_VALUE
</code></pre>
<p>Here the docstring appears to be <em>gone</em>. The function does nothing more than return <code>None</code>. The docstring is instead stored as a constant:</p>
<pre><code>&gt;&gt;&gt; codeobj.co_consts[0].co_consts
('docstring', None)
</code></pre>
<p>When executing the <code>MAKE_FUNCTION</code> opcode, it is that first constant, provided it is a string, that is turned into the <code>__doc__</code> attribute for the function object.</p>
<p>Once compiled, we can execute the code object with the <a href="https://docs.python.org/3/library/functions.html#exec" rel="noreferrer"><code>exec()</code> function</a> into a given namespace, which adds a function object with a docstring:</p>
<pre><code>&gt;&gt;&gt; namespace = {}
&gt;&gt;&gt; exec(codeobj, namespace)
&gt;&gt;&gt; namespace['foo']
&lt;function foo at 0x107c23e18&gt;
&gt;&gt;&gt; namespace['foo'].__doc__
'docstring'
</code></pre>
<p>So it's the job of the compiler to determine when something is a docstring. This is done in C code, in the <a href="https://github.com/python/cpython/blob/v3.6.1/Python/compile.c#L1323-L1333" rel="noreferrer"><code>compiler_isdocstring()</code> function</a>:</p>
<pre class="lang-c prettyprint-override"><code>static int
compiler_isdocstring(stmt_ty s)
{
    if (s-&gt;kind != Expr_kind)
        return 0;
    if (s-&gt;v.Expr.value-&gt;kind == Str_kind)
        return 1;
    if (s-&gt;v.Expr.value-&gt;kind == Constant_kind)
        return PyUnicode_CheckExact(s-&gt;v.Expr.value-&gt;v.Constant.value);
    return 0;
}
</code></pre>
<p>This is called from locations where a docstring makes sense; for modules and classes, in <a href="https://github.com/python/cpython/blob/v3.6.1/Python/compile.c#L1433-L1465" rel="noreferrer"><code>compiler_body()</code></a>, and for functions, in <a href="https://github.com/python/cpython/blob/v3.6.1/Python/compile.c#L1807-L1906" rel="noreferrer"><code>compiler_function()</code></a>.</p>
<hr/>
<p><strong>TLDR</strong>: comments are not part of the grammar, because the grammar parser never even sees comments. They are skipped by the tokenizer. Docstrings are not part of the grammar, because to the grammar parser they are just string literals. It is the compilation step (taking the parse tree output of the parser) that interprets those string expressions as docstrings.</p>
<hr/>
<p><sup>*</sup> The full grammar rule path is <code>simple_stmt</code> -&gt; <code>small_stmt</code> -&gt; <code>expr_stmt</code> -&gt; <code>testlist_star_expr</code> -&gt; <code>star_expr</code> -&gt; <code>expr</code> -&gt; <code>xor_expr</code> -&gt; <code>and_expr</code> -&gt; <code>shift_expr</code> -&gt; <code>arith_expr</code> -&gt; <code>term</code> -&gt; <code>factor</code> -&gt; <code>power</code> -&gt; <code>atom_expr</code> -&gt; <code>atom</code> -&gt; <code>STRING+</code></p>
</div>
<span class="comment-copy">This is a good answer. I will mark as accepted if you can explain how docstrings are parsed as well, since they are in my original question.</span>
<span class="comment-copy">Thank you. I'll mark as accepted. I would appreciate it if you could move your comment to your answer. :)</span>
<span class="comment-copy">Docstrings are <i>not</i> "discarded completely", they're stored in the <code>__doc__</code> attribute. And <code>help(functionname)</code> shows them.</span>
<span class="comment-copy">@MartijnPieters Mmh, cool... edited. Thanks. This answer belongs to you more than it does me.</span>
<span class="comment-copy">@Coldspeed: see <a href="https://github.com/python/cpython/blob/v3.6.1/Python/compile.c#L1335-L1349" rel="nofollow noreferrer"><code>is_const()</code></a> for what is dropped (if the resulting expression statement AST passes that test it is ignored and never added to the byteccode) and <a href="https://github.com/python/cpython/blob/v3.6.1/Python/peephole.c" rel="nofollow noreferrer"><code>peephole.c</code></a> for a whole host of other optimisation tricks.</span>
<span class="comment-copy">I tried following the logic for <code>simple_stmt</code>, and I don't see where it finally yields <code>atom</code>. I managed to trace <code>atom</code> all the way up to <code>with_expr</code>, which is not used by <code>simple_stmt</code>. Can you point to a single chain where it finally gets to <code>atom</code>? Further, how is it recognised that exactly three <code>'</code> make up a single docstring? Does the <code>+</code> in <code>STRING+</code> handle this?</span>
<span class="comment-copy">Is there a "true" full grammar specification somewhere? What if I want to look up how Python comments look? And is anything else missing from that "Full Grammar specification" page?</span>
<span class="comment-copy">I tried getting a full picture by googling <code>python tokenizer</code> but only found the <a href="https://docs.python.org/3/library/tokenize.html" rel="nofollow noreferrer"><code>tokenize</code></a> module. That must not be it, as its documentation says <i>"The scanner in this module returns comments as tokens as well, making it useful for implementing “pretty-printers,” including colorizers for on-screen displays."</i></span>
<span class="comment-copy">@StefanPochmann: the <code>tokenize</code> module echoes the <a href="https://github.com/python/cpython/blob/master/Parser/tokenizer.c" rel="nofollow noreferrer">C implementation</a> with added niceties like treating comments as tokens <i>anyway</i>.</span>
<span class="comment-copy">@StefanPochmann: the full grammar is the <a href="https://github.com/python/cpython/blob/master/Grammar/Grammar" rel="nofollow noreferrer">full grammar</a> used as input to the <a href="https://github.com/python/cpython/blob/master/Parser/pgen.c" rel="nofollow noreferrer"><code>pgen</code> parser generator</a>. See <a href="http://eli.thegreenplace.net/2010/06/30/python-internals-adding-a-new-statement-to-python/" rel="nofollow noreferrer">eli.thegreenplace.net/2010/06/30/…</a> if you want to learn how this all works.</span>
