<div class="post-text" itemprop="text">
<p>I create a new class that is a subclass of multiprocessing.Process and I would like to invoke methods on this class.  The methods change class members but take no arguments, and I think should work transparently.  For instance, in the MWE below I create a class that inherits from Process and has a stop() function which just sets an instance member flag.  When this flag is set though the run() method doesn't seem to notice a change.  This all seemed to work when I was inheriting from threading.Thread, thoughts?</p>
<pre><code>from queue import Empty
import multiprocessing


class Worker(multiprocessing.Process):
    def __init__(self, queue):
        multiprocessing.Process.__init__(self) # , daemon=True)
        self.queue = queue
        self.close = False

    def stop(self):
        self.close = True
        print(self.close)

    def run(self):
        while (not self.close) or self.queue.qsize() &gt; 0:
            print(self.close)
            print(self.queue.qsize())
            for item in range(0, self.queue.qsize()):
                try:
                    self.queue.get_nowait()
                except Empty:
                    continue

queue = multiprocessing.Queue()
dbq = Worker(queue)
dbq.start()
queue.put("d")
dbq.stop()
dbq.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You have to use something like <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Value" rel="nofollow"><code>multiprocessing.Value</code></a> for synchronization between processes.</p>
<p>Sample code: </p>
<pre><code>from queue import Empty
from ctypes import c_bool
import multiprocessing

class Worker(multiprocessing.Process):
    def __init__(self, queue):
        multiprocessing.Process.__init__(self) # , daemon=True)
        self.queue = queue
        self.close = multiprocessing.Value(c_bool, False)

    def stop(self):
        self.close.value = True
        print(self.close)

    def run(self):
        while (not self.close.value) or self.queue.qsize() &gt; 0:
            print(self.close)
            print(self.queue.qsize())
            for item in range(0, self.queue.qsize()):
                try:
                    self.queue.get_nowait()
                except Empty:
                    continue

if __name__ == '__main__':
    queue = multiprocessing.Queue()
    dbq = Worker(queue)
    dbq.start()
    queue.put("d")
    dbq.stop()
    dbq.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Processes do not share memory space with their parent in the same way threads do. When a process is <code>fork</code>ed it will get a new copy of the parent's memory so you can't share as easily as with threads (effectively... realistically there is <a href="http://en.wikipedia.org/wiki/Copy-on-write" rel="nofollow">copy-on-write</a>).</p>
<p>I recommend that in order to kill workers you use an synchronization primitive like <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Event" rel="nofollow"><code>Event</code></a>, because usually workers are killed together in response to something that happened.</p>
<p>You will end up with something like this (notice, no <code>stop</code> method for workers):</p>
<pre><code>from queue import Empty
import multiprocessing


class Worker(multiprocessing.Process):
    # added the event to the initializing function
    def __init__(self, queue, close_event):
        multiprocessing.Process.__init__(self) # , daemon=True)
        self.queue = queue
        self.close = close_event

    def run(self):
        while (not self.close.is_set()) or self.queue.qsize() &gt; 0:
            print(self.close)
            print(self.queue.qsize())
            for item in range(0, self.queue.qsize()):
                try:
                    self.queue.get_nowait()
                except Empty:
                    continue

queue = multiprocessing.Queue()
# create a shared event for processes to react to
close_event = multiprocessing.Event()
# send event to all processes
dbq = Worker(queue, close_event)
dbq.start()
queue.put("d")
# set the event to stop workers
close_event.set()
dbq.join()
</code></pre>
</div>
<span class="comment-copy">Use a synchronization object instead of just a basic type to do any interprocess communication. I recommend multiprocessing.Event</span>
<span class="comment-copy">Thanks, I want to keep my api the same so using this method seems like it should work.</span>
<span class="comment-copy">I appreciate the explanation.  I don't understand what is happening when I call stop() in my original code.  We call start() on the multiprocessing.Process child and it forks, but why don't calls to the functions on this object happen in the new process?  I thought the methods would be marshaled through IPC, such as sockets or the like, automatically?  Thus a call to stop() later on  the original object would be really a call to a proxy which would use IPC to call the remote object function, and thus change the remote object memory space?</span>
<span class="comment-copy">The new process is a completely different memory mapping. If you understand that <code>start</code> forks, you probably understand any calls later on only affects the memory of the <b>parent</b> process. That's why you need to handle communication with synchronization objects. They are shared memory for both processes to access.</span>
<span class="comment-copy">Thanks @Reut Sharabani.  I think what I am lacking is an understanding of why the parent even has the ability to invoke methods on an object that inherits from multiprocessing.Process.  Are there two instances of Worker now, one in the parent memory space and one in the subprocess memory space?  When I create the object, then call start(), what is the "thing" I'm left with in the parent process?  I thought it was a proxy where methods were tied across to an instance in the new subprocess.  But this doesn't seem to be true?  Is it just an object in the parent that hasn't had start called on it?</span>
<span class="comment-copy">If you're familiar with any c code that sues <code>fork</code>, you can easily understand why the two can <b>start</b> from the same memory image, but <b>proceed</b> to different places. I don't know exactly how Python implements processes, but I assume you are only left with means to communicate with the Process object in the parent, while the child proceeds to run the target function.</span>
