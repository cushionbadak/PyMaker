<div class="post-text" itemprop="text">
<p>I am trying to multithread the following code and I just can't seem to get it working. </p>
<p>The following code (of which I removed most of the code just for illustrative purposes) currently works smoothly, but slowly (approximately 5 minutes for a list of 3600 tweets).</p>
<pre><code>import dataset
import datetime
import json

with open("postgresConnecString.txt", 'r') as f:
    DB_CONNECTIONSTRING = f.readline()
DB = dataset.connect(DB_CONNECTIONSTRING)

def load_tweet(tweet, tweets_saved):
    """Takes a tweet (dictionary) and upserts its contents to a PostgreSQL database"""
    try:
        data = {'tweet_id': tweet['tweet_id',
                'tweet_json': json.dumps(tweet)} # Dictionary that contains the data I need from the tweet
        DB['tweets'].upsert(data, ['tweet_id'])
        tweets_saved += 1
        if tweets_saved % 100 == 0:
            print('Saved ' + str(tweets_saved) + ' tweets')
        return tweets_saved
    except KeyError:
        return tweets_saved

if __name__ == "__main__":
    tweets['tweet1', 'tweet2']
    for tweet in tweets:
        tweets_saved = load_tweet(tweet, tweets_saved)
</code></pre>
<p>As such, I was looking for an option to do this multithreaded. However, I have not yet found a way in which I can:</p>
<ul>
<li>Multithread the extraction process;</li>
<li>Print a counter per 100, 500 or 1000 tweets;</li>
</ul>
<p>Going through <a href="http://www.tutorialspoint.com/python/python_multithreading.htm" rel="nofollow">this tutorial</a> hasn't given me the understanding to do this yet: the concepts of a class for each thread, what I need to customize in the class and implementing a Queue at the moment is a lot for me to grasp at the moment; I'm only semi-starting out.</p>
<ul>
<li>Could someone provide hints as to how I would incorporate the script above utilizing multiple threads? </li>
<li>How many threads should I use? Python currently uses ~1% of my CPU while running the script and ~ 10% of RAM (my <a href="http://www.asus.com/Notebooks_Ultrabooks/ASUS_ROG_G750JS/specifications/" rel="nofollow">system specs</a>)</li>
<li>How do I take care of incrementing a counter (using the Lock()?), and printing upon hitting counter % 100?</li>
</ul>
<p>EDIT: As requested: here are the big shots from the profiler result (with dataset.upsert):</p>
<pre><code>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    5898  245.133    0.042  245.133    0.042 :0(_connect)
    5898   12.137    0.002   12.206    0.002 :0(execute)
</code></pre>
<p>Here is a second try with 'dataset.insert' instead of 'dataset.upsert':</p>
<pre><code>1386332 function calls (1382960 primitive calls) in 137.255 seconds

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  2955  122.646    0.042  122.646    0.042 :0             (_connect)
</code></pre>
<p>Last (and definitely not least), here's the timing when running raw psycopg2 code. </p>
<pre><code>63694 function calls (63680 primitive calls) in 2.203 seconds
</code></pre>
<p>Concluding, don't use dataset for performance (though writing the psycopg2 code took me 10 minutes which is &gt;&gt; the 10 seconds for a dataset.upsert)</p>
<ul>
<li>Now, as for the original question. Will I be able to reduce the ~ 2 second per file even more by multithreading? How?</li>
</ul>
<p>The full code can be found <a href="https://github.com/MVersteeg/Twitter-Archive-Loader/blob/master/pyTwitter.py" rel="nofollow">here</a></p>
</div>
<div class="post-text" itemprop="text">
<p>Several things that can be improved: </p>
<p>Run the whole batch on a single transaction. Using transaction means the database are not required to actually commit (write the data to disk) on every single writes, but rather it can buffer uncommitted data on memory. This usually leads to more efficient resource usage.</p>
<p>Add a unique index over tweet_id. Without a unique index, you may be forcing the database to do a sequential scan on every upserts, which leads bulk upsert to scale by O(n**2).</p>
<p>Split the insert and updates, use .insert_many() whenever you can rather than .upsert(). Before doing the bulk upsert, you do a preflight query to find out the list of tweet_ids that exists in both the database and your list of tweets. Use .insert_many() to insert items that don't already exists in the database and plain .update() for those that already exists.</p>
</div>
<div class="post-text" itemprop="text">
<p>I dont know if you'll be able to improve performance. But as for how I think you'll want concurrent.futures.Executor.map. ProcessPoolExecutor rather than ThreadPoolExecutor should be what you want, although I'm no expert.</p>
<p><a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.map" rel="nofollow">https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.map</a></p>
<p>If you want to show progress have a look at concurrent.futures.as_completed from the same module.</p>
</div>
<span class="comment-copy">It looks like you are not CPU bound so multithreading might only slow you down. If you are I/O bound, as I suspect, you will get a much better performance boost by buying a faster hard disk - as sad as it sounds. But check other processes, such as your RDBMS' before you go this route.</span>
<span class="comment-copy">Python is currently running from my SSD, the database (PostgreSQL) is running from a USB 3.0 connected external drive. The files I am reading from are on a 7200rpm HDD (though I imagine that this can't affect performance as the list is already in memory?). Would you recommend a different setup? I couldn't imagine it being the hardware?</span>
<span class="comment-copy">Before we can write effective concurrent code, we need to understand what the bottleneck is. I don't understand why it takes 5 mins to process 3600 tweets, for example. Do you know what specific lines are consuming the most time? <a href="http://pymotw.com/2/profile/" rel="nofollow noreferrer">Profiling the code</a> can help you answer that.</span>
<span class="comment-copy">Profiling is a good idea. Your example leads one to believe that <code>dataset</code> has horribly bad performance. If so, threading won't help. You could experiment with doing <code>.insert</code> instead of <code>.upsert</code> but you'd really want to compare against a standard sql insert (esp one with multiple values).</span>
<span class="comment-copy">Added the profiler result, seems like '_connect' has run for ~ 90% of the time. Now trying alternate results with alternate calls (.insert, directly with psycopg2).</span>
