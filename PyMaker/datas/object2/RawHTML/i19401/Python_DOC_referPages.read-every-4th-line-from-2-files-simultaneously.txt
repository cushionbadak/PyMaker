<div class="post-text" itemprop="text">
<p>I am processing largish text files (10 MB gziped). There are always 2 files that belong together, both of equal length and structure: 4 lines per dataset. </p>
<p>I need to process data from line 2 in every block of 4, from both files at the same time.</p>
<p>My question: what is the most time-efficient approach to this?</p>
<p>Right now I'm doing this:</p>
<pre><code>def read_groupwise(iterable, n, fillvalue=None):
    args = [iter(iterable)] * n
    return itertools.izip_longest(fillvalue=fillvalue, *args)

f1 = gzip.open(file1,"r")
f2 = gzip.open(file2,"r")
for (fline1,fline2,fline3,fline4), (rline1, rline2, rline3, rline4) in zip(read_groupwise(f1, 4), read_groupwise(f2, 4)):
    # process fline2, rline2
</code></pre>
<p>But since I only need line2 of each, I'm guessing there's probably a much more efficent way to do this?</p>
</div>
<div class="post-text" itemprop="text">
<p>This could be done by constructing your own generator:</p>
<pre><code>def get_nth(iterable, n, after=1):
    if after &gt; 1:
        consume(iterable, after-1)
    while True:
        yield next(iterable)
        consume(iterable, n-1)

with gzip.open(file1, "r") as f1, gzip.open(file2, "r") as f2:
    every = (4, 2)
    for line_f1, line_f2 in zip(get_nth(f1, *every), get_nth(f2, *every)):
        ...
</code></pre>
<p>The generator advances to the first item to be given (in this case, we want the second item, so we skip one to place the iterator before the second item), and then yields one value, and advances to place itself before the next item. This is quite a simple way to achieve the task at hand.</p>
<p>Here using <a href="http://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow"><code>consume()</code> from <code>itertools</code>' recipes</a>:</p>
<pre><code>def consume(iterator, n):
    "Advance the iterator n-steps ahead. If n is none, consume entirely."
    # Use functions that consume iterators at C speed.
    if n is None:
        # feed the entire iterator into a zero-length deque
        collections.deque(iterator, maxlen=0)
    else:
        # advance to the empty slice starting at position n
        next(islice(iterator, n, n), None)
</code></pre>
<p>As a final note, I'm not sure if <code>gzip.open()</code> gives a context manager, if it doesn't, you'll want to use <code>contextlib.closing()</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>I would suggest to use a straight away <a href="http://docs.python.org/2/library/itertools.html#itertools.izip_longest" rel="nofollow">itertools.izip_longest</a> to zip the content of both the files and <a href="http://docs.python.org/2/library/itertools.html#itertools.islice" rel="nofollow">itertools.islice</a> to select every fourth element starting from line 2</p>
<pre><code>&gt;&gt;&gt; def get_nth(iterable, n, after=1, fillvalue = ""):
    return islice(izip_longest(*iterable,fillvalue=fillvalue), n, None, after)

&gt;&gt;&gt; with gzip.open(file1, "r") as f1, gzip.open(file2, "r") as f2:
    for line in get_nth([f1, f2], n = 2):
        print map(str.strip, line)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If you have the memory then try:</p>
<pre><code>ln1 = f1.readlines()[2::4]
ln2 = f2.readlines()[2::4]
for fline, rline in zip(ln1, ln2):
    ...
</code></pre>
<p>But only if you have the memory.</p>
</div>
<span class="comment-copy">I've tried it out, but it's not faster than what I had before. Thanks for the suggestion, though. Sidequestion: what's the benefit of using "with open(file) as f..." compared to "f = open(file)"?</span>
<span class="comment-copy">Faster isn't the most important thing - readability is the more important issue. As to <code>with</code>, it closes the file when you move out of the scope of the with block, and does so in a <code>try: ... finally: ...</code> block, which means it will do so even if there is an exception. It is more readable, gets rid of 'file.close()' clutter, ensures the file gets closed in all situations, and it generally a good idea.</span>
<span class="comment-copy">I've tried it out, but it's not faster than what I had before. Thanks for the suggestion, though.</span>
