<div class="post-text" itemprop="text">
<p>Say I have a very large list and I'm performing an operation like so:</p>
<pre><code>for item in items:
    try:
        api.my_operation(item)
    except:
        print 'error with item'
</code></pre>
<p>My issue is two fold:</p>
<ul>
<li>There are a lot of items</li>
<li>api.my_operation takes forever to return</li>
</ul>
<p>I'd like to use multi-threading to spin up a bunch of api.my_operations at once so I can process maybe 5 or 10 or even 100 items at once. </p>
<p>If my_operation() returns an exception (because maybe I already processed that item) - that's OK. It won't break anything. The loop can continue to the next item.</p>
<p><strong>Note</strong>: this is for Python 2.7.3</p>
</div>
<div class="post-text" itemprop="text">
<p>First, in Python, if your code is CPU-bound, multithreading won't help, because only one thread can hold the Global Interpreter Lock, and therefore run Python code, at a time. So, you need to use processes, not threads.</p>
<p>This is not true if your operation "takes forever to return" because it's IO-bound—that is, waiting on the network or disk copies or the like. I'll come back to that later.</p>
<hr/>
<p>Next, the way to process 5 or 10 or 100 items at once is to create a pool of 5 or 10 or 100 workers, and put the items into a queue that the workers service. Fortunately, the stdlib <a href="http://docs.python.org/3/library/multiprocessing.html"><code>multiprocessing</code></a> and <a href="http://docs.python.org/3/library/concurrent.futures.html"><code>concurrent.futures</code></a> libraries both wraps up most of the details for you. </p>
<p>The former is more powerful and flexible for traditional programming; the latter is simpler if you need to compose future-waiting; for trivial cases, it really doesn't matter which you choose. (In this case, the most obvious implementation with each takes 3 lines with <code>futures</code>, 4 lines with <code>multiprocessing</code>.) </p>
<p>If you're using 2.6-2.7 or 3.0-3.1, <code>futures</code> isn't built in, but you can install it from <a href="https://pypi.python.org/pypi/futures">PyPI</a> (<code>pip install futures</code>).</p>
<hr/>
<p>Finally, it's usually a lot simpler to parallelize things if you can turn the entire loop iteration into a function call (something you could, e.g., pass to <code>map</code>), so let's do that first:</p>
<pre><code>def try_my_operation(item):
    try:
        api.my_operation(item)
    except:
        print('error with item')
</code></pre>
<hr/>
<p>Putting it all together:</p>
<pre><code>executor = concurrent.futures.ProcessPoolExecutor(10)
futures = [executor.submit(try_my_operation, item) for item in items]
concurrent.futures.wait(futures)
</code></pre>
<hr/>
<p>If you have lots of relatively small jobs, the overhead of multiprocessing might swamp the gains. The way to solve that is to batch up the work into larger jobs. For example (using <code>grouper</code> from the <a href="http://docs.python.org/3/library/itertools.html#recipes"><code>itertools</code> recipes</a>, which you can copy and paste into your code, or get from the <code>more-itertools</code> project on PyPI):</p>
<pre><code>def try_multiple_operations(items):
    for item in items:
        try:
            api.my_operation(item)
        except:
            print('error with item')

executor = concurrent.futures.ProcessPoolExecutor(10)
futures = [executor.submit(try_multiple_operations, group) 
           for group in grouper(5, items)]
concurrent.futures.wait(futures)
</code></pre>
<hr/>
<p>Finally, what if your code is IO bound? Then threads are just as good as processes, and with less overhead (and fewer limitations, but those limitations usually won't affect you in cases like this). Sometimes that "less overhead" is enough to mean you don't need batching with threads, but you do with processes, which is a nice win.</p>
<p>So, how do you use threads instead of processes? Just change <code>ProcessPoolExecutor</code> to <code>ThreadPoolExecutor</code>.</p>
<p>If you're not sure whether your code is CPU-bound or IO-bound, just try it both ways.</p>
<hr/>
<blockquote>
<p>Can I do this for multiple functions in my python script? For example, if I had another for loop elsewhere in the code that I wanted to parallelize. Is it possible to do two multi threaded functions in the same script?</p>
</blockquote>
<p>Yes. In fact, there are two different ways to do it.</p>
<p>First, you can share the same (thread or process) executor and use it from multiple places with no problem. The whole point of tasks and futures is that they're self-contained; you don't care where they run, just that you queue them up and eventually get the answer back.</p>
<p>Alternatively, you can have two executors in the same program with no problem. This has a performance cost—if you're using both executors at the same time, you'll end up trying to run (for example) 16 busy threads on 8 cores, which means there's going to be some context switching. But sometimes it's worth doing because, say, the two executors are rarely busy at the same time, and it makes your code a lot simpler. Or maybe one executor is running very large tasks that can take a while to complete, and the other is running very small tasks that need to complete as quickly as possible, because responsiveness is more important than throughput for part of your program.</p>
<p>If you don't know which is appropriate for your program, usually it's the first.</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Edit 2018-02-06</strong>: revision based on <a href="https://stackoverflow.com/questions/15143837/how-to-multi-thread-an-operation-within-a-loop-in-python/15144765?noredirect=1#comment84270429_15144765">this comment</a></p>
<p><strong>Edit</strong>: forgot to mention that this works on Python 2.7.x</p>
<p>There's multiprocesing.pool, and the following sample illustrates how to use one of them:</p>
<pre><code>from multiprocessing.pool import ThreadPool as Pool
# from multiprocessing import Pool

pool_size = 5  # your "parallelness"

# define worker function before a Pool is instantiated
def worker(item):
    try:
        api.my_operation(item)
    except:
        print('error with item')

pool = Pool(pool_size)

for item in items:
    pool.apply_async(worker, (item,))

pool.close()
pool.join()
</code></pre>
<p>Now if you indeed identify that your process is CPU bound as @abarnert mentioned, change ThreadPool to the process pool implementation (commented under ThreadPool import). You can find more details here: <a href="http://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers" rel="nofollow noreferrer">http://docs.python.org/2/library/multiprocessing.html#using-a-pool-of-workers</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You can split the processing into a specified number of threads using an approach like this:</p>
<pre><code>import threading                                                                

def process(items, start, end):                                                 
    for item in items[start:end]:                                               
        try:                                                                    
            api.my_operation(item)                                              
        except Exception:                                                       
            print('error with item')                                            


def split_processing(items, num_splits=4):                                      
    split_size = len(items) // num_splits                                       
    threads = []                                                                
    for i in range(num_splits):                                                 
        # determine the indices of the list this thread will handle             
        start = i * split_size                                                  
        # special case on the last chunk to account for uneven splits           
        end = None if i+1 == num_splits else (i+1) * split_size                 
        # create the thread                                                     
        threads.append(                                                         
            threading.Thread(target=process, args=(items, start, end)))         
        threads[-1].start() # start the thread we just created                  

    # wait for all threads to finish                                            
    for t in threads:                                                           
        t.join()                                                                



split_processing(items)
</code></pre>
</div>
<span class="comment-copy">Trying to enable my code (for loop) to run this way but not sure where api is coming from. NameError: name 'api' is not defined</span>
<span class="comment-copy">Do I need to install concurrent? How? I'm using Python 2.7.3 and it can't find the concurrent module. <i>edit</i> looks like concurrent is only available in 3.2. Bummer.</span>
<span class="comment-copy">@doremi: I thought you were using 3.x, because you're calling <code>print</code> as a function rather than using it as a statement. But if you're using 2.x, you can install <code>futures</code> from <a href="https://pypi.python.org/pypi/futures" rel="nofollow noreferrer">PyPI</a> (e.g., <code>pip install futures</code>) and just <code>import futures</code> instead of <code>import concurrent.futures</code>. (Or you can use `multiprocessing, which is not much more complicated—it just means 4 lines of code instead of 3.)</span>
<span class="comment-copy">This is a very in depth answer @abarnert, as usual :) Please see my answer as well for a highly portable version of the implementation (using all that's available from 2.7.x).</span>
<span class="comment-copy">@jeffrey: Yes. Let me edit the answer, because the response is just a bit too long to fit in a comment.</span>
<span class="comment-copy">@jeffrey: Well, even with only one function, you can only safely use it with an executor if it doesn't rely on futures from another iteration. Using two functions does make it <i>easier</i> to inadvertently depend on a sequencing that doesn't exist, but it's already possible. Anyway, the usual way to avoid that is to actually put the sequencing in explicitly—e.g., schedule the second function in a callback on the first. The fact that futures are chainable and otherwise composable is part of what makes them a useful abstraction over pool tasks.</span>
<span class="comment-copy">Note that <code>multiprocessing.ThreadPool</code> is not documented at all. However, <code>multiprocessing.dummy.Pool</code> is the same class, and it <i>is</i> <a href="https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy" rel="nofollow noreferrer">documented</a>. It's arguable that the first one is more explicitly meaningful, and worth using even though it isn't technically guaranteed to work. Either way, you might want a comment  (explaining that the latter is a threadpool despite the name, or the former is present in all CPython 2.7 versions even though it isn't documented).</span>
<span class="comment-copy">For all other Python noobs like me: if this isn't working for you, make sure that <code>worker</code> is defined before you create your pool! (per <a href="https://stackoverflow.com/a/2783017/398316">stackoverflow.com/a/2783017/398316</a>)</span>
<span class="comment-copy">@M2X good call. I'll revise the answer accordingly</span>
