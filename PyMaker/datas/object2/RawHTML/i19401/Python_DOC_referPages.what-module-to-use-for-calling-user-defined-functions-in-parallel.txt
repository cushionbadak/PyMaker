<div class="post-text" itemprop="text">
<p>I've written an irc bot that runs some commands when told so, the commands are predefined python functions that will be called on the server where the bot is running.</p>
<p>I have to call those functions without knowing exactly what they'll do
(more I/O or something computationally expensive, nothing harmful since I review them when I accept them), but I need to get their return value in order to give a reply back to the irc channel.</p>
<p>What module do you recommend for running several of these callbacks in parallel and why?</p>
<p>The <code>threading</code> or <code>multiprocessing</code> modules, something else?</p>
<p>I heard about twisted, but I don't know how it will fit in my current implementation since I know nothing about it and the bot is fully functional from the point of view of the protocol.</p>
<p>Also requiring the commands to do things asynchronously is not an option since I want the bot to be easily extensible.</p>
</div>
<div class="post-text" itemprop="text">
<p>First, the tl;dr:</p>
<p>Use <a href="http://docs.python.org/3/library/concurrent.futures.html" rel="nofollow"><code>concurrent.futures</code></a> if you're using 3.2+, or the <a href="https://pypi.python.org/pypi/futures" rel="nofollow"><code>futures</code></a> module on PyPI that backports the same thing if you're using 2.x.</p>
<p>You can write your code with a <a href="http://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor-example" rel="nofollow"><code>ThreadPoolExecutor</code></a> and switch it to a <a href="http://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor" rel="nofollow"><code>ProcessPoolExecutor</code></a> as a one-liner change. And the API is so minimal and simple that there's nothing to get confused by.</p>
<hr/>
<blockquote>
<p>Also requiring the commands to do things asynchronously is not an option since I want the bot to be easily extensible.</p>
</blockquote>
<p>I don't see how that follows. There's nothing about async code that makes it less extensible. Of course you have to know how to write async code in order to extend it, but thousands of novice JS programmers are doing a nearly-passable job of that every day, and Python makes it a whole lot easier (see <a href="https://github.com/saucelabs/monocle" rel="nofollow"><code>monocle</code></a>, <a href="http://twistedmatrix.com/documents/8.1.0/api/twisted.internet.defer.html#inlineCallbacks" rel="nofollow"><code>inlineCallbacks</code></a> in <code>twisted</code>, <code>tulip</code>, etc.). Also, the fact that you explicitly refer to these things as "callbacks" in your description implies that you're already thinking in those terms…</p>
<p>If you're convinced this actually is a requirement, then <code>twisted</code> is not acceptable. But <a href="http://gevent.org/intro.html" rel="nofollow"><code>gevent</code></a> (and <code>eventlet</code>, etc.) may be—you can just write code that looks completely synchronous, and it runs asynchronously.</p>
<p>Next:</p>
<blockquote>
<p>What module do you recommend for running several of these callbacks in parallel and why?</p>
</blockquote>
<p>Do you really need to run them in parallel (you can take advantage of multiple cores to run multiple CPU-bound jobs at the same time), concurrently (a long-running job won't block other jobs), or neither (as long as the jobs get done, it doesn't matter whether they're parallelized, interleaved, or serialized)?</p>
<p>If you need parallelism, you need <code>multiprocessing</code>. There's really no way around that; the GIL will prevent you from using multiple cores in a single process.</p>
<p>If you only need concurrency, you can use either <code>threading</code> or <code>multiprocessing</code>. Processes may mean more overhead and/or more portability issues between Windows and Unix (and even sometimes between Unixes), and it sometimes forces you to think about how to pass data around—or, if you must, share it. On the other hand, by <em>not</em> forcing you to think about passing or sharing data, threads make it easier to accidentally create races and other bugs. (See isedev's great answer for more on the tradeoffs.)</p>
<p>If you need neither, you can use <code>gevent</code> (or something similar), <code>threading</code>, or <code>multiprocessing</code>. You can create and switch between 10000 green threads as easily as you can create a few hundreds threads or processes, and with much less overhead. However, a single long-running CPU-bound command can stall your entire system.</p>
<hr/>
<p>Whichever one you use, you most likely want to use a pool of greenlets, threads, or processes pulling commands off a queue (rather than spinning off a new one for each command, or building something more complex).</p>
<p>While <code>multiprocessing</code> has such a thing built in, <code>threading</code> does not. (Actually, there <em>is</em> a <code>threading</code>-based threadpool—but it's in <code>multiprocessing</code>, not <code>threading</code>. And it's not part of the public API.)</p>
<p>There's a lot of amazingly cool stuff in <code>multiprocessing</code>, and if you need it, definitely use it. (There are also some third-party libraries with even cooler stuff in them, which can make complex use cases a whole lot easier, or do things that <code>multiprocessing</code> just can't do.) But if not, <code>futures</code> is a lot simpler, and the ability to test the same system with threads and processes with a one-liner change (or even trivially doing it at runtime) is very nice.</p>
</div>
<div class="post-text" itemprop="text">
<p>There is no definitive answer to your question: it really depends what the functions do, how often they are called and what level of parallelism you need.</p>
<p>The <code>threading</code> and <code>multiprocessing</code> modules work in radically different ways.</p>
<p><code>threading</code> implements native threads within the Python interpreter: fairly inexpensive to create but limited in parallelism due to Python's Global Interpreter Lock (GIL). Threads share the same address space, so may interfere with each other (e.g. if a thread causes the interpreter to crash, all threads, including your app, die), but inter-thread communication is cheap and fast as a result.</p>
<p><code>multiprocessing</code> implements parallelism using distinct processes: the setup is far more expensive than threads (required creation of a new process), but each process runs its own copy of the interpreter (hence no GIL related locking issues) and run in different address spaces (isolating your main app). The child processes communicate with the parent over IPC channels and required Python objects to be pickled/unpickled - so again, more expensive than threads.</p>
<p>You need to figure out what trade-off is best suited to your purpose.</p>
</div>
<span class="comment-copy">the question is not specific enough. You could try <a href="http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html#application" rel="nofollow noreferrer"><code>celery</code></a> that supports a wide-range of requirements.</span>
<span class="comment-copy"><code>I don't see how that follows. There's nothing about async code that makes it less extensible.</code> I said that because I don't want the users that extend the bot to think whether their code will stall the bot or not, I just want their code to run without them having to worry too much. It's better that I worry once, write the bot cleverly and then no one will have to think about solving this problem again when writing commands for the bot. Thank you for the detailed answer, I'll read it carefully tomorrow since there are things I don't know about.</span>
<span class="comment-copy">@Paul: If you just can't trust users to explicitly mark IO waits, something like <code>gevent</code> magically handles that. But if you can't event trust them to avoid doing long runs of CPU-bound work with no yields in the middle, you have to use preemptive multitasking, whether thread-based or process-based. (It really is as simple as writing <code>yield</code> every so often… but maybe that's still too much.)</span>
<span class="comment-copy"><code>multiprocessing.dummy</code> module that provides the same interface as <code>multiprocessing</code> and uses threads instead of processes <a href="http://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.dummy" rel="nofollow noreferrer">is public API</a>.</span>
<span class="comment-copy">Thank you, I'll use <code>concurrent.futures</code>.</span>
<span class="comment-copy">That's the problem, the bot can be extended to do anything. And the functions can be called as many times the users on the irc channels the bot is connected to require. I think I made my mind about this because of what you said: <code>if a thread causes the interpreter to crash, all threads, including your app, die</code> so I think is more important to keep the bot up and running than having a performance boost.</span>
<span class="comment-copy"><code>threading</code> doesn't implement lightweight threads within the interpreter; it uses native OS threads. (Of course "lightweight thread" is a bit of a vague term, but usually it means user-level threads without native context switching.)</span>
<span class="comment-copy">yes, perhaps not the best way to express it. I meant lightweight in comparison to fork/exec children... not LWP. Will update all the same.</span>
<span class="comment-copy">Actually, they <i>are</i> LWPs. It's horribly confusing that "lightweight process" and "heavyweight thread" mean pretty much the same thing. (I think that's Sun's fault, but I'm not sure.)</span>
