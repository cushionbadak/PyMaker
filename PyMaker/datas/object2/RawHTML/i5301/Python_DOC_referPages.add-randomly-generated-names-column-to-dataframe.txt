<div class="post-text" itemprop="text">
<p>I know how to create random string, like:</p>
<pre><code>''.join(secrets.choice(string.ascii_uppercase + string.digits) for _ in range(N))
</code></pre>
<p>However, there should be no duplicates so what I am currently just checking if the key already exists in a list, like shown in the following code:</p>
<pre><code>import secrets
import string
import numpy as np


amount_of_keys = 40000

keys = []

for i in range(0,amount_of_keys):
    N = np.random.randint(12,20)
    n_key = ''.join(secrets.choice(string.ascii_uppercase + string.digits) for _ in range(N))
    if not n_key in keys:
        keys.append(n_key)
</code></pre>
<p>Which is okay for a small amount of keys like <code>40000</code>, however the problem does not scale well the more keys there are. So I am wondering if there is a faster way to get to the result for even more keys, like <code>999999</code></p>
</div>
<div class="post-text" itemprop="text">
<h2>Basic improvements, sets and local names</h2>
<p>Use a <em>set</em>, not a list, and testing for uniqueness is much faster; set membership testing takes constant time independent of the set size, while lists take O(N) linear time. Use a set comprehension to produce a series of keys at a time to avoid having to look up and call the <code>set.add()</code> method in a loop; properly random, larger keys have a very small chance of producing duplicates anyway.</p>
<p>Because this is done in a tight loop, it is worth your while optimising away all name lookups as much as possible:</p>
<pre><code>import secrets
import numpy as np
from functools import partial

def produce_amount_keys(amount_of_keys, _randint=np.random.randint):
    keys = set()
    pickchar = partial(secrets.choice, string.ascii_uppercase + string.digits)
    while len(keys) &lt; amount_of_keys:
        keys |= {''.join([pickchar() for _ in range(_randint(12, 20))]) for _ in range(amount_of_keys - len(keys))}
    return keys
</code></pre>
<p>The <code>_randint</code> keyword argument binds the <code>np.random.randint</code> name to a local in the function, which are faster to reference than globals, especially when attribute lookups are involved.</p>
<p>The <code>pickchar()</code> partial avoids looking up attributes on modules or more locals; it is a single callable that has all the references in place, so is faster in execute, especially when done in a loop.</p>
<p>The <code>while</code> loop keeps iterating only if there were duplicates produced. We produce enough keys in a single set comprehension to fill the remainder if there are no duplicates.</p>
<h2>Timings for that first improvement</h2>
<p>For 100 items, the difference is not that big:</p>
<pre><code>&gt;&gt;&gt; timeit('p(100)', 'from __main__ import produce_amount_keys_list as p', number=1000)
8.720592894009314
&gt;&gt;&gt; timeit('p(100)', 'from __main__ import produce_amount_keys_set as p', number=1000)
7.680242831003852
</code></pre>
<p>but when you start scaling this up, you'll notice that the O(N) membership test cost against a list really drags your version down:</p>
<pre><code>&gt;&gt;&gt; timeit('p(10000)', 'from __main__ import produce_amount_keys_list as p', number=10)
15.46253142200294
&gt;&gt;&gt; timeit('p(10000)', 'from __main__ import produce_amount_keys_set as p', number=10)
8.047800761007238
</code></pre>
<p>My version is already almost twice as fast as 10k items; 40k items can be run 10 times in about 32 seconds:</p>
<pre><code>&gt;&gt;&gt; timeit('p(40000)', 'from __main__ import produce_amount_keys_list as p', number=10)
138.84072386901244
&gt;&gt;&gt; timeit('p(40000)', 'from __main__ import produce_amount_keys_set as p', number=10)
32.40720253501786
</code></pre>
<p>The list version took over 2 minutes, more than ten times as long.</p>
<h2>Numpy's random.choice function, not cryptographically strong</h2>
<p>You can make this faster still by forgoing the <code>secrets</code> module and using <code>np.random.choice()</code> instead; this won't produce a cryptographic level randomness however, but picking a random character is twice as fast:</p>
<pre><code>def produce_amount_keys(amount_of_keys, _randint=np.random.randint):
    keys = set()
    pickchar = partial(
        np.random.choice,
        np.array(list(string.ascii_uppercase + string.digits)))
    while len(keys) &lt; amount_of_keys:
        keys |= {''.join([pickchar() for _ in range(_randint(12, 20))]) for _ in range(amount_of_keys - len(keys))}
    return keys
</code></pre>
<p>This makes a huge difference, now 10 times 40k keys can be produced in just 16 seconds:</p>
<pre><code>&gt;&gt;&gt; timeit('p(40000)', 'from __main__ import produce_amount_keys_npchoice as p', number=10)
15.632006907981122
</code></pre>
<h2>Further tweaks with the itertools module and a generator</h2>
<p>We can also take the <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="noreferrer"><code>unique_everseen()</code> function</a> from the <code>itertools</code> module <em>Recipes</em> section to have it take care of the uniqueness, then use an infinite generator and the <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="noreferrer"><code>itertools.islice()</code> function</a> to limit the results to just the number we want:</p>
<pre><code># additional imports
from itertools import islice, repeat

# assumption: unique_everseen defined or imported

def produce_amount_keys(amount_of_keys):
    pickchar = partial(
        np.random.choice,
        np.array(list(string.ascii_uppercase + string.digits)))
    def gen_keys(_range=range, _randint=np.random.randint):
        while True:
            yield ''.join([pickchar() for _ in _range(_randint(12, 20))])
    return list(islice(unique_everseen(gen_keys()), amount_of_keys))
</code></pre>
<p>This is slightly faster still, but only marginally so:</p>
<pre><code>&gt;&gt;&gt; timeit('p(40000)', 'from __main__ import produce_amount_keys_itertools as p', number=10)
14.698191125993617
</code></pre>
<h2>os.urandom() bytes and a different method of producing strings</h2>
<p>Next, we can follow on on <a href="https://stackoverflow.com/a/48422376/100297">Adam Barnes's ideas</a> for using UUID4 (which is basically just a wrapper around <a href="https://docs.python.org/3/library/os.html#os.urandom" rel="noreferrer"><code>os.urandom()</code></a>) and Base64. But by case-folding Base64 and replacing 2 characters with randomly picked ones, his method severely limits the entropy in those strings (you won't produce the full range of unique values possible, a 20-character string only using <code>(256 ** 15) / (36 ** 20)</code> == 1 in every 99437 bits of entropy!).</p>
<p>The Base64 encoding uses both upper and lower case characters and digits but also <em>adds</em> the <code>-</code> and <code>/</code> characters (or <code>+</code> and <code>_</code> for the URL-safe variant). For only uppercase letters and digits, you'd have to uppercase the output and map those extra two characters to other random characters, a process that throws away a large amount of entropy from the random data provided by <code>os.urandom()</code>. Instead of using Base64, you could also use the Base32 encoding, which uses uppercase letters and the digits 2 through 8, so produces strings with 32 ** n possibilities versus 36 ** n. However, this can speed things up further from the above attempts:</p>
<pre><code>import os
import base64
import math

def produce_amount_keys(amount_of_keys):
    def gen_keys(_urandom=os.urandom, _encode=base64.b32encode, _randint=np.random.randint):
        # (count / math.log(256, 32)), rounded up, gives us the number of bytes
        # needed to produce *at least* count encoded characters
        factor = math.log(256, 32)
        input_length = [None] * 12 + [math.ceil(l / factor) for l in range(12, 20)]
        while True:
            count = _randint(12, 20)
            yield _encode(_urandom(input_length[count]))[:count].decode('ascii')
    return list(islice(unique_everseen(gen_keys()), amount_of_keys))
</code></pre>
<p>This is <strong>really</strong> fast:</p>
<pre><code>&gt;&gt;&gt; timeit('p(40000)', 'from __main__ import produce_amount_keys_b32 as p', number=10)
4.572628145979252
</code></pre>
<p>40k keys, 10 times, in just over 4 seconds. So about 75 times as fast; the speed of using <code>os.urandom()</code> as a source is undeniable.</p>
<p>This is, <em>cryptographically strong again</em>; <code>os.urandom()</code> produces bytes for cryptographic use. On the other hand, we reduced the number of possible strings produced by more than 90% (<code>((36 ** 20) - (32 ** 20)) / (36 ** 20) * 100</code> is 90.5), we are no longer using the <code>0</code>, <code>1</code>, <code>8</code> and <code>9</code> digits in the outputs.</p>
<p>So perhaps we should use the <code>urandom()</code> trick to produce a proper Base36 encoding; we'll have to produce our own <code>b36encode()</code> function:</p>
<pre><code>import string
import math

def b36encode(b, 
        _range=range, _ceil=math.ceil, _log=math.log, _fb=int.from_bytes, _len=len, _b=bytes,
        _c=(string.ascii_uppercase + string.digits).encode()):
    """Encode a bytes value to Base36 (uppercase ASCII and digits)

    This isn't too friendly on memory because we convert the whole bytes
    object to an int, but for smaller inputs this should be fine.
    """
    b_int = _fb(b, 'big')
    length = _len(b) and _ceil(_log((256 ** _len(b)) - 1, 36))
    return _b(_c[(b_int // 36 ** i) % 36] for i in _range(length - 1, -1, -1))
</code></pre>
<p>and use that:</p>
<pre><code>def produce_amount_keys(amount_of_keys):
    def gen_keys(_urandom=os.urandom, _encode=b36encode, _randint=np.random.randint):
        # (count / math.log(256, 36)), rounded up, gives us the number of bytes
        # needed to produce *at least* count encoded characters
        factor = math.log(256, 36)
        input_length = [None] * 12 + [math.ceil(l / factor) for l in range(12, 20)]
        while True:
            count = _randint(12, 20)
            yield _encode(_urandom(input_length[count]))[-count:].decode('ascii')
    return list(islice(unique_everseen(gen_keys()), amount_of_keys))
</code></pre>
<p>This is reasonably fast, and above all produces the full range of 36 uppercase letters and digits:</p>
<pre><code>&gt;&gt;&gt; timeit('p(40000)', 'from __main__ import produce_amount_keys_b36 as p', number=10)
8.099918447987875
</code></pre>
<p>Granted, the base32 version is almost twice as fast as this one (thanks to an efficient Python implementation using a table) but using a custom Base36 encoder is still twice the speed of the non-cryptographically secure <code>numpy.random.choice()</code> version.</p>
<p>However, using <code>os.urandom()</code> <em>produces bias</em> again; we have to produce more bits of entropy than is required for between 12 to 19 base36 'digits'. For 17 digits, for example, we can't produce 36 ** 17 different values using bytes, only the nearest equivalent of 256 ** 11 bytes, which is about 1.08 times too high, and so we'll end up with a bias towards <code>A</code>, <code>B</code>, and to a lesser extent, <code>C</code> (thanks <a href="https://stackoverflow.com/users/1672429/stefan-pochmann">Stefan Pochmann</a> for pointing this out).</p>
<h2>Picking an integer below <code>(36 ** length)</code> and mapping integers to base36</h2>
<p>So we need to reach out to a secure random method that can give us values evenly distributed between <code>0</code> (inclusive) and <code>36 ** (desired length)</code> (exclusive). We can then map the number directly to the desired string.</p>
<p>First, mapping the integer to a string; the following has been tweaked to produce the output string the fastest:</p>
<pre><code>def b36number(n, length, _range=range, _c=string.ascii_uppercase + string.digits):
    """Convert an integer to Base36 (uppercase ASCII and digits)"""
    chars = [_c[0]] * length
    while n:
        length -= 1
        chars[length] = _c[n % 36]
        n //= 36
    return ''.join(chars)
</code></pre>
<p>Next, we need a fast and <em>cryptographically secure</em> method of picking our number in a range. You can still use <code>os.urandom()</code> for this, but then you'd have to mask the bytes down to a maximum number of bits, and then loop until your actual value is below the limit. This is actually already implemented, by the <a href="https://docs.python.org/3/library/secrets.html#secrets.randbelow" rel="noreferrer"><code>secrets.randbelow()</code> function</a>. In Python versions &lt; 3.6 you can use <a href="https://docs.python.org/3/library/random.html#random.randrange" rel="noreferrer"><code>random.SystemRandom().randrange()</code></a>, which uses the exact same method with some extra wrapping to support a lower bound greater than 0 and a step size.</p>
<p>Using <code>secrets.randbelow()</code> the function becomes:</p>
<pre><code>import secrets

def produce_amount_keys(amount_of_keys):
    def gen_keys(_below=secrets.randbelow, _encode=b36number, _randint=np.random.randint):
        limit = [None] * 12 + [36 ** l for l in range(12, 20)]
        while True:
            count = _randint(12, 20)
            yield _encode(_below(limit[count]), count)
    return list(islice(unique_everseen(gen_keys()), amount_of_keys))
</code></pre>
<p>and this then is quite close to the (probably biased) base64 solution:</p>
<pre><code>&gt;&gt;&gt; timeit('p(40000)', 'from __main__ import produce_amount_keys_below as p', number=10)
5.135716405988205
</code></pre>
<p>This is almost as fast as the Base32 approach, but produces the full range of keys!</p>
</div>
<div class="post-text" itemprop="text">
<p>So it's a speed race is it?</p>
<p>Building on the work of Martijn Pieters, I've got a solution which cleverly leverages another library for generating random strings:  <code>uuid</code>.</p>
<p>My solution is to generate a <code>uuid4</code>, base64 encode it and uppercase it, to get only the characters we're after, then slice it to a random length.</p>
<p>This works for this case because the length of outputs we're after, (12-20), is shorter than the shortest base64 encoding of a uuid4.  It's also really fast, because <code>uuid</code> is very fast.</p>
<p>I also made it a generator instead of a regular function, because they can be more efficient.</p>
<p>Interestingly, using the standard library's <code>randint</code> function was faster than <code>numpy</code>'s.</p>
<p>Here is the test output:</p>
<pre><code>Timing 40k keys 10 times with produce_amount_keys
20.899942063027993
Timing 40k keys 10 times with produce_amount_keys, stdlib randint
20.85920040300698
Timing 40k keys 10 times with uuidgen
3.852462349983398
Timing 40k keys 10 times with uuidgen, stdlib randint
3.136272903997451
</code></pre>
<p>Here is the code for <code>uuidgen()</code>:</p>
<pre><code>def uuidgen(count, _randint=np.random.randint):
    generated = set()

    while True:
        if len(generated) == count:
            return

        candidate = b64encode(uuid4().hex.encode()).upper()[:_randint(12, 20)]
        if candidate not in generated:
            generated.add(candidate)
            yield candidate
</code></pre>
<p>And <a href="https://github.com/asday/uuidgen" rel="nofollow noreferrer">here</a> is the entire project.  (At commit <a href="https://github.com/Asday/uuidgen/tree/d9925de3c08688a1fb9026fe1a9041774a9ddd97" rel="nofollow noreferrer">d9925d</a> at the time of writing).</p>
<hr/>
<p>Thanks to feedback from Martijn Pieters, I've improved the method somewhat, increasing the entropy, and speeding it up by a factor of about 1/6th.</p>
<p>There is still a lot of entropy lost in casting all lowercase letters to uppercase.  If that's important, then it's possibly advisable to use <code>b32encode()</code> instead, which has the characters we want, minus <code>0</code>, <code>1</code>, <code>8</code>, and <code>9</code>.</p>
<p>The new solution reads as follows:</p>
<pre><code>def urandomgen(count):
    generated = set()

    while True:
        if len(generated) == count:
            return

        desired_length = randint(12, 20)

        # # Faster than math.ceil
        # urandom_bytes = urandom(((desired_length + 1) * 3) // 4)
        #
        # candidate = b64encode(urandom_bytes, b'//').upper()
        #
        # The above is rolled into one line to cut down on execution
        # time stemming from locals() dictionary access.

        candidate = b64encode(
            urandom(((desired_length + 1) * 3) // 4),
            b'//',
        ).upper()[:desired_length]

        while b'/' in candidate:
            candidate = candidate.replace(b'/', choice(ALLOWED_CHARS), 1)

        if candidate not in generated:
            generated.add(candidate)
            yield candidate.decode()
</code></pre>
<p>And the test output:</p>
<pre><code>Timing 40k keys 10 times with produce_amount_keys, stdlib randint
19.64966493297834
Timing 40k keys 10 times with uuidgen, stdlib randint
4.063803717988776
Timing 40k keys 10 times with urandomgen, stdlib randint
2.4056471119984053
</code></pre>
<p>The new commit in my repository is <a href="https://github.com/Asday/uuidgen/tree/5625fdfd4bc167d34d865009a4d18682e10fb293" rel="nofollow noreferrer">5625fd</a>.</p>
<hr/>
<p>Martijn's comments on entropy got me thinking.  The method I used with <code>base64</code> and <code>.upper()</code> makes letters SO much more common than numbers.  I revisited the problem with a more binary mind on.</p>
<p>The idea was to take output from <code>os.urandom()</code>, interpret it as a long string of 6-bit unsigned numbers, and use those numbers as an index to a rolling array of the allowed characters.  The first 6-bit number would select a character from the range <code>A..Z0..9A..Z01</code>, the second 6-bit number would select a character from the range <code>2..9A..Z0..9A..T</code>, and so on.</p>
<p>This has a slight crushing of entropy in that the first character will be slightly less likely to contain <code>2..9</code>, the second character less likely to contain <code>U..Z0</code>, and so on, but it's so much better than before.</p>
<p>It's slightly faster than <code>uuidgen()</code>, and slightly slower than <code>urandomgen()</code>, as shown below:</p>
<pre><code>Timing 40k keys 10 times with produce_amount_keys, stdlib randint
20.440480664998177
Timing 40k keys 10 times with uuidgen, stdlib randint
3.430628580001212
Timing 40k keys 10 times with urandomgen, stdlib randint
2.0875444510020316
Timing 40k keys 10 times with bytegen, stdlib randint
2.8740892770001665
</code></pre>
<p>I'm not entirely sure how to eliminate the last bit of entropy crushing; offsetting the start point for the characters will just move the pattern along a little, randomising the offset will be slow, shuffling the map will still have a period...  I'm open to ideas.</p>
<p>The new code is as follows:</p>
<pre><code>from os import urandom
from random import randint
from string import ascii_uppercase, digits

# Masks for extracting the numbers we want from the maximum possible
# length of `urandom_bytes`.
bitmasks = [(0b111111 &lt;&lt; (i * 6), i) for i in range(20)]
allowed_chars = (ascii_uppercase + digits) * 16  # 576 chars long


def bytegen(count):
    generated = set()

    while True:
        if len(generated) == count:
            return

        # Generate 9 characters from 9x6 bits
        desired_length = randint(12, 20)
        bytes_needed = (((desired_length * 6) - 1) // 8) + 1

        # Endianness doesn't matter.
        urandom_bytes = int.from_bytes(urandom(bytes_needed), 'big')

        chars = [
            allowed_chars[
                (((urandom_bytes &amp; bitmask) &gt;&gt; (i * 6)) + (0b111111 * i)) % 576
            ]
            for bitmask, i in bitmasks
        ][:desired_length]

        candidate = ''.join(chars)

        if candidate not in generated:
            generated.add(candidate)
            yield candidate
</code></pre>
<p>And the full code, along with a more in-depth README on the implementation, is over at <a href="https://github.com/Asday/uuidgen/commit/de0db8e62189e263528eae883b3f57a3dfb071e1/implementations" rel="nofollow noreferrer">de0db8</a>.</p>
<p>I tried several things to speed the implementation up, as visible in the repo.  Something that would definitely help is a character encoding where the numbers and ASCII uppercase letters are sequential.</p>
</div>
<div class="post-text" itemprop="text">
<p>A simple and fast one:</p>
<pre><code>def b36(n, N, chars=string.ascii_uppercase + string.digits):
    s = ''
    for _ in range(N):
        s += chars[n % 36]
        n //= 36
    return s

def produce_amount_keys(amount_of_keys):
    keys = set()
    while len(keys) &lt; amount_of_keys:
        N = np.random.randint(12, 20)
        keys.add(b36(secrets.randbelow(36**N), N))
    return keys
</code></pre>
<p>-- <strong>Edit:</strong> The below refers to a previous revision of Martijn's answer. After our discussion he added another solution to it, which is essentially the same as mine but with some optimizations. They don't help much, though, it's only about 3.4% faster than mine in my testing, so in my opinion they mostly just complicate things. --</p>
<p>Compared with Martijn's final solution in <a href="https://stackoverflow.com/a/48421303/1672429">his accepted answer</a> mine is a lot simpler, about factor 1.7 faster, and not biased:</p>
<pre><code>Stefan
8.246490597876106 seconds.
8 different lengths from 12 to 19
  Least common length 19 appeared 124357 times.
  Most common length 16 appeared 125424 times.
36 different characters from 0 to Z
  Least common character Q appeared 429324 times.
  Most common character Y appeared 431433 times.
36 different first characters from 0 to Z
  Least common first character C appeared 27381 times.
  Most common first character Q appeared 28139 times.
36 different last characters from 0 to Z
  Least common last character Q appeared 27301 times.
  Most common last character E appeared 28109 times.

Martijn
14.253227412021943 seconds.
8 different lengths from 12 to 19
  Least common length 13 appeared 124753 times.
  Most common length 15 appeared 125339 times.
36 different characters from 0 to Z
  Least common character 9 appeared 428176 times.
  Most common character C appeared 434029 times.
36 different first characters from 0 to Z
  Least common first character 8 appeared 25774 times.
  Most common first character A appeared 31620 times.
36 different last characters from 0 to Z
  Least common last character Y appeared 27440 times.
  Most common last character X appeared 28168 times.
</code></pre>
<p>Martijn's has a bias in the first character, <code>A</code> appears far too often and <code>8</code> far to seldom. I ran my test ten times, his most common first character was always <code>A</code> or <code>B</code> (five times each), and his least common character was always <code>7</code>, <code>8</code> or <code>9</code> (two, three and five times, respectively). I also checked the lengths separately, length 17 was particularly bad, his most common first character always appeared about 51500 times while his least common first character appeared about 25400 times.</p>
<p>Fun side note: I'm using the <code>secrets</code> module that Martijn dismissed :-)</p>
<p>My whole script:</p>
<pre><code>import string
import secrets
import numpy as np
import os
from itertools import islice, filterfalse
import math

#------------------------------------------------------------------------------------
#   Stefan
#------------------------------------------------------------------------------------

def b36(n, N, chars=string.ascii_uppercase + string.digits):
    s = ''
    for _ in range(N):
        s += chars[n % 36]
        n //= 36
    return s

def produce_amount_keys_stefan(amount_of_keys):
    keys = set()
    while len(keys) &lt; amount_of_keys:
        N = np.random.randint(12, 20)
        keys.add(b36(secrets.randbelow(36**N), N))
    return keys

#------------------------------------------------------------------------------------
#   Martijn
#------------------------------------------------------------------------------------

def b36encode(b, 
        _range=range, _ceil=math.ceil, _log=math.log, _fb=int.from_bytes, _len=len, _b=bytes,
        _c=(string.ascii_uppercase + string.digits).encode()):
    b_int = _fb(b, 'big')
    length = _len(b) and _ceil(_log((256 ** _len(b)) - 1, 36))
    return _b(_c[(b_int // 36 ** i) % 36] for i in _range(length - 1, -1, -1))

def produce_amount_keys_martijn(amount_of_keys):
    def gen_keys(_urandom=os.urandom, _encode=b36encode, _randint=np.random.randint, _factor=math.log(256, 36)):
        while True:
            count = _randint(12, 20)
            yield _encode(_urandom(math.ceil(count / _factor)))[-count:].decode('ascii')
    return list(islice(unique_everseen(gen_keys()), amount_of_keys))

#------------------------------------------------------------------------------------
#   Needed for Martijn
#------------------------------------------------------------------------------------

def unique_everseen(iterable, key=None):
    seen = set()
    seen_add = seen.add
    if key is None:
        for element in filterfalse(seen.__contains__, iterable):
            seen_add(element)
            yield element
    else:
        for element in iterable:
            k = key(element)
            if k not in seen:
                seen_add(k)
                yield element

#------------------------------------------------------------------------------------
#   Benchmark and quality check
#------------------------------------------------------------------------------------

from timeit import timeit
from collections import Counter

def check(name, func):
    print()
    print(name)

    # Get 999999 keys and report the time.
    keys = None
    def getkeys():
        nonlocal keys
        keys = func(999999)
    t = timeit(getkeys, number=1)
    print(t, 'seconds.')

    # Report statistics about lengths and characters
    def statistics(label, values):
        ctr = Counter(values)
        least = min(ctr, key=ctr.get)
        most = max(ctr, key=ctr.get)
        print(len(ctr), f'different {label}s from', min(ctr), 'to', max(ctr))
        print(f'  Least common {label}', least, 'appeared', ctr[least], 'times.')
        print(f'  Most common {label}', most, 'appeared', ctr[most], 'times.')
    statistics('length', map(len, keys))
    statistics('character', ''.join(keys))
    statistics('first character', (k[0] for k in keys))
    statistics('last character', (k[-1] for k in keys))

for _ in range(2):
    check('Stefan', produce_amount_keys_stefan)
    check('Martijn', produce_amount_keys_martijn)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>Caveat: This is not cryptographically secure</strong>. I want to give an alternative <code>numpy</code> approach to the one in Martijn's great answer.</p>
<p><code>numpy</code> functions aren't really optimised to be called repeatedly in a loop for small tasks; rather, it's better to perform each operation in bulk. This approach gives more keys than you need (massively so in this case because I over-exaggerated the need to overestimate) and so is less memory efficient but is still super fast.</p>
<ol>
<li><p>We know that all your string lengths are between 12 and 20. Just generate all the string lengths in one go. We know that the final <code>set</code> has the possibility of trimming down the final list of strings, so we should anticipate that and make more "string lengths" than we need. 20,000 extra is excessive, but it's to make a point:</p>
<p><code>string_lengths = np.random.randint(12, 20, 60000)</code></p></li>
<li><p>Rather than create all our sequences in a <code>for</code> loop, create a 1D list of characters that is long enough to be cut into 40,000 lists. In the <em>absolute</em> worst case scenario, all the random string lengths in (1) were the max length of 20. That means we need 800,000 characters.</p>
<p><code>pool = list(string.ascii_letters + string.digits)</code></p>
<p><code>random_letters = np.random.choice(pool, size=800000)</code></p></li>
<li><p>Now we just need to chop that list of random characters up. Using <code>np.cumsum()</code> we can get sequential starting indices for the sublists, and <code>np.roll()</code> will offset that array of indices by 1, to give a corresponding array of end indices.
<br/></p>
<p><code>starts = string_lengths.cumsum()</code></p>
<p><code>ends = np.roll(string_lengths.cumsum(), -1)</code></p></li>
<li><p>Chop up the list of random characters by the indices.
<br/></p>
<p><code>final = [''.join(random_letters[starts[x]:ends[x]]) for x, _ in enumerate(starts)]</code></p></li>
</ol>
<p>Putting it all together:</p>
<pre><code>def numpy_approach():
    pool = list(string.ascii_letters + string.digits)
    string_lengths = np.random.randint(12, 20, 60000)   
    ends = np.roll(string_lengths.cumsum(), -1) 
    starts = string_lengths.cumsum()
    random_letters = np.random.choice(pool, size=800000)
    final = [''.join(random_letters[starts[x]:ends[x]]) for x, _ in enumerate(starts)]
    return final
</code></pre>
<p>And <code>timeit</code> results:</p>
<pre><code>322 ms ± 7.97 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<h1>Alternate approach: Uniqueness in creation rather than by test</h1>
<p>The obvious approach to your question would be to generate random output, and then check whether it is unique. Though I do not offer an implementation, here is an alternate approach:</p>
<ol>
<li>Generate output that looks as random as possible</li>
<li>Generate output that is guaranteed to be unique, and looks somewhat random</li>
<li>Combine them</li>
</ol>
<p>Now you have output that is guaranteed to be unique, and appears to be random.</p>
<h2>Example</h2>
<p>Suppose you would want to generate 999999 strings with lengths from 12 and 20. The approach will of course work for all character sets, but lets keep it simple and assume you want to use only 0-9.</p>
<ol>
<li>Generate random output with lengths from 6 to 14</li>
<li>Randomly permute the numbers 000000 to 999999 (yes 6 digits is quite a lot to 'sacrifice' in apparent randomness, but with a larger characterset you won't need this many characters)</li>
<li>Now combine them in a way that the uniqueness must be preserved. The most trivial way would be simple concatenation of the entities, but you can of course think of less obvious solutions. </li>
</ol>
<h2>Small scale example</h2>
<ol>
<li><p>Generate randomness:</p>
<p>sdfdsf 
xxer 
ver </p></li>
<li><p>Generate uniqueness</p>
<p>xd
ae
bd</p></li>
<li><p>Combine</p>
<p>xdsdfdsf
aexxer
bdver</p></li>
</ol>
<p>Note that this method assumes that you have a minimum number of characters per entry, which seems to be the case in your question.</p>
</div>
<span class="comment-copy">Is there any particular reason you're using numpy for random numbers rather than the stdlib <code>random</code> module?</span>
<span class="comment-copy">no, is random faster?</span>
<span class="comment-copy">Note that you don't actually produce 40k keys. You are producing <i>fewer</i> keys as you don't generate more if there are duplicates.</span>
<span class="comment-copy">@MartijnPieters Well... with at least 99.999999997829302352% probability they actually do produce 40k keys: <a href="https://eval.in/951632" rel="nofollow noreferrer">eval.in/951632</a></span>
<span class="comment-copy">Well define random ... how random do you want it ?</span>
<span class="comment-copy">@AdamBarnes: actually, that led me to further improvements; a compromise between the fast baseXX functions and limited entropy, and producing random data faster with <code>os.urandom()</code>.</span>
<span class="comment-copy">I like that you present multiple approaches. I have added a different kind of approach <a href="https://stackoverflow.com/a/48426820/983722">here</a>. I am not in a position to test, but theoretically removing the need for tests could lead to a fast solution :)</span>
<span class="comment-copy">You can pass a generator expression instead of a list to <code>join</code>.</span>
<span class="comment-copy">@Michael: and that would make the join slower, so I explicitly don't.</span>
<span class="comment-copy">@Michael: see <a href="//stackoverflow.com/a/9061024">List comprehension without [ ] in Python</a></span>
<span class="comment-copy">Why b64encode a hex string? You are severely reducing the entropy, basing your output on bytes that only have 16 different values each. This severely reduces the range of keys you are pr during. At the very least, encode the binary data to base 64. Next, base 64 uses a different set of characters (not just letters and digits, but <code>+</code> and <code>/</code> this too; this could easily matter, you might want to map those to a valid key character.</span>
<span class="comment-copy">I was actually unaware b64 used <code>+</code> and <code>/</code>, I'll go and play with that now.</span>
<span class="comment-copy">Also, <code>b64encode()</code> produces a <code>bytes</code> object, not a <code>str</code> string, so you'll have to decode again.</span>
<span class="comment-copy">If I use <code>uuid4().bytes</code> instead of <code>uuid4().hex.encode()</code>, I can easily produce a million unique 12-character values, while the latter produces about 5 duplicates every million. And it is of course faster as you don't have to produce the hex bytestring.</span>
<span class="comment-copy">Base32 lacks <code>0</code> and <code>1</code> as well, not just <code>8</code> and <code>9</code>.</span>
<span class="comment-copy">Nice! Yours is a bit faster again because it forgoes int -&gt; bytes and bytes -&gt; int transitions.</span>
<span class="comment-copy">Do note that your <code>b36</code> number is essentially backwards. Not that it matters here. It appears to be the cause of the bias in my case however, as I produce integer values greater than <code>36 ** (desired length)</code>, and this adds more information I discard again. This is especially pronounced in length 17, as <code>17 / math.log(256, 36)</code> is almost, but not quite, 11. The remaining 0.0139 fraction of a byte creates a larger bias than the larger margins in the other values. All because base36 doesn't cleanly divide across powers of 2. Looks like I can only avoid the bias with <code>randbelow()</code>!</span>
<span class="comment-copy">@MartijnPieters Yes, that "backwardness" is why I renamed it from <code>base36</code> to <code>b36</code>. With the word "base" it felt wrong because that sounds like the mathematical base representation. But with just "b" I feel ok to do that, it's not as loaded. If at all, "b36" sounds like the encodings in programming, which don't take one arbitrarily large number but <i>bytes</i>, and then I argue that what I'm doing is similar to encoding the number with little-endian. And yes, <code>randbelow</code> might be the best option to avoid bias. Or in your approach you could discard byteses too large, but that seems complicated.</span>
<span class="comment-copy">Heyya, your PC will be different to mine and Martijn's; do you mind <code>timeit</code>'ing one of our implementations on your machine alongside yours, so we have some manner of context?</span>
<span class="comment-copy">@AdamBarnes sure, but it will be quite a few hours before I'm at that PC. Will update you once done.</span>
<span class="comment-copy">Highly appreciated.</span>
<span class="comment-copy">@AdamBarnes in the course of trying to set up a test case, I <i>think</i> there might be an issue in your output. I measured the lengths of all your output strings: <code>len_dict = {12: 4451, 13: 4352, 14: 4492, 15: 26705}</code>. However, creating and exhausting the generator comes out at a reasonably similar time (<code>262 ms ± 2.41 ms</code>) - yours is faster - for me.</span>
<span class="comment-copy">That ended up being REALLY simple; I'd only generated bitmasks up to 15 characters.  How silly.</span>
<span class="comment-copy">The main problem with this (besides lack of entropy), is that it still requires somehow building a string from allowed characters, quickly.  Martijn and myself got around this with the <code>base64</code> library, which takes bytes, and returns ASCII, and once you're doing that, you may as well hand it <code>urandom()</code> bytes.  The uniqueness is not a slowness factor, thanks to sets.  The difference in time between my solution without checking for uniqueness is so small that one is faster than the other at random due to system load.</span>
<span class="comment-copy">@AdamBarnes Thanks for your comment. I was afraid that the checking uniqueness would be too efficient for this method to be worthwile. I will leave it here for entertainment value. -- One last poke: Are you sure the speed difference is not so small because you already accepted the potential overhead of using a set (which is good at checking for uniqueness?)</span>
<span class="comment-copy">I am "confident", I am not "sure".  As you didn't provide a code sample, I wasn't able to test your implementation, and though I could write it myself, I was busy extending my own answer to ridiculous lengths...  (Sorry about that).  If you feel like joining in the fun, go nuts and throw an implementation together and see for yourself.  If you like, you could open a PR against <a href="https://github.com/asday/uuidgen" rel="nofollow noreferrer">my repository</a>.</span>
