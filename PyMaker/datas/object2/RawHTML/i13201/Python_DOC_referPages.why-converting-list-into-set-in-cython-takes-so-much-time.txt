<div class="post-text" itemprop="text">
<p><code>huge_list</code> parameter is something like <code>[[12,12,14],[43,356,23]]</code>. And my code to convert list to set is:  </p>
<pre><code>cpdef list_to_set(list huge_list):
    cdef list ids
    cdef list final_ids=[]
    for ids in huge_list:
        final_ids.append(set(ids))

    return final_ids
</code></pre>
<p>I have 2800 list elements, each has 30,000 id. It takes around 19 second. How to improve performance?</p>
<hr/>
<p><strong>EDIT 1:</strong><br/>
Instead of <code>set</code> I used <code>unique</code> in <code>numpy</code> as below and <code>numpy</code> speeds up by ~7 seconds:  </p>
<pre><code>df['ids'] = df['ids'].apply(lambda x: numpy.unique(x))
</code></pre>
<p>Now it takes 14 seconds (Previously it was ~20 seconds). I don't think this time is acceptable yet. :|</p>
</div>
<div class="post-text" itemprop="text">
<p>Cython cannot speed up anything. The most time is spent building sets, e.g. calculating hash values of your elements and storing them in maps. This is already done in C, so no speed up possible. The pure python version:</p>
<pre><code>final_ids = [set(ids) for ids in huge_list]
</code></pre>
<p>whould lead to the same result.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you just want to convert the nested lists to set you can simply use <a href="https://docs.python.org/3/library/functions.html#map" rel="nofollow"><code>map</code></a> function :</p>
<pre><code>final_ids=map(set,huge_list)
</code></pre>
</div>
<span class="comment-copy">1) <code>final_ids</code> is a list, not a set? 2) What's wrong with <code>my_set = set(some_list)</code> (without using Cython)?</span>
<span class="comment-copy">@DavidW, TypeError: unhashable type: 'list'.</span>
<span class="comment-copy">Fair enough that makes sense. Not sure your code above works though (final_ids doesn't have update I think?)</span>
<span class="comment-copy">@DavidW, it is <code>append()</code>. My mistake.</span>
<span class="comment-copy">Be more specific. You want to remove duplicates of a list, as fast as possible. How many unique integers the list contains in average? What is the largest number?</span>
<span class="comment-copy">I used <code>df['ids'] = df['ids'].apply(lambda x: numpy.unique(x))</code> and it speeds up by ~7 seconds. but needs more improvement. Isn't it possible? How people manage their huge data then?</span>
<span class="comment-copy">As I said these are not efficient. I turned my set into <code>numpy array</code> and handling duplicates by <code>numpy.unique</code>, I save 7 seconds but it is not acceptable yet.</span>
<span class="comment-copy">@AlirezaHos Where did you said that? this is extremely different from what you have done.And can you say that what 's wrong with using <code>map</code>, this is the most pythonic way to go with this task!</span>
<span class="comment-copy">I didn't :-| I'm not looking for a most pythonic way to handle my problem, I'm looking for a solution which would take much less time than 19 seconds. I appreciate for the time you've put on this.</span>
<span class="comment-copy">@AlirezaHos What?? did you test this solution at all? or ran any benchmark on it?</span>
<span class="comment-copy">No no... I used time built-in function to see approximately how much time it takes.</span>
