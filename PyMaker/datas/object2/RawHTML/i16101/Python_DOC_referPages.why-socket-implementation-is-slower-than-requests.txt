<div class="post-text" itemprop="text">
<p>I have a python 3.4 script fetching multiple web pages. At first, I used requests library to fetch pages:</p>
<pre><code>def get_page_requsets(url):
    r = requests.get(url)
    return r.content
</code></pre>
<p>Above code gives an average speed of 4.6 requests per second.
To increase speed I rewrote function to use sockets library:</p>
<pre><code>def get_page_socket(url):

    url = urlparse(url)
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.connect((url.netloc, 80))
    req = '''
GET {} HTTP/1.1\r
Host: {}\r
Connection: Keep-Alive\r
\r
    '''.format(url.path, url.host, uagent)
    sock.send(req.encode())
    reply = b''
    while True:
        chunk = sock.recv(65535)
        if chunk:
            reply += chunk
        else:
            break
    sock.close()
    return reply
</code></pre>
<p>And average speed fell to 4.04 requests per second. I was not hoping for drammatic speed boost, but was hoping for slight increase, as socket is more low level. 
Is this library issue or I'm doing something wrong?</p>
</div>
<div class="post-text" itemprop="text">
<p><code>requests</code> uses <a href="http://urllib3.readthedocs.org/en/latest/"><code>urllib3</code></a>, which handles HTTP connections very efficiently. Connections to the same server are re-used wherever possible, saving you the socket connection and teardown costs:</p>
<blockquote>
<ul>
<li>Re-use the same socket connection for multiple requests, with optional client-side certificate verification. See: <code>HTTPConnectionPool</code> and <code>HTTPSConnectionPool</code></li>
</ul>
</blockquote>
<p>In addition, <code>urllib3</code> and <code>requests</code> advertise to the server that they can handle <em>compressed</em> responses; with compression you can transfer more data in the same amount of time, leading to more requests per second.</p>
<blockquote>
<ul>
<li>Supports gzip and deflate decoding. See: <code>decode_gzip()</code> and <code>decode_deflate()</code></li>
</ul>
</blockquote>
<p><code>urllib3</code> uses sockets too (albeit via the <a href="https://docs.python.org/3/library/http.client.html"><code>http.client</code> module</a>); there is little point in reinventing this wheel. Perhaps you should think about fetching URLs in parallel instead, using threading or multiprocessing, or eventlets; the <code>requests</code> author has a <a href="https://github.com/kennethreitz/grequests">gevents-requests integration package</a> that can help there.</p>
</div>
<div class="post-text" itemprop="text">
<p>The slowness is probably simply because you are doing HTTP wrong: You issue a HTTP/1.1 request and even explicitly specify connection keep-alive (not even needed because this is implicit with HTTP/1.1). But then you just read from the socket and expect the server to close the connection after the request is done. But the server will not do that, it will instead wait for more requests from you because of keep-alive and only close the connection after some time of inactivity, which depends on the server configuration. You are lucky to connect to a server with a very short timeout where you are still getting 4.04 requests per seconds, with  others servers it would be only few requests per minute with your code.</p>
<p>If you want to make a simple HTTP request with a plain socket use HTTP/1.0 and do not use keep-alive. Then you could read just until the server closes and you also don't have to deal with chunked transfer encoding which was introduced with HTTP/1.1. You also don't have to deal with compressed encoding because you don't specifically accept them (but some broken servers will send them anyway).  </p>
<p>But, while this will make your code faster than it is now, it will not be as fast as requests, because all this keep-alive, compression etc was added to improve speed. To re-implement all of this correctly is not that easy, so I recommend you stay with the requests library.</p>
</div>
<span class="comment-copy">Perhaps the requests library's implementation is approximately the same as your implementation, in which case it's no surprise that they perform similarly.</span>
<span class="comment-copy">At 4 or 5 requests per second, you are not limited by client resources, you are clearly limited either by network or the server. Thus it makes little difference if you use sockets directly. In fact, high-level library ought to be more clever and thus faster. If it's network latency, use <a href="http://en.wikipedia.org/wiki/HTTP_pipelining" rel="nofollow noreferrer">en.wikipedia.org/wiki/HTTP_pipelining</a> ; if it's throughput, use compression; if server is throttled, use parallel connections. A quick solution is to use <code>curl_multi</code> interface or <a href="http://docs.python-requests.org/en/v0.10.6/user/advanced/#asynchronous-requests" rel="nofollow noreferrer">docs.python-requests.org/en/v0.10.6/user/advanced/â€¦</a></span>
<span class="comment-copy">requests is nowhere near that implementation. :-)</span>
