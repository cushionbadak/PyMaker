<div class="post-text" itemprop="text">
<p>I need to take some large files of strings and replace each string with an id from 1 up in a separate file. There are repeats of some of the strings within each file, and there are common strings between files so those need to get the same id. I have implemented this with a dictionary, which works, but due to the size of the files and the amount of strings, this solution seems to work slowly. Is there a data structure or perhaps hashing technique that would be better suited for this?</p>
<p>______________________edited_______________________________________________________</p>
<p>My implementation for dict</p>
<pre><code>index = {}
lastindex = 0
for row in reader:
    if row[0] not in index:
        lastindex += 1
        index[row[0]] = lastindex
    w.write(index[row[0]])
</code></pre>
<p>An input sample </p>
<pre><code>feifei77.w70-e2.ezcname.com
reseauocoz.cluster007.ovh.net
cse-web-cl.comunique-se.com.br
ext-cust.squarespace.com
ext-cust.squarespace.com
ext-cust.squarespace.com
ext-cust.squarespace.com
ghs.googlehosted.com
isutility.web9.hubspot.com
sendv54sxu8f12g.ihance.net
sites.smarsh.io
www.triblocal.com.s3-website-us-east-1.amazonaws.com
*.2bask.com
*.819.cn
</code></pre>
<p>this should return</p>
<pre><code>1
2
3
4
4
4
4
5
6
7
8
9
10
...
</code></pre>
<p>I should clarify, it does not necessarily need to be ordered in that way, though it does need to include every integer from 1 to the number of strings.
4 2 3 1 1 1 1 5 6 7 8 9 10 would be valid as well</p>
</div>
<div class="post-text" itemprop="text">
<p>Slightly more memory friendly would be to use a <code>set</code> instead of a <code>dict</code>. Using the <code>unique_everseen()</code> example from the <code>itertools</code> docs at <a href="https://docs.python.org/3/library/itertools.html" rel="nofollow">https://docs.python.org/3/library/itertools.html</a> you could do this:</p>
<pre><code>for idx, word in enumerate(unique_everseen(reader), 1):
    print(idx)
</code></pre>
<p>An alternative that would scale to much larger data sets would be to use some sort of persistent key/value store that stores data on disk (instead of an in-memory mapping), e.g. with LevelDB (using Plyvel) it could look like this:</p>
<pre><code>import itertools
import plyvel

db = plyvel.DB('my-database', create_if_missing=True)
cnt = itertools.count(1)  # start counting at 1
for word in reader:
    key = word.encode('utf-8')
    value = db.get(key)
    if value is not None:
        # We've seen this word before.
        idx = int(value)
    else:
        # We've not seen this word before.
        idx = next(cnt)
        db.put(key, str(idx).encode('ascii'))

    print(idx)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The bottleneck of your code is the w.write during the for loop. Generate the dict first and then write the file it will run much faster.</p>
</div>
<span class="comment-copy">Your bottleneck is likely just the sheer volume of data you need to process. <code>dict</code>s are already highly optimized, but you'll need to show more detail (code, sample input, etc) if you want any specific suggestions.</span>
<span class="comment-copy">Did you profile your code ? Python's <code>dict</code> are hash tables, and the lookup time is O(1), so I'm not sure your dict is the culprit.</span>
<span class="comment-copy">I guess I was looking for a way to do this without storing all the strings in one memory bank, but I'm not sure if that is possible. A dict is fast, but if theres a way to do it through hashing, holding millions of strings is going to cost a lot more than that</span>
<span class="comment-copy">What's exactly present in <code>row</code>?</span>
<span class="comment-copy">I looks weird to me that you write out something on each index-operation. This should have a massive impact on runtime. Try batching writes, and only write out every now and then.</span>
<span class="comment-copy">I'm still testing speed but so far this seems to work pretty well. Thanks for the help</span>
