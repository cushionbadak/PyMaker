<div class="post-text" itemprop="text">
<p>I have a large flat file which I need to parse using a list which contains the variable name, the starting point, and the length of the variable along with the type. e.g.</p>
<pre><code>columns = [['LOAD_CYCLE', 131, 6, 'int'],
           ['OPERATOR', 59, 8, 'Char (8)'],
           ['APP_DATE', 131, 8, 'Date'],
           ['UNIQUE_KEY', 245, 25, 'Char (25)']]
</code></pre>
<p>This list contains 1,600 items. The only really important columns are the starting point and the length of the variable. This is used to split each line in the flat file into a list of variables, which is used to create a new file to be inserted into a database. The data type is important, but I can always do that section later.</p>
<p>Currently, my method is to read the file in chunks (it is a very large file; over 6GB), and then process the chunk piece by piece:</p>
<pre><code>line = data_file.read(chunk*1000)
for x in range(1000):
    offset = chunk*x
    for item in columns:
        piece = line[item[1]+offset:item[1]+item[2]+offset].replace('\n','')
        #Depending on the data type, a piece may undergo one or two checks before being
        #added to a list which is then written to an output file
</code></pre>
<p>The time consuming part is iterating through the columns. Is this the only way to do this? Or is there perhaps a more efficient way to split the string? Something involving maps?</p>
</div>
<div class="post-text" itemprop="text">
<p>This seems like a great case for the <a href="https://docs.python.org/3/library/struct.html" rel="nofollow"><code>struct</code></a> module. Assuming you're using CPython, this effectively moves the loop over the columns into C.</p>
<p>First, you need to build up the format string.</p>
<p>Since your columns appear to be specified in arbitrary order, rather than ordered by starting point, and may have gaps between them, this isn't quite trivial… but it shouldn't be <em>too</em> hard. Something like this:</p>
<pre><code>sorted_columns = sorted(columns, key=operator.itemgetter(1))
formats = []
offset = 0
for name, start, length, vtype in sortedcolumns:
    # add padding bytes
    if start &gt; offset:
        formats.append('{}x'.format(start-offset))
    formats.append('{}s'.format(length))
format = struct.Struct('=' + ''.join(formats))
</code></pre>
<p>Then:</p>
<pre><code>offset = chunk*x
values = format.unpack_from(line, offset)
</code></pre>
<p>And now you have a tuple of 1600 items.</p>
<p>Of course to do anything with that tuple, you may have to iterate over it anyway. But maybe you can do that in C as well. For example, if you're just inserting the values into a SQL database, then creating a giant SQL statement with 1600 parameters (in the same order as in <code>sorted_columns</code>) and passing the tuple as the arguments may take care of that for you:</p>
<pre><code>cursor.execute(giant_insert_sql, values)
</code></pre>
<p>But if you need to do something more complicated to each value, then you'll need to do something like one of the following:</p>
<ul>
<li>Use <a href="http://numpy.org" rel="nofollow">NumPy</a> and/or <a href="http://pandas.pydata.org/" rel="nofollow">Pandas</a> to vectorize the loop. (Note that they can also be used to just load the whole file into memory, vectorizing the outer loop as well as the inner one, if you've got the RAM… but that shouldn't be nearly as much of a performance gain.)</li>
<li>Run your existing code in <a href="http://pypy.org" rel="nofollow">PyPy</a> instead of CPython.</li>
<li>Use <a href="http://numba.pydata.org/" rel="nofollow">Numba</a> to JIT the code within CPython.</li>
<li>Write a C extension to replace your inner loop—which, if you're lucky, may be as simple as just moving your Python code to a function in a separate file and compiling it with <a href="http://cython.org/" rel="nofollow">Cython</a>.</li>
</ul>
</div>
<span class="comment-copy">I'm a bit confused.  Is your example for <b>columns</b> a valid example?</span>
<span class="comment-copy">Is your source file a dbf?</span>
<span class="comment-copy">@user1269942 Yes, it is a valid example of the way I am parsing the source file. Source file is a plain text file (6GB).</span>
<span class="comment-copy">Since your method works, it's a good starting point.  You can try Cython or pypy (<a href="http://pypy.org/performance.html" rel="nofollow noreferrer">pypy.org/performance.html</a>) to boost your speed. Or maybe preprocess your columns so you can grab offsets and lengths without needing to add numbers.  Also, the replace command...that may give a small performance hit too..is it needed? if you just want to take the trailing \n off of the line end, you can use .strip()</span>
<span class="comment-copy">How large is the file? If it's less than the memory on your computer, you can load it into memory with pandas and then your operations should be pretty quick.</span>
<span class="comment-copy">Thanks, trying to modify my code now. The columns are in "arbitrary" since the SQL table differs in order compared to the flat file. Also, as in the example above, I have some derived fields that read from the same position. For example Load_cycle is YYYYMM which is derived from a date field which I also need. I'll see how that works with the code.</span>
<span class="comment-copy">@DavidBekker: You rarely want to rely on the declared order of columns in a SQL table anyway; specify the columns in the <code>INSERT</code> statement in the order you want. (For interactive use, it's usually safe to not specify the columns, and it saves a lot of typing on every command you run. But in code, sacrificing robustness for a few keystrokes that you only have to enter once isn't usually worth it.)</span>
