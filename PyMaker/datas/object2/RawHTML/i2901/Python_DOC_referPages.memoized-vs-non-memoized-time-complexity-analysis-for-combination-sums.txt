<div class="post-text" itemprop="text">
<p>I'm trying to understand why using <code>lru_cache</code> to solve this problem yields a slower performance of the code. </p>
<p><a href="https://leetcode.com/problems/combination-sum/description/" rel="nofollow noreferrer">The question</a> is essentially to return all combinations that add up to a certain target.</p>
<p>I'm using the <code>lru_cache</code> decorator to do the memoization (<a href="https://docs.python.org/3/library/functools.html" rel="nofollow noreferrer">docs</a>), and this is my solution:</p>
<pre><code>from functools import lru_cache

def combinationSum(candidates, target):
    return dfs(tuple(candidates), 0, target)

@lru_cache(maxsize=None)
def dfs(candidates, i, target):
    if target &lt; 0:
        return []

    if target == 0:
        return [[]]

    if i == len(candidates):
        return []

    final_results = []
    for j in range(i, len(candidates)):

        results = dfs(candidates, j, target - candidates[j])

        for x in results:
            final_results.append([candidates[j]] + x)

    return final_results
</code></pre>
<p>It seems like when the <code>lru_cache</code> decorator is commented out, I get almost a 50% increase in the runtime speed of this algorithm. This seems a little counter intuitive as I thought the time complexity of the solution should be reduced, even with the increased overhead of function calls to retrieve results from the memoization.</p>
<p>For the memoized solution, I believe the time complexity should be <code>O(n^2*k*2^n)</code> where <code>n</code> is the length of the array, and <code>k</code> being all numbers in the range from <code>0</code> to <code>target</code>. </p>
<p>This is my analysis (need a little help verifying):</p>
<pre><code>time complexity 
= possible states for memoization x work done at each step
= (n * k) * (n * maximum size of results)
= n * k * n * 2^n
</code></pre>
<p>I'm also missing some gaps in my knowledge on how to analyze the time complexity of the recursive solution, I could use some help in doing so!</p>
<p>EDIT:</p>
<p>I'm using <code>range(1, 10000)</code> as a test input, here are the benchmarks:</p>
<pre><code># with lru_cache
$ time python3 combination_sum.py
CacheInfo(hits=59984, misses=49996, maxsize=None, currsize=49996)

real    0m4.031s
user    0m3.996s
sys     0m0.024s

# without lru_cache
$ time python3 combination_sum.py

real    0m0.073s
user    0m0.060s
sys     0m0.010s
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You didn't give <em>both</em> arguments, and they're both important.  I can make either version much faster than the other by picking <em>specific</em> pairs.  If you're passing <code>range(1, 10000)</code> as <code>candidates</code>, then each cache lookup has to (among other things) do 9999 comparisons just to determine that the candidates are always the same - and that's huge overhead.  Try, e.g.,</p>
<pre><code>combinationSum(range(1, 1000), 45) # not ten thousand, just one thousand
</code></pre>
<p>for a case where the cached version is much faster.  After which:</p>
<pre><code>&gt;&gt;&gt; dfs.cache_info()
CacheInfo(hits=930864, misses=44956, maxsize=None, currsize=44956)
</code></pre>
<p>"Analysis" is futile if you don't account for the expense of doing a cache lookup, and you're apparently <em>trying</em> cases where cache lookup is extremely expensive.  Dict lookup is expected-case <code>O(1)</code>, but the hidden constant factor can be arbitrarily large depending on how expensive equality-testing is (and for a key involving an <code>N</code>-element tuple, establishing equality requires at least <code>N</code> comparisons).</p>
<p>Which should suggest a major improvement:  keep <code>candidates</code> out of the argument list.  It's invariant so there's really no need to pass it.  Then the cache just needs to store fast-to-compare <code>(i, target)</code> pairs.</p>
<h2>EDIT: PRACTICAL CHANGES</h2>
<p>Here's another version of the code that doesn't pass in <code>candidates</code>.  For</p>
<pre><code>combinationSum(range(1, 10000), 45)
</code></pre>
<p>it's faster by at least a factor of 50 on my box.  There's one other material change:  don't do a recursive call when <code>target</code> is reduced below zero.  An enormous number of cache entries were recording empty-list results for <code>(j, negative_integer)</code> arguments.  This change reduced the final cache size from 449956 to 1036 in the case above - and cut the number of hits from 9444864 to 6853.</p>
<pre><code>def combinationSum(candidates, target):

    @lru_cache(maxsize=None)
    def dfs(i, target):
        if target == 0:
            return [[]]
        assert target &gt; 0
        if i == n:
            return []
        final_results = []
        for j in range(i, n):
            cand = candidates[j]
            if cand &lt;= target:
                results = dfs(j, target - cand)
                for x in results:
                    final_results.append([cand] + x)
        return final_results

    n = len(candidates)
    result = dfs(0, target)
    print(dfs.cache_info())
    return result
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Try running the following on your result</p>
<pre><code>&gt;&gt;&gt;dfs.cache_info()
</code></pre>
<p>you should get a result of something like this</p>
<pre><code>CacheInfo(hits=2, misses=216, maxsize=None, currsize=216)
</code></pre>
<p>because your function parameters are very long so they don't match often with the cached values, I blame the target parameter here, restructuring the program might improve the hits drastically.</p>
</div>
<span class="comment-copy">Please give the specific arguments you're using.  Whether a cache helps or hurts depends a whole lot on the cache hit rate, and it's impossible to <i>guess</i> which hit rate you're getting without knowing the arguments you're using.</span>
<span class="comment-copy">what type <code>candidates</code> is?</span>
<span class="comment-copy">Lower complexity does not guarantee lower time.  It just means it grows more slowly at large values.  For small values it is perfectly valid that it takes longer.</span>
<span class="comment-copy">@skyboyer candidates is a list of integers</span>
<span class="comment-copy">@TimPeters I've updated the question with the input I'm using!</span>
<span class="comment-copy">I'm such a fool, I completely overlooked the time complexity of the comparison of <code>candidates</code> throughout problems I worked through &gt;&lt;. Yeah, it seems like when I was passing in small values like <code>5</code> for <code>target</code>, it's easy for the recursion tree to be pruned as well, but when larger values are passed in, the effect of memoization becomes more apparent.</span>
<span class="comment-copy">Now I have more intuition on why the memoized solution is faster for a larger input space, how can I analyze the time complexity of the plain recursive solution to compare them?</span>
<span class="comment-copy">I've updated the question with the cache info results! Yes, probably restructuring will work, but I want to figure out the impact of using memoization on the solution â€“ and why does the runtime slow down with memoization</span>
