<div class="post-text" itemprop="text">
<p>I have the following scenario:</p>
<ul>
<li>Python 3.6+</li>
<li>The input data is read from a file, line by line.</li>
<li>A coroutine sends the data to an API (using <code>aiohttp</code>) and saves the result of the call to Mongo (using <code>motor</code>). So there's a lot of IO going on.</li>
</ul>
<p>The code is written using <code>async</code> / <code>await</code>, and works just fine for individual calls executed manually.</p>
<p>What I don't know how to do is to consume the input data en masse.</p>
<p>All <code>asyncio</code> examples I've seen demonstrate <code>asyncio.wait</code> by sending a finite list as a parameter. But I can't simply send a list of tasks to it, because the input file may have millions of rows.</p>
<p>My scenario is about streaming data as through a conveyor belt to a consumer.</p>
<p>What else can I do? I want the program to process the data in the file using all the resources it can muster, but without getting overwhelmed.</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>My scenario is about streaming data as through a conveyor belt to a consumer. What else can I do?</p>
</blockquote>
<p>You can create a fixed number of tasks roughly corresponding to the capacity of your conveyor belt, and pop them off a <a href="https://docs.python.org/3/library/asyncio-queue.html#asyncio.Queue" rel="nofollow noreferrer">queue</a>. For example:</p>
<pre><code>async def consumer(queue):
    while True:
        line = await queue.get()
        # connect to API, Mongo, etc.
        ...
        queue.task_done()

async def producer():
    N_TASKS = 10
    loop = asyncio.get_event_loop()
    queue = asyncio.Queue(N_TASKS)
    tasks = [loop.create_task(consume(queue)) for _ in range(N_TASKS)]
    try:
        with open('input') as f:
            for line in f:
                await queue.put(line)
        await queue.join()
    finally:
        for t in tasks:
            t.cancel()
</code></pre>
<p>Since, unlike threads, tasks are lightweight and do not hog operating system resources, it is fine to err on the side of creating "too many" of them. asyncio can handle thousands of tasks without a hitch, although that is probably overkill for this tasks - tens will suffice.</p>
</div>
<span class="comment-copy">Thanks, this looks like the right way to go, marked as the solution.  One complication I had was that the program wouldn't wait for all tasks to complete. I added an <code>await asyncio.sleep(1)</code> before cancelling the tasks in the <code>finally</code> block.</span>
<span class="comment-copy">Update: my next iteration has the tasks <code>break</code> out of the <code>while True</code> loop when they receive a <code>None</code>.  So after the queue is done ingesting the input data, I put in it as many <code>None</code>s as there are <code>N_TASKS</code>, which allows each task to shut down gracefully.  I then <code>await asyncio.wait(tasks)</code>, and no longer need to <code>cancel()</code> them.</span>
<span class="comment-copy">@Andrei This problem just occurred to me when I took another look at the code. Your second solution will work great, though I added a slightly different one to the answer, to showcase the <code>join()</code> method on the queue which is designed exactly for this kind of scenario.</span>
<span class="comment-copy">I recently came back to this codebase, and the <code>join()</code> works perfectly. To anyone else reading this, make sure to call <code>queue.task_done()</code> after each iteration of the consumer.</span>
