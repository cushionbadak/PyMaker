<div class="post-text" itemprop="text">
<p>Using python 3.6, I have a problem like so: (1) there's a <code>joblib.Parallel</code> loop over embarrassingly parallel jobs (2) the jobs themselves are fairly time intensive c++ native objects that occasionally segfault and whose code I cannot modify.</p>
<p>To guard against the segfaults, I attempted to wrap the jobs themselves inside a multiprocessing Process. Unfortunately, python itself throws an assertion error with <code>daemonic processes are not allowed to have children</code> with this solution.</p>
<p>So I took the solution posted here and tried inheriting from Process: <a href="https://stackoverflow.com/a/8963618/614684">https://stackoverflow.com/a/8963618/614684</a></p>
<p>That didn't work either, and so I came up with the following solution which DOES work:</p>
<pre><code>class NoDaemonProcess(multiprocessing.Process):
  def __init__(self, group=None, target=None, name=None, args=(), kwargs={},
               *, daemon=None):
    super(NoDaemonProcess, self).__init__(group, target, name, args, kwargs, daemon=daemon)
    if 'daemon' in multiprocessing.process._current_process._config:
      del multiprocessing.process._current_process._config['daemon']
    self._config = multiprocessing.process._current_process._config.copy()
  # make 'daemon' attribute always return False                                                                                                                                              
  def _get_daemon(self):
    return False
  def _set_daemon(self, value):
    pass
  daemon = property(_get_daemon, _set_daemon)
</code></pre>
<p>Basically, I modify the global state of the multiprocessing package to delete the fact that the current process is a daemon.</p>
<p>Is there a better way to do this? I would appreciate any help in making this more robust and reliable.</p>
</div>
<div class="post-text" itemprop="text">
<p>Keep It Simple, Stupid :)</p>
<p>Your error message is at <a href="https://github.com/python/cpython/blob/master/Lib/multiprocessing/process.py#L110" rel="nofollow noreferrer">https://github.com/python/cpython/blob/master/Lib/multiprocessing/process.py#L110</a> .</p>
<p>You can use a regular <a href="https://docs.python.org/3/library/subprocess.html" rel="nofollow noreferrer"><code>subprocess</code></a> as part of your worker payload. It won't be subject to any <code>multiprocessing</code> logic. To avoid creating a separate Python program specifically for that, just do</p>
<pre><code>subprocess.Popen([sys.executable,__file__,"&lt;arg&gt;"])
</code></pre>
<p>and make <code>&lt;arg&gt;</code> trigger the necessary logic in the <code>if __name__ == '__main__'</code> block.</p>
<p>To get the result from the subprocess, either print it to stdout in any machine-readable format, or use any IPC mechanism.</p>
</div>
<span class="comment-copy">Do they have to be deamons? Now they just behave like normal Process.</span>
<span class="comment-copy">@MegaIng <code>multiprocessing.Pool</code> sets daemon flag on workers: <a href="https://github.com/python/cpython/blob/master/Lib/multiprocessing/pool.py#L182" rel="nofollow noreferrer">github.com/python/cpython/blob/master/Lib/multiprocessing/â€¦</a></span>
<span class="comment-copy">I'm aware of the source of the error message. I arrived at my solution by looking at the multiprocessing source. And subprocess won't work. The job is with c++ library whose access is provided through cython. It's not a cmdline call.</span>
<span class="comment-copy">@JasonMond You can launch the same Python file with a command line argument that will trigger the necessary logic in the <code>if __name__ == '__main__'</code> block.</span>
<span class="comment-copy">that won't help either. Among many arguments to the job is a list of xml objects. Which can be in the order of 100s of MBs. Outside of writing those files to a file system where space is at a premium (and I have 1000s of these jobs running in parallel), I don't see how those things can be passed as arguments through subprocess in a clean manner. And then the return value is a structured python object created in cython. I could probably create a json based serialization and deserialization function so that it can go through subprocess, but that's also a decent amount of work.</span>
<span class="comment-copy">@JasonMond just the same, stdin or IPC. <code>multiprocessing</code> uses <code>pickle</code> under the hood, so it's as simple as <code>pickle.dump(args,process.stdin)</code> and <code>.load(sys.stdin)</code> on the other end (<code>sys.stdin.buffer</code> in Py3 to read in binary mode).</span>
<span class="comment-copy">@JasonMond and if you use fork, you don't need to send input data at all, only get the result back.</span>
