<div class="post-text" itemprop="text">
<p>So I've got some code</p>
<pre><code>tensors = [] //its filled with 3D float tensors
total = sum(tensors)
</code></pre>
<p>if I change that last line to</p>
<pre><code>total = tf.add_n(tensors)
</code></pre>
<p>then the code produces the same output but runs much more slowly and soon causes
an out-of-memory exception. Whats going on here? Can someone explain how pythons built in sum function and tf.add_n interact with an array of tensors respectively and why pythons sum would seemingly just be a better version?</p>
</div>
<div class="post-text" itemprop="text">
<p>When you use <code>sum</code>, you call a standard python algorithm that call <code>__add__</code> recursively on the elements of the array. Since <code>__add__</code> (or <code>+</code>) indeed is overloaded on tensorflow's tensors, it works as expected: it creates a graph that can be executed during a session. It is not optimal, however, because you add as many operation as there are elements in your list; also, you are enforcing the order of the operation (add the first two elements, then the third to the result, and so on), which is also not optimal.</p>
<p>By contrast, <code>add_n</code> is a specialized operation to do just that. Looking at the graph is really telling I think:</p>
<pre><code>import tensorflow as tf

with tf.variable_scope('sum'):
  xs = [tf.zeros(()) for _ in range(10)]
  sum(xs)

with tf.variable_scope('add_n'):
  xs = [tf.zeros(()) for _ in range(10)]
  tf.add_n(xs)
</code></pre>
<p><a href="https://i.stack.imgur.com/TOUzC.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/TOUzC.png"/></a></p>
<p>However – contrary to what I thought earlier – <code>add_n</code> takes up more memory because it waits – and store – for all incoming inputs before storing them. If the number of inputs is large, then the difference can be substantial.</p>
<p>The behavior I was expecting from <code>add_n</code>, that is, summation of inputs as they are available, is actually achieved by <a href="https://www.tensorflow.org/api_docs/python/tf/accumulate_n" rel="nofollow noreferrer"><code>tf.accumulate_n</code></a>. This should be the superior alternative, as it takes less memory than <code>add_n</code> but does not enforce the order of summation like <code>sum</code>.</p>
<p>Why did the authors of tensorflow-wavenet used <code>sum</code> instead of <code>tf.accumulate_n</code>? Certainly because before this function is not differentiable on TF &lt; 1.7. So if you have to support TF &lt; 1.7 <em>and</em> be memory efficient, good old <code>sum</code> is actually quite a good option.</p>
</div>
<div class="post-text" itemprop="text">
<p>The <a href="https://docs.python.org/3/library/functions.html#sum" rel="nofollow noreferrer">sum() built-in</a> only takes iterables and therefor would seem to gain the advantage of using generators in regards to memory profile.</p>
<p>the <a href="https://www.tensorflow.org/api_docs/python/tf/add_n" rel="nofollow noreferrer">add_n() function</a> for tensor takes a list of tensors and seem to retain that data structure throughout handling based on it's requirement for shape comparison.</p>
<pre><code>In [29]: y = [1,2,3,4,5,6,7,8,9,10]  

In [30]: y.__sizeof__()
Out[30]: 120

In [31]: x = iter(y)

In [32]: x.__sizeof__()
Out[32]: 32
</code></pre>
</div>
<span class="comment-copy">Well its not my code. If you're really interested here is the repo. <a href="https://github.com/ibab/tensorflow-wavenet" rel="nofollow noreferrer">github.com/ibab/tensorflow-wavenet</a> Simply changing the line in model.py _create_network function causes it to reach OOM exception very quickly on my GPU</span>
