<div class="post-text" itemprop="text">
<p>I am doing a CPU expensive computation that can be easily divided into many workers. However, I have to join the results at the end. The problem is that you can not use shared memory when using multiple isolated processes, and the only way of sending the computed output from the child to the parent is using <code>multiprocessing.Queue</code>, <code>multiprocessing.Manager</code> or <code>multiprocessing.Pool</code>. All these approaches pickle the object and sends it using some kind of IPC. </p>
<p>I timed all the different parts of the operation, and processing the data is way faster with the processes, however getting the object is too slow, and its always faster if I do not use multiprocessing.</p>
<p>Is there a way to achieve the same level of shared memory we get when using <code>multithreading library</code> ? I would like to be able to do something like:</p>
<pre><code>process = [None]*numProcess
#List where the processes should write in memory the output.
results = [None]*numProcess
m = float(len(nflow))/numProcess
nflow_for_process = [nflow[int(m*i):int(m*(i+1))] for i in range(numProcess)]


for i in xrange(numProcess):
    p = Process(target=self.gatherFlowsProcess, args=(nflow_for_process[i]))
    p.daemon = True
    processes.append(p)
    p.start()

#here I join all the results again. 
results_tmp = results[0]
for d in results[1:]:
    for tuple in d:
        if results_tmp.has_key(tuple):
            results_tmp[tuple].update(d[tuple])
        else:
            results_tmp[tuple] = d[tuple]


return results_tmp
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>you can try pickle dictionaries your self, i recommend using dill module for that (but there are other module you can use too), so it would look something like this:</p>
<pre><code>import dill as pickle
dic = {...}
f = open('save.pkl', 'wb')
g = pickle.dump(dic, f)
f.close()
</code></pre>
<p>than you can use to get dictionary:</p>
<pre><code>f = open('save.pkl', 'rb')
c = pickle.load(f)    # c is now equal to dic
f.close()
</code></pre>
</div>
<span class="comment-copy">I think dictionaries solve your problem.</span>
<span class="comment-copy">Dictionaries where? A dictionary its what the children has to return, or append it to the list. However, since they don't have shared memory I can not find an efficient way to do it. Pickle needs a lot of time to pickle the returning dictionaries.</span>
<span class="comment-copy">In my description I already talk about that. You can pass the dictionaries to the main process using multithreading queues, pipes, etc. All those approaches serialize/unserialize the object you want to get. However, my dictionaries are to big and that becomes the bottleneck of my program. I would like to have a python list in shared memory (I have seen that it can be done with numpy.arrays, I tried but I was getting segmentation faults)</span>
<span class="comment-copy">I get that, I just that this might be faster, when you say your dictionaries are big, how big?</span>
<span class="comment-copy">They can be 50mb, and they have dictionaries inside the dictionary. Computing them its faster it takes much less than with one process, but then when the parent process gets them it becomes super slow and summing up the whole thing is slower than using a single process. If I send the dictionaries using threads that use shared memory it takes 1ms, so with shared memory I would completely solve the problem, but multiprocessing lib does not offer shared memory for a list of dictionaries... and I have not found anything that could do that.</span>
<span class="comment-copy">have you tried this <a href="https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow noreferrer">docs.python.org/3/library/â€¦</a></span>
<span class="comment-copy">Yes, I tried the manager and its the same speed. And sharing state between processes its very limited. You have to have a list of elements of the same type and they have to be 'ctype'. I want to have a list of length the number of processes and every process would return the computed dictionary inside that list. Using the structures that they allow would make my program even slower because I would have to convert the dictionary to something that Array() could contain.</span>
