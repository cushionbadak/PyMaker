<div class="post-text" itemprop="text">
<p>I'm trying to go through each feature in one file (1 per line) and find all matching features based on one column of that line in a second file. I have this solution, which does what I want on small files, but it's very slow on big files (my files have &gt;20,000,000 lines). <a href="https://gist.github.com/ethanagbaker/dc7bc62a413cbbc32bbf944125845afd" rel="nofollow">Here's a sample of the two input files.</a></p>
<p>My (slow) code:</p>
<pre><code>FEATUREFILE = 'S2_STARRseq_rep1_vsControl_peaks.bed'
CONSERVATIONFILEDIR = './conservation/'
with open(str(FEATUREFILE),'r') as peakFile, open('featureConservation.td',"w+") as outfile:
for line in peakFile.readlines():
    chrom = line.split('\t')[0]
    startPos = int(line.split('\t')[1])
    endPos = int(line.split('\t')[2])
    peakName = line.split('\t')[3]
    enrichVal = float(line.split('\t')[4])

    #Reject negative peak starts, if they exist (sometimes this can happen w/ MACS)
    if startPos &gt; 0:
        with open(str(CONSERVATIONFILEDIR) + str(chrom)+'.bed','r') as conservationFile:
            cumulConserv = 0.
            n = 0
            for conservLine in conservationFile.readlines():
                position = int(conservLine.split('\t')[1])
                conservScore = float(conservLine.split('\t')[3])
                if position &gt;= startPos and position &lt;= endPos:
                    cumulConserv += conservScore
                    n+=1
        featureConservation = cumulConserv/(n)
        outfile.write(str(chrom) + '\t' + str(startPos) + '\t' + str(endPos) + '\t' + str(peakName) + '\t' + str(enrichVal) + '\t' + str(featureConservation) + '\n')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The best solution for my purposes seems to be rewriting the above code for pandas. Here's what's working well for me on some very large files:</p>
<pre><code>from __future__ import division
import pandas as pd

FEATUREFILE = 'S2_STARRseq_rep1_vsControl_peaks.bed'
CONSERVATIONFILEDIR = './conservation/'

peakDF = pd.read_csv(str(FEATUREFILE), sep = '\t', header=None, names=['chrom','start','end','name','enrichmentVal'])
#Reject negative peak starts, if they exist (sometimes this can happen w/ MACS)
peakDF.drop(peakDF[peakDF.start &lt;= 0].index, inplace=True)
peakDF.reset_index(inplace=True)
peakDF.drop('index', axis=1, inplace=True)
peakDF['conservation'] = 1.0 #placeholder

chromNames = peakDF.chrom.unique()

for chromosome in chromNames: 
    chromSubset = peakDF[peakDF.chrom == str(chromosome)]
    chromDF = pd.read_csv(str(CONSERVATIONFILEDIR) + str(chromosome)+'.bed', sep='\t', header=None, names=['chrom','start','end','conserveScore'])

for i in xrange(0,len(chromSubset.index)):
    x = chromDF[chromDF.start &gt;= chromSubset['start'][chromSubset.index[i]]]
    featureSubset = x[x.start &lt; chromSubset['end'][chromSubset.index[i]]]
    x=None
    featureConservation = float(sum(featureSubset.conserveScore)/(chromSubset['end'][chromSubset.index[i]]-chromSubset['start'][chromSubset.index[i]]))
    peakDF.set_value(chromSubset.index[i],'conservation',featureConservation)
    featureSubset=None

 peakDF.to_csv("featureConservation.td", sep = '\t')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>To start with you are looping over ALL of <code>conservationFile</code> every time you read a single line from <code>peakFile</code> so stick a <code>break</code> after <code>n+=1</code> in the if statement and that should help somewhat. Assuming that there is only one match that is.</p>
<p>Another option is to try using <a href="https://docs.python.org/3/library/mmap.html" rel="nofollow">mmap</a> which may help with buffering</p>
</div>
<div class="post-text" itemprop="text">
<p>Bedtools was made for this, specifically the <code>intersect</code> function:</p>
<p><a href="http://bedtools.readthedocs.io/en/latest/content/tools/intersect.html" rel="nofollow">http://bedtools.readthedocs.io/en/latest/content/tools/intersect.html</a></p>
</div>
<span class="comment-copy">have you considered sorting <code>conservationFile</code> by the position, this way you can reduce the complexity of this to N rather N^2 assuming the <code>peakFile</code> is sorted as well. (you can use <code>sort -r -k2</code> for that)</span>
<span class="comment-copy">Use Pandas.  It might change your life.</span>
<span class="comment-copy">@Chris OP wasn't looking for a library recommendation.</span>
<span class="comment-copy"><code>for line in peakFile.readlines():</code> will read the whole file into memory at once. <code>for line in peakFile:</code> will read one line at a time.</span>
<span class="comment-copy">@Chris I'm familiar w/ Pandas, but didn't think parsing these files into data tables would speed it up. Also there's an issue with needing to open a particular file based on information in the first file.</span>
<span class="comment-copy">nice. If this works for you, good job :)</span>
<span class="comment-copy">Ah, there's more than one match. The peakFile has information on a range of positions that constitute that feature (ie peak 1 is position 1-200). conservationFile has a score for each individual position, so I need to find all of those conservationFile entries that are within the range of the peakFile feature.</span>
<span class="comment-copy">Ok, my bad. but like @Chris said seriously check out <a href="http://pandas.pydata.org/" rel="nofollow noreferrer">Pandas</a> it might change you life</span>
<span class="comment-copy">I was having problems preserving some of the extra data in additional columns when I tried bedtools.</span>
<span class="comment-copy">You can merge them with a delimiter, such as underscore (<code>_</code>) then split them after e.g. <code>perl -p -e "s/_/\t/g;"</code>. I acknowledge that bedtools is somewhat ill-suited for non-BED data, but  the benefits out weigh the drawbacks as it is highly optmizied for performance and has a plethora of command and options.  You can also split out the extra cols and add them back later with <code>awk</code> or <code>paste</code> depending on the output you obtain. I am happy to help out on this.</span>
