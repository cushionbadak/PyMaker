<div class="post-text" itemprop="text">
<p>I have a csv file with many millions of rows. I want to start iterating from the 10,000,000 row. At the moment I have the code:</p>
<pre><code>    with open(csv_file, encoding='UTF-8') as f: 
        r = csv.reader(f)
        for row_number, row in enumerate(r):    
            if row_number &lt; 10000000:
                continue
            else:
                process_row(row)      
</code></pre>
<p>This works, however take several seconds to run before the rows of interest appear. Presumably all the unrequired rows are loaded into python unnecessarily, slowing it down. Is there a way of starting the iteration process on a certain row - i.e. without the start of the data read in.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could use <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow"><em>islice</em></a>:</p>
<pre><code>from itertools import islice

with open(csv_file, encoding='UTF-8') as f:
    r = csv.reader(f)
    for row in islice(r,  10000000, None):
            process_row(row)  
</code></pre>
<p>It still iterates over all the rows but does it a lot more efficiently. </p>
<p>You could also use the <a href="https://docs.python.org/3/library/itertools.html#recipes" rel="nofollow">consume recipe</a> which calls <em>functions that consume iterators at C speed</em>, calling it on the file object <em>before</em> you pass it to the <em>csv.reader</em>, so you also avoid needlessly processing those lines with the reader:</p>
<pre><code>import collections
from itertools import islice
def consume(iterator, n):
    "Advance the iterator n-steps ahead. If n is none, consume entirely."
    # Use functions that consume iterators at C speed.
    if n is None:
        # feed the entire iterator into a zero-length deque
        collections.deque(iterator, maxlen=0)
    else:
        # advance to the empty slice starting at position n
        next(islice(iterator, n, n), None)


with open(csv_file, encoding='UTF-8') as f:
    consume(f, 9999999)
    r = csv.reader(f)
    for row  in r:
          process_row(row)  
</code></pre>
<p>As Shadowranger commented, if a file could conatin embedded newlines then you would have to consume the reader and pass <code>newline=""</code> but if that is not the case then use do consume the file object as the performance difference will be considerable especially if you have a lot of columns.</p>
</div>
<span class="comment-copy">Any reason you can't use <code>tail</code> to skip the first N lines and pipe that to your python script?</span>
<span class="comment-copy">Side-note: You want to pass <code>newline=''</code> to the <code>open</code> call; the <code>csv</code> module expects you to leave newline interpolation to it, you don't want <code>open</code> performing line ending conversions.</span>
<span class="comment-copy">You shouldn't run the <code>consume</code> on the raw file handle if there is a chance that a field could contain embedded newlines (legal in most if not all CSV dialects). Skipping before <code>csv.reader</code> wrapping means you'll incorrectly interpret field embedded newlines as record separators.</span>
<span class="comment-copy">@ShadowRanger, true, I added a note.</span>
