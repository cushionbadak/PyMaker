<div class="post-text" itemprop="text">
<p>Are there some edit-distance in python that take account of the accent.
Where for exemple hold the following property</p>
<pre><code>d('ab', 'ac') &gt; d('àb', 'ab') &gt; 0
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>With the <a href="https://pypi.python.org/pypi/python-Levenshtein/" rel="noreferrer">Levenshtein module</a>:</p>
<pre><code>In [1]: import unicodedata, string

In [2]: from Levenshtein import distance

In [3]: def remove_accents(data):
   ...:     return ''.join(x for x in unicodedata.normalize('NFKD', data)
   ...:                             if x in string.ascii_letters).lower()

In [4]: def norm_dist(s1, s2):
   ...:     norm1, norm2 = remove_accents(s1), remove_accents(s2)
   ...:     d1, d2 = distance(s1, s2), distance(norm1, norm2)
   ...:     return (d1+d2)/2.

In [5]: norm_dist(u'ab', u'ac')
Out[5]: 1.0

In [6]: norm_dist(u'àb', u'ab')
Out[6]: 0.5
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Unicode allows decomposition of accented characters into the base character plus a combining accent character; e.g. <code>à</code> decomposes into <code>a</code> followed by a combining grave accent.</p>
<p>You want to convert both strings using normalization form NFKD, which decomposes accented characters and converts compatibility characters to their canonical forms, then use an edit distance metric that ranks substitutions above insertions and deletions.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here's a solution based on <a href="https://docs.python.org/3/library/difflib.html" rel="nofollow noreferrer">difflib</a> and <a href="https://docs.python.org/3/library/unicodedata.html" rel="nofollow noreferrer">unicodedata</a> with no dependencies whatsoever:</p>
<pre><code>import unicodedata
from difflib import Differ

# function taken from https://stackoverflow.com/a/517974/1222951
def remove_accents(input_str):
    nfkd_form = unicodedata.normalize('NFKD', input_str)
    only_ascii = nfkd_form.encode('ASCII', 'ignore').decode()
    return only_ascii

def compare(wrong, right):
    # normalize both strings to make sure equivalent (but
    # different) unicode characters are canonicalized 
    wrong = unicodedata.normalize('NFKC', wrong)
    right = unicodedata.normalize('NFKC', right)

    num_diffs = 0
    index = 0
    differences = list(Differ().compare(wrong, right))
    while True:
        try:
            diff = differences[index]
        except IndexError:
            break

        # diff is a string like "+ a" (meaning the character "a" was inserted)
        # extract the operation and the character
        op = diff[0]
        char = diff[-1]

        # if the character isn't equal in both
        # strings, increase the difference counter
        if op != ' ':
            num_diffs += 1

        # if a character is wrong, there will be two operations: one
        # "+" and one "-" operation
        # we want to count this as a single mistake, not as two mistakes
        if op in '+-':
            try:
                next_diff = differences[index+1]
            except IndexError:
                pass
            else:
                next_op = next_diff[0]
                if next_op in '+-' and next_op != op:
                    # skip the next operation, we don't want to count
                    # it as another mistake
                    index += 1

                    # we know that the character is wrong, but
                    # how wrong is it?
                    # if the only difference is the accent, it's
                    # a minor mistake
                    next_char = next_diff[-1]
                    if remove_accents(char) == remove_accents(next_char):
                        num_diffs -= 0.5

        index += 1

    # output the difference as a ratio of
    # (# of wrong characters) / (length of longest input string)
    return num_diffs / max(len(wrong), len(right))
</code></pre>
<hr/>
<p>Tests:</p>
<pre><code>for w, r in (('ab','ac'),
            ('àb','ab'),
            ('être','etre'),
            ('très','trés'),
            ):
    print('"{}" and "{}": {}% difference'.format(w, r, compare(w, r)*100))
</code></pre>
<pre><code>"ab" and "ac": 50.0% difference
"àb" and "ab": 25.0% difference
"être" and "etre": 12.5% difference
"très" and "trés": 12.5% difference
</code></pre>
</div>
<span class="comment-copy">Wouldn't just replacing accented letters with non-accented in both the strings, then calculating the distance work?</span>
<span class="comment-copy">I second that. Using Unidecode might help: <a href="https://pypi.python.org/pypi/Unidecode/0.04.1" rel="nofollow noreferrer">pypi.python.org/pypi/Unidecode/0.04.1</a></span>
<span class="comment-copy">ok thanks, but at this point I have d('àa','aa') = 0.</span>
<span class="comment-copy">Where is the problem? You don't know how to tell if a given character is an "accented" version of another, or how to integrate this fact in the distance itself?(or both?)</span>
<span class="comment-copy">@vigte, what values do you want then?</span>
