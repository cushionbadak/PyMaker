<div class="post-text" itemprop="text">
<p>I want to do batch processing of files on multiple cores. I have following scenario:</p>
<ol>
<li>I have 20 files.</li>
<li>I have a function that takes a filename, processes it and produces an integer result. I want to apply the function to all the 20 files, calculate integer output for each of them and finally sum individual outputs and print the total result.</li>
<li>Since I have 4 cores, I can only process 4 files at time. Thus I want to run 5 rounds of processing 4 files at a time (4*5 = 20).</li>
<li>That is I want to create 4 processes each processing 5 files one after another (1st process processes files 0, 4, 8, 12, 16, 2nd process processes files 1, 5, 9, 13, 17, etc.).</li>
</ol>
<p>How do I achieve this? I am confused by <code>multiprocessing.Pool()</code>, <code>multiprocessing.Process()</code> and various other options.</p>
<p>Thanks.</p>
</div>
<div class="post-text" itemprop="text">
<p>In order to demonstrate <code>Pool</code> I'm going to suppose your working function, which consumes a filename and produces a number, is named <code>work</code> and that the 20 files are labeled <code>1.txt</code>,... <code>20.txt</code>. One way to set this up would be as follows,</p>
<pre><code>from multiprocessing import Pool

pool = Pool(processes=4)
result = pool.map_async(work, ("%d.txt"%n for n in xrange(1,21)))
print sum(result.get())
</code></pre>
<p>This method will do the work of steps 3 and 4 for you.</p>
</div>
<div class="post-text" itemprop="text">
<p>It's pretty simple.</p>
<pre><code>from multiprocessing import Pool

def process_file(filename):
    return filename

if __name__ == '__main__':
    pool = Pool()
    files = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    results = pool.imap(process_file, files)

    for result in results:
        print result
</code></pre>
<p><code>Pool</code> automatically defaults to the number of processor cores that you have. Also, make sure that your processing function is importable from the file and that your multiprocessing code is inside of the <code>if __name__ == '__main__':</code>. If not, you'll make a fork bomb and lock up your computer.</p>
</div>
<div class="post-text" itemprop="text">
<p>Although Jared's answer is great, I personally would use a <code>ProcessPoolExecutor</code> from the <a href="http://docs.python.org/3/library/concurrent.futures.html" rel="nofollow"><code>futures</code></a> module, and not even worry about <code>multiprocessing</code>:</p>
<pre><code>with ProcessPoolExecutor(max_workers=4) as executor:
    result = sum(executor.map(process_file, files))
</code></pre>
<p>When it gets a little more complicated, the <code>future</code> object, or <code>futures.as_completed</code>, can be really nifty compared to the <code>multiprocessing</code> equivalents. When it gets a lot more complicated, <code>multiprocessing</code> is a whole lot more flexible and powerful. But when it's this trivial, really, it's almost hard to tell the difference.</p>
</div>
<span class="comment-copy">Do you really "want to run 5 rounds of processing 4 files at a time", or just "process 20 files, no more than 4 at a time, as quickly as possible"? Because the latter is a lot simpler (as well as being faster).</span>
<span class="comment-copy">@abarnert: You are correct. I want to process 20 files as quickly as possible and I do not care if each processor strictly gets 5 files each. If one of the processor spends more than average time on one of the files, then I expect other processors pick up remaining work and finish them and not necessarily stop doing work because they have processed their quota of 5 files.</span>
<span class="comment-copy">OK, good, that's exactly what I meant by "as well as being faster". Then Jared's 3-line answer is literally all you need.</span>
<span class="comment-copy">Thanks for the quick response. Now, what's the difference between Pool.map(), Pool.map_async() and Pool.imap(). Which one should be used in which scenario?</span>
<span class="comment-copy">@abhinavkulkarni: You can sum an iterator just as easily. Basically, it will block on evaluating the iterator inside <code>sum</code>, instead of blocking on returning the <code>list</code>. When you're dealing with only 20 objects, the difference won't make any difference either way.</span>
<span class="comment-copy">@abhinavkulkarni: Yes. Or, if your function just returned an <code>int</code>, but you were calling it 5 million times instead of 20, and didn't want to build up a huge list. It's basically the same tradeoff as <code>range</code> vs. <code>xrange</code>, <code>map</code> vs. <code>imap</code>, etc. all over Python 2.x. (In 3.x, for the most part, you don't get the list versions of most things anymore.)</span>
<span class="comment-copy">@abhinavkulkarni: You should also look at the difference between <code>map</code> and <code>map_async</code>, and between <code>imap</code> and <code>imap_unordered</code>.</span>
<span class="comment-copy">@abhinavkulkarni: One more thing to look at is the <code>chunksize</code> parameter. For many cases, you don't careâ€”just let <code>multiprocessing</code> guess at a good chunk size and it'll be more than good enough. But if you have a small number of large and highly variable tasks, you might want to test with different explicit chunk sizes. (Too much chunking means less parallelism; too little means more scheduling overhead.)</span>
