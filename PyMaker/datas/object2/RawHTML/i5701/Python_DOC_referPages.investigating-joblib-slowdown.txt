<div class="post-text" itemprop="text">
<p>I'm trying to use <a href="https://pythonhosted.org/joblib/parallel.html" rel="nofollow noreferrer">joblib</a> to make a custom random forest implementation train in parallel.</p>
<p>The task is embarrassingly parallel, so I assumed getting a speedup shouldn't be too hard with joblib.</p>
<p>Here's some sample code:</p>
<pre><code>class RandomForest(object):
    def __init__(self, settings, data):
        self.forest = [None] * settings.n_trees
        self.parallel = Parallel(n_jobs=settings.njobs, backend="threading")

    def fit(self, data, train_ids_current_minibatch, settings, param, cache):
        self.forest = self.parallel(
            delayed(_parallel_build_trees_batch)(
                i_t, data, train_ids_current_minibatch, settings, param, cache)
            for i_t, tree in enumerate(self.forest))

    def partial_fit(self, data, train_ids_current_minibatch, settings, param, cache):
        self.forest = self.parallel(
            delayed(_parallel_build_trees_partial)(
                tree, i_t, data, train_ids_current_minibatch, settings, param, cache)
            for i_t, tree in enumerate(self.forest))
</code></pre>
<p>However, the training is much slower when using more than one jobs, both in the batch and incremental case. The data and cache arguments are dicts that contain (large) numpy arrays, so I'm wondering if that is the cause.</p>
<p>I've tried coding the same using <code>multiprocessing.Pool</code> and the results are even worse, as is not using the <code>threading</code> backend of joblib, I assume because the fit functions make heavy use of numpy/scipy code.</p>
<p>Any ideas on how to debug/fix the slowdown?</p>
</div>
<div class="post-text" itemprop="text">
<p>Your analysis seems correct to me: The slowdown is caused by <code>data</code> and <code>cache</code> being large objects. Now, in a multiprocessing environment you don't have shared memory, so you need to somehow share these objects. Python support this via <a href="https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes" rel="nofollow noreferrer">shared objects</a>: there is a "main process" which really holds the object. But then the other processes need to send all updates over some mechanism (AFAIK the object is pickled and then sent via a pipe or queue) which slows it down.</p>
<p>I see some options for you:</p>
<ul>
<li>transform your code so it uses <strong>partitions</strong>: I'm not familiar with random forest. I guess every process has <code>data</code> as an initial data set and then you try to find an "optimum". If you could push process 1 to find all "type A" optimums, and process 2 to find all "type B" optimums and then let every process e.g. write their findings on disk in file <code>rf_process_x.txt</code> then you don't need a shared memory state</li>
<li>transform your code so it uses <strong>queues</strong> (see last example on <a href="https://docs.python.org/3/library/multiprocessing.html#examples" rel="nofollow noreferrer">this page</a>): If partitioning doesn't work, then maybe you could:

<ol>
<li>start up n worker processes</li>
<li>every process builds up his <code>data</code> set for himself (so it is not in shared memory)</li>
<li>in the main process you put "jobs" into the task_queue, e.g. find random forest with this specific set of parameters. The worker gets the job from the task_queue, computes it and puts its result on a result_queue. This is only fast if the tasks and results are slow as these objects need to be pickled and sent over a pipe from the parent process to the worker process. </li>
</ol></li>
<li>use <strong>joblibs memmapping</strong>: <a href="https://pythonhosted.org/joblib/parallel.html#working-with-numerical-data-in-shared-memory-memmaping" rel="nofollow noreferrer">Joblibs supports</a> dumping the object onto disk and then give each object a memory-mapped access to that file</li>
<li>if your operation is <em>not</em> CPU bound (does heavy disk or network operations) you could move to <strong>multithreading</strong>. This way you really have a shared memory. But as far as I can see you <em>are</em> cpu bound and will run into the "GIL lock" issue (in cpython only one thread runs at a time)</li>
<li>you may find <strong>other ways of speeding up</strong> random forest, e.g. <a href="https://stackoverflow.com/a/37213922/119861">this SO answer</a> mentions a few</li>
</ul>
</div>
<span class="comment-copy">did my post answer your question?</span>
