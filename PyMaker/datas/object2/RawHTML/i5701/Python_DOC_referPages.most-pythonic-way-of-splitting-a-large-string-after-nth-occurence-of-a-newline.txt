<div class="post-text" itemprop="text">
<p>I have a very large multiline string, and would like to split it into an array, let's say, after each 50th occurrence of the newline character (\n)
What is, in your opinion, the most Pythonic and efficient way to do so?</p>
</div>
<div class="post-text" itemprop="text">
<p>A couple approaches that avoid storing all the lines as separate strings in memory (which any use of <code>split</code>/<code>splitlines</code> would require, involving quite a bit of overhead thanks to the fairly high per-object overhead for each string) would be to use a file-like object wrapper to get lines one-by-one, and <code>itertools.islice</code> (or a clever <code>zip</code> based trick) to batch them up.</p>
<pre><code>from io import StringIO  # On Py2, for plain str, you'd use from cStringIO import StringIO
from itertools import islice

def batch_lines(data, batchsize=50):
    with StringIO(data) as f:
        while True:
            block = ''.join(islice(f, batchsize))
            if not block: break
            yield block
</code></pre>
<p>Equivalently, though a bit more obscurely, you could use <code>zip_longest</code>:</p>
<pre><code>from io import StringIO  # On Py2, for plain str, you'd use from cStringIOimport StringIO
from itertools import islice, zip_longest

def batch_lines(data, batchsize=50):
    with StringIO(data) as f:
        yield from map(''.join, zip_longest(*[f] * batchsize, fillvalue=''))
</code></pre>
<p>The second approach appears to be the faster of the two. Comparing them to a non-memory sensitive approach that <code>splitlines</code> and <code>join</code>s slices of the result:</p>
<pre><code>def batch_lines(data, batchsize=50):
    lines = data.splitlines(True)
    yield from (''.join(lines[i:i+batchsize]) for i in range(0, len(lines), batchsize))
</code></pre>
<p>the <code>zip_longest</code> approach and the <code>splitlines</code> based approach have roughly identical timings for me (for large numbers of short lines), while the <code>islice</code> approach takes about 40% longer. For some inputs, the <code>zip_longest</code> approach is slower than <code>splitlines</code>, though if the data has huge numbers of lines (enough to cause memory pressure when you make millions of individual <code>str</code>s from the lines up front), you'll gain more from reduced memory than it costs you in CPU. </p>
<p>On my Python (64 bit Python 3.6.1 on Windows), the per <code>str</code> overhead (ignoring the cost of storing the actual data) for ASCII <code>str</code> is 49 bytes (it goes up for non-ASCII). So if <code>data</code> is 1 GB of data, comprising 10 million lines, holding the split lines in memory simultaneously would cost you another 1 GB for the split up data, plus another ~470 MB for the object headers associated with each <code>str</code> (add in the cost of the <code>list</code> to store them, and you're a little over 540 MB of extra overhead). If your system has 3 GB of RAM, and the OS and other applications are using 800 MB of it, the cost of that extra 540 MB will be paid in agonizing slowdowns from page thrashing. If your <code>data</code> is smaller than that, sure, go for the simple approach, but if you might approach system memory limits, you may want to use a lazier approach to line splitting.</p>
</div>
<div class="post-text" itemprop="text">
<p>you could use split and join every i:i+n lines. Not sure if it is the most pythonic way.</p>
<pre><code>data = 'one\ntwo\nthree\nfour\n'
n = 2 # in your case it will be 50
lines = data.split()
print ['\n'.join(lines[i:i+n]) for i in range(0, len(lines), n)]
</code></pre>
<p>results in </p>
<pre><code>['one\ntwo', 'three\nfour']
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's one way that should be quite efficient. I can't claim it's Pythonic, if it was it would probably be shorter. But it does use <code>yield</code>.</p>
<pre><code>def line_groups(string, n=50):
    start = 0
    while start &lt; len(string):
        end = start
        for i in range(n):
            pos = string.find('\n', end, len(string))
            if pos &lt; 0:
                end = len(string)
                break
            end = pos + 1
        yield string[start:end]
        start = end
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>It's probably not the most pythonic, but here's my stab at the matter:</p>
<pre><code>#python 2.7
def split(string):
 string_lines = string.splitlines(True)
 return [sum(stringlines[i:i+50]) for i in xrange(0, len(string_lines), 50)]
</code></pre>
<p>It works by splitting the string into each line, then grabbing them in groups of 50. </p>
<p>I've been told that using <code>sum</code> on strings is a <em>very</em> bad idea, so here's another take:</p>
<pre><code>def find_next_index(string):
  for index,value in enumerate(string):
    if value == '\n':
      yield (index,True)
  yield (len(string),False)
def split2(string):
 output = []
 count = 0
 last_index = 0
 temp = (0,True)
 While temp[1]:
   temp = find_next_index(string)
   count += 1
   if count==50:
      output.append(string[last_index:temp])
      last_index = temp
      count = 0
 return output
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This is a short, pythonic version:</p>
<pre><code>from itertools import zip_longest
def batch_lines(data, batchsize=50):
     args = [iter(data.split())] * batchsize
     return zip_longest(*args, fillvalue='')
</code></pre>
<p>It does not do stream processing of the string, like @ShadowRanger but it's simpler and still contained in 2 lines (without imports). It returns an iterator as well. The solution comes from the recipe for the <code>grouper</code> function available at <a href="https://docs.python.org/3/library/itertools.html#itertools.islice" rel="nofollow noreferrer">https://docs.python.org/3/library/itertools.html#itertools.islice</a></p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Recursive Algorithm for Generator on Stream Input</strong></p>
<p>Take a recursive approach:</p>
<pre><code>import sys
INTMIN = -sys.maxsize

def find_next_nth(s, sub, n=1):
    cut = s.find(sub)
    if cut == -1:
        return INTMIN
    if n == 1:
        return cut
    return cut + find_next_nth(s[cut+1:], sub, n=n-1) + 1
</code></pre>
<p>which gives the indices of the every n-th cuts, then split around the cuts, like so:</p>
<pre><code>def split_every_nth(s, sub, n=1):
    cut = find_next_nth(s, sub, n)
    while cut &gt; 0:
        yield s[:cut]
        s = s[cut+1:]
        cut = find_next_nth(s, sub, n)
    yield s
    raise StopIteration
</code></pre>
<p>This approach gives a generator that also works well with <code>s</code> coming in as stream, since the algorithm does not require the whole string <code>s</code>, but only the head of it. Very gentle on RAM.</p>
<p>You call the function with</p>
<pre><code>for chunk in split_every_nth(my_str, my_char, n=n):
    #do work here
</code></pre>
<p>Pretty Pythonic, at least I think so;/</p>
</div>
<span class="comment-copy">What's your take on the matter?</span>
<span class="comment-copy">What is your opinion? Do you have some code that you have been working on? I'd suggest a generator.</span>
<span class="comment-copy">You are essentially asking how to chunk a file-iterator...</span>
<span class="comment-copy">post some sample input and expected output .</span>
<span class="comment-copy">Ah, there's the function I was looking for. What's the asymptotic complexity of join, though?</span>
<span class="comment-copy">@JakobLovern O(n).</span>
<span class="comment-copy">@juanpa.arrivillaga <i>how</i> do you know all this?</span>
<span class="comment-copy">@JakobLovern the usual idiom is instead of summing a bunch of stirngs directly, accumulate them into a container like a <code>list</code> which has linear append, then join, which again is linear, and this keeps you in linear time.</span>
<span class="comment-copy">Presumably, since it's by line, you'd want <code>data.splitlines()</code> (splits on line breaks), not <code>data.split()</code> (splits on any whitespace). It's equivalent for your test input, but not for the general case of lines. You could also pass <code>keepends=True</code> to <code>splitlines</code>, and use plain <code>''.join</code> (not <code>'\n'.join</code>) and preserve the source data's newlines precisely.</span>
<span class="comment-copy">Don't do <code>reduce(lambda x,y: x+y</code>, just use <code>sum</code>, but <i>definitely</i> don't do it with <code>str</code> types, because that is a quadratic algorithm.</span>
<span class="comment-copy">ugh, I can never remember whether <code>sum</code> allows you to add strings together. Thanks, though. It's been edited out.</span>
<span class="comment-copy">You still shouldn't, the reason it prevents you is because using <code>sum</code> is quadratic time with sequence types, including list, loop and extend or whatever... Just think about what <code>for sublist in list_of_lists: final_list = final_list + sublist</code> does...</span>
<span class="comment-copy">@JakobLovern: You can still use <code>sum</code>-like behaviors, just use <code>''.join</code> instead (which is designed to avoid the quadratic scaling issue, in exchange for specializing for strings): <code>return [''.join(stringlines[i:i+50]) for i in xrange(0, len(string_lines), 50)]</code></span>
