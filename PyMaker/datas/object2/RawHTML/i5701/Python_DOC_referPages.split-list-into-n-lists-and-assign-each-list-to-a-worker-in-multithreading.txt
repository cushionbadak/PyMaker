<div class="post-text" itemprop="text">
<p>I'm writing a script that takes N records from a table, and processes the said records via multithreading.</p>
<p>Previously I simply used Order by RAND() in my SQL statement within each worker definition, and hoped that there would be no duplicates.</p>
<p>This sort of works (deduping is done later), however, I would like to make my script more efficient by:</p>
<p><strong>1)</strong> querying the table once, extract N records, and assign them to a list</p>
<p><strong>2)</strong> split the big list into ~equally-sized lists of Y lists, which can be accomplished via :</p>
<pre><code>number_of_workers = 2
first_names = ['Steve', 'Jane', 'Sara', 'Mary','Jack']
def chunkify(lst,n):
     return [lst[i::n] for i in xrange(n)]
list1 = chunkify(first_names, number_of_workers)
print list1
</code></pre>
<p><strong>3)</strong> When defining the worker function in multithreading, pass on a different sublist to each worker.  Note - the number of workers (and parts I want to split the query result into) is defined at the beginning of the function.
However, as I'm fairly new to Python, I have no idea how to pass on each sublist to a separate worker (or is it even doable?)<br/>
Any help, other suggestions, etc. would be much appreciated!  </p>
<p>Example of multithreading code is below.  How would I use </p>
<pre><code>import threading
import random

def worker():

    assign sublistN to worker N 
    print sublistN

threads = []
for i in range(number_of_workers):
    print i
    print ""
    t = threading.Thread(target=worker)
    threads.append(t)
    t.start()
</code></pre>
<p>Thank you in advance!</p>
</div>
<div class="post-text" itemprop="text">
<p>Two things:</p>
<p>First, take a look at the <a href="https://docs.python.org/3/library/queue.html" rel="nofollow noreferrer">Queue</a> object. You don't even need to split the lists apart yourself this way. It's used for splitting a collection of objects between multiple threads (there's also a multi-process varient, which is where I'm getting to). The docs contain very good examples that fit your requirements.</p>
<p>Second, unless your workers involve waiting on things such as IO, network requests etc. threading in python is no quicker (probably slower actually) than processing sequentially. Threading does not make use of multi-processing, only one thread is ever running at one time. If this is your case, you'll probably want <a href="https://docs.python.org/3/library/multiprocessing.html" rel="nofollow noreferrer">Multiprocessing</a> which actually spins up a whole new python process for working. You've got similar tools such as <a href="https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues" rel="nofollow noreferrer">queues</a> in here.</p>
</div>
<div class="post-text" itemprop="text">
<p>As SCB mentioned, this was solved by utilizing que.
Here is a quick example that takes a list of names -&gt; passes a name to each worker (2 workers) -&gt; each workers simply prints the name they were given.</p>
<pre><code>from Queue import Queue
from threading import Thread
from time import sleep
first_names = ['Steve', 'Jane', 'Sara', 'Mary','Jack','tara','bobby']


q = Queue(first_names)
num_threads = 2

def do_stuff(q):
    while True:
        print q.get()
        sleep(1)
        q.task_done()



for i in range(num_threads):
    worker = Thread(target=do_stuff, args=(q,))
    worker.start()

for x in first_names:
    q.put(x)

q.join()
</code></pre>
<p>Code adapted from <a href="http://l4wisdom.com/python/python_threadwithqueue.php" rel="nofollow noreferrer">here</a>.</p>
</div>
<span class="comment-copy">Thanks for the info.  Will look into Queue.   As for threading - I'm building a scraping script, and the bottle neck is network/requests via proxies, hence threading does seem to speed up the processing up until the single cpu core tops out.  I did read great things about multiprocessing, but I was not able to get it to work, my guess is because im running code via jupyter notebook.  Once I get better, I will definitely get multiprocessing another shot , and hopefully utilize all them cores :)</span>
