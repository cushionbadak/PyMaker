<div class="post-text" itemprop="text">
<p>Say there are many (about 300,000) JSON files that take much time (about 30 minutes) to load into a list of Python objects. Profiling revealed that it is in fact not the file access but the decoding, which takes most of the time. Is there a format that I can convert these files to, which can be loaded much faster into a python list of objects?</p>
<p><strong>My attempt:</strong> I converted the files to ProtoBuf (aka Google's <strong>Proto</strong>col <strong>Buf</strong>fers) but even though I got really small files (reduced to ~20% of their original size), the time to load them did not improve that dramatically (still more than 20 minutes to load them all).</p>
</div>
<div class="post-text" itemprop="text">
<p>You might be looking into the wrong direction with the conversion as it will probably not cut your loading times as much as you would like. If the decoding is taking a lot of time, it will probably take quite some time from other formats as well, assuming that the JSON decoder is not really badly written. I am assuming the standard library functions have decent implementations, and JSON is not a lousy format for data storage speed-wise.</p>
<p>You could try running your program with <a href="https://pypy.org/" rel="nofollow noreferrer">PyPy</a> instead of the default CPython implementation that I will assume you are using. PyPy could decrease the execution time tremendously. It has a faster JSON module and uses a JIT which might speed up your program <strong>a lot</strong>.</p>
<p>If you are using Python 3 you could also try using <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor" rel="nofollow noreferrer">ProcessPoolExecutor</a> to run the file loading and data deserialization / decoding concurrently. You will have to experiment with the degree of concurrency, but a good starting point is the number of your CPU cores, which you can halve or double. If your program waits for I/O a lot, you should run a higher degree of concurrency, if the degree of I/O is smaller you can try and reduce the concurrency. If you write each executor so that they load the data into Python objects and simply return them, you should be able to cut your loading times significantly. Note that you must use a process-driven approach, using threads will not work with the <a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow noreferrer">GIL</a>.</p>
<p>You could also use a <a href="http://artem.krylysov.com/blog/2015/09/29/benchmark-python-json-libraries/" rel="nofollow noreferrer">faster JSON library</a> which could speed up your execution times two or three-fold in an optimal case. In a real-world use case the speed up will probably be smaller. <em>Do note that these might not work with PyPy since it uses an alternative CFFI implementation and will not work with CPython programs, and PyPy has a good JSON module anyway.</em></p>
</div>
<div class="post-text" itemprop="text">
<p>Try <a href="https://github.com/esnme/ultrajson" rel="nofollow noreferrer"><code>ujson</code></a>, it's quite a bit faster.</p>
<p>"Decoding takes most of the time" can be seen as "building the Python objects takes all the time". Do you really need all these things as Python objects in RAM all the time? It must be quite a lot. </p>
<p>I'd consider using a proper database for e.g. querying data of such size.</p>
<p>If you need mass processing of a different kind, e.g. stats or matrix processing, I'd take a look at <code>pandas</code>.</p>
</div>
<span class="comment-copy">Try <a href="https://msgpack.org/" rel="nofollow noreferrer"><code>MessagePack</code></a> (<a href="https://github.com/msgpack/msgpack-python" rel="nofollow noreferrer"><code>msgpack-python</code></a>). You won't get much faster than that with a general data structure. Of course, you can always tailor-made your format to fit your data to squeeze out that last ounce of performance but at that point I'd first re-think my strategy.</span>
<span class="comment-copy">The suggestion to use a <code>ProcessPoolExecutor</code> has proven to be very valuable to me! The execution time with ProtoBuf + <code>ProcessPoolExecutor</code> is now only ~5 minutes. As for the other suggestions (especially PyPy) I will have to try them at a later date. For now this is good enough. Oh, and thank you :)</span>
