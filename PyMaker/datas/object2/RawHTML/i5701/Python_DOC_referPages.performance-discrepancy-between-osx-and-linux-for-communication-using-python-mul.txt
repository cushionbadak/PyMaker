<div class="post-text" itemprop="text">
<p>I have been trying to learn more about Python's <code>multiprocessing</code> module and to evaluate different techniques for communication between processes. I wrote a benchmark that compares the performance of <code>Pipe</code>, <code>Queue</code>, and <code>Array</code> (all from <code>multiprocessing</code>) for transferring <code>numpy</code> arrays between processes. The full benchmark can be found <a href="https://gist.github.com/drdavella/87a903fdd769711ad7382f072aafde95" rel="noreferrer">here</a>. Here's a snippet of the test for <code>Queue</code>:</p>
<pre><code>def process_with_queue(input_queue, output_queue):
    source = input_queue.get()
    dest = source**2
    output_queue.put(dest)


def test_with_queue(size):

    source = np.random.random(size)

    input_queue = Queue()
    output_queue = Queue()

    p = Process(target=process_with_queue, args=(input_queue, output_queue))
    start = timer()
    p.start()
    input_queue.put(source)
    result = output_queue.get()
    end = timer()

    np.testing.assert_allclose(source**2, result)

    return end - start
</code></pre>
<p>I ran this test on my Linux laptop and got the following results for an array size of 1000000:</p>
<pre><code>Using mp.Array: time for 20 iters: total=2.4869s, avg=0.12435s
Using mp.Queue: time for 20 iters: total=0.6583s, avg=0.032915s
Using mp.Pipe:  time for 20 iters: total=0.63691s, avg=0.031845s
</code></pre>
<p>I was a little surprised to see <code>Array</code> perform so poorly since it uses shared memory and presumably doesn't require pickling, but I assume there must be some copying in <code>numpy</code> that I can't control.</p>
<p>However, I ran the same test (again for array size 1000000) on a Macbook, and got the following results:</p>
<pre><code>Using mp.Array: time for 20 iters: total=1.6917s, avg=0.084587s
Using mp.Queue: time for 20 iters: total=2.3478s, avg=0.11739s
Using mp.Pipe:  time for 20 iters: total=8.7709s, avg=0.43855s
</code></pre>
<p>The real timing differences aren't that surprising since of course different systems would exhibit different performance. What <strong>is</strong> so surprising are the differences in relative timing.</p>
<p>What could account for this? This is a pretty surprising result to me. I wouldn't be surprised to see such stark differences between Linux and Windows, or OSX and Windows, but I sort of assumed that these things would behave very similarly between OSX and Linux.</p>
<p><a href="https://stackoverflow.com/questions/1289813/python-multiprocessing-vs-threading-for-cpu-bound-work-on-windows-and-linux">This question</a> addresses performance differences between Windows and OSX, which seems more expected.</p>
</div>
<div class="post-text" itemprop="text">
<h1>TL;DR: OSX is faster with Array because calls to the C library slow Array down on Linux</h1>
<p>Using <code>Array</code> from <code>multiprocessing</code> uses the <a href="https://docs.python.org/3.7/library/ctypes.html#module-ctypes" rel="nofollow noreferrer">C types Python library</a> to make a C call to set memory for the Array. This takes relatively more time on Linux than on OSX. You can also observe this on OSX by using pypy. Setting memory takes much longer using pypy (and GCC and LLVM) than using python3 on OSX (using Clang).</p>
<h1>TL;DR: the difference between Windows and OSX lies in the way multiprocessing starts new processes</h1>
<p>The major difference is in the implementation of <code>multiprocessing</code>, which works different under OSX than in Windows. The most important difference is the way <code>multiprocessing</code> starts a new process. There are three ways this can be done: using <code>spawn</code>, <code>fork</code> or <code>forkserver</code>. The default (<em>and only supported</em>) way under Windows is <code>spawn</code>. The default way under *nix (including OSX) is <code>fork</code>. This is documented in the <a href="https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods" rel="nofollow noreferrer">Contexts and start methods</a> section of the <code>multiprocessing</code> documentation.</p>
<p>One other reason for the deviation in results is the low number of iterations you take.</p>
<p>If you increase the number of iterations and calculate the number of handled function calls per time unit, you get relatively consistent results between the three methods.</p>
<h1>Further analysis: look at the function calls with cProfile</h1>
<p>I removed your <code>timeit</code> timer functions and wrapped your code in the <code>cProfile</code> profiler.</p>
<p>I added this wrapper function:</p>
<pre><code>def run_test(iters, size, func):
    for _ in range(iters):
        func(size)
</code></pre>
<p>And I replaced the loop in <code>main()</code> with:</p>
<pre><code>for func in [test_with_array, test_with_pipe, test_with_queue]:
    print(f"*** Running {func.__name__} ***")
    pr = cProfile.Profile()
    pr.enable()
    run_test(args.iters, args.size, func)
    pr.disable()
    ps = pstats.Stats(pr, stream=sys.stdout)
    ps.strip_dirs().sort_stats('cumtime').print_stats()
</code></pre>
<h1>Analysis of the OSX - Linux difference with Array</h1>
<p>What I see is that Queue is faster than Pipe, which is faster than Array. Regardsless of the platform (OSX/Linux/Windows), Queue is between 2 and 3 times faster than Pipe. On OSX and Windows, Pipe is around 1.2 and 1.5 times faster than Array. But on Linux, Pipe is around 3.6 times faster than Array. In other words, On Linux, Array is relatively much slower than on Windows and OSX. This is strange.</p>
<p>Using the cProfile data, I compared the performance ratio between OSX and Linux. There are two function calls that take a lot of time: <code>Array</code> and <code>RawArray</code> in <code>sharedctypes.py</code>. These functions are only called in the Array scenario (not in Pipe or Queue). On Linux, these calls take almost 70% of the time, while on OSX only 42% of the time. So this a major factor.</p>
<p>If we zoom in <a href="https://github.com/python/cpython/blob/master/Lib/multiprocessing/sharedctypes.py" rel="nofollow noreferrer">to the code</a>, we see that <code>Array</code> (line 84) calls <code>RawArray</code>, and <code>RawArray</code> (line 54) does nothing special, except a call to <code>ctypes.memset</code> (<a href="https://docs.python.org/3.7/library/ctypes.html#utility-functions" rel="nofollow noreferrer">documentation</a>). So there we have a suspect. Let's test it.</p>
<p>The following code uses timeit to test the performance of setting 1 MB of memory buffer to 'A'.</p>
<pre><code>import timeit
cmds = """\
import ctypes
s=ctypes.create_string_buffer(1024*1024)
ctypes.memset(ctypes.addressof(s), 65, ctypes.sizeof(s))"""
timeit.timeit(cmds, number=100000)
</code></pre>
<p>Running this on my MacBookPro and on my Linux server confirms the behaviour that this runs much slower on Linux than on OSX. Knowing that <a href="https://pypy.org/" rel="nofollow noreferrer">pypy</a> is on OSX compiled using <a href="https://gcc.gnu.org/" rel="nofollow noreferrer">GCC</a> and Apples <a href="https://llvm.org/" rel="nofollow noreferrer">LLVM</a>, this is more akin to the Linux world than Python, which is on OSX compiled directly against <a href="https://clang.llvm.org/" rel="nofollow noreferrer">Clang</a>. Normally, Python programs runs faster on pypy than on CPython, but the code above runs 6.4 times slower on pypy (on the same hardware!).</p>
<p>My knowlegde of C toolchains and C libraries is limited, so I can't dig deeper. So my conclusion is: <strong><em>OSX and Windows are faster with Array because memory calls to the C library slow Array down on Linux</em></strong>.</p>
<h1>Analysis of the OSX - Windows performance difference</h1>
<p>Next I ran this on my dual-boot MacBook Pro under OSX and under Windows. The advantage is that the underlying hardware is the same; only the OS is different. I increased the number of iterations to 1000 and the size to 10.000.</p>
<p>The results are as follows:</p>
<ul>
<li>OSX:

<ul>
<li>Array: 225668 calls in 10.895 seconds </li>
<li>Pipe: 209552 calls in 6.894 seconds</li>
<li>Queue: 728173 calls in 7.892 seconds</li>
</ul></li>
<li>Windows:

<ul>
<li>Array: 354076 calls in 296.050 seconds </li>
<li>Pipe: 374229 calls in 234.996 seconds</li>
<li>Queue: 903705 calls in 250.966 seconds</li>
</ul></li>
</ul>
<p>We can see that:</p>
<ol>
<li>The Windows implementation (using <code>spawn</code>) takes more calls than OSX (using <code>fork</code>);</li>
<li>The Windows implementation takes much more time per call than OSX.</li>
</ol>
<p>What's not immediately evident, but relevant to note is that if you look at the <em>average time per call</em>, the relative pattern between the three multiprocessing methodes (Array, Queue and Pipe) is the same (see graphs below). In other words: <strong><em>the differences in performance between Array, Queue and Pipe in OSX and Windows can be completely explained by two factors: 1. the difference in Python performance between the two platforms; 2. the different ways both platforms handle multiprocessing.</em></strong></p>
<p>In other words: the difference in the number of calls is explained by the <a href="https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods" rel="nofollow noreferrer">Contexts and start methods</a> section of the <code>multiprocessing</code> documentation. The difference in execution time is explained in the performance difference of Python between OSX and Windows. If you factor out those two components, the relative performance of Array, Queue and Pipe are (more or less) comparable on OSX and Windows, as is shown in the graphs below.</p>
<p><a href="https://i.stack.imgur.com/23xip.png" rel="nofollow noreferrer"><img alt="Performance differences of Array, Queue and Pipe between OSX and Windows" src="https://i.stack.imgur.com/23xip.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>Well, When we talk about multi-process with python these things happens:</p>
<ul>
<li>The OS does all the multi-tasking work</li>
<li>The only option for multi-core concurrency</li>
<li>Duplicated use of system resources</li>
</ul>
<p>There are huge differences between osx and linux. and osx is based on Unix and treats multi tasking process in other way than linux.</p>
<p>Unix installation requires a strict and well-defined hardware machinery and works only on specific CPU machines, and maybe osx is not designed to speed up python processes. This reason may be the cause.</p>
<p>For more details you can read the <a href="https://docs.python.org/2/library/multiprocessing.html" rel="nofollow noreferrer">MultiProcessing</a> documentation.</p>
<p>I hope it helps.</p>
</div>
<span class="comment-copy">The <code>Value</code> and <code>Array</code> types rely on a <code>Lock</code> to ensure data safety. Acquiring a lock is a fairly expensive action as it requires to switch to kernel mode. On the other hand, serializing simple data structures is what modern CPUs do most of the time so its cost is fairly low. Removing the <code>Lock</code> from the <code>Array</code> should show better performance but you cannot exclude race conditions over the data.</span>
<span class="comment-copy">@noxdafox if you look at the full benchmark code you'll see that I am actually not using a lock for the <code>Array</code> portion of the benchmark. And even then this would only account for the poor relative performance of <code>Array</code> on Linux, but it does not necessarily account for the discrepancy between Linux and OSX.</span>
<span class="comment-copy">Does your macbook have a solid state drive and your linux laptop a rotating disk?</span>
<span class="comment-copy">It could explain the Array slowness in Linux.  Python shared memory implementation appears to create files on file system (see <a href="https://stackoverflow.com/questions/44747145/writing-to-shared-memory-in-python-is-very-slow" title="writing to shared memory in python is very slow">stackoverflow.com/questions/44747145/…</a>). I would assume SSD versus a rotating disk would explain the difference there. It does not explain why pipe is so slow on mac, though.</span>
<span class="comment-copy">You should consider measuring CPU time instead of wall clock time.</span>
<span class="comment-copy">comprehensive answer, but the question wasn't about Windows...  The OP asked about difference between Mac and Linux.</span>
<span class="comment-copy">@CoreyGoldberg : ow... darn. That’s stupid... I ran it on Linux as well. Will add that in a few hours...</span>
<span class="comment-copy">@CoreyGoldberg added analysis of the OSX vs. Linux using Array.</span>
<span class="comment-copy">@agtoever thanks for the very detailed analysis. So to distill your results even further, you're saying that it basically boils down to the difference in performance of <code>ctypes.memset</code> on these platforms? I have no idea why that should be the case. I wonder what the relative performance of <code>memset</code> is in pure C code on these platforms?</span>
<span class="comment-copy">I would love to learn more about which differences between OSX and Linux are having an effect here. Could you expand your answer a bit on this topic?</span>
<span class="comment-copy">I believe that OSX andy other OSes is not designed for python.</span>
