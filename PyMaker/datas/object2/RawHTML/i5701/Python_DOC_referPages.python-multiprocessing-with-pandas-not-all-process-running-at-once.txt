<div class="post-text" itemprop="text">
<p><a href="https://i.stack.imgur.com/IojUD.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/IojUD.png"/></a></p>
<p>I am reading a csv in chunk and passing the chunk to a pool of 4 processes.</p>
<pre><code>pool = Pool(processes=4)
            chunk_index = 1
            for df in pd.read_csv(downloaded_file, chunksize=chunksize, compression='gzip', skipinitialspace=True, encoding='utf-8'):
                output_file_name = output_path + merchant['output_file_format'].format(
                    file_index, chunk_index)
                pool.map(wrapper_process, [
                         (df, transformer, output_file_name)])
                chunk_index += 1
</code></pre>
<p>With this code my understanding is it should show me 4 process continuously running. But in the screenshot of htop below, It is always 2 running. One is htop command it self. It means that only 1 python process in running at the time.</p>
<p><a href="https://i.stack.imgur.com/lpC34.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/lpC34.png"/></a>
From the memory usage, It is 12 gb which i think will only be possible when the 4 chunks are loaded in memory <strong>provided 1 chunk is 2gb almost</strong></p>
<p>How can i use for processors at once.</p>
</div>
<div class="post-text" itemprop="text">
<p>The problem is that you misuderstood how map works.
From <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map" rel="nofollow noreferrer">the doc</a>:</p>
<blockquote>
<p><code>map(func, iterable[, chunksize])</code>
  This method chops the iterable into a number of chunks which it submits
  to the process pool as separate tasks. The (approximate) size of these
  chunks can be specified by setting chunksize to a positive integer.</p>
</blockquote>
<p>As iterable you provide a list with only one element: the tuple <code>(df, ...)</code>.
But you'd need to provide a iterable with many elements. To make this work, you'd need to
prepare the list first and only then send it to the processes (hint:
you can just write <code>Pool()</code> and let python find out the number of cores
itself)</p>
<pre><code>pool = Pool()
chunk_index = 1
list = []
for df in pd.read_csv(downloaded_file, 
        chunksize=chunksize, 
        compression='gzip', 
        skipinitialspace=True, 
        encoding='utf-8'):
    output_file_name = output_path + 
        merchant['output_file_format'].format(file_index, chunk_index)
    list.append((df, transformer, output_file_name)])
    chunk_index += 1
pool.map(wrapper_process, list)
</code></pre>
<p>But now you have the problem that you need to hold the full csv data in
memory which <em>might</em> be ok, but is usually not. To come around this problem
you could switch to using a queue: You would</p>
<ul>
<li>build up an empty queue</li>
<li>start the processes and tell them to get items from the queue (which is still empty at start)</li>
<li>feed the queue with your main process (and maybe check that the queue is not getting too long so memory consumption doesn't go into the roof)</li>
<li>put a <code>STOP</code> element to the queue so the processes quit themselves</li>
</ul>
<p>There's a good example in <a href="https://docs.python.org/3/library/multiprocessing.html#examples" rel="nofollow noreferrer">the official doc (look at the last example on the page)</a> which explains you would approach that.</p>
<p>One last word: Are you sure your operation is CPU bound? Do you do a lot of
processing in <code>wrapper_process</code> (and possibly also <code>transformer</code>)? Because if you just split the CSV in separate
files without much processing your program is IO bound and not CPU bound
and then the multiprocessing wouldn't make any sense.</p>
</div>
<span class="comment-copy">I just fixed a stupid mistake in my example below. Were you able to test it?</span>
<span class="comment-copy">This will work for small data. My one chunk is 2gb and it is not realistic to hold 24 gb of data in my memory. Any ways this tells clearly how map works so i will accept is a correct solution. I managed to do with <code>apply_aysnc</code> and forced to get resposne once my pool limit reaches. Thanks</span>
