<div class="post-text" itemprop="text">
<p>My task is to find the frequency of each word in a list. There are two ways to it. </p>
<p><strong>Method1</strong></p>
<pre><code>def f(words):
    wdict = {}
    for word in words:
        if word not in wdict:
            wdict[word] = 0
        wdict[word] += 1
    return wdict
</code></pre>
<p><strong>Method2</strong></p>
<pre><code>def g(words):
    wdict = {}
    for word in words:
        try:
            wdict[word] += 1
        except KeyError:
            wdict[word] = 1
</code></pre>
<p>Why is Method 2 efficient? Isn't in both the cases, the number of hash functions calls is same in contradiction to this <a href="http://blackecho.github.io/blog/programming/2016/03/23/python-underlying-data-structures.html" rel="nofollow noreferrer">http://blackecho.github.io/blog/programming/2016/03/23/python-underlying-data-structures.html</a>?</p>
</div>
<div class="post-text" itemprop="text">
<p>It depends on the input. If on average most words are already in the dict then you will not get many exceptions. If most words are unique then the overhead of the exceptions will make the second method slower.</p>
</div>
<div class="post-text" itemprop="text">
<p>Lets simulate few cases.</p>
<p>Example: "A bird is flying"</p>
<pre><code>words = ["A", "bird", "is", flying"]
</code></pre>
<p>In your first method: 
for every word it will search in the dictionary 3 times so it will access total 3 * <code>len(words)</code> or 3 * 4 = 12</p>
<p>In second method
it will only search 2 times if not found; otherwise 1 time: so 2 * 4 = 8</p>
<p>Theoretically both have same time complexity.</p>
<p><strong>Update:</strong></p>
<p>Thanks to <a href="https://stackoverflow.com/users/550094/thierry-lathuille">Thierry Lathuille</a> for pointing out. Indeed method 1 should be more efficient than method 2. Python dictionary use hashmap so accessing a key complexity would be O(n) but in average case it is O(1). and cpython implementation is quite efficient. On the other hand try/catch exception handling is slow. </p>
<p>you can use <a href="https://docs.python.org/2/library/collections.html#collections.defaultdict" rel="nofollow noreferrer">defaultdict</a> in your method 1 for more clean code.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are two main differences: </p>
<ul>
<li>Method 1 will perform the <code>in</code> operation for every word, while method 2 will update directly whenever possible.  </li>
<li>Whenever Method1 inserts a new word, the count is later updated. Method2 starts to count at 1.</li>
</ul>
<p>It ultimately depends on the input but if there are a sufficient amount of repetitions there will be less operations.</p>
<p>Example:<br/>
Let's just go through the code here to get the general idea (not actual operations).</p>
<p><code>['a', 'a']</code></p>
<p>Method1<br/>
1 - 'a' not in wdict - True<br/>
2 - assign 'a'<br/>
3 - update 'a'<br/>
4 - 'a' not in dict - False<br/>
5 - update 'a'  </p>
<p>Method2<br/>
1 - access 'a'<br/>
2 - error<br/>
3 - assign 'a'  directly to 1<br/>
4 - update 'a' (second 'a')  </p>
<p>Although these steps are not precisely the amount of operations that go on when executing, they are indicative that Method2 is leaner and goes through less 'steps'.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are several approaches to this answer. You can use the loop and still get the expected answer. I focus on two methods:</p>
<p><strong>List comprehension</strong></p>
<pre><code>wordstring = 'it was the best of times it was the worst of times '
wordstring += 'it was the age of wisdom it was the age of foolishness'
wordlist = wordstring.split()
# Count each word
wordfreq = [wordlist.count(w) for w in wordlist] # a list comprehension
# Convert to set to remove repetitions
frequencies=set(zip(wordlist, wordfreq))
print(frequencies)
</code></pre>
<p>Output:</p>
<pre><code>{('of', 4), ('best', 1), ('the', 4), ('worst', 1), ('age', 2), ('wisdom', 1), ('it', 4), ('was', 4), ('times', 2), ('foolishness', 1)}
</code></pre>
<p><strong>Method two: standard library</strong></p>
<pre><code>import collections
wordstring = 'it was the best of times it was the worst of times '
wordstring += 'it was the age of wisdom it was the age of foolishness'
wordlist = wordstring.split()
# Count frequency
freq=collections.Counter(wordlist)
print(freq)
</code></pre>
<p>Output:</p>
<pre><code>Counter({'it': 4, 'was': 4, 'the': 4, 'of': 4, 'times': 2, 'age': 2, 'best': 1, 'worst': 1, 'wisdom': 1, 'foolishness': 1})
</code></pre>
<p>The method of choice depends on the size of text you are working with. The methods above are good for small text size.</p>
</div>
<span class="comment-copy">The most efficient probably is <code>Counter</code> from the standard library: <code>from collections import Counter; c = Counter(words)</code>.</span>
<span class="comment-copy">there are many methods to do it. Some of which are more efficient than <code>Method2</code>. See <code>collections.defaultDict</code> or even better <code>collections.Counter</code>.</span>
<span class="comment-copy">Ev.kounis. My question is how come Method2 is efficient than Method1 in case of number of hash function calls?</span>
<span class="comment-copy">Just test it by yourself: <a href="https://gist.github.com/BrunoDesthuilliers/256138f62c8eafbc9a0561fbb14bc35a" rel="nofollow noreferrer">gist.github.com/BrunoDesthuilliers/…</a></span>
<span class="comment-copy">triplee, Can you see this once:  <a href="http://blackecho.github.io/blog/programming/2016/03/23/python-underlying-data-structures.html" rel="nofollow noreferrer">blackecho.github.io/blog/programming/2016/03/23/…</a></span>
<span class="comment-copy">I had a quick look but I don't see what sort of reaction you hope for. Many random blogs are happy to provide incomplete or dubious programming advice.</span>
<span class="comment-copy">The blog is saying number of function calls in Method 1 is more compared to the Method 2. I unable to digest it. I feel that number of function calls in both the cases is same.</span>
<span class="comment-copy">It says it performs a lot of unnecessary <i>hash</i> function computations, but that seems like a false claim. Why is this a comment to my answer, and not e.g. a new question, or a comment to the blog's author?</span>
<span class="comment-copy">@SheikhArbaz you can check the validity of this blog post's assertion by setting up a quick and simple benchmark  like this one: <a href="https://gist.github.com/BrunoDesthuilliers/256138f62c8eafbc9a0561fbb14bc35a" rel="nofollow noreferrer">gist.github.com/BrunoDesthuilliers/…</a> - as you'll find out, on a long enough real-life text (with lot of different words), the containment test is significantly faster (by a factor 2 in python 3.6, and by a factor <i>four</i> with python 2.7). For a dummy text with the same three words repeated over and over.</span>
<span class="comment-copy">No, that's wrong. Accessing a key in a dict or checking if it exists are both approximately O(1), so both methods are approximately O(n). With your example, I get 1.36µs for for method 1, and 2µs for method 2, which is in fact much slower. The reason is that exceptions are much more expensive to treat than tests, so method 1 only will start being faster if there are many duplicate words.</span>
<span class="comment-copy">it's "list comprehension", not "list compression" ;) (fixed).</span>
<span class="comment-copy">Should be list compression in python not  <b>comprehension</b></span>
<span class="comment-copy">Please check your facts: <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" rel="nofollow noreferrer">docs.python.org/3/tutorial/…</a></span>
<span class="comment-copy">Sure, I now see it. Thanks for correction.</span>
<span class="comment-copy">Unrelated but your first solution would perform better by first doing a set of the words and only then counting them, ie: <code>frequencies = [(word, wordlist.count(word)) for word in set(wordlist)]</code></span>
