<div class="post-text" itemprop="text">
<p>I'm using :<br/></p>
<ul>
<li>python 3.6 </li>
<li>django==2.1.1</li>
<li>gunicorn==19.9.0</li>
</ul>
<p>i have done the following:</p>
<ul>
<li>created a django project called <code>api</code> </li>
<li>created an <code>apiapp</code> (an app in my project) </li>
</ul>
<p>and i have this code in api_app's <code>apps.py</code> :</p>
<pre><code>from django.apps import AppConfig
from api import settings

class ApiappConfig(AppConfig):
    name = 'apiapp'
    verbose_name = "random_name"

    def ready(self):
        self.job()


    @classmethod
    def job(cls):
        ### doing whatever here for example :
        print(settings.SHARED_VARIABLE)
</code></pre>
<p>and the following in api_app's <code>__init__.py</code>:</p>
<pre><code>import os
default_app_config = 'apiapp.apps.ApiappConfig'
</code></pre>
<p>i'm creating an API so i am required to use multiple workers when deploying:</p>
<pre><code>gunicorn api.wsgi -w 10
</code></pre>
<p>now, my issue is that the function <code>job</code> which is called when the server is started, is getting called 10 times because i'm using 10 gunicorn workers, i would like to call it only once</p>
<p>another thing that i would like to do is to have the 
<code>settings.SHARED_VARIABLE</code> variable, shared between the different workers. this variable will be updated only by the worker that will launch the <code>app.py</code> on server start.</p>
<p>Thank you ! </p>
</div>
<div class="post-text" itemprop="text">
<p>gunicorn has a setting to do this: <a href="http://docs.gunicorn.org/en/stable/settings.html#preload-app" rel="nofollow noreferrer"><code>--preload</code></a></p>
<p>So, after I add this in <code>settings.py</code>: <code>SHARED_VARIABLE = 'content of SHARED_VARIABLE'</code> (and fixed <code>apiapp/__init__.py</code> to use the real app name), I can run gunicorn with the application loaded only once:</p>
<pre><code>$ gunicorn api.wsgi -w 10 --preload
content of SHARED_VARIABLE
[2018-12-31 10:12:15 +0000] [394] [INFO] Starting gunicorn 19.6.0
[2018-12-31 10:12:15 +0000] [394] [INFO] Listening at: http://127.0.0.1:8000 (394)
[2018-12-31 10:12:15 +0000] [394] [INFO] Using worker: sync
[2018-12-31 10:12:15 +0000] [399] [INFO] Booting worker with pid: 399
[2018-12-31 10:12:15 +0000] [400] [INFO] Booting worker with pid: 400
[2018-12-31 10:12:15 +0000] [401] [INFO] Booting worker with pid: 401
[2018-12-31 10:12:15 +0000] [403] [INFO] Booting worker with pid: 403
[2018-12-31 10:12:15 +0000] [404] [INFO] Booting worker with pid: 404
[2018-12-31 10:12:15 +0000] [405] [INFO] Booting worker with pid: 405
[2018-12-31 10:12:15 +0000] [406] [INFO] Booting worker with pid: 406
[2018-12-31 10:12:15 +0000] [408] [INFO] Booting worker with pid: 408
[2018-12-31 10:12:15 +0000] [410] [INFO] Booting worker with pid: 410
[2018-12-31 10:12:15 +0000] [411] [INFO] Booting worker with pid: 411
</code></pre>
</div>
<span class="comment-copy">This wouldn't make sense. Data is not shared between processes. If you only invoked it once, it would only be available to that single worker.</span>
<span class="comment-copy">@DanielRoseman oki, i'll worry about the data sharing later, what about the apps.py issue ?</span>
<span class="comment-copy">not only apps.py, but settings.py will be called ten times if you have multiple works.How about use multi thread mode:<code>gunicorn api.wsgi -k gthread -w 1 --threads 8</code></span>
<span class="comment-copy">Gunicorn workers are for servicing APIs.  In general, I see an issue with the whole architecture: your job should be an external Django command (see <a href="https://docs.djangoproject.com/en/2.1/howto/custom-management-commands/" rel="nofollow noreferrer">docs.djangoproject.com/en/2.1/howto/custom-management-commands</a>).  How it is started is completely different topic but it is far easier to start two different processes than trying to cope with gunicorn agents. You could also use the <code>on_starting</code> guncorn function and invoke the above mentioned custom admin command.</span>
<span class="comment-copy">Roseman is right. Why don't you put the shared variable in the database? Since you need it to be consistency?</span>
<span class="comment-copy">i will have to save the shared variable in a file or db for it to be changed in the workers everytime it is changed in the thread created by the preload ? (i will create a thread in preload that does scheduled task that will update the content of the shared variable, i'm using threading and schedule modules)</span>
<span class="comment-copy">Either that or use one of the tools provided by <code>multiprocessing</code> to communicate between processes. eg <a href="https://docs.python.org/3/library/multiprocessing.html#proxy-objects" rel="nofollow noreferrer">docs.python.org/3/library/multiprocessing.html#proxy-objects</a></span>
<span class="comment-copy">but then how would i send those objects to the workers ?</span>
<span class="comment-copy">just create the proxies before forking</span>
