<div class="post-text" itemprop="text">
<p>Assuming that I have an expression similar to the one below (in reality this is an SQL statement):</p>
<pre><code>"v1 and (v2 and (v3 or v4))"
</code></pre>
<p>I want to parse it in order to process strings and maintain parentheses' precedence. To do so, I've used the following recursive function</p>
<pre><code>def parse_conditions(expr):
    def _helper(iter):
        items = []
        for item in iter:
            if item == '(':
                result, closeparen = _helper(iter)
                if not closeparen:
                    raise ValueError("Unbalanced parentheses")
                items.append(result)
            elif item == ')':
                return items, True
            else:
                items.append(item)
        return items, False
    return _helper(iter(expr))[0] 
</code></pre>
<p>that gives the following output: </p>
<pre><code>print(parse_conditions("v1 and (v2 and (v3 or v4))"))

['v', '1', ' ', 'a', 'n', 'd', ' ', ['v', '2', ' ', 'a', 'n', 'd', ' ', ['v', '3', ' ', 'o', 'r', ' ', 'v', '4']]]
</code></pre>
<p>The expected output however, would either be </p>
<pre><code>['v1 and', ['v2 and', ['v3 or v4']]]
</code></pre>
<p>or </p>
<pre><code>['v1', and', ['v2', and', ['v3', 'or', 'v4']]]
</code></pre>
<p>Any thoughts how to achieve this?</p>
</div>
<div class="post-text" itemprop="text">
<p>You want to <a href="https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" rel="nofollow noreferrer"><em>tokenize</em></a> your input. The simplest tokenizer needed to parse your balanced expressions could be a simple regular expression here, splitting on <code>(</code> and <code>)</code>, ignoring whitespace:</p>
<pre><code>import re

_tokenizer = re.compile(r'\s*([()])\s*').split
def tokenize(s):
    return filter(None, _tokenizer(s))
</code></pre>
<p>and use <code>tokenize())</code> instead of <code>iter()</code>:</p>
<pre><code>def parse_conditions(expr):
    def _helper(tokens):
        items = []
        for item in tokens:
            if item == '(':
                result, closeparen = _helper(tokens)
                if not closeparen:
                    raise ValueError("Unbalanced parentheses")
                items.append(result)
            elif item == ')':
                return items, True
            else:
                items.append(item)
        return items, False
    return _helper(tokenize(expr))[0] 
</code></pre>
<p>The <code>filter(None, ...)</code> call filters out the empty strings that <code>re.split()</code> produces at points where the input starts or ends with a <code>(</code> or <code>)</code>, or if two <code>(</code> or <code>)</code> characters directly follow one another.</p>
<p>Demo:</p>
<pre><code>&gt;&gt;&gt; s = 'v1 and (v2 and (v3 or v4))'
&gt;&gt;&gt; parse_conditions(s)
['v1 and', ['v2 and', ['v3 or v4']]]
</code></pre>
<p>To split out operators too, you can either add valid operators to the splitting expression, or just add whitespace as a delimiter.</p>
<p>Splitting on whitespace, where we don't include the spaces in the tokens:</p>
<pre><code>_tokenizer = re.compile(r'(?:([()])|\s+)').split
</code></pre>
<p>produces:</p>
<pre><code>&gt;&gt;&gt; parse_conditions(s)
['v1', 'and', ['v2', 'and', ['v3', 'or', 'v4']]]
</code></pre>
<p>while focusing on valid operators would be:</p>
<pre><code>_tokenizer = re.compile(r'\s*([()]|\b(?:or|and)\b)\s*').split
</code></pre>
<p>and for your sample input that produces the same result.</p>
<p>Note that your code has a bug; it won't detect an unbalanced <em>closing</em> parenthesis:</p>
<pre><code>&gt;&gt;&gt; parse_conditions('foo) and bar')
['foo']
</code></pre>
<p>You need to validate that the first <code>_helper()</code> call returns <code>False</code> for the second element in the returned tuple. Instead of <code>return _helper(tokenize(expr))[0]</code>, use:</p>
<pre><code>items, closing = _helper(tokenize(expr))
if closing:  # indicating there was a closing ) without opening (
    raise ValueError("Unbalanced parentheses")
return items
</code></pre>
<p>Finally, I'd not use recursion here, and instead use an explicit stack to replace the call stack on which recursion builds. Your own stack is only limited by memory, where a recursion stack is limited to a fixed size (1000 by default):</p>
<pre><code>def parse_conditions(expr):
    stack = []  # or a `collections.deque()` object, which is a little faster
    top = items = []
    for token in tokenize(expr):
        if token == '(':
            stack.append(items)
            items.append([])
            items = items[-1]
        elif token == ')':
            if not stack:
                raise ValueError("Unbalanced parentheses")
            items = stack.pop()  
        else:
            items.append(token)
    if stack:
        raise ValueError("Unbalanced parentheses")
    return top
</code></pre>
<p>You might be interested in looking at the <a href="https://docs.python.org/3/library/tokenize.html" rel="nofollow noreferrer"><code>tokenize</code> module</a>, which implements a tokenizer for Python code; the <a href="https://github.com/python/cpython/blob/3.7/Lib/tokenize.py" rel="nofollow noreferrer">source code</a> uses a series of regular expressions to split out Python source code into tokens (where a token not only contains the token text, but also a <a href="https://docs.python.org/3/library/token.html" rel="nofollow noreferrer">token type</a>, the start and end positions (a column, line tuple) and the full line it came from).</p>
</div>
<span class="comment-copy">It's a two-step process. First, <i>tokenize</i> the input string to produce <code>["v1", "and", "(", "v2", "and", "(", "v3", "or", "v4", ")", ")"]</code>. Next, <i>parse</i> the list of tokens instead of the list of characters.</span>
<span class="comment-copy">If you're parsing SQL, and unless you really want to write a parser, you should take a look at sqlparse (<a href="https://github.com/andialbrecht/sqlparse" rel="nofollow noreferrer">github.com/andialbrecht/sqlparse</a>), even if you just end up using their lexer/tokenizer (<a href="https://github.com/andialbrecht/sqlparse/blob/master/sqlparse/lexer.py" rel="nofollow noreferrer">github.com/andialbrecht/sqlparse/blob/master/sqlparse/lexer.py</a>)</span>
<span class="comment-copy">@thebjorn I've had a look at <code>sqlparse</code> but was not able to find out how to parse expressions similar to the one I've posted and maintain expressions' precedence. If you could provide an example I would appreciate it.</span>
<span class="comment-copy">Then you're probably better off following Martijn's code..</span>
<span class="comment-copy">Not if you add your operators to the split group; e.g. <code>[()&lt;&gt;]</code>. Tokenisation just means splitting out an opaque string into interesting components, and if your components are not always space delimited then expand the code regex to handle single character and muti-character operators, keywords and other components specifically.</span>
<span class="comment-copy">I note that there was no mention of a more complex grammar in your question however, if your real problem has additional complexities, can I suggest you ask a new question?</span>
<span class="comment-copy">Would you mind if I edit this question and include some more complex examples? I just wanted to include a minimal, simple example - my bad.</span>
<span class="comment-copy">Kind-of, yes. That’s called “shifting the goal posts”, moving from problem to new problem.</span>
<span class="comment-copy">Here's the <a href="https://stackoverflow.com/questions/54980916/parse-sql-expressions-and-maintain-parentheses-precedence">new question</a>. I would appreciate any help.</span>
