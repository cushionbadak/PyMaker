<div class="post-text" itemprop="text">
<p>I'd like to have a process pool in a mostly async I/O application, because sometimes CPU bound tasks need to be done that shouldn't stall the main application. Furthermore I want to limit the number of processes.</p>
<p>According to the documentation the right way is to use <code>run_in_executor</code>. The code below works but it doesn't terminate the processes after the work was done.</p>
<pre class="lang-py prettyprint-override"><code>import asyncio
from concurrent.futures.process import ProcessPoolExecutor

class App:
    def __init__(self):
        self.process_pool = ProcessPoolExecutor(4)
        self.loop = asyncio.get_event_loop()

    async def get_regular(self):
        return await regular()

    async def get_expensive(self):
        return await self.loop.run_in_executor(
            self.process_pool, expensive
        )
</code></pre>
<p>How do you reuse processes in the process pool or terminate them to obey the upper limit?</p>
</div>
<div class="post-text" itemprop="text">
<p>The process pool will have strange behavior if you reuse it. So I suggest to create a new pool every time and wrap it in a <code>with</code> structure as demonstrated in the <a href="https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor-example" rel="nofollow noreferrer">Example</a>.</p>
<p>If you insist to reuse the pool, the responsibility to manage its life time falls on your shoulder. After usage, you can kill all subprocesses in the pool by </p>
<pre><code>self.process_pool.shutdown()
</code></pre>
</div>
<span class="comment-copy">Have tried the <code>map()</code> approach from the <a href="https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor-example" rel="nofollow noreferrer">example</a>?</span>
<span class="comment-copy">Wouldn't this create a new process pool (with n processes) every time <code>get_expensive</code> is called? Furthermore I only have single element tasks.</span>
<span class="comment-copy">The point of a process <i>pool</i> is to keep the processes ready for when you need them later. The idea is that you pay the process startup price only once, and then get instantaneous response from already existing processes.</span>
<span class="comment-copy">How much overhead does this have compared to a solution that only spawns a new process?</span>
<span class="comment-copy">The overhead is the time to startup the sub-processes, in my machine it is less than 100 ms.</span>
<span class="comment-copy">The "strange" behavior I said is about printing message to stdout. If you don't use <code>print</code> in the <code>expensive</code> function, it would be fine to reuse the process pool.</span>
<span class="comment-copy">Considering that it takes a couple of seconds to run <code>expensive</code> 100ms is negligible. I will just spawn a new pool everytime for now. Thanks!</span>
