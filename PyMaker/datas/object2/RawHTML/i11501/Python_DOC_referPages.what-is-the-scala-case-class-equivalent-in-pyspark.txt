<div class="post-text" itemprop="text">
<p>How would you go about employing and/or implementing a case class equivalent in PySpark?</p>
</div>
<div class="post-text" itemprop="text">
<p><a href="https://stackoverflow.com/questions/37147450/what-is-the-scala-case-class-equivalent-in-pyspark/49949762#comment61833624_37147450">As mentioned</a> by <a href="https://stackoverflow.com/users/2482744/alex-hall">Alex Hall</a> a real equivalent of named product type, is a <code>namedtuple</code>.</p>
<p>Unlike <code>Row</code>, suggested in <a href="https://stackoverflow.com/a/48681481/9613318">the other answer</a>, it has a number of useful properties:</p>
<ul>
<li><p>Has well defined shape and can be reliably used for structural pattern matching:</p>
<pre><code>&gt;&gt;&gt; from collections import namedtuple
&gt;&gt;&gt;
&gt;&gt;&gt; FooBar = namedtuple("FooBar", ["foo", "bar"])
&gt;&gt;&gt; foobar = FooBar(42, -42)
&gt;&gt;&gt; foo, bar = foobar
&gt;&gt;&gt; foo
42
&gt;&gt;&gt; bar
-42
</code></pre>
<p>In contrast <code>Rows</code> <a href="https://issues.apache.org/jira/browse/SPARK-13802" rel="noreferrer">are not reliable when used with keyword arguments</a>:</p>
<pre><code>&gt;&gt;&gt; from pyspark.sql import Row
&gt;&gt;&gt;
&gt;&gt;&gt; foobar = Row(foo=42, bar=-42)
&gt;&gt;&gt; foo, bar = foobar
&gt;&gt;&gt; foo
-42
&gt;&gt;&gt; bar
42
</code></pre>
<p>although if defined with positional arguments:</p>
<pre><code>&gt;&gt;&gt; FooBar = Row("foo", "bar")
&gt;&gt;&gt; foobar = FooBar(42, -42)
&gt;&gt;&gt; foo, bar = foobar
&gt;&gt;&gt; foo
42
&gt;&gt;&gt; bar
-42
</code></pre>
<p>the order is preserved.</p></li>
<li><p>Define proper types</p>
<pre><code>&gt;&gt;&gt; from functools import singledispatch
&gt;&gt;&gt; 
&gt;&gt;&gt; FooBar = namedtuple("FooBar", ["foo", "bar"])
&gt;&gt;&gt; type(FooBar)
&lt;class 'type'&gt;
&gt;&gt;&gt; isinstance(FooBar(42, -42), FooBar)
True
</code></pre>
<p>and can be used whenever type handling is required, especially with single:</p>
<pre><code>&gt;&gt;&gt; Circle = namedtuple("Circle", ["x", "y", "r"])
&gt;&gt;&gt; Rectangle = namedtuple("Rectangle", ["x1", "y1", "x2", "y2"])
&gt;&gt;&gt;
&gt;&gt;&gt; @singledispatch
... def area(x):
...     raise NotImplementedError
... 
... 
&gt;&gt;&gt; @area.register(Rectangle)
... def _(x):
...     return abs(x.x1 - x.x2) * abs(x.y1 - x.y2)
... 
... 
&gt;&gt;&gt; @area.register(Circle)
... def _(x):
...     return math.pi * x.r ** 2
... 
... 
&gt;&gt;&gt;
&gt;&gt;&gt; area(Rectangle(0, 0, 4, 4))
16
&gt;&gt;&gt; &gt;&gt;&gt; area(Circle(0, 0, 4))
50.26548245743669
</code></pre>
<p>and <a href="https://github.com/mrocklin/multipledispatch/" rel="noreferrer">multiple</a> dispatch:</p>
<pre><code>&gt;&gt;&gt; from multipledispatch import dispatch
&gt;&gt;&gt; from numbers import Rational
&gt;&gt;&gt;
&gt;&gt;&gt; @dispatch(Rectangle, Rational)
... def scale(x, y):
...     return Rectangle(x.x1, x.y1, x.x2 * y, x.y2 * y)
... 
... 
&gt;&gt;&gt; @dispatch(Circle, Rational)
... def scale(x, y):
...     return Circle(x.x, x.y, x.r * y)
...
...
&gt;&gt;&gt; scale(Rectangle(0, 0, 4, 4), 2)
Rectangle(x1=0, y1=0, x2=8, y2=8)
&gt;&gt;&gt; scale(Circle(0, 0, 11), 2)
Circle(x=0, y=0, r=22)
</code></pre>
<p>and combined with the first property, there can be used in wide ranges of pattern matching scenarios. <code>namedtuples</code> also support standard inheritance and <a href="https://docs.python.org/3/library/typing.html#typing.NamedTuple" rel="noreferrer">type hints</a>.</p>
<p><code>Rows</code> don't:</p>
<pre><code>&gt;&gt;&gt; FooBar = Row("foo", "bar")
&gt;&gt;&gt; type(FooBar)
&lt;class 'pyspark.sql.types.Row'&gt;
&gt;&gt;&gt; isinstance(FooBar(42, -42), FooBar)  # Expected failure
Traceback (most recent call last):
...
TypeError: isinstance() arg 2 must be a type or tuple of types
&gt;&gt;&gt; BarFoo = Row("bar", "foo")
&gt;&gt;&gt; isinstance(FooBar(42, -42), type(BarFoo))
True
&gt;&gt;&gt; isinstance(BarFoo(42, -42), type(FooBar))
True
</code></pre></li>
<li><p>Provide highly optimized representation. Unlike <code>Row</code> objects, tuple don't use <code>__dict__</code> and carry field names with each instance. As a result there are can be order of magnitude faster to initialize:</p>
<pre><code>&gt;&gt;&gt; FooBar = namedtuple("FooBar", ["foo", "bar"])
&gt;&gt;&gt; %timeit FooBar(42, -42)
587 ns ± 5.28 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre>
<p>compared to different <code>Row</code> constructors:</p>
<pre><code>&gt;&gt;&gt; %timeit Row(foo=42, bar=-42)
3.91 µs ± 7.67 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
&gt;&gt;&gt; FooBar = Row("foo", "bar")
&gt;&gt;&gt; %timeit FooBar(42, -42)
2 µs ± 25.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>
<p>and are significantly more memory efficient (very important property when working with large scale data):</p>
<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; FooBar = namedtuple("FooBar", ["foo", "bar"])
&gt;&gt;&gt; sys.getsizeof(FooBar(42, -42))
64
</code></pre>
<p>compared to equivalent <code>Row</code></p>
<pre><code>&gt;&gt;&gt; sys.getsizeof(Row(foo=42, bar=-42))
72
</code></pre>
<p>Finally attribute access is order of magnitude faster with <code>namedtuple</code>:</p>
<pre><code>&gt;&gt;&gt; FooBar = namedtuple("FooBar", ["foo", "bar"])
&gt;&gt;&gt; foobar = FooBar(42, -42)
&gt;&gt;&gt; %timeit foobar.foo
102 ns ± 1.33 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
</code></pre>
<p>compared to equivalent operation on <code>Row</code> object:</p>
<pre><code>&gt;&gt;&gt; foobar = Row(foo=42, bar=-42)
&gt;&gt;&gt; %timeit foobar.foo
2.58 µs ± 26.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre></li>
<li><p>Last but not least <code>namedtuples</code> are properly supported in Spark SQL</p>
<pre><code>&gt;&gt;&gt; Record = namedtuple("Record", ["id", "name", "value"])
&gt;&gt;&gt; spark.createDataFrame([Record(1, "foo", 42)])
DataFrame[id: bigint, name: string, value: bigint]
</code></pre></li>
</ul>
<p><strong>Summary</strong>:</p>
<p>It should be clear that <code>Row</code> is a very poor substitute for an <a href="https://en.wikipedia.org/wiki/Product_type" rel="noreferrer">actual product type</a>, and should be avoided unless enforced by Spark API.</p>
<p>It should be also clear that <code>pyspark.sql.Row</code> is not intended to be a replacement of a case class when you consider that, it is direct equivalent of <code>org.apache.spark.sql.Row</code> - type which is pretty far from an actual product, and behaves like <code>Seq[Any]</code> (depending on a subclass, with names added). Both Python and Scala implementations were introduced as an useful, albeit awkward interface between external code and internal Spark SQL representation. </p>
<p><strong>See also</strong>:</p>
<ul>
<li><p>It would be a shame not to mention awesome <a href="https://pypi.org/project/MacroPy/" rel="noreferrer">MacroPy</a> developed by <a href="https://stackoverflow.com/users/871202/li-haoyi">Li Haoyi</a> and its port (<a href="https://github.com/azazel75/macropy" rel="noreferrer">MacroPy3</a>) by Alberto Berti:</p>
<pre><code>&gt;&gt;&gt; import macropy.console
0=[]=====&gt; MacroPy Enabled &lt;=====[]=0
&gt;&gt;&gt; from macropy.case_classes import macros, case
&gt;&gt;&gt; @case
... class FooBar(foo, bar): pass
... 
&gt;&gt;&gt; foobar = FooBar(42, -42)
&gt;&gt;&gt; foo, bar = foobar
&gt;&gt;&gt; foo
42
&gt;&gt;&gt; bar
-42
</code></pre>
<p>which comes with a rich set of other features including, but not limited to, advanced pattern matching and neat lambda expression syntax.</p></li>
<li><p>Python <a href="https://docs.python.org/3/library/dataclasses.html" rel="noreferrer"><code>dataclasses</code></a> (Python 3.7+).</p></li>
</ul>
</div>
<div class="post-text" itemprop="text">
<p>If you go to <a href="https://spark.apache.org/docs/1.5.2/sql-programming-guide.html" rel="nofollow noreferrer">sql-programming-guide</a> in <strong>Inferring the Schema Using Reflection</strong> section, you will see <code>case class</code> being defined as </p>
<blockquote>
<p>case class defines the schema of the table. The names of the arguments to the case class are read using reflection and become the names of the columns. Case classes can also be nested or contain complex types such as Sequences or Arrays.</p>
</blockquote>
<p>with example as </p>
<pre><code>val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._
case class Person(name: String, age: Int)
val people = sc.textFile("examples/src/main/resources/people.txt").map(_.split(",")).map(p =&gt; Person(p(0), p(1).trim.toInt)).toDF()
</code></pre>
<p>In the same section, if you switch to <em>python</em> i.e. <em>pyspark</em>, you will see <code>Row</code> being used and defined as </p>
<blockquote>
<p>Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by looking at the first row.</p>
</blockquote>
<p>with example as </p>
<pre><code>from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)
lines = sc.textFile("examples/src/main/resources/people.txt")
parts = lines.map(lambda l: l.split(","))
people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))
schemaPeople = sqlContext.createDataFrame(people)
</code></pre>
<p><strong>So the conclusion of the explanation is that <code>Row</code> can be used as <code>case class</code> in <em>pyspark</em></strong></p>
</div>
<span class="comment-copy">Python's <code>collections.namedtuple</code> is pretty similar.</span>
<span class="comment-copy">@AlexHall So you're ultimately saying you can use some generic Python class... there is no Spark optimized case class equivalent that ships with PySpark correct?</span>
<span class="comment-copy">I don't know much about PySpark, it was just a general Python recommendation.</span>
<span class="comment-copy">@conner.xyz No, there isn't because without static typing case classes (or <code>Product</code> types in general) are not that useful. Typically plain Python tuple is just enough. Named tuples are great but <a href="http://stackoverflow.com/q/33574009/1560062">require distributing over the workers</a>.</span>
<span class="comment-copy">This is a fantastic answer!</span>
<span class="comment-copy">Very good illustration.</span>
