<div class="post-text" itemprop="text">
<p>I'm trying to do something in Python 2.7, and I can't quite figure it out.
What I want is to carry out two sets of actions simultaneously, and in addition there is some need for the two threads to communicate with each other.</p>
<p>More specifically: I want to send a series of HTTP requests, and at the same time (in parallel) send a similar series of HTTP requests. This way I don't have to wait for a (potentially delayed) response, because the other series can just continue on. </p>
<p>The thing is, the number of requests per second cannot exceed a certain value; let's say one request per second. So I need to make sure that the combined request-frequency of the two parallel threads does not exceed this value.</p>
<p>Any help would be appreciated. Apologies if the  solution is obvious, I'm still pretty new to python.</p>
</div>
<div class="post-text" itemprop="text">
<p>Seems like you need a "semaphore". From the python2.7 docs:</p>
<blockquote>
<p>A semaphore manages an internal counter which is decremented by each acquire() call and incremented by each release() call. The counter can never go below zero; when acquire() finds that it is zero, it blocks, waiting until some other thread calls release().</p>
</blockquote>
<p>So this semaphore of yours is basically a counter of calls, that reset to the allowed rate every second, shared by all the HTTP threads. If it reaches 0 no thread can make request no more, until another thread release the connection or a second passes and the Counter is filled again.</p>
<p>You can set-up your script with x HTTP request workers and one HTTP Call Rate Resetter worker:</p>
<ul>
<li>the resetter destroys and regen the semaphore</li>
<li>each worker acquire() every HTTP is made.</li>
</ul>
<p>If you are using Python2.7 and threading you can find all the docs here:
<a href="https://docs.python.org/2/library/threading.html" rel="nofollow noreferrer">https://docs.python.org/2/library/threading.html</a>.</p>
<p>And a nice tutorial here:
<a href="https://pymotw.com/2/threading/" rel="nofollow noreferrer">https://pymotw.com/2/threading/</a></p>
</div>
<div class="post-text" itemprop="text">
<p>Raymond Hettinger gave a really good keynote talk about the proper way to think about concurrency and multithreading here: <a href="https://www.youtube.com/watch?v=Bv25Dwe84g0&amp;t=2" rel="nofollow noreferrer">https://www.youtube.com/watch?v=Bv25Dwe84g0&amp;t=2</a></p>
<p>And his notes can be found here: <a href="https://dl.dropboxusercontent.com/u/3967849/pyru/_build/html/index.html" rel="nofollow noreferrer">https://dl.dropboxusercontent.com/u/3967849/pyru/_build/html/index.html</a></p>
<p>What I recommend, which is from the talk, is to use an atomic message queue to "talk" between the threads. However, this talk and Raymond's work is done in 3.5 or 3.6. This library <a href="https://docs.python.org/3/library/queue.html" rel="nofollow noreferrer">https://docs.python.org/3/library/queue.html</a> will help you significantly. </p>
</div>
<div class="post-text" itemprop="text">
<p>A common way to enforce your rate-limiting requirement is to use a <a href="https://en.wikipedia.org/wiki/Token_bucket" rel="nofollow noreferrer">Token Bucket</a> approach. </p>
<p>Specifically in Python, you'd have a queue shared between the threads, and a 3rd thread (perhaps the original initiating thread) which puts one plug object into the queue per second. (That is, it's a simple loop: wait 1 second, put an object, repeat.)</p>
<p>The two worker threads each try to take an object from the queue, and for each object they take, they issue one request. Voila! The workers can't issue more requests, in total, than tokens made available (which equal to the number of seconds that have passed. Even if one thread is stuck on a long-running request, the other can just be the one to repeatedly obtain a token. It's generalizable to N threads: they're all just competing to get the next allow-one-request token from the shared queue. </p>
<p>If many threads are stuck on long-running requests, multiple tokens collect in the queue, allowing a burst of catch-up requests – but still only reaching the overall target average-number-of-requests over a longer period. (By adjusting the maximum size of the queue, or whether it is preloaded with a small surplus of tokens, the exact enforcement of the limit can be adjusted – for example, so that it converges to the correct limit within 10 seconds, or 30, or 3600, whatever.)</p>
<p>The shared queue can also be the mechanism that is used to cleanly tell the worker threads to quit. That is, instead of pushing-into-the-queue whatever signalling-object means, "do one request", an external control thread can push-into-the-queue an object meaning, "finish and exit". Pushing in N such objects will cause the N worker threads to each get the command. </p>
</div>
<span class="comment-copy">Can you share your code?</span>
<span class="comment-copy">Thanks a lot! I will check out the vid.</span>
