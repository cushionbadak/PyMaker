<div class="post-text" itemprop="text">
<p>I'm pretty new to asyncio and I'm not able to figure this out by myself at this time and would greatly appreciate any help.</p>
<p>The use case is as follows:</p>
<ul>
<li>There is a web service that I need to fetch data from that throttles and blacklists when too many requests are sent to it.</li>
<li>I need to make a large number of requests to this web service for data</li>
<li>The web service sends data in a paged fashion i.e. when there is too much data for a given request, subsequent requests need to be made to get more pages.</li>
<li>Whether more pages need to be fetched can be figured out by examining the response from a particular request.</li>
<li>Once data is received on the client-side it needs to written to disk</li>
</ul>
<p>So, in my mind, the setup could be as follows:
 - An initial list of requests that need to be made is prepared
 - A Semaphore controls how many requests are made per unit time to control throttling.
 - All the initial requests are added to a loop.
 - When a response is received a separate coroutine (or maybe a thread?) is dispatched to persist the data. I don't want persistence to be blocking the fetching of more data.
 - When a response is received it is examined to see if more pages need to be requested to get the full data. If more pages are needed then another task is added to the loop to fetch the next page.</p>
<p>I've written some code for a minimal example that should provide the framework for what I'm trying to achieve:</p>
<pre><code>import asyncio
import time 
import datetime
from random import random

sema = asyncio.Semaphore(2)

async def my_worker():
    async with sema:
        print("{}".format(datetime.datetime.now()))
        print("I'm going to fetch some data")
        result = await data_fetcher()

        print("I'm going to save data to disk")
        await write_result_to_disk(result)

        if random() &gt; 0.5:
            print("I need to create and run a new worker here to fetch more data")

async def data_fetcher():
    await asyncio.sleep(3)
    return "Bla bla bla"

async def write_result_to_disk(result):
    await asyncio.sleep(3)
    print(result)

blah = [my_worker(), my_worker(), my_worker(), my_worker(), my_worker()]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.gather(*blah))
print("All Workers Completed")
loop.close()
</code></pre>
<p>This seems to set up the semaphore properly and run the workers but it leaves several questions unanswered:</p>
<ul>
<li>First and foremost, how do I dynamically add more workers (to fetch subsequent pages) to the loop?</li>
<li>How can I handle the persistence so that it doesn't block data fetching?</li>
<li>Assuming multiple pages need to end up in the same file, how can I safely collect all of the data from these requests, merge it, and then persist without blocking other data fetching requests?</li>
</ul>
<p>Thank you in advance for any help!</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>First and foremost, how do I dynamically add more workers (to fetch subsequent pages) to the loop?</p>
</blockquote>
<p>You can enqueue new coroutines to the event loop with <a href="https://docs.python.org/3/library/asyncio-task.html#asyncio.ensure_future" rel="nofollow noreferrer"><code>asyncio.ensure_future</code></a>.</p>
<blockquote>
<p>How can I handle the persistence so that it doesn't block data fetching?</p>
</blockquote>
<p>If you're talking about writing to a database, there are libraries for that. If you're talking about writing to a file, then that's tricky-- local file IO is almost always blocking, so you'd have to delegate work to a separate thread. Luckily, asyncio provides a helper for that: <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop.run_in_executor" rel="nofollow noreferrer"><code>loop.run_in_executor</code></a>.</p>
<blockquote>
<p>Assuming multiple pages need to end up in the same file, how can I safely collect all of the data from these requests, merge it, and then persist without blocking other data fetching requests?</p>
</blockquote>
<p>This starts getting outside the bounds of what's a good question for SO. You should read up on different concurrency patterns for this.</p>
</div>
<span class="comment-copy">What I'd recommend for the collection and merging is to have a queue that you put data in when it's been received, and have a worker in another thread continually take the data and append it to the file.</span>
<span class="comment-copy">Thanks for your answer. Could you kindly put it in the context of the code that I provided? Specifically, how does my_worker() spawn a new my_worker (to get the next page)? How does my_worker() get access to the loop to call loop.run_in_executor()? Thanks.</span>
<span class="comment-copy">@llevar You can obtain the currently running event loop with <code>loop = asyncio.get_event_loop()</code>.</span>
<span class="comment-copy">@llevar Also, if your writes are not large, you probably needn't bother with <code>run_in_executor</code> because the OS will write the contents to the page cache and flush it in the background later.</span>
<span class="comment-copy">Thanks for feedback so far. I'm still not clear on how to accomplish my primary goal. If I call ensure_future() passing in a my_worker() inside my_worker() and I don't await it then there is an error that the loop is done but some Futures are still pending. If I do await the new worker then the original worker can't complete until the new one completes. The goal is to schedule a new worker from inside a currently running one, add it to the loop, and keep executing.</span>
