<div class="post-text" itemprop="text">
<p>To copy a nested list in an existing list, it is unfortunately not sufficient to simply multiply it, otherwise references are created and not independent lists in the list, see this example:</p>
<pre><code>x = [[1, 2, 3]] * 2
x[0] is x[1]  # will evaluate to True
</code></pre>
<p>To achieve your goal, you could use the range function in a list comprehension, for example, see this:</p>
<pre><code>x = [[1, 2, 3] for _ in range(2)]
x[0] is x[1]  # will evaluate to False (wanted behaviour)
</code></pre>
<p>This is a good way to multiply items in a list without just creating references, and this is also explained multiple times on many different websites.</p>
<p>However, there is a more efficient way to copy the list elements. That code seems a little faster to me (measured by timeit via command line and with different paramater n âˆˆ {1, 50, 100, 10000} for code below and range(n) in code above):</p>
<pre><code>x = [[1, 2, 3] for _ in [0] * n]
</code></pre>
<p>But I wonder, why does this code run faster? Are there other disadvantages (more memory consumption or similar)?</p>
<pre><code>python -m timeit '[[1, 2, 3] for _ in range(1)]'
1000000 loops, best of 3: 0.243 usec per loop

python -m timeit '[[1, 2, 3] for _ in range(50)]'
100000 loops, best of 3: 3.79 usec per loop

python -m timeit '[[1, 2, 3] for _ in range(100)]'
100000 loops, best of 3: 7.39 usec per loop

python -m timeit '[[1, 2, 3] for _ in range(10000)]'
1000 loops, best of 3: 940 usec per loop

python -m timeit '[[1, 2, 3] for _ in [0] * 1]'
1000000 loops, best of 3: 0.242 usec per loop

python -m timeit '[[1, 2, 3] for _ in [0] * 50]'
100000 loops, best of 3: 3.77 usec per loop

python -m timeit '[[1, 2, 3] for _ in [0] * 100]'
100000 loops, best of 3: 7.3 usec per loop

python -m timeit '[[1, 2, 3] for _ in [0] * 10000]'
1000 loops, best of 3: 927 usec per loop


# difference will be greater for larger n

python -m timeit '[[1, 2, 3] for _ in range(1000000)]'
10 loops, best of 3: 144 msec per loop

python -m timeit '[[1, 2, 3] for _ in [0] * 1000000]'
10 loops, best of 3: 126 msec per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This is correct; <code>range</code>, even in Python 3 where it produces a compact range object, is more complicated than a list, in the classical tradeoff between computation and storage. </p>
<p>As the list grows too large to fit in cache (the primary question if we're concerned with performance), the range object runs into a different issue: as each number in the range is created, it destroys and creates new <code>int</code> objects (the first 256 or so are less costly because they're interned, but their difference can still cost a few cache misses). The list would keep referring to the same one. </p>
<p>There are still more efficient options, though; a bytearray, for instance, would consume far less memory than the list. Probably the best function for the task is hidden away in <code>itertools</code>: <a href="https://docs.python.org/3/library/itertools.html#itertools.repeat" rel="nofollow noreferrer"><code>repeat</code></a>. Like a range object, it doesn't need storage for all the copies, but like the repeated list it doesn't need to create distinct objects. Something like <code>for _ in repeat(None, x)</code> would therefore just poke at the same few cache lines (iteration count and reference count for the object). </p>
<p>In the end, the main reason people stick to using <code>range</code> is because it's what's prominently presented (both in the idiom for a fixed count loop and among the builtins). </p>
<p>In other Python implementations, it's quite possible for range to be faster than repeat; this would be because the counter itself already holds the value. I'd expect such behaviours from Cython or PyPy. </p>
</div>
<span class="comment-copy">That difference is so exceptionally tiny, it can be considered within the margin of error. Generally as software engineers we only care about factors of 2 or greater.</span>
<span class="comment-copy">I ran the same example for <code>n == 1</code>, and the <code>range</code> version came out 45 ns faster. There is nothing significant here.</span>
<span class="comment-copy">I wonder, because it's every time faster (on my machine) and greater values will show also greater differences. Adding greater n....</span>
<span class="comment-copy">@FHTMitchell Not really: for small <code>n</code> (about 1000) <code>n*[0]</code> wins by a large margin (larger than statistical error). For larger <code>n</code> (about 1e6), the results that I get are close (larger than 2 sigma) but they are <i>consistently larger</i> (I would expect that <code>range</code> would win at least sometimes if the results were statistically close).</span>
<span class="comment-copy">@AGNGazer OK I suppose there's a difference between <i>statistical</i> error and <i>useful</i> difference.</span>
<span class="comment-copy">Paragraphs might help this answer.</span>
