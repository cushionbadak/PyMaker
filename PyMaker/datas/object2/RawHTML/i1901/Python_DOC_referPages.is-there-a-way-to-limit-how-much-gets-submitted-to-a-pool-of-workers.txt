<div class="post-text" itemprop="text">
<p>I have a Pool of workers and use <code>apply_async</code> to submit work to them.
I do not care for the result of the function applied to each item.
The pool seems to accept any number of <code>apply_async</code> calls, no matter how large the data or how quickly the workers can keep up with the work. </p>
<p>Is there a way to make <code>apply_async</code> block as soon as a certain number of items are waiting to be processed? I am sure internally, the pool is using a Queue, so it would be trivial to just use a maximum size for the Queue?</p>
<p>If this is not supported, would it make sense to submit a big report because this look like very basic functionality and rather trivial to add?</p>
<p>It would be a shame if one had to essentially re-implement the whole logic of Pool just to make this work.</p>
<p>Here is some very basic code:</p>
<pre><code>from multiprocessing import Pool
dowork(item):
    # process the item (for side effects, no return value needed)
    pass 

pool = Pool(nprocesses)
for work in getmorework():
    # this should block if we already have too many work waiting!        
    pool.apply_async(dowork, (work,))
pool.close()
pool.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>So something like this?</p>
<pre><code>import multiprocessing
import time

worker_count = 4
mp = multiprocessing.Pool(processes=worker_count)
workers = [None] * worker_count

while True:
    try:
        for i in range(worker_count):
            if workers[i] is None or workers[i].ready():
                workers[i] = mp.apply_async(dowork, args=next(getmorework()))
    except StopIteration:
        break
    time.sleep(1)
</code></pre>
<p>I dunno how fast you're expecting each worker to finish, the <code>time.sleep</code> may or may not be necessary or might need to be a different time or whatever. </p>
</div>
<div class="post-text" itemprop="text">
<p>an alternative might be to use <code>Queue</code>'s directly:</p>
<pre><code>from multiprocessing import Process, JoinableQueue
from time import sleep
from random import random

def do_work(i):
    print(f"worker {i}")
    sleep(random())
    print(f"done {i}")

def worker():
    while True:
        item = q.get()
        if item is None:
            break
        do_work(item)
        q.task_done()

def generator(n):
    for i in range(n):
        print(f"gen {i}")
        yield i

# 1 = allow generator to get this far ahead
q = JoinableQueue(1)

# 2 = maximum amount of parallelism
procs = [Process(target=worker) for _ in range(2)]
# and get them running
for p in procs:
    p.daemon = True
    p.start()

# schedule 10 items for processing
for item in generator(10):
    q.put(item)

# wait for jobs to finish executing
q.join()

# signal workers to finish up
for p in procs:
    q.put(None)
# wait for workers to actually finish
for p in procs:
    p.join()
</code></pre>
<p>mostly stolen from example Python's <code>queue</code> module:</p>
<p><a href="https://docs.python.org/3/library/queue.html#queue.Queue.join" rel="nofollow noreferrer">https://docs.python.org/3/library/queue.html#queue.Queue.join</a></p>
</div>
<span class="comment-copy">what is the purpose of blocking?  maybe a <code>Pool</code> is the wrong abstraction, why not just a <code>Queue</code> and some <code>Process</code>es?  that module has been around many years and I doubt it'll change, it's pretty complicated already let alone with exposing the many combinations of blocking your suggestion would entail (or maybe it wouldn't be too bad, but it seems non-trivial to me)</span>
<span class="comment-copy">The only purpose would be to simply limit the amount of work that can be queued to the workers without having to manually monitor how much work the workers already have completed. The process that fills the queue can often provide the data much much much faster than it can get consumed which may lead to memory problems. As I said, Queue already has the feature to limit the size, so adding this should be trivial.</span>
<span class="comment-copy">Thanks, I think this may actually be the easiest way to achieve this in the current situation. It is really a shame that the Pool constructor does not allow to pass on a maximum size to the Queue that is used internally.</span>
