<div class="post-text" itemprop="text">
<p>I am trying to eliminate all non-english words from many (100k) preprocssed text files (porter stemmed and lowercased, dropped all non a-z characters). I already parallelized the process to speed things up, but it is still painfully slow. Is there any more efficient way to do this in python?   </p>
<pre><code>englishwords = list(set(nltk.corpus.words.words()))
englishwords = [x.lower() for x in list(englishwords)]
englishwords = [ps.stem(w) for w in englishwords]
# this step takes too long:
shareholderletter= ' '.join(w for w in nltk.wordpunct_tokenize(shareholderletter) if w in englishwords)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You are checking for <code>somthing in otherthing</code> - and your <code>otherthing</code> is a list. </p>
<p>Lists are good for storing stuff, but lookup of "does x is in" your list takes <code>O(n)</code>. </p>
<p>Use a <code>set</code> instead, that drops the lookup to <code>O(1)</code> <em>and</em> it eleminate any dupes so your base -size of things to look up in drops as well if you got duplicates.</p>
<p>If your set does not change afterwards, go and use a <code>frozenset</code> - which is immutable.</p>
<p>Read: <a href="https://docs.python.org/3/tutorial/datastructures.html#sets" rel="nofollow noreferrer">Documentation of sets</a></p>
<p>If you use follow @DeepSpace 's suggestion, and leverage set operations you get even better performance:</p>
<pre><code>s = set( t.lower().strip() for t in ["Some","text","in","set"])

t = set("Some text in a string that holds other words as well".lower().split())

print ( s&amp;t )  # show me all things that are in both sets (aka intersection)
</code></pre>
<p>Output:</p>
<pre><code>set(['text', 'some', 'in'])
</code></pre>
<hr/>
<p>See <a href="https://docs.python.org/3/library/stdtypes.html#frozenset" rel="nofollow noreferrer">set operations</a></p>
<hr/>
<p>O(n): worstcase: your word is the last of 200k words in your list and you check the whole list - which takes 200k checks.</p>
<p>O(1): lookup time is constant, no matter how many items are in your datastructure, it takes the same amount of time to check if its in. To get this benefit, a <code>set</code> has a more complex storage-solution that takes slightly more memory (then a list) to perform so well on lookups.</p>
<hr/>
<p>Edit: worst case scenario for <em>not</em> finding a word inside a set/list:</p>
<pre><code>import timeit

setupcode = """# list with some dupes
l = [str(i) for i in range(10000)] + [str(i) for i in range(10000)] + [str(i) for i in range(10000)]
# set of this list
s = set( l )
"""

print(timeit.timeit("""k = "10000" in l """,setup = setupcode, number=100))
print(timeit.timeit("""k = "10000" in s """,setup = setupcode, number=100))

0.03919574100000034    # checking 100 times if "10000" is in the list
0.00000512200000457    # checking 100 times if "10000" us in the set
</code></pre>
</div>
<span class="comment-copy">add <code>englishwords=set(englishwords)</code> to remove dupes.</span>
<span class="comment-copy">or use set comprehension instead of list comprehension.</span>
<span class="comment-copy">While at it, make <code>something</code> a set as well and then just do set intersection. No need for a loop at all. This should speed things up in at least an order of magnitude.</span>
<span class="comment-copy">This however remove duplicate words. Consider:  <code>s = set( t.lower().strip() for t in ["Some","text","in","set"])</code> <code>t = set("Some text in a string that holds other words as well and then some more text in ways".lower().split())</code> <code>print ( s&amp;t )  # show me all things that are in both sets (aka intersection)</code>  The second instance are of "text" and "in" are being ignored by the result.</span>
<span class="comment-copy">@SAFEX - if you need the duplicates in your output, you can not use the <code>set &amp; set</code> operation <i><b>but</b></i> you still profit from the faster lookup in sets : <code>' '.join(w for w in nltk.wordpunct_tokenize(shareholderletter) if w in englishwords)</code>  if you convert <code>englishwords</code> into a set instead of a list. If you use set-operations (which would be fastest) you loose the dupes.</span>
<span class="comment-copy">@SAFEX see timing difference in case a word is not in a list (using strings of numbers "0"-"9999" and looking for "10000" in a list with dupes and a set of said list</span>
<span class="comment-copy">@PatrickArtner indeed, converting the whitelist into a set instead of a list significantly improves performance. Thanks again!</span>
