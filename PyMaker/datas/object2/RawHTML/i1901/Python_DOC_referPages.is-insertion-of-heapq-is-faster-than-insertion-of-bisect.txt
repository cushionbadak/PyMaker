<div class="post-text" itemprop="text">
<p>I have a question about bisect and heapq.</p>
<p>First I will show you 2 versions of code and then ask question about it.</p>
<p><strong>version of using bisect</strong>:</p>
<pre><code>while len(scoville) &gt; 1:
    a = scoville.pop(0)
    #pops out smallest unit
    if a &gt;= K:
        break
    b = scoville.pop(0)
    #pops out smallest unit
    c = a + b * 2
    bisect.insort(scoville, c)
</code></pre>
<p><strong>version of using heapq</strong></p>
<pre><code>while len(scoville) &gt; 1:
    a = heapq.heappop(scoville)
    #pops out smallest unit
    if a &gt;= K:
        break
    b = heapq.heappop(scoville)
    #pops out smallest unit
    c = a + b * 2
    heapq.heappush(scoville, c)
</code></pre>
<p>Both algorithms use 2 pops and 1 insert. </p>
<p>As I know, at version of using bisect, pop operation of list is O(1), and insertion operation of bisect class is O(logn).  </p>
<p>And at version of using heapq, pop operation of heap is O(1), and insertion operation of heap is O(logn) in average.</p>
<p>So both code should have same time efficiency approximately. However, version of using bisect is keep failing time efficiency test at some code challenge site.</p>
<p>Does anybody have a good guess?</p>
<p>*scoville is a list of integers</p>
</div>
<div class="post-text" itemprop="text">
<p>Your assumptions are  wrong. Neither is <code>pop(0)</code> O(1), nor is <code>bisect.insort</code> O(logn).</p>
<p>The problem is that in both cases, all the elements after the element you pop or insert have to be shifted one position to the left or might, making both operations O(n).</p>
<p>From the <a href="https://docs.python.org/3/library/bisect.html#bisect.insort_left" rel="nofollow noreferrer"><code>bisect.insort</code></a> documentation:</p>
<blockquote>
<p><code>bisect.insort_left(a, x, lo=0, hi=len(a))</code></p>
<p>Insert x in a in sorted order. This is equivalent to a.insert(bisect.bisect_left(a, x, lo, hi), x) assuming that a is already sorted. <strong>Keep in mind that the O(log n) search is dominated by the slow O(n) insertion step.</strong></p>
</blockquote>
<p>You can test this by creating a really long list, say <code>l = list(range(10**8))</code> and then doing <code>l.pop(0)</code> or <code>l.pop()</code> and <code>bisect.insort(l, 0)</code> or <code>bisect.insort(l, 10**9)</code>. The operations popping and inserting at the end shoul be instantaneous, while the others have a short but noticeable delay.
You can also use <code>%timeit</code> to test it repeatedly on shorter lists, if you alternatingly pop and insert so that the length of the list remains constant over many thousands of runs:</p>
<pre><code>&gt;&gt;&gt; l = list(range(10**6))
&gt;&gt;&gt; %timeit l.pop(); bisect.insort(l, 10**6)
100000 loops, best of 3: 2.21 us per loop
&gt;&gt;&gt; %timeit l.pop(0); bisect.insort(l, 0)
100 loops, best of 3: 14.2 ms per loop
</code></pre>
<p>Thus, the version using <code>bisect</code> is O(n) and the one with <code>heapq</code> is O(logn).</p>
</div>
<span class="comment-copy">Addendum: i) This assumes that the list is an actual Python <code>list</code>; other implementations may have faster <code>pop</code> or <code>insert</code> operations. ii) <code>heappop</code> does not have O(1), either, but O(logn) for restoring the heap invariant after the pop, but this does not change the overall complexity.</span>
