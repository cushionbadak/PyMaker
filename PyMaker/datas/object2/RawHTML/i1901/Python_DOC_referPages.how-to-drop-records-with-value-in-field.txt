<div class="post-text" itemprop="text">
<p>I have a <code>DataFrame</code> with 4 columns of which 2 contain string values. I was wondering if there was a way to select rows based on a partial string match against a particular column?</p>
<p>In other words, a function or lambda function that would do something like </p>
<pre><code>re.search(pattern, cell_in_question) 
</code></pre>
<p>returning a boolean. I am familiar with the syntax of <code>df[df['A'] == "hello world"]</code> but can't seem to find a way to do the same with a partial string match say <code>'hello'</code>.</p>
<p>Would someone be able to point me in the right direction?</p>
</div>
<div class="post-text" itemprop="text">
<p>Based on github issue <a href="https://github.com/pydata/pandas/issues/620" rel="noreferrer">#620</a>, it looks like you'll soon be able to do the following:</p>
<pre><code>df[df['A'].str.contains("hello")]
</code></pre>
<p>Update: <a href="http://pandas.pydata.org/pandas-docs/stable/text.html#text-string-methods" rel="noreferrer">vectorized string methods (i.e., Series.str)</a> are available in pandas 0.8.1 and up. </p>
</div>
<div class="post-text" itemprop="text">
<p>I am using pandas 0.14.1 on macos in ipython notebook.  I tried the proposed line above:</p>
<pre><code>df[df['A'].str.contains("Hello|Britain")]
</code></pre>
<p>and got an error: </p>
<pre><code>"cannot index with vector containing NA / NaN values"
</code></pre>
<p>but it worked perfectly when an "==True" condition was added, like this:</p>
<pre><code>df[df['A'].str.contains("Hello|Britain")==True]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If anyone wonders how to perform a related problem: <strong><em>"Select column by partial string"</em></strong> </p>
<p>Use:</p>
<pre><code>df.filter(like='hello')  # select columns which contain the word hello
</code></pre>
<p>And to select rows by partial string matching, pass <code>axis=0</code> to filter:</p>
<pre><code># selects rows which contain the word hello in their index label
df.filter(like='hello', axis=0)  
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Quick note: if you want to do selection based on a partial string contained in the index, try the following:</p>
<pre><code>df['stridx']=df.index
df[df['stridx'].str.contains("Hello|Britain")]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Say you have the following <code>DataFrame</code>:</p>
<pre><code>&gt;&gt;&gt; df = pd.DataFrame([['hello', 'hello world'], ['abcd', 'defg']], columns=['a','b'])
&gt;&gt;&gt; df
       a            b
0  hello  hello world
1   abcd         defg
</code></pre>
<p>You can always use the <code>in</code> operator in a lambda expression to create your filter.</p>
<pre><code>&gt;&gt;&gt; df.apply(lambda x: x['a'] in x['b'], axis=1)
0     True
1    False
dtype: bool
</code></pre>
<p>The trick here is to use the <code>axis=1</code> option in the <code>apply</code> to pass elements to the lambda function row by row, as opposed to column by column.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here's what I ended up doing for partial string matches.  If anyone has a more efficient way of doing this please let me know.</p>
<pre><code>def stringSearchColumn_DataFrame(df, colName, regex):
    newdf = DataFrame()
    for idx, record in df[colName].iteritems():

        if re.search(regex, record):
            newdf = concat([df[df[colName] == record], newdf], ignore_index=True)

    return newdf
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Since I've seen a lot of questions on similar topics, I thought it would be good to leave this here.</p>
<h3>Basic Substring Search</h3>
<pre><code>df1 = pd.DataFrame({'col': ['foo', 'foobar', 'bar', 'baz']})
df1

      col
0     foo
1  foobar
2     bar
3     baz
</code></pre>
<p>To select all rows containing "foo", use <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.contains.html" rel="nofollow noreferrer"><code>str.contains</code></a>:</p>
<pre><code>df1[df1['col'].str.contains('foo')]

      col
0     foo
1  foobar
</code></pre>
<p>Note that this is a pure substring search, so you can safely disable regex based matching. </p>
<pre><code>df1[df1['col'].str.contains('foo', regex=False)]

      col
0     foo
1  foobar
</code></pre>
<p>Performance wise, this does make a difference.</p>
<pre><code>df2 = pd.concat([df1] * 1000, ignore_index=True)

%timeit df2[df2['col'].str.contains('foo')]
%timeit df2[df2['col'].str.contains('foo', regex=False)]

6.31 ms ± 126 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
2.8 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<p>Avoid using regex-based search if you don't need it.</p>
<blockquote>
<p><strong>Note</strong><br/>
  Partial substring searches that are anchored at the start or end of strings can be done using <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.startswith.html" rel="nofollow noreferrer"><code>str.startswith</code></a> or <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.endswith.html" rel="nofollow noreferrer"><code>str.endswith</code></a>
  respectively.</p>
<p>Additionally, for regex based searches anchored at the start, use <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.match.html" rel="nofollow noreferrer"><code>str.match</code></a>.</p>
</blockquote>
<p><strong>Regex-based Search</strong><br/>
Most <code>str</code> methods support regular expressions. For example, to find rows in <code>df1</code> which contain "foo" followed by something else, we can use </p>
<pre><code>df1[df1['col'].str.contains(r'foo(?!$)')]

      col
1  foobar
</code></pre>
<hr/>
<h3><strong>Matching Entire Word(s)</strong></h3>
<p>By default, the substring search searches for the specified substring/pattern regardless of whether it is full word or not. To only match full words, we will need to make use of regular expressions here—in particular, our pattern will need to specify word boundaries (<code>\b</code>).</p>
<p>For example, </p>
<pre><code>df3 = pd.DataFrame({'col': ['the sky is blue', 'bluejay by the window']})
df3

                     col
0        the sky is blue
1  bluejay by the window
</code></pre>
<p>Now consider,</p>
<pre><code>df3[df3['col'].str.contains('blue')]

                     col
0        the sky is blue
1  bluejay by the window
</code></pre>
<p>v/s </p>
<pre><code>df3[df3['col'].str.contains(r'\bblue\b')]

               col
0  the sky is blue
</code></pre>
<hr/>
<h3><strong>Multiple Substring Search</strong></h3>
<p>This is most easily achieved through a regex search using the regex OR pipe.</p>
<pre><code># Slightly modified example.
df4 = pd.DataFrame({'col': ['foo abc', 'foobar xyz', 'bar32', 'baz 45']})
df4

          col
0     foo abc
1  foobar xyz
2       bar32
3      baz 45

df4[df4['col'].str.contains(r'foo|baz')]

          col
0     foo abc
1  foobar xyz
3      baz 45
</code></pre>
<p>You can also create a list of terms, then join them:</p>
<pre><code>terms = ['foo', 'baz']
df4[df4['col'].str.contains('|'.join(terms))]

          col
0     foo abc
1  foobar xyz
3      baz 45
</code></pre>
<p>Sometimes, it is wise to escape your terms in case they have characters that can be interpreted as <a href="https://docs.python.org/3/howto/regex.html#matching-characters" rel="nofollow noreferrer">regex metacharacters</a>. If your terms contain any of the following characters...</p>
<pre><code>. ^ $ * + ? { } [ ] \ | ( )
</code></pre>
<p>Then, you'll need to use <a href="https://docs.python.org/3/library/re.html#re.escape" rel="nofollow noreferrer"><code>re.escape</code></a> to <em>escape</em> them:</p>
<pre><code>import re
df4[df4['col'].str.contains('|'.join(map(re.escape, terms)))]

          col
0     foo abc
1  foobar xyz
3      baz 45
</code></pre>
<p><code>re.escape</code> has the effect of escaping the special characters so they're treated literally.</p>
<pre><code>re.escape(r'.foo^')
# '\\.foo\\^'
</code></pre>
<hr/>
<h3><strong>Multiple Whole Word Search</strong></h3>
<p>Similar to the above, except we add a word boundary (<code>\b</code>) to the joined pattern.</p>
<pre><code>p = r'\b(?:{})\b'.format('|'.join(map(re.escape, terms)))
df4[df4['col'].str.contains(p)]

       col
0  foo abc
3   baz 45
</code></pre>
<p>Where <code>p</code> looks like this,</p>
<pre><code>p
# '\\b(?:foo|baz)\\b'
</code></pre>
<hr/>
<h3>A Great Alternative: Use <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" rel="nofollow noreferrer">List Comprehensions</a>!</h3>
<p>Because you can! <a href="https://stackoverflow.com/questions/54028199/for-loops-with-pandas-when-should-i-care">And you should!</a> They are usually a little bit faster than string methods, because string methods are hard to vectorise and usually have loopy implementations. </p>
<p>Instead of,</p>
<pre><code>df1[df1['col'].str.contains('foo', regex=False)]
</code></pre>
<p>Use the <code>in</code> operator inside a list comp,</p>
<pre><code>df1[['foo' in x for x in df1['col']]]

       col
0  foo abc
1   foobar
</code></pre>
<p>Instead of,</p>
<pre><code>regex_pattern = r'foo(?!$)'
df1[df1['col'].str.contains(regex_pattern)]
</code></pre>
<p>Use <a href="https://docs.python.org/3/library/re.html#re.compile" rel="nofollow noreferrer"><code>re.compile</code></a> (to cache your regex) + <a href="https://docs.python.org/3/library/re.html#re.Pattern.search" rel="nofollow noreferrer"><code>Pattern.search</code></a> inside a list comp,</p>
<pre><code>p = re.compile(regex_pattern, flags=re.IGNORECASE)
df1[[bool(p.search(x)) for x in df1['col']]]

      col
1  foobar
</code></pre>
<p>If "col" has NaNs, then instead of </p>
<pre><code>df1[df1['col'].str.contains(regex_pattern, na=False)]
</code></pre>
<p>Use,</p>
<pre><code>def try_search(p, x):
    try:
        return bool(p.search(x))
    except TypeError:
        return False

p = re.compile(regex_pattern)
df1[[try_search(p, x) for x in df1['col']]]

      col
1  foobar
</code></pre>
<hr/>
<h3>More Options for Partial String Matching: <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.core.defchararray.find.html#numpy.core.defchararray.find" rel="nofollow noreferrer"><code>np.char.find</code></a>, <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html" rel="nofollow noreferrer"><code>np.vectorize</code></a>, <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html" rel="nofollow noreferrer"><code>DataFrame.query</code></a>.</h3>
<p>In addition to <code>str.contains</code> and list comprehensions, you can also use the following alternatives.</p>
<p><strong><code>np.char.find</code></strong><br/>
Supports substring searches (read: no regex) only.</p>
<pre><code>df4[np.char.find(df4['col'].values.astype(str), 'foo') &gt; -1]

          col
0     foo abc
1  foobar xyz
</code></pre>
<p><strong><code>np.vectorize</code></strong><br/>
This is a wrapper around a loop, but with lesser overhead than most pandas <code>str</code> methods.</p>
<pre><code>f = np.vectorize(lambda haystack, needle: needle in haystack)
f(df1['col'], 'foo')
# array([ True,  True, False, False])

df1[f(df1['col'], 'foo')]

       col
0  foo abc
1   foobar
</code></pre>
<p>Regex solutions possible:</p>
<pre><code>regex_pattern = r'foo(?!$)'
p = re.compile(regex_pattern)
f = np.vectorize(lambda x: pd.notna(x) and bool(p.search(x)))
df1[f(df1['col'])]

      col
1  foobar
</code></pre>
<p><strong><code>DataFrame.query</code></strong><br/>
Supports string methods through the python engine. This offers no visible performance benefits, but is nonetheless useful to know if you need to dynamically generate your queries.</p>
<pre><code>df1.query('col.str.contains("foo")', engine='python')

      col
0     foo
1  foobar
</code></pre>
<p>More information on <code>query</code> and <code>eval</code> family of methods can be found at <a href="https://stackoverflow.com/questions/53779986/dynamic-expression-evaluation-in-pandas-using-pd-eval">Dynamic Expression Evaluation in pandas using pd.eval()</a>.</p>
<hr/>
<h3>Recommended Usage Precedence</h3>
<ol>
<li>(First) <code>str.contains</code>, for its simplicity</li>
<li>List comprehensions, for its performance</li>
<li><code>np.vectorize</code></li>
<li>(Last) <code>df.query</code></li>
</ol>
</div>
<div class="post-text" itemprop="text">
<p>Why don't you try with <code>df[df["COLUMN_ID"].str.contains("SUBSTRING")]</code>.</p>
<p>Just replace the <code>COLUMN_ID</code> with string referring to column where you want to search for your sub-string and replace <code>SUBSTRING</code> with the text you want to be searched for ("Hello" in your case).</p>
</div>
<span class="comment-copy">If you are looking for 1) partial word matches, 2) full word matches, 3) multiple substring matches, 4) more performant alternatives using NumPy or list comprehensions, please take a look at <a href="https://stackoverflow.com/a/55335207/4909087">this answer</a>.</span>
<span class="comment-copy">This is implemented now</span>
<span class="comment-copy">Since str.* methods treat the input pattern as a regular expression, you can use <code>df[df['A'].str.contains("Hello|Britain")]</code></span>
<span class="comment-copy">Is it possible to convert <code>.str.contains</code> to use <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html#pandas.DataFrame.query" rel="nofollow noreferrer"><code>.query()</code> api</a>?</span>
<span class="comment-copy">@zyxue <a href="https://stackoverflow.com/q/44933071/395857">Select rows by partial string query with pandas</a></span>
<span class="comment-copy"><code>df[df['value'].astype(str).str.contains('1234.+')]</code> for filtering out non-string-type columns.</span>
<span class="comment-copy">Or you can do: df[df['A'].str.contains("Hello|Britain", na=False)]</span>
<span class="comment-copy">This can be distilled to: <code>df.loc[:, df.columns.str.contains('a')]</code></span>
<span class="comment-copy">which can be further distilled to <code>df.filter(like='a')</code></span>
<span class="comment-copy">You can just df[df.index.to_series().str.contains('LLChit')]</span>
<span class="comment-copy">How do I modify above to say that x['a'] exists only in beginning of x['b']?</span>
<span class="comment-copy">apply is a bad idea here in terms of performance and memory. See <a href="https://stackoverflow.com/questions/54432583/when-should-i-ever-want-to-use-pandas-apply-in-my-code">this answer</a>.</span>
<span class="comment-copy">Should be 2x to 3x faster if you compile regex before loop: regex = re.compile(regex) and then if regex.search(record)</span>
<span class="comment-copy">@MarkokraM <a href="https://docs.python.org/3.6/library/re.html#re.compile" rel="nofollow noreferrer">docs.python.org/3.6/library/re.html#re.compile</a> says that the most recent regexs are cached for you, so you don't need to compile yourself.</span>
<span class="comment-copy">Do not use iteritems to iterate over a DataFrame. It ranks last in terms of pandorability and performance</span>
<span class="comment-copy">This is just a repeat of what has already been mentioned.</span>
