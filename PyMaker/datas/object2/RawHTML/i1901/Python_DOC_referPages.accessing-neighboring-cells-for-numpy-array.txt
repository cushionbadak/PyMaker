<div class="post-text" itemprop="text">
<p>How can I access and modify the surrounding 8 cells for a 2D numpy array in an efficient manner?</p>
<p>I have a 2D numpy array like this:</p>
<pre><code>arr = np.random.rand(720, 1440)
</code></pre>
<p>For each grid cell, I want to reduce by 10% of the center cell, the surrounding 8 cells (fewer for corner cells), but only if the surrounding cell value exceeds 0.25. I suspect that the only way to do this is using a for loop but would like to see if there are better/faster solutions.</p>
<p>-- EDIT: For loop based soln:</p>
<pre><code>arr = np.random.rand(720, 1440)

for (x, y), value in np.ndenumerate(arr):
    # Find 10% of current cell
    reduce_by = value * 0.1

    # Reduce the nearby 8 cells by 'reduce_by' but only if the cell value exceeds 0.25
    # [0] [1] [2]
    # [3] [*] [5]
    # [6] [7] [8]
    # * refers to current cell

    # cell [0]
    arr[x-1][y+1] = arr[x-1][y+1] * reduce_by if arr[x-1][y+1] &gt; 0.25 else arr[x-1][y+1]

    # cell [1]
    arr[x][y+1] = arr[x][y+1] * reduce_by if arr[x][y+1] &gt; 0.25 else arr[x][y+1]

    # cell [2]
    arr[x+1][y+1] = arr[x+1][y+1] * reduce_by if arr[x+1][y+1] &gt; 0.25 else arr[x+1][y+1]

    # cell [3]
    arr[x-1][y] = arr[x-1][y] * reduce_by if arr[x-1][y] &gt; 0.25 else arr[x-1][y]

    # cell [4] or current cell
    # do nothing

    # cell [5]
    arr[x+1][y] = arr[x+1][y] * reduce_by if arr[x+1][y] &gt; 0.25 else arr[x+1][y]

    # cell [6]
    arr[x-1][y-1] = arr[x-1][y-1] * reduce_by if arr[x-1][y-1] &gt; 0.25 else arr[x-1][y-1]

    # cell [7]
    arr[x][y-1] = arr[x][y-1] * reduce_by if arr[x][y-1] &gt; 0.25 else arr[x][y-1]

    # cell [8]
    arr[x+1][y-1] = arr[x+1][y-1] * reduce_by if arr[x+1][y-1] &gt; 0.25 else arr[x+1][y-1]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This answer assumes that you <em>really</em> want to do exactly what you wrote in your question. Well, almost exactly, since your code crashes because indices get out of bounds. The easiest way to fix that is to add conditions, like, e.g.,</p>
<pre><code>if x &gt; 0 and y &lt; y_max:
    arr[x-1][y+1] = ...
</code></pre>
<p>The reason why the main operation <strong>cannot</strong> be vectorized using numpy or scipy is that all cells are “reduced” by some neighbor cells that have <em>already</em> been “reduced”. Numpy or scipy would use the unaffected values of the neighbors on each operation. In my other answer I show how to do this with numpy if you are allowed to group operations in 8 steps, each along the direction of one particular neighbor, but each using the <em>unaffected</em> value in that step for that neighbor. As I said, here I presume you have to proceed sequentially.</p>
<p>Before I continue, let me swap <code>x</code> and <code>y</code> in your code. Your array has a typical screen size, where 720 is the height and 1440 the width. Images are usually stored by rows, and the rightmost index in an ndarray is, by default, the one that varies more rapidly, so everything makes sense. It's admittedly counter-intuitive, but the correct indexing is <code>arr[y, x]</code>.</p>
<p>The major optimization that can be applied to your code (that cuts execution time from ~9 s to ~3.9 s on my Mac) is not to assign a cell to itself when it's not necessary, coupled with <a href="https://docs.python.org/3/reference/datamodel.html#object.__imul__" rel="nofollow noreferrer">in-place multiplication</a> <em>and</em> with <code>[y, x]</code> instead of <code>[y][x]</code> indexing. Like this:</p>
<pre><code>y_size, x_size = arr.shape
y_max, x_max = y_size - 1, x_size - 1
for (y, x), value in np.ndenumerate(arr):
    reduce_by = value * 0.1
    if y &gt; 0 and x &lt; x_max:
        if arr[y - 1, x + 1] &gt; 0.25: arr[y - 1, x + 1] *= reduce_by
    if x &lt; x_max:
        if arr[y    , x + 1] &gt; 0.25: arr[y    , x + 1] *= reduce_by
    if y &lt; y_max and x &lt; x_max:
        if arr[y + 1, x + 1] &gt; 0.25: arr[y + 1, x + 1] *= reduce_by
    if y &gt; 0:
        if arr[y - 1, x    ] &gt; 0.25: arr[y - 1, x    ] *= reduce_by
    if y &lt; y_max:
        if arr[y + 1, x    ] &gt; 0.25: arr[y + 1, x    ] *= reduce_by
    if y &gt; 0 and x &gt; 0:
        if arr[y - 1, x - 1] &gt; 0.25: arr[y - 1, x - 1] *= reduce_by
    if x &gt; 0:
        if arr[y    , x - 1] &gt; 0.25: arr[y    , x - 1] *= reduce_by
    if y &lt; y_max and x &gt; 0:
        if arr[y + 1, x - 1] &gt; 0.25: arr[y + 1, x - 1] *= reduce_by
</code></pre>
<p>The other optimization (that brings execution time further down to ~3.0 s on my Mac) is to avoid the boundary checks by using an array with extra boundary cells. We don't care what value the boundary contains, because it will never be used. Here is the code:</p>
<pre><code>y_size, x_size = arr.shape
arr1 = np.empty((y_size + 2, x_size + 2))
arr1[1:-1, 1:-1] = arr
for y in range(1, y_size + 1):
    for x in range(1, x_size + 1):
        reduce_by = arr1[y, x] * 0.1
        if arr1[y - 1, x + 1] &gt; 0.25: arr1[y - 1, x + 1] *= reduce_by
        if arr1[y    , x + 1] &gt; 0.25: arr1[y    , x + 1] *= reduce_by
        if arr1[y + 1, x + 1] &gt; 0.25: arr1[y + 1, x + 1] *= reduce_by
        if arr1[y - 1, x    ] &gt; 0.25: arr1[y - 1, x    ] *= reduce_by
        if arr1[y + 1, x    ] &gt; 0.25: arr1[y + 1, x    ] *= reduce_by
        if arr1[y - 1, x - 1] &gt; 0.25: arr1[y - 1, x - 1] *= reduce_by
        if arr1[y    , x - 1] &gt; 0.25: arr1[y    , x - 1] *= reduce_by
        if arr1[y + 1, x - 1] &gt; 0.25: arr1[y + 1, x - 1] *= reduce_by
arr = arr1[1:-1, 1:-1]
</code></pre>
<p>For the records, if the operations could be vectorized using numpy or scipy, the speed-up with respect to this solution would be at least by a factor of 35 (measured on my Mac).</p>
<p>N.B.: if numpy <em>did</em> operations on array slices sequentially, the following would yield factorials (i.e., products of positive integers up to a number) – but it does not:</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; arr = np.arange(1, 11)
&gt;&gt;&gt; arr
array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
&gt;&gt;&gt; arr[1:] *= arr[:-1]
&gt;&gt;&gt; arr
array([ 1,  2,  6, 12, 20, 30, 42, 56, 72, 90])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>Please clarify your question</strong></p>
<ul>
<li>Is it really intended that one loop iteration depends on the other, as mentioned by @jakevdp in the comments?</li>
<li>If this is the case, how exactly should be border pixels be handeled? This will affect the whole result due to the dependence from one loop iteration to the others</li>
<li>Please add a working reference implementation (You are getting an out of bounds error in your reference implementation)</li>
</ul>
<p><strong>Borders untouched, dependend loop iterations</strong></p>
<p>I don't see any other way than using a compiler in this way. In this example I use <code>Numba</code>, but you can also do quite the same in <code>Cython</code> if this is preverred.</p>
<pre><code>import numpy as np
import numba as nb

@nb.njit(fastmath=True)
def without_borders(arr):
  for x in range(1,arr.shape[0]-1):
    for y in range(1,arr.shape[1]-1):
      # Find 10% of current cell
      reduce_by = arr[x,y] * 0.1

      # Reduce the nearby 8 cells by 'reduce_by' but only if the cell value exceeds 0.25
      # [0] [1] [2]
      # [3] [*] [5]
      # [6] [7] [8]
      # * refers to current cell

      # cell [0]
      arr[x-1][y+1] = arr[x-1][y+1] * reduce_by if arr[x-1][y+1] &gt; 0.25 else arr[x-1][y+1]

      # cell [1]
      arr[x][y+1] = arr[x][y+1] * reduce_by if arr[x][y+1] &gt; 0.25 else arr[x][y+1]

      # cell [2]
      arr[x+1][y+1] = arr[x+1][y+1] * reduce_by if arr[x+1][y+1] &gt; 0.25 else arr[x+1][y+1]

      # cell [3]
      arr[x-1][y] = arr[x-1][y] * reduce_by if arr[x-1][y] &gt; 0.25 else arr[x-1][y]

      # cell [4] or current cell
      # do nothing

      # cell [5]
      arr[x+1][y] = arr[x+1][y] * reduce_by if arr[x+1][y] &gt; 0.25 else arr[x+1][y]

      # cell [6]
      arr[x-1][y-1] = arr[x-1][y-1] * reduce_by if arr[x-1][y-1] &gt; 0.25 else arr[x-1][y-1]

      # cell [7]
      arr[x][y-1] = arr[x][y-1] * reduce_by if arr[x][y-1] &gt; 0.25 else arr[x][y-1]

      # cell [8]
      arr[x+1][y-1] = arr[x+1][y-1] * reduce_by if arr[x+1][y-1] &gt; 0.25 else arr[x+1][y-1]
  return arr
</code></pre>
<p><strong>Timings</strong></p>
<pre><code>arr = np.random.rand(720, 1440)

#non-compiled verson: 6.7s
#compiled version:    6ms (the first call takes about 450ms due to compilation overhead)
</code></pre>
<p>This is realy easy to do an gives about a factor of 1000x. Depending on the first 3 Points there might be some more optimizations possible.</p>
</div>
<div class="post-text" itemprop="text">
<p>No need for loops, avoid the usual python loops, they are very slow. For greater efficiency, rely on numpy's build in matrix operation, "universal" functions, filters, masks and conditions  whenever you can. <a href="https://realpython.com/numpy-array-programmin" rel="nofollow noreferrer">https://realpython.com/numpy-array-programmin</a> For complicated computations vectorization is not too bad see some chart and benchmarks <a href="https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array">Most efficient way to map function over numpy array</a> (just do not use it for simpler matrix operations, like squaring of cells, build in functions will overperform)</p>
<p>Easy to see that each internal cell would be multiplied on .9 up to 8 times due 8 neighbors (that is reduced by .1), and additionally due to be a central cell,
yet it cannot be reduced below .25/.9 = 5/18. For border and corner cell number of decreases fells to 6 and 3 times.</p>
<p>Therefore </p>
<pre><code>x1 = 700  # for debugging use lesser arrays
x2 = 1400

neighbors = 8 # each internal cell has 8 neighbors


for i in range(neighbors):
     view1 = arr[1:-1, 1:-1] # internal cells only
     arr [1:x1, 1:-1] = np.multiply(view1,.9, where = view1 &gt; .25)

arr [1:-1, 1:-1] *= .9 
</code></pre>
<p>Borders and corners are be treated in same way with neighbours = 5 and 3 respectively and different views. I guess all three cases can be joined in one formula with complicated where case, yet speed up would be moderate, as borders and corners take a small fraction of all cells.</p>
<p>Here I used a small loop, yet it just 8 repetitions. It should be can get rid of the loop too, using power, log, integer part and max functions, resulting in a bit clumsy, but somewhat faster one-liner, something around </p>
<pre><code>      numpy.multiply( view1, x ** numpy.max( numpy.ceil( (numpy.log (* view1/x... / log(.9)
</code></pre>
<p>We can also try another useful technique, vectorization.
The vectorization is building a function which then can be applied to all the elements of the array.</p>
<p>For a change, lets preset margins/thresholds to find out exact coefficient to multiply on . Here is what code to look like</p>
<pre><code>n = 8
decrease_by = numpy.logspace(1,N,num=n, base=x, endpoint=False)

margins = decrease_by * .25

# to do : save border rows for further analysis, skip this for simplicity now    
view1 = a [1: -1, 1: -1]

def decrease(x):


    k = numpy.searchsorted(margin, a)
    return x * decrease_by[k]

f = numpy.vectorize(decrease)
f(view1)
</code></pre>
<p><strong>Remark 1</strong> One can try use different combinations of approaches, e.g. use precomputed margins with matrix arithmetics rather than vectorization. Perhaps there are even more tricks to slightly speed up each of above solutions or combinations of above. </p>
<p><strong>Remark 2</strong> PyTorch has many similarity with Numpy functionality but can greatly benefit from GPU. If you have a decent GPU consider PyTorch. There were attempt on gpu based numpy (gluon, abandoned gnumpy, minpy) More on gpu's
<a href="https://stsievert.com/blog/2016/07/01/numpy-gpu/" rel="nofollow noreferrer">https://stsievert.com/blog/2016/07/01/numpy-gpu/</a></p>
</div>
<div class="post-text" itemprop="text">
<p>Your size of the array is a typical screen size, so I guess that cells are pixel values in the range [0, 1). Now, pixel values are never multiplied by each other. If they were, operations would depend on the range (e.g., [0, 1) or [0, 255]), but they never do. So I would assume that when you say “reduce by 10% of a cell” you mean “subtract 10% of a cell”. But even so, the operation remains dependent on the order it is applied to the cells, because the usual way of calculating the total variation of a cell first and then applying it (like in a convolution) would cause some cell values to become negative (e.g., 0.251 - 8 * 0.1 * 0.999) , which does not make sense if they are pixels.</p>
<p>Let me assume for now that you <em>really</em> want to multiply cells by each other and by a factor, and that you want to do that by first having each cell affected by its neighbor number 0 (your numbering), then by its neighbor number 1, and so on for neighbors number 2, 3, 5, 7 and 8. As a rule, it's easier to define this kind of operations from the “point of view” of the target cells than from that of the source cells. Since numpy operates quickly on full arrays (or views thereof), the way to do this is to shift all neighbors in the position of the cell that is to be modified. Numpy has no <code>shift()</code>, but it has a <a href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.roll.html" rel="nofollow noreferrer"><code>roll()</code></a> which for our purpose is just as good, because we don't care about the boundary cells, that, as per your comment, can be restored to the original value as a last step. Here is the code:</p>
<pre><code>import numpy as np

arr = np.random.rand(720, 1440)
threshold = 0.25
factor    = 0.1
#                                                0 1 2
#                                    neighbors:  3   5
#                                                6 7 8
#                                                       ∆y  ∆x    axes
arr0 = np.where(arr  &gt; threshold, arr  * np.roll(arr,   (1,  1), (0, 1)) * factor, arr)
arr1 = np.where(arr0 &gt; threshold, arr0 * np.roll(arr0,   1,       0    ) * factor, arr0)
arr2 = np.where(arr1 &gt; threshold, arr1 * np.roll(arr1,  (1, -1), (0, 1)) * factor, arr1)
arr3 = np.where(arr2 &gt; threshold, arr2 * np.roll(arr2,       1,      1 ) * factor, arr2)
arr5 = np.where(arr3 &gt; threshold, arr3 * np.roll(arr3,      -1,      1 ) * factor, arr3)
arr6 = np.where(arr5 &gt; threshold, arr5 * np.roll(arr5, (-1,  1), (0, 1)) * factor, arr5)
arr7 = np.where(arr6 &gt; threshold, arr6 * np.roll(arr6,  -1,       0    ) * factor, arr6)
res  = np.where(arr7 &gt; threshold, arr7 * np.roll(arr7, (-1, -1), (0, 1)) * factor, arr7)
# fix the boundary:
res[:,  0] = arr[:,  0]
res[:, -1] = arr[:, -1]
res[ 0, :] = arr[ 0, :]
res[-1, :] = arr[-1, :]
</code></pre>
<p>Please note that even so, the main steps are different from what you do in your solution. But they necessarily are, because rewriting your solution in numpy would cause arrays to be read and written to in the same operation, and this is not something that numpy can do in a predictable way.</p>
<p>If you should change your mind, and decide to subtract instead of multiplying, you only need to change the column of <code>*</code>s before <code>np.roll</code> to a column of <code>-</code>s. But this would only be the first step in the direction of a proper convolution (a common and important operation on 2D images), for which you would need to completely reformulate your question, though.</p>
<p>Two notes: in your example code you indexed the array like <code>arr[x][y]</code>, but in numpy arrays, by default, the leftmost index is the most slowly varying one, i.e., in 2D, the vertical one, so that the correct indexing is <code>arr[y][x]</code>. This is confirmed by the order of the sizes of your array. Secondly, in images, matrices, and in numpy, the vertical dimension is usually represented as increasing downwards. This causes your numbering of the neighbors to differ from mine. Just multiply the vertical shifts by -1 if necessary.</p>
<hr/>
<p>EDIT</p>
<p>Here is an alternative implementation that yields exactly the same results. It is slightly faster, but modifies the array in place:</p>
<pre><code>arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[ :-2,  :-2] * factor, arr[1:-1, 1:-1])
arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[ :-2, 1:-1] * factor, arr[1:-1, 1:-1])
arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[ :-2, 2:  ] * factor, arr[1:-1, 1:-1])
arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[1:-1,  :-2] * factor, arr[1:-1, 1:-1])
arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[1:-1, 2:  ] * factor, arr[1:-1, 1:-1])
arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[2:  ,  :-2] * factor, arr[1:-1, 1:-1])
arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[2:  , 1:-1] * factor, arr[1:-1, 1:-1])
arr[1:-1, 1:-1] = np.where(arr[1:-1, 1:-1] &gt; threshold, arr[1:-1, 1:-1] * arr[2:  , 2:  ] * factor, arr[1:-1, 1:-1])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>EDIT: ah, I see that when you say "reduce" you mean multiply, not subtract. I also failed to recognize that you want reductions to compound, which this solution does not do. So it's incorrect, but I'll leave it up in case it's helpful.</p>
<p>You can do this in a vectorized manner using <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html" rel="nofollow noreferrer"><code>scipy.signal.convolve2d</code></a>:</p>
<pre><code>import numpy as np
from scipy.signal import convolve2d

arr = np.random.rand(720, 1440)

mask = np.zeros((arr.shape[0] + 2, arr.shape[1] + 2))
mask[1:-1, 1:-1] = arr
mask[mask &lt; 0.25] = 0
conv = np.ones((3, 3))
conv[1, 1] = 0

arr -= 0.1 * convolve2d(mask, conv, mode='valid')
</code></pre>
<p>This comes from thinking about your problem the other way around: each square should have 0.1 times all the surrounding values subtracted from it. The <code>conv</code> array encodes this, and we slide it over the <code>mask</code> array using <code>scipy.signal.convolve2d</code> to accumulate the values that should be subtracted.</p>
</div>
<div class="post-text" itemprop="text">
<p>We can do this using linear indices. As described your implementation depends on how you iterate through the array. So I assume we want to fix the array, work out what to multiply each element by, then simply apply the multiplication. So it doesnt matter how we go through the array.</p>
<p>How much to multiply each element is given by:</p>
<pre><code>1 if a[i,j] &lt; 0.25 else np.prod(neighbours_a*0.1)
</code></pre>
<p>so we will first go through the whole array, and get the 8 neighbours of each element, multiply them together, with a factor of 0.1^8, and then apply a conditional elementwise multiplication of those values with a. </p>
<p>To do this we will use linear indexing, and offseting them. So for an array with m rows, n columns, the i,jth element has linear index i<em>n + j. To move down a row we can just add n as the (i+1),jth element has linear index (i+1)n + j = (i</em>n + j) + n. This arithmatic provides a good way to get the neighbours of every point, as the neighbours are all fixed offsets from each point.</p>
<pre><code>import numpy as np

# make some random array
columns = 3
rows = 3
a = np.random.random([rows, columns])

# this contains all the reduce by values, as well as padding values of 1.
# on the top, bot left and right. we pad the array so we dont have to worry 
# about edge cases, when gathering neighbours. 
pad_row, pad_col = [1, 1], [1,1]
reduce_by = np.pad(a*0.1, [pad_row, pad_col], 'constant', constant_values=1.)

# build linear indices into the [row + 2, column + 2] array. 
pad_offset = 1
linear_inds_col = np.arange(pad_offset, columns + pad_offset)
linear_row_offsets = np.arange(pad_offset, rows + pad_offset)*(columns + 2*pad_offset)
linear_inds_for_array = linear_inds_col[None, :] + linear_row_offsets[:, None]

# get all posible row, col offsets, as linear offsets. We start by making
# normal indices eg. [-1, 1] up 1 row, along 1 col, then make these into single
# linear offsets such as -1*(columns + 2) + 1 for the [-1, 1] example
offsets = np.array(np.meshgrid([1, -1, 0], [1, -1, 0])).T.reshape([-1, 2])[:-1, :]
offsets[:,0] *= (columns + 2*pad_offset)
offsets = offsets.sum(axis=1)

# to every element in the flat linear indices we made, we just have to add
# the corresponding linear offsets, to get the neighbours
linear_inds_for_neighbours = linear_inds_for_array[:,:,None] + offsets[None,None,:]

# we can take these values from reduce by and multiply along the channels
# then the resulting [rows, columns] matrix will contain the potential
# total multiplicative factor to reduce by (if a[i,j] &gt; 0.25)
relavent_values = np.take(reduce_by, linear_inds_for_neighbours)
reduce_by = np.prod(relavent_values, axis=2)

# do reduction
val_numpy = np.where(a &gt; 0.25, a*reduce_by, a)

# check same as loop
val_loop = np.copy(a)
for i in range(rows):
    for j in range(columns):
        reduce_by = a[i,j]*0.1
        for off_row in range(-1, 2):
            for off_col in range(-1, 2):
                if off_row == 0 and off_col == 0:
                    continue
                if 0 &lt;= (i + off_row) &lt;= rows - 1 and 0 &lt;= (j + off_col) &lt;= columns - 1:
                    mult = reduce_by if a[i + off_row, j + off_col] &gt; 0.25 else 1.
                    val_loop[i + off_row, j + off_col] *= mult


print('a')
print(a)
print('reduced np')
print(val_numpy)
print('reduce loop')
print(val_loop)
print('equal {}'.format(np.allclose(val_numpy, val_loop)))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Try using pandas</p>
<pre><code>import pandas as pd
# create random array as pandas DataFrame
df = pd.DataFrame(pd.np.random.rand(720, 1440))  
# define the centers location for each 9x9
Center_Locations = (df.index % 3 == 1,
                    df.columns.values % 3 == 1)
# new values for the centers, to be use later
df_center = df.iloc[Center_Locations] * 1.25
# change the df, include center
df = df * 0.9 
# replacing only the centers values   
df.iloc[Center_Locations] = df_center 
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>It's not possible to avoid the loop because the reduction is performed sequentially, not in parallel.</p>
<p>Here's my implementation. For each <code>(i,j)</code> create 3x3 block-view of <code>a</code> centered at <code>a[i,j]</code> (the value of which I set temporarily to 0 so that it is below the threshold, since we don't want to reduce it). For the <code>(i,j)</code> at the boundary, the block is 2x2 at the corners and 2x3 or 3x2 elsewhere. Then the block is masked by the threshold and the unmasked elements are multiplied by <code>a_ij*0.1</code>.</p>
<pre><code>def reduce(a, threshold=0.25, r=0.1):
    for (i, j), a_ij in np.ndenumerate(a):
        a[i,j] = 0       
        block = a[0 if i == 0 else (i-1):i+2, 0 if j == 0 else (j-1):j+2]   
        np.putmask(block, block&gt;threshold, block*a_ij*r)  
        a[i,j] = a_ij   
    return a
</code></pre>
<p>Note that the reduction is also performed from the boundary cells on the cells surrounding the them, i.e the loop starts from the first corner of the array, <code>a[0, 0]</code> which has 3 neighbors: <code>a[0,1]</code>, <code>a[1,0]</code> and <code>a[1,1]</code>, which are reduced by <code>a[0,0]*0.1</code> if they are &gt; 0.25. Then it goes to the cell <code>a[0,1]</code> which has 5 neighbors etc. If you want to operate strictly on cells that have 8 neighbors, i.e. window of size 3x3, the loop should go from <code>a[1,1]</code> to <code>a[-2, -2]</code>, and the function should be modified as follows:</p>
<pre><code>def reduce_(a, threshold=0.25, r=0.1):
    ''' without borders -- as in OP's solution'''
    for (i, j), a_ij in np.ndenumerate(a[1:-1,1:-1]):
        block = a[i:i+3, j:j+3]
        mask = ~np.diag([False, True, False])*(block &gt; threshold)
        np.putmask(block, mask, block*a_ij*r)   
    return a
</code></pre>
<p>Example:       </p>
<pre><code>&gt;&gt;&gt; a = np.random.rand(4, 4)
array([[0.55197876, 0.95840616, 0.88332771, 0.97894739],
       [0.06717366, 0.39165116, 0.10248439, 0.42335457],
       [0.73611318, 0.09655115, 0.79041814, 0.40971255],
       [0.34336608, 0.39239233, 0.14236677, 0.92172401]])

&gt;&gt;&gt; reduce(a.copy())    
array([[0.00292008, 0.05290198, 0.00467298, 0.00045746],
       [0.06717366, 0.02161831, 0.10248439, 0.00019783],
       [0.00494474, 0.09655115, 0.00170875, 0.00419891],
       [0.00016979, 0.00019403, 0.14236677, 0.0001575 ]])

&gt;&gt;&gt; reduce_(a.copy())
array([[0.02161831, 0.03753609, 0.03459563, 0.01003268],
       [0.06717366, 0.00401381, 0.10248439, 0.00433872],
       [0.02882996, 0.09655115, 0.03095682, 0.00419891],
       [0.00331524, 0.00378859, 0.14236677, 0.00285336]])
</code></pre>
<p>Another example for 3x2 array:</p>
<pre><code>&gt;&gt;&gt; a = np.random.rand(3, 2)
array([[0.17246979, 0.42743388],
       [0.1911065 , 0.41250723],
       [0.73389051, 0.22333497]])

&gt;&gt;&gt; reduce(a.copy())
array([[0.17246979, 0.00737194],
       [0.1911065 , 0.0071145 ],
       [0.01402513, 0.22333497]])

&gt;&gt;&gt; reduce_(a.copy())  # same as a because there are no cells with 8 neighbors
array([[0.17246979, 0.42743388],
       [0.1911065 , 0.41250723],
       [0.73389051, 0.22333497]])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>By analyzing the problem to smaller ones, we see, that actully @jakevdp solution does the job, but forgets about checking the term <code>mask&lt;0.25</code> <strong>after</strong> convolution with the mask so that some values may drop later behind 0.25 (there maybe 8 tests for every pixel), so there must be a for loop, unless there's a built-in function for that I didn't heard of..</p>
<p>Here's my proposal:</p>
<pre><code># x or y first depends if u want rows or cols , .. different results
for x in range(arr.shape[1]-3):
    for y in range(arr.shape[0]-3):
        k = arr[y:y+3,x:x+3]
        arr[y:y+3,x:x+3] = k/10**(k&gt;0.25)
</code></pre>
</div>
<span class="comment-copy">Add a working loopy solution?</span>
<span class="comment-copy">The result very much depends on traversal order, but eh. The only improvement I can suggest over yours is to use numpy's views <code>a=arr[x-1:x+1, y-1:y+1]; a-=value; a[1,1]+=value; a=np.clip(a, 0.25)</code> you get the idea.</span>
<span class="comment-copy">@WalterTross, I would be ok if the boundary cells were left unchanged.</span>
<span class="comment-copy">To be clear: as written, values are already reduced when you reference them. That is, <code>a[0, 0]</code> might be 0.4, but then is reduced to 0.2  by the time your loop gets to <code>a[1, 0]</code>, and so the initial value does not affect <code>a[1,0]</code>. Is that intentional?</span>
<span class="comment-copy">I get the feeling, that this can only be done iteratively, because one step affects the next one</span>
<span class="comment-copy">Thanks for the article link!  However, I'm afraid that <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html" rel="nofollow noreferrer"><code>np.vectorize()</code></a> is "is essentially a for loop."</span>
<span class="comment-copy">Can you please confirm if your solution gives correct results? E.g. compare to the results returned by the max9111 function <code>without_borders(arr)</code> (which is the OP's original solution sped-up by numba) or my function <code>reduce_(arr)</code>, both of which return same (correct) results.</span>
<span class="comment-copy">1.I din t test it might have a typo or bug, yet in either case I don t think the code provided corresponds well to the problem statement or requester needs. It looks like other commenters and or moderators cowed some code from requester.2.Even if so the question might have more than one correct solutions. For instance order of reductions is not necessary important even thouhg it affects result. I Imagine something like let s try reduce contrast get rid of noice etc task</span>
<span class="comment-copy">Both Andy and max gave great and accurate answers. Yet personally I find Walter's solution more interesting, as question was about more about possibility of avoiding loops.</span>
<span class="comment-copy">Actually I more liked the 'rolling' solution of Walter ( boundaries are easy to fix with numpy.pad)</span>
<span class="comment-copy">numpy has no shift, but you can process border separately. Or just pad array(s) with 10 s. (for subtracting with 0s)</span>
<span class="comment-copy">The question is clearly referring to a convolution. This is the proper solution, good job. Although it could be improved with a high pass filter so you don't need to apply a mask there!</span>
<span class="comment-copy">@jakevdp as you pointed out in your comment, this is not a linear filter. In other words: unlike convolution, the entries of <code>a</code> are changed and referenced in the same loop, so the results are not exactly as in the given loopy solution.</span>
<span class="comment-copy">I am afraid this is not correct, besides the reduction here being a multiplication and not subtraction. Convolution operates on the whole array with its original cells, but we want to do it sequentially, cell by cell, with the reduction done on previous steps affecting the next steps.</span>
<span class="comment-copy">I do not think that we want to operate sequentially, just your. The guy who asked question must shared his code under pressure, the sequentiality is not mentioned in the problem statement. Order obviously does not matter for him, since he is did not answer multiple clarification requests.</span>
<span class="comment-copy">That is some strong panda magics. Mind expanding a little on what that does?</span>
<span class="comment-copy">Define the locations where it is "center" by n%3==1, and save it for later (df_center). Change all by 0.9, and put back the saved with *1.25</span>
<span class="comment-copy">Here's an counter-example: <code>arr = np.array([[0.17246979, 0.42743388], [0.1911065 , 0.41250723], [0.73389051, 0.22333497]])</code>. Your code returns the same arr without any changes. See the examples on my answer.</span>
<span class="comment-copy">What about: <code>arr = np.array([[0.06322375, 0.03942972, 0.73541247, 0.84798866, 0.71042087],        [0.20283542, 0.27995178, 0.84733291, 0.93385641, 0.9154688 ],        [0.16607985, 0.08221938, 0.83687028, 0.04745399, 0.56243368],        [0.59424876, 0.08783288, 0.9240022 , 0.60541983, 0.58984991],        [0.90215043, 0.47615277, 0.53946544, 0.71912684, 0.84109332]])</code>, I think your code gives incorrect result. E.g. the new <code>arr[1,1]</code> should be 0.00176996, but you have 0.0279952 (which is the original value).</span>
<span class="comment-copy">@AndyK , I prefer to let the OP decide that</span>
<span class="comment-copy">Sure the OP will decide, but you should be able to explain why your code does what it does. In the last example I provided, the result that your code returns is clearly wrong: it changes some of the items of arr simply by multiplying them by <code>0.1</code>. E.g. <code>arr[1,1] = 0.279952 -&gt; 0.0279952</code>, <code>arr[2,2] = 0.83687 -&gt; 0.083687</code>, <code>a[1,2] = 0.847333 -&gt; 0.0847333</code>, etc. But of course I might be wrong and that's why I asked you to confirm.</span>
