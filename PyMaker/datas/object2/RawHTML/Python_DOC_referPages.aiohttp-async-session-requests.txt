<div class="post-text" itemprop="text">
<p>So i've been scraping a website (www.cardsphere.com) protected pages with requests, using session, like so:</p>
<pre><code>import requests

payload = {
            'email': &lt;enter-email-here&gt;,
            'password': &lt;enter-site-password-here&gt;
          }

with requests.Session() as request:
   requests.get(&lt;site-login-page&gt;)
   request.post(&lt;site-login-here&gt;, data=payload)
   request.get(&lt;site-protected-page1&gt;)
   save-stuff-from-page1
   request.get(&lt;site-protected-page2&gt;)
   save-stuff-from-page2
   .
   .
   .
   request.get(&lt;site-protected-pageN&gt;)
   save-stuff-from-pageN
the-end
</code></pre>
<p>Now since it's quite a bit of pages i wanted to speed it up with Aiohttp + asyncio...but i'm missing something. I've been able to more or less use it to scrap unprotected pages, like so:</p>
<pre><code>import asyncio
import aiohttp

async def get_cards(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            data = await resp.text()
            &lt;do-stuff-with-data&gt;

urls  = [
         'https://www.&lt;url1&gt;.com'
         'https://www.&lt;url2&gt;.com'
         .
         .
         . 
         'https://www.&lt;urlN&gt;.com'
        ]

loop = asyncio.get_event_loop()
loop.run_until_complete(
    asyncio.gather(
        *(get_cards(url) for url in urls)
    )
)
</code></pre>
<p>That gave some results but how do i do it for pages that require login? I tried adding <code>session.post(&lt;login-url&gt;,data=payload)</code> inside the async function but that obviously didn't work out well, it will just keep logging in. Is there a way to "set" an aiohttp ClientSession before the loop function? As i need to login first and then, on the same session, get data from a bunch of protected links with asyncio + aiohttp?</p>
<p>Still rather new to python, async even more so, i'm missing some key concept here. If anybody would point me in the right direction i'll greatly appreciate it.</p>
</div>
<div class="post-text" itemprop="text">
<p>This is the simplest I can come up with, depending on what you do in <code>&lt;do-stuff-with-data&gt;</code> you may run into some other troubles regarding concurrency, down the rabbit hole you go... just kidding, its a little bit more complicated to wrap your head around coros and promises and tasks but once you get it is as simple as sequential programming</p>
<pre><code>import asyncio
import aiohttp


async def get_cards(url, session, sem):
    async with sem, session.get(url) as resp:
        data = await resp.text()
        # &lt;do-stuff-with-data&gt;


urls = [
    'https://www.&lt;url1&gt;.com',
    'https://www.&lt;url2&gt;.com',
    'https://www.&lt;urlN&gt;.com'
]


async def main():
    sem = asyncio.Semaphore(100)
    async with aiohttp.ClientSession() as session:
        await session.get('auth_url')
        await session.post('auth_url', data={'user': None, 'pass': None})
        tasks = [asyncio.create_task(get_cards(url, session, sem)) for url in urls]
        results = await asyncio.gather(*tasks)
        return results


asyncio.run(main())
</code></pre>
</div>
<span class="comment-copy">It will depend on the authentication method being used by the website how you authenticate your requests. Does the website use Basic Auth, OAuth, is it token based, etc?</span>
<span class="comment-copy">@Timothy Jannace : i access the website exactly like posted, i make a get to the login page first, then make a post to the same login-url but with the payload with my login information...so cookie based? Not much of an expert on this, edited to add the website.</span>
<span class="comment-copy">ah, I see. So after logging in the auth cookie is saved to the session which allows the subsequent logins. In that case the authentication call must be done first (and will block the other calls) and the subsequent calls can be done async.</span>
<span class="comment-copy">@Timothy Jannace yeah but how do i do that? with a simple request first or some other way? I tried multiple combinations, like doing a post after <code>with aiohttp.ClientSession() as session: </code> before doing the the async loop functions but that didn't work</span>
<span class="comment-copy">That seems to be working but it's taking about the same time (around 30 minutes for 35.000 urls). What i do after i get the data from url is only fill a dict with some info from it. Also getting a <code>concurrent.futures._base.CancelledError</code> after <code>results = await asyncio.gather(*tasks)</code></span>
<span class="comment-copy">@Last_crusaider Posting a full backtrace for the cancelled error might be enlightening. Also, for such a large number of URLs, you probably want to add a semaphore that limits their number. E.g. add <code>sem = Semaphore(100)</code> to <code>main()</code>, then pass <code>sem</code> go <code>get_cards()</code>, and add <code>async with sem</code> around the existing code.</span>
<span class="comment-copy">For the task taking the same time you will need to debug your script to find the bottleneck, most likely network congestion, or the coroutines fighting for resources, there are no restraints on the amount of concurrent tasks, and its not parallelization either, so all that context switching is taking a toll, 35k are a lot, you may want to implement a semaphore to limit the amount of concurrent tasks</span>
<span class="comment-copy">Added a semaphore, but I didn't tested it tough</span>
<span class="comment-copy">Managed to do it by passing cookies around. Although i was crashing the target site's servers because of the load, semaphore helped there. Thank you!</span>
