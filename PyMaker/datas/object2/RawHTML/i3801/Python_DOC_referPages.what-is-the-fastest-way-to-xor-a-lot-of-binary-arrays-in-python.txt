<div class="post-text" itemprop="text">
<p>I am tasked with calculating hamming distances between 1D binary arrays in two groups - a group of 3000 arrays and a group of 10000 arrays, and every array is 100 items(bits) long. So thats 3000x10000 HD calculations on 100 bit long objects.And all that must be done in at most a dozen minutes</p>
<p>Here's the best of what I came up with</p>
<pre><code>#X - 3000 by 100 bool np.array 
#Y - 10000 by 100 bool np.array
hd = []
i=1
for x in X:
    print("object nr " + str(i) + "/" + str(len(X)))
    arr = np.array([x] * len(Y)) 
    C = Y^arr # just xor this array by all the arrays in the other group simultainously
    hd.append([sum(c) for c in C]) #add up all the bits to get the hamming distance
    i+=1

return np.array(hd)
</code></pre>
<p>And it's still going to take 1-1.5 hours for it to finish. How do I go about making this faster?</p>
</div>
<div class="post-text" itemprop="text">
<p>You should be able to dramatically improve the summing speed by using <code>numpy</code> to perform it, rather than using a list comprehension and the built-in <code>sum</code> function (that takes no advantage of <code>numpy</code> vectorized operations).</p>
<p>Just replace:</p>
<pre><code>hd.append([sum(c) for c in C])
</code></pre>
<p>with:</p>
<pre><code># Explicitly use uint16 to reduce memory cost; if array sizes might increase
# you can use uint32 to leave some wiggle room
hd.append(C.sum(1, dtype=np.uint16))
</code></pre>
<p>which, for a 2D array, will return a new 1D array where each value is the sum of the corresponding row (thanks to specifying it should operate on <code>axis</code> <code>1</code>). For example:</p>
<pre><code>&gt;&gt;&gt; arr = np.array([[True,False,True], [False,False,True], [True, True,True]], dtype=np.bool)
&gt;&gt;&gt; arr.sum(1, np.uint16)
array([ 2, 1, 3], dtype=uint16)
</code></pre>
<p>Since it performs all the work at the C layer in a single operation without type conversions (instead of your original approach that requires a Python level loop that operates on each row, then an implicit loop that, while at the C layer, must still implicitly convert each <code>numpy</code> value one by one from <code>np.bool</code> to Python level <code>int</code>s just to sum them), this should run substantially faster for the array scales you're describing.</p>
<p>Side-note: While not the source of your performance problems, there is no reason to manually maintain your index value; <a href="https://docs.python.org/3/library/functions.html#enumerate" rel="nofollow noreferrer"><code>enumerate</code> can do that</a> more quickly and easily. Simply replace:</p>
<pre><code>i=1
for x in X:
    ... rest of loop ...
    i+=1
</code></pre>
<p>with:</p>
<pre><code>for i, x in enumerate(X, 1):
    ... rest of loop ...
</code></pre>
<p>and you'll get the same behavior, but slightly faster, more concise and cleaner in general.</p>
</div>
<div class="post-text" itemprop="text">
<p>IIUC, you can use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_xor.html" rel="nofollow noreferrer"><code>np.logical_xor</code></a> and list comprehension:</p>
<pre><code>result = np.array([[np.logical_xor(X[a], Y[b].T).sum() for b in range(len(Y))] 
                                                       for a in range(len(X))])
</code></pre>
<p>The whole operation runs in 7 seconds in my machine.</p>
<pre><code>0:00:07.226645
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Just in case you are not limited to using Python, this is a solution in C++ using <code>bitset</code>:</p>
<pre><code>#include &lt;iostream&gt;
#include &lt;bitset&gt;
#include &lt;vector&gt;
#include &lt;random&gt;
#include &lt;chrono&gt;

using real = double;

std::mt19937_64 rng;
std::uniform_real_distribution&lt;real&gt; bitset_dist(0, 1);
real prob(0.75);

std::bitset&lt;100&gt; rand_bitset()
{
    std::bitset&lt;100&gt; bitset;
    for (size_t idx = 0; idx &lt; bitset.size(); ++idx)
    {
         bitset[idx] = (bitset_dist(rng) &lt; prob) ? true : false;
    }
    return std::move(bitset);
}

int main()
{
    rng.seed(std::chrono::high_resolution_clock::now().time_since_epoch().count());

    size_t v1_size(3000);
    size_t v2_size(10000);

    std::vector&lt;size_t&gt; hd;
    std::vector&lt;std::bitset&lt;100&gt;&gt; vec1;
    std::vector&lt;std::bitset&lt;100&gt;&gt; vec2;
    vec1.reserve(v1_size);
    vec2.reserve(v2_size);
    hd.reserve(v1_size * v2_size); /// Edited from hd.reserve(v1_size);

    for (size_t i = 0; i &lt; v1_size; ++i)
    {
        vec1.emplace_back(rand_bitset());
    }
    for (size_t i = 0; i &lt; v2_size; ++i)
    {
        vec2.emplace_back(rand_bitset());
    }

    std::cout &lt;&lt; "vec1 size: " &lt;&lt; vec1.size() &lt;&lt; '\n'
              &lt;&lt; "vec2 size: " &lt;&lt; vec2.size() &lt;&lt; '\n';

    auto start(std::chrono::high_resolution_clock::now());
    for (const auto&amp; b1 : vec1)
    {
        for (const auto&amp; b2 : vec2)
        {
            /// Count only the bits that are set and store them
            hd.emplace_back((b1 ^ b2).count());
        }
    }
    auto time(std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(std::chrono::high_resolution_clock::now() - start).count());

    std::cout &lt;&lt; vec1.size() &lt;&lt; " x " &lt;&lt; vec2.size()
              &lt;&lt; " xor operations on 100 bits took " &lt;&lt; time &lt;&lt; " ms\n";
    return 0;
}
</code></pre>
<p>On my machine, the whole operation (<code>3000</code> x <code>10000</code>) takes about <code>300</code> ms.</p>
<p>You could put this into a function, compile it into a library and call it from Python. Another option is to store the distances to a file and then read them in Python.</p>
<hr/>
<p>EDIT: I had the wrong size for the hd vector. Reserving the proper amount of memory reduces the operation to about <code>190</code> ms because relocations are avoided.</p>
</div>
<span class="comment-copy">Do you <i>have to</i> use Python?</span>
<span class="comment-copy">Performance-wise, on an input array of 100 rows of 10000 elements each, <code>[sum(c) for c in C)]</code> took about 2 seconds to run on my machine; <code>C.sum(1, dtype=np.uint16)</code> took 737 µs, or about 0.037% of the original runtime (so 1-1.5 hours would reasonably be expected to drop to the 1-2 second range on this change alone). For 10000 rows of 100 elements each, the timings are longer, but still similar; the listcomp+<code>sum</code> solution took 2.11 seconds, the <code>numpy</code> solution took 914 µs. Either way, it should dramatically reduce runtime.</span>
<span class="comment-copy">Holy Guacamole, you Sir are a genius! It runs so fast I honestly didn't think it was even possible!</span>
