<div class="post-text" itemprop="text">
<p>I'm parsing data from a binary format and want to stream the resulting JSON strings to a listening server. These streams are independent I would like to have each stream run concurrently to speed up ingest of my data into the server.</p>
<p>I've tried using the <code>multithreading</code> library: </p>
<pre><code>import multiprocessing as mp

def write_tcp_stream(host, port, packet):
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.connect((host, port)) 
    except socket.error as msg:
        sys.stderr.write("[ERROR] %s\n", msg[1])
        sock.close()
        return

    sock.sendall(packet)

    sock.close()

...
p = mp.Pool(4)
for s in objects_to_stream.values() # assume s is a JSON string
    p.apply_async(write_tcp_stream, args=(HOST,PORT, s.encode())
p.close()
p.join()
</code></pre>
<p>but depending on the file I'm parsing I may get out-of-memory errors. I'm guessing it has to do with using the pool, but I don't know enough to understand what's happening behind the scenes.</p>
<p>I don't think I actually need to use <code>multiprocessing</code> but I don't know if it's possible to open multiple TCP sockets and write to them concurrently? I want to "fire-and-forget" the TCP writes. Is this possible?</p>
</div>
<div class="post-text" itemprop="text">
<p>Your question is a bit light on details to give a definitive answer (how large are the JSON packets? Is the task I/O bound or CPU bound? Does all your data come from one binary file?) but here are some options that might lead you in the right direction</p>
<ol>
<li><p>Simple: Write the JSON to stdout and use netcat to stream it to the server. Depending on how your data is structured, you could start up multiple instances to increase parallelism.</p></li>
<li><p>Non Blocking: If your task is I/O bound then I would keep it all in a single thread. Using <a href="https://docs.python.org/2/howto/sockets.html#non-blocking-sockets" rel="nofollow noreferrer">non-blocking sockets</a> you could have multiple sockets open at once and write data to them, but as your packets are large you would likely need to feed the data to the socket in chunks - this may quickly get messy.</p></li>
<li><p>Event Framework: use an event framework to handle the non blocking sockets for you (eg <a href="https://twistedmatrix.com" rel="nofollow noreferrer">Twisted Python</a>, or <a href="https://docs.python.org/3/library/asyncio-stream.html" rel="nofollow noreferrer">asyncio-stream</a> in Python 3). The idea here is you have an event loop which runs a given coroutine until it performs some action that blocks (writing to a socket say), then it switches to another coroutine until that blocks. You basically end up implementing this functionality if you want to use non-blocking sockets yourself. </p></li>
<li><p>Threads: If your task is CPU bound (by decoding the binary data say) then it may be optimal to run multiple processes to process the data in parallel. Threads won't work for this as the CPython GIL doesn't allow separate threads to run at the same time. Use the <code>multiprocessing</code> module, or just start multiple instances of your process</p></li>
</ol>
<p>What ever method you choose, you should probably look at how you can process your data in chunks instead of loading it all into memory at once.</p>
<p>I would suggest that <code>asyncio</code> would be a good place to start if you're using Python 3. By keeping it all in the same thread, you can easily pass around data, and you'll get much of the functionality you need out of the box.</p>
</div>
<span class="comment-copy">You can do this in a single thread if you use an event-loop model. See <a href="https://twistedmatrix.com/trac/" rel="nofollow noreferrer">Twisted</a> as one example.</span>
<span class="comment-copy">Is each <code>s</code> a huge thing that could conceivably get even huger when pickled? If so, then the error does mean what it sounds like. If not, something else is going on.</span>
<span class="comment-copy">But meanwhile: You're right that you don't need <code>multiprocessing</code> for this. Threads will be just fine, because the code isn't doing any CPU-intensive work, just waiting on IO. You can use a <code>multiprocessing.dummy.Pool</code>, or a <code>concurrent.futures.ThreadPoolExecutor</code>. The latter guarantees that you won't have to pickle the objects to pass them as arguments to a task; they just get shared directly between threads. (Although I suspect that isn't your actual problem in the first place…)</span>
<span class="comment-copy">Threads are probably better than processes here, and if switching fixes your problem… then you can decide whether you want to keep trying to debug the multiprocessing problem anyway, or just use threads and not worry about it.</span>
<span class="comment-copy">@abarnet yes, <code>s</code> is large, I'm not surprised that I'm hitting out of memory errors</span>
<span class="comment-copy">Some answers to the questions... All data comes from a single binary file (TDMS), using <a href="http://nptdms.readthedocs.io/en/latest/" rel="nofollow noreferrer"><code>nptdms</code></a> to load it into memory. I'm essentially dumping every individual data point with its metadata into newline-delimited JSON objects, so when doing <code>sendall()</code> the JSON string can be over 2MB long...</span>
