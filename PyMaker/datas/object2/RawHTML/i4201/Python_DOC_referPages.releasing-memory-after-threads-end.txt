<div class="post-text" itemprop="text">
<p>I'm working on an application that is primarily an API, but also has a multithreaded background job processing system, used to perform scheduled jobs as well as ad hoc jobs that take too long for an instant API response.</p>
<p>This will be forked 10 times through gunicorn. Any single forked process is capable of picking up a job to run, so job processing is balanced between the processes alongside the servicing of API requests.</p>
<p>My challenge is with how each process will continue to claim the peak amount of memory it ever needed for job processing. Some jobs require 1.5GB-2GB worth of memory.</p>
<p>Given enough time, eventually all 10 processes will have had to work those kinds of jobs, and each will be clinging to upwards of 2GB of memory. Even if the average memory use of the process rarely exceeds 100MB.</p>
<p>These intensive jobs are only run through dedicated threads within the process.</p>
<p>Is there ANY mechanism to compel Python to release the memory claimed specifically for a thread at the thread's closure? Or any general mechanism to force a Python process to reset memory to only what's actively needed at that moment?</p>
<p>Side note: I'm also exploring forking instead of threading but thus far that's introducing other problems I'm not sure I can work around.</p>
</div>
<div class="post-text" itemprop="text">
<p>Without a concrete example of what your API and worker processes/threads are doing its hard to provide a specific answer.</p>
<p>Python is a reference counted language: when an object is not referenced by any other it is free to be garbage collected. One can force the garbage collector to run (see <a href="https://docs.python.org/3/library/gc.html" rel="nofollow noreferrer">https://docs.python.org/3/library/gc.html</a>), but almost always its best to just let it do its thing.</p>
<p>When your worker threads exit any objects created within the thread are likely to be garbage collected; and exception to this would be objects placed in some global data structure (but your use-case doesn't sound like that it something you would be doing).</p>
</div>
<div class="post-text" itemprop="text">
<p>Just to prove that threads get destroyed after their job finished, you can run this code:</p>
<pre><code>def job(o: dict):
    count = 1
    r = random.randrange(10, 20)
    while count &lt; r:
        print(f"{o['name']}={count}/{r}")
        count += 1
        time.sleep(1)

    print(f"{o['name']} finished.")


def run_thread(o: dict):
    threading.Thread(target=job, args=(o,)).start()


if __name__ == '__main__':
    obj1 = {"name": "A"}
    run_thread(obj1)

    obj2 = {"name": "B"}
    run_thread(obj2)

    while True:
        time.sleep(1)
        print(f"THREADS: {len(threading.enumerate())}")
</code></pre>
<p>The output will be something like this:</p>
<pre><code>A=1/14
B=1/10
THREADS: 3
B=2/10
A=2/14    
THREADS: 3
...
B finished.
A=10/14
A=11/14
THREADS: 2
A=12/14
THREADS: 2
A=13/14
THREADS: 2
A finished.
THREADS: 1
THREADS: 1
THREADS: 1
</code></pre>
<p>As you can see, whenever a thread ends, total thread count decreases.</p>
<p>UPDATE:</p>
<p>Ok. I hope this script will satisfy you.</p>
<pre><code>from typing import List
import random
import threading
import time
import os
import psutil


def get_mem_usage():
    return PROCESS.memory_info().rss // 1024


def show_mem_usage():
    global MAX_MEMORY
    while True:
        mem = get_mem_usage()
        print(f"Currently used memory={mem} KB")
        MAX_MEMORY = max(mem, MAX_MEMORY)
        time.sleep(5)


def job(name: str):
    print(f"{name} started.")
    job_memory: List[int] = []
    total_bit_length = 0
    while command['stop_thread'] is False:
        num = random.randrange(100000, 999999)
        job_memory.append(num)
        total_bit_length += int.bit_length(num)
        time.sleep(0.0000001)
        if len(job_memory) % 100000 == 0:
            print(f"{name} Memory={total_bit_length//1024} KB")

    print(f"{name} finished.")


def start_thread(name: str):
    threading.Thread(target=job, args=(name,), daemon=True).start()


if __name__ == '__main__':
    command = {'stop_thread': False}

    STOP_THREAD = False
    PROCESS = psutil.Process(os.getpid())

    mem_before_threads = get_mem_usage()

    MAX_MEMORY = 0

    print(f"Starting memory={mem_before_threads} KB")

    threading.Thread(target=show_mem_usage, daemon=True).start()

    input("Press enter to START threads...\n")

    for i in range(20):
        start_thread("Job" + str(i + 1))

    input("Press enter to STOP threads...\n")
    print("Stopping threads...")

    command['stop_thread'] = True

    time.sleep(2)  # give some time to stop threads

    print("Threads stopped.")

    mem_after_threads = get_mem_usage()

    print(f"Memory before threads={mem_before_threads} KB")
    print(f"Max Memory while threads running={MAX_MEMORY} KB")
    print(f"Memory after threads stopped={mem_after_threads} KB")

    input("Press enter to exit.")
</code></pre>
<p>And this is the output:</p>
<pre><code>Starting memory=12980 KB
Currently used memory=13020 KB
Press enter to START threads...

Job1 started.
Job2 started.
Job3 started.
Job4 started.
Job5 started.
Job6 started.
Job7 started.
Job8 started.
Job9 started.
Job10 started.
Job11 started.
Job12 started.
Job13 started.
Job14 started.
Job15 started.
Job16 started.
Job17 started.
Job18 started.
Job19 started.
Job20 started.

Press enter to STOP threads...
Currently used memory=16740 KB
Currently used memory=19764 KB
Currently used memory=22516 KB
Currently used memory=25420 KB
Currently used memory=28340 KB

Stopping threads...
Job12 finished.
Job20 finished.
Job11 finished.
Job7 finished.
Job18 finished.
Job2 finished.
Job4 finished.
Job19 finished.
Job16 finished.
Job10 finished.
Job1 finished.
Job9 finished.
Job6 finished.
Job13 finished.
Job15 finished.
Job17 finished.
Job3 finished.
Job5 finished.
Job8 finished.
Job14 finished.

Threads stopped.

Memory before threads=12980 KB
Max Memory while threads running=28340 KB
Memory after threads stopped=13384 KB
Press enter to exit.
Currently used memory=13388 KB
</code></pre>
<p>I really don't know why there is a 408 KB difference buy it may be overhead for using 15 MB memory.</p>
</div>
<span class="comment-copy">There are two levels of memory allocation. Python frees objects when their reference count goes to zero. Unless you have complicated circular references that can't be resolved, python frees objects when the function implementing your thread returns. But that doesn't mean it goes back to the operating system. If another process needs memory, the OS may swap this process-allocated but unused memory to disk. If your program never touches the memory again, it will stay on disk.</span>
<span class="comment-copy">@tdelaney I <i>think</i> it's returning it to the OS that I'm ultimately in need of. It seems very peculiar that there's no way for a process to give up memory that it no longer needs.</span>
<span class="comment-copy">That's OS and libc implementation dependent. Linux has a <code>malloc_trim</code> C function for instance, that tries some cleanup, but I suspect that there will be some python objects sprinkled around the heap and not much will free. I think forking is the usual way to handle this. Maybe you need to post those issues here.</span>
<span class="comment-copy">@tdelaney I might post another question for that, since it'd kind of be off-topic to this one. Short story is that these jobs run within a large Flask application with a lot of functionality that's dependent on the Flask current_app object, and initial attempts to pass that object into forked processes hasn't been kind.</span>
<span class="comment-copy">Good plan! And, ouch, that seems like an "interesting" problem.</span>
<span class="comment-copy">It's not; that's the annoying thing. The threads get access to a global data structure but (at least in my current jobs) aren't adding anything to it that would outlive the thread. I've been able to mitigate the problem a little bit since my OP by calling gc.collect() regularly, but I think that's only helping prevent memory fragmentation and the process claiming more than it actually needs. The solve would be to release memory back to the OS, but I don't know if that's possible.</span>
<span class="comment-copy">I think as Ramazan Polat's short example, your problem isn't due to the use of threads per se; the most likely explanation is that objects' reference counts and not dropping to zero and so the objects are not garbage collected. Can you provide a minimal example along the lines of your app that has the same behaviour?</span>
<span class="comment-copy">You're probably right, but I can't figure out where the problem lies. I'm profiling the memory now and I'm having low level functions returning, for example, adding 100MB but I'm only able to free half that when I delete the variable i store the response in. There's definitely other things at work here, with memory not releasing when removed from scope. The thread was just the upper-most scope I thought would dictate a release once ended.</span>
<span class="comment-copy">I know the threads are being destroyed. I have debug output that periodically displays the thread count per process. The problem is that the process isn't returning memory to the OS that's claimed strictly within a thread.</span>
<span class="comment-copy">@Rikaelus, updated the answer. Check it out.</span>
