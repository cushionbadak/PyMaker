<div class="post-text" itemprop="text">
<p>I've had the following assignment: Given a list of n integers, each integer in the list is unique and larger than 0. I am also given a number K - which is an integer larger than 0.
<strong>List slicing of any kind is not allowed</strong></p>
<p>I need to check whether there is a subset that sums up to K.
e.g: for the list [1,4,8], and k=5, I return True, because we have the subset {1,4}.</p>
<p>Now I need to implement this using a recursion:
So I did, however I needed to implement memoization:</p>
<p>And I wonder what is the difference between those functions' code:
I mean, both seem to implement memoization, however the second should work better but it doesn't. I'd really appreciate some help :)</p>
<pre><code>def subsetsum_mem(L, k):
    '''
    fill-in your code below here according to the instructions
    '''
    sum_dict={}
    return s_rec_mem(L,0,k,sum_dict)



def s_rec_mem(L, i, k, d):
    '''
    fill-in your code below here according to the instructions
    '''
    if(k==0):
        return True
    elif(k&lt;0 or i==len(L)):
        return False
    else:
        if k not in d:
            res_k=s_rec_mem(L,i+1,k-L[i],d) or s_rec_mem(L,i+1,k,d)
            d[k]=res_k
            return res_k


def subsetsum_mem2(L, k):
    '''
    fill-in your code below here according to the instructions
    '''
    sum_dict={}
    return s_rec_mem2(L,0,k,sum_dict)


def s_rec_mem2(L, i, k, d):
    '''
    fill-in your code below here according to the instructions
    '''
    if(k==0):
        return True
    elif(k&lt;0 or i==len(L)):
        return False
    else:
        if k not in d:
            res_k=s_rec_mem2(L,i+1,k-L[i],d) or s_rec_mem2(L,i+1,k,d)
            d[k]=res_k
            return res_k
        else:
            return d[k]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You have two problems with your memoization.</p>
<p>First, you're using just <code>k</code> as the cache key. But the function does different things for different values of <code>i</code>, and you're ignoring <code>i</code> in the cache, so you're going to end up returning the value from <code>L, 9, 1, d</code> for <code>L, 1, 1, d</code>.</p>
<p>Second, only in <code>s_rec_mem</code>, you never return <code>d[k]</code>; if it's present, you just fall off the end and return <code>None</code> (which is falsey).</p>
<p>So, the second one <em>does</em> come closer to working—but it still doesn't actually work.</p>
<p>You could fix it like this:</p>
<pre><code>def s_rec_mem2(L, i, k, d):
    '''
    fill-in your code below here according to the instructions
    '''
    if(k==0):
        return True
    elif(k&lt;0 or i==len(L)):
        return False
    else:
        if (i, k) not in d:
            res_k=s_rec_mem2(L,i+1,k-L[i],d) or s_rec_mem2(L,i+1,k,d)
            d[i, k]=res_k
            return res_k
        else:
            return d[i, k]
</code></pre>
<p>… or by just using <code>lru_cache</code>, either by passing down <code>tuple(L)</code> instead of <code>L</code> (because tuples, unlike lists, can be hashed as keys, and your recursive function doesn't care what kind of sequence it gets), or by making it a local function that sees <code>L</code> via closure instead of getting passed it as a parameter.</p>
<hr/>
<p>Finally, from a quick glance at your code:</p>
<ul>
<li>It looks like you're only ever going to evaluate <code>s_rec_mem</code> at most twice on each set of arguments (assuming you correctly cache on <code>i, k</code> rather than just <code>k</code>), which means memoization can only provide a 2x constant speedup at best. To get any more than that, you need to rethink your caching or your algorithm.</li>
<li>You're only memoizing within each separate top-level run, but switching to <code>lru_cache</code> on <code>tuple(L), i, k</code> means you're memoizing across all runs until you restart the program (or REPL session)—so the first test may take a long time, but subsequent runs on the same <code>L</code> (even with a different <code>k</code>) could benefit from previous caching.</li>
<li>You seem to be trying to solve a minor variation of the subset sum problem. That problem in the general case is provably NP-complete, which means it's <em>guaranteed</em> to take exponential time. And your variation seems to be if anything harder than the standard problem, not easier. If so, not only is your constant-factor speedup not going to have much benefit, there's really <em>nothing</em> you can do that will do better. In real life, people who solve equivalent problems usually use either optimization (e.g., via dynamic programming) or approximation to within an arbitrary specified limit, both of which allow for polynomial (or at least pseudo-polynomial) solutions in most cases, but can't solve all cases. Or, alternatively, there are subclasses of inputs for which you <em>can</em> solve the problem in polynomial time, and if you're lucky, you can prove your inputs fall within one of those subclasses. If I've guessed right on what you're doing, and you want to pursue any of those three options, you'll need to do a bit of research. (Maybe Wikipedia is good enough, or maybe there are good answers here or on the compsci or math SE sites to get you started.)</li>
</ul>
</div>
<span class="comment-copy">In the first one, if <code>k</code> is in <code>d</code>, you never return anything</span>
<span class="comment-copy">What does "work better" mean?</span>
<span class="comment-copy">The first one isn't doing the right thing, because it falls off the end and returns <code>None</code> rather than <code>d[k]</code> if the value is already in the cache. Since <code>None</code> is falsey, that could well make your code run a whole lot faster by just not doing most of the necessary work and returning the wrong answer very quickly.</span>
<span class="comment-copy">By the way, you might want to look at using <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache" rel="nofollow noreferrer"><code>@functools.lru_cache</code></a> for memoization. Even if you're writing your own memoization as a learning exercise (or trying to beat the performance of <code>lru_cache</code>…), it's worth having the well-tested, standard implementation to compare your version against in unit tests and benchmarks.</span>
<span class="comment-copy">@abarnert Oh I see, then the first one returning some None values might make the code run faster, but doesn't actually work.  So the question is, is the second code actually making the process more efficient? or am I missing something? abarnert - About Iru_cache, I'll look into it, thanks :)</span>
<span class="comment-copy">Thank you very much! After going through it myself for quite some time (way too much) I've figured the solution you've mentioned above. However, comparing it to lru_cache seemed to indicate it's not working so well - since the run time for this function with and without memoization are pretty much the same for various different inputs.  Do you think anything else can be done considering the problem's restrictions?</span>
<span class="comment-copy">@Guy See the updated answer for more details.</span>
<span class="comment-copy">Thank you so much for the thoughtful reply! I'll look into it :)</span>
<span class="comment-copy">@Guy One last comment: When I say “most cases” in the last bullet point, that may not actually be true. Many optimization solutions actually only work in a vanishingly small subset of all possible cases, but those cases include most of the cases you <i>usually</i> actually care about. (This is sort of the finite version of the fact that almost all functions are not continuous, and yet most functions you think of are) So you have to understand your input space and the algorithm pretty well (or cross your fingers and test) in general.</span>
<span class="comment-copy">I see, that's quite interesting to think about, thank you for your thoughtful comments :)</span>
