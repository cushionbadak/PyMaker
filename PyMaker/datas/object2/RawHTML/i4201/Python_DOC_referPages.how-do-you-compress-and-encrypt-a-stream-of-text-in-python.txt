<div class="post-text" itemprop="text">
<p>Let's say I have a stream (a generator) which produces text. They are typically JSON strings but could be any string. And I can't instantiate it as a list. Too large.</p>
<p>I want to take this stream and produce another stream which is this stream composed with compression followed by encryption. What is a good way to do that?</p>
<p>The encryption part is not so hard but compression usually requires a batch of data. Is there a pure streaming way to do it? If not, what is a good enough alternative?</p>
<p>I also don't want to create any files. </p>
</div>
<div class="post-text" itemprop="text">
<p>The stdlib <a href="https://docs.python.org/3/library/zlib.html" rel="nofollow noreferrer"><code>zlib</code></a> has support for compressing a stream of data by feeding in chunks at a time. See <code>compressobj</code> and the <code>Compress</code> type.</p>
<p>Notice that you can feed in as many or as few bytes as you want; the compressor object does its own buffering to keep zlib happy.</p>
<p>Many other compression libraries are built to mimic the interface of <code>zlib</code> to some degree.</p>
<p>And, as you already know, encryption usually isn’t a problem.</p>
<p>Since you’re starting with text, not bytes, there’s also a zeroth part to your problem: encoding the text. You could use the <code>codecs</code> module for a stream encoder, but since it sounds like you’ve already got a generator of one line at a time, <code>(line.encode('utf-8') for line in lines)</code> should be just fine. (You probably already knew this part, which is why you didn’t ask—but a future searcher with the same problem may not.)</p>
</div>
<div class="post-text" itemprop="text">
<p>Can you group your data with the grouper from the <a href="https://docs.python.org/3/library/itertools.html#itertools-recipes" rel="nofollow noreferrer">itertools recipe</a>?</p>
<pre><code>from itertools import zip_longest

def grouper(iterable, n, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    # grouper('ABCDEFG', 3, 'x') --&gt; ABC DEF Gxx"
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)

chunksize = 10
iterable = range(100)
for chunk in grouper(iterable, chunksize, fillvalue=''):
    print(chunk)
</code></pre>
<p>return:</p>
<pre><code>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)
(10, 11, 12, 13, 14, 15, 16, 17, 18, 19)
(20, 21, 22, 23, 24, 25, 26, 27, 28, 29)
...
</code></pre>
<p>Note that it never create a list with all the data!</p>
</div>
<span class="comment-copy">This probably isn’t the best answer to your question, but it is worth knowing about: In Python, the vast majority of functions that want files will actually take any “file-like object” with the right methods (in 3.x, that means the appropriate ABC from the <code>io</code> module, although you can usually get away with less than that). You can use <code>io.BytesIO</code> to make a file-like object out of a bytes-like object, or you can build your own class if you need to generate bytes on the fly.</span>
<span class="comment-copy">For compression, take a look at <a href="https://docs.python.org/3/library/zlib.html" rel="nofollow noreferrer">docs.python.org/3/library/zlib.html</a></span>
<span class="comment-copy">Yes, but io.BytesIO only takes bytes not a generator yielding bytes. I would also like to know how to take an iterable and produce a file-like object.</span>
<span class="comment-copy">@user1827975 That’s why I said “This probably isn’t the best answer to your question”, and went on to give a better answer as an answer. But meanwhile, did you not read the entire comment? You can easily build your own class that implements the ABC around a generator.</span>
<span class="comment-copy">You probably don’t want to separately compress separate chunks. That would effectively mean restarting the compression dictionary over and over, raising the cost and reducing the compression you get (and also require a bunch of headers, and require some kind of framing to delimit the compressed chunks).</span>
<span class="comment-copy">I think I misunderstood his question, I tougth he wanted to compress a chunk of data.</span>
