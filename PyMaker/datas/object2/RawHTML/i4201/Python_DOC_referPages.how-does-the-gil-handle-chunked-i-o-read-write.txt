<div class="post-text" itemprop="text">
<p>Say I had a <code>io.BytesIO()</code> I wanted to write a response to sitting on a thread:</p>
<pre><code>f = io.ByteIO()
with requests.Session() as s:
    r = s.get(url, stream = True)
    for chunk in r.iter_content(chunk_size = 1024):
        f.write(chunk)
</code></pre>
<p>Now this is not to harddisk but rather in memory (got plenty of it for my purpose), so I don't have to worry about the needle being a bottleneck. I know for blocking I/O (file read/write) the GIL is released from the <a href="https://docs.python.org/3/c-api/init.html#thread-state-and-the-global-interpreter-lock" rel="nofollow noreferrer">docs</a> and this SO <a href="https://stackoverflow.com/a/29270846/6741482">post</a> by Alex Martelli, but I wonder, does the GIL just release on <code>f.write()</code> and then reacquire on the <code>__next__()</code> call of the loop? </p>
<p>So what I end up with are  a bunch of fast GIL acquisitions and releases. Obviously I would have to time this to determine anything worth note, <em>but does writing to in memory file objects on a multithreaded web scraper in general support GIL bypass</em>?</p>
<p>If not, I'll just handle the large responses and dump them into a queue and process on <code>__main__</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>From what I can see in the <a href="https://github.com/python/cpython/blob/master/Modules/_io/bytesio.c" rel="nofollow noreferrer"><code>BytesIO</code> type's source code</a>, the GIL is not released during a call to <code>BytesIO.write</code>, since it's just doing a quick memory copy. It's only for system calls that may block that it makes sense for the GIL to be released.</p>
<p>There probably is such a syscall in the <code>__next__</code> method of the <code>r.iter_content</code> generator (when data is read from a socket), but there's none on the writing side.</p>
<p>But I think your question reflects an incorrect understanding of what it means for a builtin function to release the GIL when doing a blocking operation. It will release the GIL just before it does the potentially blocking syscall. But it will reacquire the GIL it before it returns to Python code. So it doesn't matter how many such GIL releasing operations you have in a loop, all the Python code involved will be run with the GIL held. The GIL is never released by one operation and reclaimed by different one. It's both released and reclaimed for each operation, as a single self-contained step.</p>
<p>As an example, you can look at <a href="https://github.com/python/cpython/blob/61f82e0e337f971da57f8f513abfe693edf95aa5/Python/fileutils.c#L1440" rel="nofollow noreferrer">the C code that implements writing to a file descriptor</a>. The macro <code>Py_BEGIN_ALLOW_THREADS</code> releases the GIL. A few lines later, <code>Py_END_ALLOW_THREADS</code> reacquires the GIL. No Python level runs in between those steps, only a few low-level C assignments regarding <code>errno</code>, and the <code>write</code> syscall that might block, waiting on the disk.</p>
</div>
<span class="comment-copy">How long does that <code>f.write()</code> take? I suspect for writing 1 KiB it's below a microsecond, mostly the function call overhead. I'd understand if you wrote a huge amount, hundreds of megs; maybe at that time something else could be scheduled.</span>
<span class="comment-copy">@9000 Doesn't take long, just curious about the inner workings. I'm chunking this to be kind to the network (which has low bandwidth) and since a <code>session.get()</code> method without <code>stream</code> downloads the entire response, when that response is &gt;1GB I want a clean manner to handle it. Figured storing in memory files then passing those back to a central thread (<code>__main__</code>) for parsing before harddisk saving would be wise.</span>
<span class="comment-copy">The GIL essentially goes by CPython bytecode. So the GIL is released when it gets to the C function underneath <code>BytesIO.write</code>. When that C function returns, it's reacquired to execute the next bytecode in the calling function. It may continue to be held all the way through the next <code>__next__</code>, but that's not guaranteed.</span>
<span class="comment-copy">If you want to see the bytecode for your own code, <code>dis.dis('''&lt;your code here&gt;''') will show you. After the </code>CALL_FUNCTION<code>, most likely the only other things that happen are a </code>POP_TOP` to ignore the return value and a <code>JUMP_ABSOLUTE</code> back to the <code>FOR_ITER</code>. Then, inside that <code>FOR_ITER</code> it's doing the special method lookup and call to <code>__next__</code> on your iterator in C, and the next bytecode where it could get released is the start of that <code>iter_content.__next__</code> (or, if <code>iter_content</code> is a generator function, the next instruction after the <code>YIELD_VALUE</code>).</span>
<span class="comment-copy">@abarnert So when the program gets the C function under <code>__next__()</code>, does it not execute the C function under <code>BytesIO.write()</code> and then return back to the C function that was under <code>__next__()</code>? Thus the <code>for</code> could completely bypass the GIL. I would think this could be the case, and the code within a loop would be the real culprit subject to GIL-ness.</span>
<span class="comment-copy">I think I need to try some C projects so I can discern just what is going on in that source code. Thanks for your further detail on GIL acquisition, very helpful. Based upon this, chunking a network I/O response to store in memory doesn't seem efficient. Since only a single thread can execute the <code>for chunk in r.iter_content()</code> at any time, if 4 threads were live with large files, there would be lots of context switching right?</span>
<span class="comment-copy">Only one thread could be doing the <code>f.write(chunk)</code> part of the loop, but if the most time spent is waiting on the network to download the next chunk in the generator's <code>__next__</code> method (something that will probably release the GIL), then you might still benefit from parallelism. I'd agree that the chunking part is not likely to improve performance, since there's more Python-level code that gets run that way.</span>
