<div class="post-text" itemprop="text">
<p>I have some Python code that executes an external app which works fine when the app has a small amount of output, but hangs when there is a lot. My code looks like:</p>
<pre><code>p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
errcode = p.wait()
retval = p.stdout.read()
errmess = p.stderr.read()
if errcode:
    log.error('cmd failed &lt;%s&gt;: %s' % (errcode,errmess))
</code></pre>
<p>There are comments in the docs that seem to indicate the potential issue. Under wait, there is:</p>
<blockquote>
<p>Warning: This will deadlock if the child process generates enough output to a <code>stdout</code> or <code>stderr</code> pipe such that it blocks waiting for the OS pipe buffer to accept more data. Use <code>communicate()</code> to avoid that.</p>
</blockquote>
<p>though under communicate, I see:</p>
<p>Note The data read is buffered in memory, so do not use this method if the data size is large or unlimited.</p>
<p>So it is unclear to me that I should use either of these if I have a large amount of data. They don't indicate what method I should use in that case.</p>
<p>I do need the return value from the exec and do parse and use both the <code>stdout</code> and <code>stderr</code>.</p>
<p>So what is an equivalent method in Python to exec an external app that is going to have large output?</p>
</div>
<div class="post-text" itemprop="text">
<p>You're doing blocking reads to two files; the first needs to complete before the second starts.  If the application writes a lot to <code>stderr</code>, and nothing to <code>stdout</code>, then your process will sit waiting for data on <code>stdout</code> that isn't coming, while the program you're running sits there waiting for the stuff it wrote to <code>stderr</code> to be read (which it never will be--since you're waiting for <code>stdout</code>).</p>
<p>There are a few ways you can fix this.</p>
<p>The simplest is to not intercept <code>stderr</code>; leave <code>stderr=None</code>.  Errors will be output to <code>stderr</code> directly.  You can't intercept them and display them as part of your own message.  For commandline tools, this is often OK.  For other apps, it can be a problem.</p>
<p>Another simple approach is to redirect <code>stderr</code> to <code>stdout</code>, so you only have one incoming file: set <code>stderr=STDOUT</code>.  This means you can't distinguish regular output from error output.  This may or may not be acceptable, depending on how the application writes output.</p>
<p>The complete and complicated way of handling this is <code>select</code> (<a href="http://docs.python.org/library/select.html" rel="nofollow noreferrer">http://docs.python.org/library/select.html</a>).  This lets you read in a non-blocking way: you get data whenever data appears on either <code>stdout</code> or <code>stderr</code>.  I'd only recommend this if it's really necessary.  This probably doesn't work in Windows.</p>
</div>
<div class="post-text" itemprop="text">
<p>Reading <code>stdout</code> and <code>stderr</code> independently with very large output (ie, lots of megabytes) using <code>select</code>:</p>
<pre><code>import subprocess, select

proc = subprocess.Popen(cmd, bufsize=8192, shell=False, \
    stdout=subprocess.PIPE, stderr=subprocess.PIPE)

with open(outpath, "wb") as outf:
    dataend = False
    while (proc.returncode is None) or (not dataend):
        proc.poll()
        dataend = False

        ready = select.select([proc.stdout, proc.stderr], [], [], 1.0)

        if proc.stderr in ready[0]:
            data = proc.stderr.read(1024)
            if len(data) &gt; 0:
                handle_stderr_data(data)

        if proc.stdout in ready[0]:
            data = proc.stdout.read(1024)
            if len(data) == 0: # Read of zero bytes means EOF
                dataend = True
            else:
                outf.write(data)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><em>A lot of output</em> is subjective so it's a little difficult to make a recommendation.  If the amount of output is <em>really</em> large then you likely don't want to grab it all with a single read() call anyway.  You may want to try writing the output to a file and then pull the data in incrementally like such:</p>
<pre><code>f=file('data.out','w')
p = subprocess.Popen(cmd, shell=True, stdout=f, stderr=subprocess.PIPE)
errcode = p.wait()
f.close()
if errcode:
    errmess = p.stderr.read()
    log.error('cmd failed &lt;%s&gt;: %s' % (errcode,errmess))
for line in file('data.out'):
    #do something
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Glenn Maynard is right in his comment about deadlocks. However, the best way of solving this problem is two create two threads, one for stdout and one for stderr, which read those respective streams until exhausted and do whatever you need with the output.</p>
<p>The suggestion of using temporary files may or may not work for you depending on the size of output etc. and whether you need to process the subprocess' output as it is generated.</p>
<p>As Heikki Toivonen has suggested, you should look at the <code>communicate</code> method. However, this buffers the stdout/stderr of the subprocess in memory and you get those returned from the <code>communicate</code> call - this is not ideal for some scenarios. But the source of the communicate method is worth looking at.</p>
<p>Another example is in a package I maintain, <a href="http://code.google.com/p/python-gnupg/" rel="noreferrer">python-gnupg</a>, where the <code>gpg</code> executable is spawned via <code>subprocess</code> to do the heavy lifting, and the Python wrapper spawns threads to read gpg's stdout and stderr and consume them as data is produced by gpg. You may be able to get some ideas by looking at the source there, as well. Data produced by gpg to both stdout and stderr can be quite large, in the general case.</p>
</div>
<div class="post-text" itemprop="text">
<p>You could try communicate and see if that solves your problem. If not, I'd redirect the output to a temporary file.</p>
</div>
<div class="post-text" itemprop="text">
<p>I had the same problem. If you have to handle a large output, another good option could be to use a file for stdout and stderr, and pass those files per parameter.</p>
<p>Check the tempfile module in python: <a href="https://docs.python.org/2/library/tempfile.html" rel="nofollow">https://docs.python.org/2/library/tempfile.html</a>.</p>
<p>Something like this might work</p>
<pre><code>out = tempfile.NamedTemporaryFile(delete=False)
</code></pre>
<p>Then you would do:</p>
<pre><code>Popen(... stdout=out,...)
</code></pre>
<p>Then you can read the file, and erase it later.</p>
</div>
<div class="post-text" itemprop="text">
<p>Here is simple approach which captures both regular output plus error output, all within Python so limitations in <code>stdout</code> don't apply:</p>
<pre><code>com_str = 'uname -a'
command = subprocess.Popen([com_str], stdout=subprocess.PIPE, shell=True)
(output, error) = command.communicate()
print output

Linux 3.11.0-20-generic SMP Fri May 2 21:32:55 UTC 2014 
</code></pre>
<p>and</p>
<pre><code>com_str = 'id'
command = subprocess.Popen([com_str], stdout=subprocess.PIPE, shell=True)
(output, error) = command.communicate()
print output

uid=1000(myname) gid=1000(mygrp) groups=1000(cell),0(root)
</code></pre>
</div>
<span class="comment-copy">It seems "large" in the communicate documentation is <i>much larger</i> than you are likely expecting, and certainly much larger than common.  For example, you can output 10MB of text and most systems would be fine with communicate.  Output of 1GB when you only have 1GB of RAM would be another story.</span>
<span class="comment-copy">Since the specific case I'm dealing with will have a lot of stdout and a small amount or no stderr, I'm going to try out the file redirection Mark suggested first, but the more complete coverage of the issue is very helpful.</span>
<span class="comment-copy">This by far makes the most sense to me in overcoming the in memory buffer issues. I even tried the subprocess <code>cmd</code> as <code>bash -c "cat /dev/urandom | tr -dc 'a-zA-Z0-9'"</code> which works great. My mental block were around what these lines mean - [1] <code>ready[0]</code> and why does [2] <code>len(proc.stdout.read(1024)) == 0</code> mean EOF? [3] Why not check the <code>len(proc.stderr.read(1024))</code>? [4] Why is read flush not needed? Sorry, several questions all lumped into one comment :/</span>
<span class="comment-copy">@neowulf33  [1] ready is a list of lists, ready[0] is the list which can contain either stdout, stderr, or both. see select docs.  [2] "An empty string is returned when EOF is encountered immediately." <a href="https://docs.python.org/2.7/library/stdtypes.html#file.read" rel="nofollow noreferrer">docs.python.org/2.7/library/stdtypes.html#file.read</a>  [3] because you'd lose the data! [4] I don't understand, how flush?</span>
<span class="comment-copy">Thanks! My bad - I was definitely half asleep when I wrote "read flush"!</span>
<span class="comment-copy">[5] why not read more than <code>1024</code> (1kb)? [6] how is <code>[proc.stdout, proc.stderr]</code> or <code>read[0]</code> related to <code>8192</code> (8kb)?  Thanks! Doc links - <a href="https://docs.python.org/3/library/subprocess.html#subprocess.Popen" rel="nofollow noreferrer">subprocess.Popen</a> and <a href="https://docs.python.org/3/library/select.html#select.select" rel="nofollow noreferrer">select.select</a></span>
<span class="comment-copy">@neowulf33 [5] [6] yes, probably the numbers I chose (1024, 8192) are kind of arbitrary, they are just large enough buffer sizes, AFAIK they don't have any special significance.</span>
<span class="comment-copy">This can also easily deadlock.  If the forked process writes more data than the OS will buffer to stderr before exiting with an error code, this code will sit forever waiting for it to exit, while the process sits on a blocking write to stderr waiting for you to read it.</span>
<span class="comment-copy">1) assumes the large data output is stderr which would be odd but not unheard of), 2) if stderr IS the source of large data amount the solution is same, make stderr a file as well</span>
<span class="comment-copy">In this instance, the process can potentially have a great deal of stdout, but will not have much, if any, stderr, so this is a reasonable solution for me.</span>
<span class="comment-copy">Will check out python-gnupg as an example. Thanks.</span>
<span class="comment-copy">Relevant links to the interesting methods - <a href="https://bitbucket.org/vinay.sajip/python-gnupg/src/952281d4c966608403a23af76429f11df9e0a852/gnupg.py?at=default&amp;fileviewer=file-view-default#gnupg.py-825" rel="nofollow noreferrer"><code>_open_subprocess</code></a> and <a href="https://bitbucket.org/vinay.sajip/python-gnupg/src/952281d4c966608403a23af76429f11df9e0a852/gnupg.py?at=default&amp;fileviewer=file-view-default#gnupg.py-903" rel="nofollow noreferrer"><code>_collect_output</code></a></span>
<span class="comment-copy">Since communicate explicitly warns away from usage if you have a great deal of output, I'm going to look at the other options.</span>
