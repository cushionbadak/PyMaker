<div class="post-text" itemprop="text">
<p>In Python, how do I read in a binary file and loop over each byte of that file?</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Python 2.4 and Earlier</strong></p>
<pre><code>f = open("myfile", "rb")
try:
    byte = f.read(1)
    while byte != "":
        # Do stuff with byte.
        byte = f.read(1)
finally:
    f.close()
</code></pre>
<p><strong>Python 2.5-2.7</strong></p>
<pre><code>with open("myfile", "rb") as f:
    byte = f.read(1)
    while byte != "":
        # Do stuff with byte.
        byte = f.read(1)
</code></pre>
<p>Note that the with statement is not available in versions of Python below 2.5. To use it in v 2.5 you'll need to import it:</p>
<pre><code>from __future__ import with_statement
</code></pre>
<p>In 2.6 this is not needed.</p>
<p><strong>Python 3</strong></p>
<p>In Python 3, it's a bit different. We will no longer get raw characters from the stream in byte mode but byte objects, thus we need to alter the condition:</p>
<pre><code>with open("myfile", "rb") as f:
    byte = f.read(1)
    while byte != b"":
        # Do stuff with byte.
        byte = f.read(1)
</code></pre>
<p>Or as benhoyt says, skip the not equal and take advantage of the fact that <code>b""</code> evaluates to false. This makes the code compatible between 2.6 and 3.x without any changes. It would also save you from changing the condition if you go from byte mode to text or the reverse.</p>
<pre><code>with open("myfile", "rb") as f:
    byte = f.read(1)
    while byte:
        # Do stuff with byte.
        byte = f.read(1)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This generator yields bytes from a file, reading the file in chunks:</p>
<pre><code>def bytes_from_file(filename, chunksize=8192):
    with open(filename, "rb") as f:
        while True:
            chunk = f.read(chunksize)
            if chunk:
                for b in chunk:
                    yield b
            else:
                break

# example:
for b in bytes_from_file('filename'):
    do_stuff_with(b)
</code></pre>
<p>See the Python documentation for information on <a href="http://docs.python.org/3/tutorial/classes.html#iterators" rel="noreferrer">iterators</a> and <a href="http://docs.python.org/3/tutorial/classes.html#generators" rel="noreferrer">generators</a>.</p>
</div>
<div class="post-text" itemprop="text">
<p>If the file is not too big that holding it in memory is a problem:</p>
<pre><code>bytes_read = open("filename", "rb").read()
for b in bytes_read:
    process_byte(b)
</code></pre>
<p>where process_byte represents some operation you want to perform on the passed-in byte.</p>
<p>If you want to process a chunk at a time:</p>
<pre><code>file = open("filename", "rb")
try:
    bytes_read = file.read(CHUNKSIZE)
    while bytes_read:
        for b in bytes_read:
            process_byte(b)
        bytes_read = file.read(CHUNKSIZE)
finally:
    file.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>To read a file — one byte at a time (ignoring the buffering)  — you could use the <a href="http://docs.python.org/3/library/functions.html#iter" rel="noreferrer">two-argument <code>iter(callable, sentinel)</code> built-in function</a>:</p>
<pre><code>with open(filename, 'rb') as file:
    for byte in iter(lambda: file.read(1), b''):
        # Do stuff with byte
</code></pre>
<p>It calls <code>file.read(1)</code> until it returns nothing <code>b''</code> (empty bytestring).  The memory doesn't grow unlimited for large files. You could pass <code>buffering=0</code>  to <code>open()</code>, to disable the buffering — it guarantees that only one byte is read per iteration (slow).</p>
<p><code>with</code>-statement closes the file automatically — including the case when the code underneath raises an exception.</p>
<p>Despite the presence of internal buffering by default, it is still inefficient to process one byte at a time. For example, here's the <code>blackhole.py</code> utility that eats everything it is given:</p>
<pre><code>#!/usr/bin/env python3
"""Discard all input. `cat &gt; /dev/null` analog."""
import sys
from functools import partial
from collections import deque

chunksize = int(sys.argv[1]) if len(sys.argv) &gt; 1 else (1 &lt;&lt; 15)
deque(iter(partial(sys.stdin.detach().read, chunksize), b''), maxlen=0)
</code></pre>
<p>Example:</p>
<pre><code>$ dd if=/dev/zero bs=1M count=1000 | python3 blackhole.py
</code></pre>
<p>It processes <em>~1.5 GB/s</em> when <code>chunksize == 32768</code> on my machine and only <em>~7.5 MB/s</em> when <code>chunksize == 1</code>. That is, it is 200 times slower to read one byte at a time. Take it into account if you can rewrite your processing to use more than one byte at a time and <em>if</em> you need performance.</p>
<p><a href="http://docs.python.org/2/library/mmap.html" rel="noreferrer"><code>mmap</code></a> allows you to treat a file as a <a href="http://docs.python.org/2/library/functions.html#bytearray" rel="noreferrer"><code>bytearray</code></a> and a file object simultaneously. It can serve as an alternative to loading the whole file in memory if you need access both interfaces. In particular, you can iterate one byte at a time over a memory-mapped file just using a plain <code>for</code>-loop:</p>
<pre><code>from mmap import ACCESS_READ, mmap

with open(filename, 'rb', 0) as f, mmap(f.fileno(), 0, access=ACCESS_READ) as s:
    for byte in s: # length is equal to the current file size
        # Do stuff with byte
</code></pre>
<p><code>mmap</code> supports the slice notation. For example, <code>mm[i:i+len]</code> returns <code>len</code> bytes from the file starting at position <code>i</code>. The context manager protocol is not supported before Python 3.2; you need to call <code>mm.close()</code> explicitly in this case. Iterating over each byte using <code>mmap</code> consumes more memory than <code>file.read(1)</code>, but <code>mmap</code> is an order of magnitude faster.</p>
</div>
<div class="post-text" itemprop="text">
<p>To sum up all the brilliant points of chrispy, Skurmedel, Ben Hoyt and Peter Hansen, this would be the optimal solution for processing a binary file one byte at a time:</p>
<pre><code>with open("myfile", "rb") as f:
    while True:
        byte = f.read(1)
        if not byte:
            break
        do_stuff_with(ord(byte))
</code></pre>
<p>For python versions 2.6 and above, because:</p>
<ul>
<li>python buffers internally - no need to read chunks </li>
<li>DRY principle - do not repeat the read line</li>
<li>with statement ensures a clean file close</li>
<li>'byte' evaluates to false when there are no more bytes (not when a byte is zero)</li>
</ul>
<p>Or use J. F. Sebastians solution for improved speed</p>
<pre><code>from functools import partial

with open(filename, 'rb') as file:
    for byte in iter(partial(file.read, 1), b''):
        # Do stuff with byte
</code></pre>
<p>Or if you want it as a generator function like demonstrated by codeape:</p>
<pre><code>def bytes_from_file(filename):
    with open(filename, "rb") as f:
        while True:
            byte = f.read(1)
            if not byte:
                break
            yield(ord(byte))

# example:
for b in bytes_from_file('filename'):
    do_stuff_with(b)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<h2>Reading binary file in Python and looping over each byte</h2>
</blockquote>
<p>New in Python 3.5 is the <code>pathlib</code> module, which has a convenience method specifically to read in a file as bytes, allowing us to iterate over the bytes. I consider this a decent (if quick and dirty) answer:</p>
<pre><code>import pathlib

for byte in pathlib.Path(path).read_bytes():
    print(byte)
</code></pre>
<p>Interesting that this is the only answer to mention <code>pathlib</code>.</p>
<p>In Python 2, you probably would do this (as Vinay Sajip also suggests):</p>
<pre><code>with open(path, 'b') as file:
    for byte in file.read():
        print(byte)
</code></pre>
<p>In the case that the file may be too large to iterate over in-memory, you would chunk it, idiomatically, using the <code>iter</code> function with the <code>callable, sentinel</code> signature - the Python 2 version:</p>
<pre><code>with open(path, 'b') as file:
    callable = lambda: file.read(1024)
    sentinel = bytes() # or b''
    for chunk in iter(callable, sentinel): 
        for byte in chunk:
            print(byte)
</code></pre>
<p>(Several other answers mention this, but few offer a sensible read size.)</p>
<h2>Best practice for large files or buffered/interactive reading</h2>
<p>Let's create a function to do this, including idiomatic uses of the standard library for Python 3.5+:</p>
<pre><code>from pathlib import Path
from functools import partial
from io import DEFAULT_BUFFER_SIZE

def file_byte_iterator(path):
    """given a path, return an iterator over the file
    that lazily loads the file
    """
    path = Path(path)
    with path.open('rb') as file:
        reader = partial(file.read1, DEFAULT_BUFFER_SIZE)
        file_iterator = iter(reader, bytes())
        for chunk in file_iterator:
            for byte in chunk:
                yield byte
</code></pre>
<p>Note that we use <code>file.read1</code>. <code>file.read</code> blocks until it gets all the bytes requested of it or <code>EOF</code>. <code>file.read1</code> allows us to avoid blocking, and it can return more quickly because of this. No other answers mention this as well.</p>
<h3>Demonstration of best practice usage:</h3>
<p>Let's make a file with a megabyte (actually mebibyte) of pseudorandom data:</p>
<pre><code>import random
import pathlib
path = 'pseudorandom_bytes'
pathobj = pathlib.Path(path)

pathobj.write_bytes(
  bytes(random.randint(0, 255) for _ in range(2**20)))
</code></pre>
<p>Now let's iterate over it and materialize it in memory: </p>
<pre><code>&gt;&gt;&gt; l = list(file_byte_iterator(path))
&gt;&gt;&gt; len(l)
1048576
</code></pre>
<p>We can inspect any part of the data, for example, the last 100 and first 100 bytes:</p>
<pre><code>&gt;&gt;&gt; l[-100:]
[208, 5, 156, 186, 58, 107, 24, 12, 75, 15, 1, 252, 216, 183, 235, 6, 136, 50, 222, 218, 7, 65, 234, 129, 240, 195, 165, 215, 245, 201, 222, 95, 87, 71, 232, 235, 36, 224, 190, 185, 12, 40, 131, 54, 79, 93, 210, 6, 154, 184, 82, 222, 80, 141, 117, 110, 254, 82, 29, 166, 91, 42, 232, 72, 231, 235, 33, 180, 238, 29, 61, 250, 38, 86, 120, 38, 49, 141, 17, 190, 191, 107, 95, 223, 222, 162, 116, 153, 232, 85, 100, 97, 41, 61, 219, 233, 237, 55, 246, 181]
&gt;&gt;&gt; l[:100]
[28, 172, 79, 126, 36, 99, 103, 191, 146, 225, 24, 48, 113, 187, 48, 185, 31, 142, 216, 187, 27, 146, 215, 61, 111, 218, 171, 4, 160, 250, 110, 51, 128, 106, 3, 10, 116, 123, 128, 31, 73, 152, 58, 49, 184, 223, 17, 176, 166, 195, 6, 35, 206, 206, 39, 231, 89, 249, 21, 112, 168, 4, 88, 169, 215, 132, 255, 168, 129, 127, 60, 252, 244, 160, 80, 155, 246, 147, 234, 227, 157, 137, 101, 84, 115, 103, 77, 44, 84, 134, 140, 77, 224, 176, 242, 254, 171, 115, 193, 29]
</code></pre>
<h3>Don't iterate by lines for binary files</h3>
<p>Don't do the following - this pulls a chunk of arbitrary size until it gets to a newline character - too slow when the chunks are too small, and possibly too large as well:</p>
<pre><code>    with open(path, 'rb') as file:
        for chunk in file: # text newline iteration - not for bytes
            for byte in chunk:
                yield byte
</code></pre>
<p>The above is only good for what are semantically human readable text files (like plain text, code, markup, markdown etc... essentially anything ascii, utf, latin, etc... encoded).</p>
</div>
<div class="post-text" itemprop="text">
<p>Python 3, read all of the file at once:</p>
<pre><code>with open("filename", "rb") as binary_file:
    # Read the whole file at once
    data = binary_file.read()
    print(data)
</code></pre>
<p>You can iterate whatever you want using <code>data</code> variable.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you have a lot of binary data to read, you might want to consider the <a href="https://docs.python.org/3/library/struct.html" rel="nofollow">struct module</a>.  It is documented as converting "between C and Python types", but of course, bytes are bytes, and whether those were created as C types does not matter.  For example, if your binary data contains two 2-byte integers and one 4-byte integer, you can read them as follows (example taken from <code>struct</code> documentation):</p>
<pre><code>&gt;&gt;&gt; struct.unpack('hhl', b'\x00\x01\x00\x02\x00\x00\x00\x03')
(1, 2, 3)
</code></pre>
<p>You might find this more convenient, faster, or both, than explicitly looping over the content of a file.</p>
</div>
<div class="post-text" itemprop="text">
<p>if you are looking for something speedy, here's a method I've been using that's worked for years:</p>
<pre><code>from array import array

with open( path, 'rb' ) as file:
    data = array( 'B', file.read() ) # buffer the file

# evaluate it's data
for byte in data:
    v = byte # int value
    c = chr(byte)
</code></pre>
<p>if you want to iterate chars instead of ints, you can simply use <code>data = file.read()</code>, which should be a bytes() object in py3.</p>
</div>
<div class="post-text" itemprop="text">
<p>After trying all the above and using the answer from @Aaron Hall, I was getting memory errors for a ~90 Mb file on a computer running Window 10, 8 Gb RAM and Python 3.5 32-bit. I was recommended by a colleague to use <code>numpy</code> instead and it works wonders.</p>
<p>By far, the fastest to read an entire binary file (that I have tested) is:</p>
<pre><code>import numpy as np

file = "binary_file.bin"
data = np.fromfile(file, 'u1')
</code></pre>
<p><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html" rel="nofollow noreferrer">Reference</a></p>
<p>Multitudes faster than any other methods so far. Hope it helps someone!</p>
</div>
<span class="comment-copy">Reading a file byte-wise is a performance nightmare. This cannot be the best solution available in python. This code should be used with care.</span>
<span class="comment-copy">@usr: Well the file objects are buffered internally, and even so this is what was asked for. Not every script needs optimal performance.</span>
<span class="comment-copy">@usr Totally agree: reading bytewise is a waste of cycles. The codeape solution looks better.</span>
<span class="comment-copy">@mezhaka: So you change it from read(1) to read(bufsize) and in the while-loop you do a for-in... the example still stands.</span>
<span class="comment-copy">@usr: the performance difference can be as much as 200 times <a href="http://stackoverflow.com/a/20014805/4279">for the code I've tried</a>.</span>
<span class="comment-copy">@codeape Just what I am looking for. But, how do you determine chunksize? Can it be an arbitrary value?</span>
<span class="comment-copy">@swdev: The example uses a chunksize of 8192 <i>Bytes</i>. The parameter for the file.read()-function simply specifies the size, i.e. the number of Bytes to be read. codeape chose <code>8192 Byte = 8 kB</code> (actually it's <code>KiB</code> but that's not as commonly known). The value is "totally" random but 8 kB seems to be an appropriate value: not too much memory is wasted and still there are not "too many" read operations as in the accepted answer by Skurmedel...</span>
<span class="comment-copy">The filesystem already buffers chunks of data, so this code is redundant.  It's better to read a byte at a time.</span>
<span class="comment-copy">While already faster than the accepted answer, this could be sped-up by another 20-25% by replacing the entire inner-most <code>for b in chunk:</code> loop with <code>yield from chunk</code>. This form of <code>yield</code> was added in Python 3.3 (see <a href="https://docs.python.org/3/reference/expressions.html#yield-expressions" rel="nofollow noreferrer"><i>Yield Expressions</i></a>).</span>
<span class="comment-copy">@stack: What you say about it being redundant doing it this way does not appear to be true in actual timing tests I have run comparing this approach to that's in Skurmedel's answer.</span>
<span class="comment-copy">Is a call to <code>close</code> missing in your first (<code>bytes_read = open("filename", "rb").read()</code>) solution?</span>
<span class="comment-copy">@Wolf yes, it is, it's just showing the broad approach.</span>
<span class="comment-copy">@Wolf: Using Python's <a href="https://www.python.org/dev/peps/pep-0343/" rel="nofollow noreferrer"><code>with</code></a> statement, which was added in 2005 to Python 2.5, would be a better, more concise, way to take care of closing the file nowadays.</span>
<span class="comment-copy">I found the last example very interesting. Too bad there's no equivalent <code>numpy</code> memory-mapped (byte) arrays.</span>
<span class="comment-copy">@martineau there is <code>numpy.memmap()</code> and you can get the data one byte at a time (ctypes.data). You could think of numpy arrays as just a little more than blobs in memory + metadata.</span>
<span class="comment-copy">jfs: Thanks, excellent news! Didn't know such a thing it existed. Great answer, BTW.</span>
<span class="comment-copy">As the linked answer says, reading/processing one byte at a time is still slow in Python even if the reads are buffered. The performance can be improved drastically if several bytes at a time could be processed as in the example in the linked answer: 1.5GB/s vs. 7.5MB/s.</span>
<span class="comment-copy">This is SO much better... thank you for doing this. I know it's not always fun to go back to a two year old answer, but I appreciate that you did it. I particularly like the "Don't iterate by lines" subheading :-)</span>
<span class="comment-copy">Hi Aaron, is there any reason why you chose to use <code>path = Path(path),     with path.open('rb') as file:</code> rather than use the built-in open function instead? They both do the same thing correct?</span>
<span class="comment-copy">@JoshuaYonathan I use the <code>Path</code> object because it's a very convenient new way to handle  paths. Instead of passing around a string into the carefully chosen "right" functions, we can simply call the methods on the path object, which essentially contains most of the important functionality you want with what is semantically a path string. With IDEs that can inspect, we can more easily get autocompletion as well. We could accomplish the same with the <code>open</code> builtin, but there are lots of upsides when writing the program for the programmer to use the <code>Path</code> object instead.</span>
<span class="comment-copy">The last method you mentioned using the function, <code>file_byte_iterator</code> is much faster than all methods I have tried on this page. Kudos to you!</span>
<span class="comment-copy">'array' is imported by 'from array import array'</span>
<span class="comment-copy">@quanly_mc yes, thanks for catching that, and sorry I forgot to include that, editing now.</span>
