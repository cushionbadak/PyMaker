<div class="post-text" itemprop="text">
<p>I use Tornado as a web server, user can submit a task through the front end page, after auditing they can start the submitted task. In this situation, i want to start an asynchronous sub process to handle the task, so i write the following code in an request handler:</p>
<pre><code>def task_handler():
    // handle task here

def start_a_process_for_task():
    p = multiprocessing.Process(target=task_handler,args=())
    p.start()
    return 0
</code></pre>
<p>I don't care about the sub process and just start a process for it and return to the front end page and tell user the task is started. The task itself will run in the background and will record it's status or results to database so user
can view on the web page later. So here i don't want to use p.join() which is blocking, but without p.join() after the task finished,the sub process becomes a defunct process and as Tornado runs as a daemon and never exits, the defunct process will never disappear. </p>
<p>Anyone knows how to fix this problem, thanks.</p>
</div>
<div class="post-text" itemprop="text">
<p>You need to join your subprocesses if you do not want to create zombies. You can do it in threads.  </p>
<p>This is a dummy example. After 10 seconds, all your subprocesses are gone instead of being zombies.  This launches a thread for every subprocess. Threads do not need to be joined or waited. A thread executes subprocess, joins it and then exits the thread as soon as the subprocess is completed. </p>
<pre><code>import multiprocessing
import threading
from time import sleep

def task_processor():
    sleep(10)

class TaskProxy(threading.Thread):
    def __init__(self):
        super(TaskProxy, self).__init__()

    def run(self):
        p = multiprocessing.Process(target=task_processor,args=())
        p.start()
        p.join()

def task_handler():
    t = TaskProxy()
    t.daemon = True
    t.start()
    return

for _ in xrange(0,20):
    task_handler()

sleep(60)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The proper way to avoid defunct children is for the parent to gracefully clean up and close all resources of the exited child. This is normally done by <code>join()</code>, but if you want to avoid that, another approach could be to set up a global handler for the <code>SIGCHLD</code> signal on the parent. </p>
<p><code>SIGCHLD</code> will be emitted whenever a child exits, and in the handler function you should either call <code>Process.join()</code> if you still have access to the process object, or even use <code>os.wait()</code> to "wait" for any child process to terminate and properly reap it. The wait time here should be 0 as you know for sure a child process has just exited. You will also be able to get the process' exit code / termination signal so it can also be a useful method to handle / log child process crashes. </p>
<p>Here's a quick example of doing this:</p>
<pre><code>from __future__ import print_function

import os
import signal
import time
from multiprocessing import Process


def child_exited(sig, frame):
    pid, exitcode = os.wait()
    print("Child process {pid} exited with code {exitcode}".format(
        pid=pid, exitcode=exitcode
    ))


def worker():
    time.sleep(5)
    print("Process {pid} has completed it's work".format(pid=os.getpid()))


def parent():
    children = []

    # Comment out the following line to see zombie children
    signal.signal(signal.SIGCHLD, child_exited)

    for i in range(5):
        c = Process(target=worker)
        c.start()
        print("Parent forked out worker process {pid}".format(pid=c.pid))
        children.append(c)
        time.sleep(1)

    print("Forked out {c} workers, hit Ctrl+C to end...".format(c=len(children)))
    while True:
        time.sleep(5)


if __name__ == '__main__':
    parent()
</code></pre>
<p>One caveat is that I am not sure if this process works on non-Unix operating systems. It should work on Linux, Mac and other Unixes.</p>
</div>
<span class="comment-copy">You should define this process in a thread and handle it the proper way or add a callback or something that will check if the process has to be joined or not.</span>
<span class="comment-copy">Given that OP already has a Tornado I/O loop, would it not be better to dump the created processes into some global list and then schedule tasks on the I/O loop to <a href="https://docs.python.org/3/library/subprocess.html#subprocess.Popen.poll" rel="nofollow noreferrer"><code>poll()</code></a> processes from that list?</span>
<span class="comment-copy">That would work as well. There is no one true way to to this. It would depend on his overall application structure. My answer demonstrates a generic way to handle this but it definitely is not the only way, and there may be better options available in each particular case.</span>
<span class="comment-copy">OKï¼Œthis also works! Thanks.</span>
<span class="comment-copy">It works, Thank you!</span>
<span class="comment-copy">If it works, mark the answer as correct or at least upvote ;)</span>
<span class="comment-copy">Hi, it works and i will upvote for it, but i will not use this method as the signal process function conflicts with  subprocess.Popen which i used a lot in my project. I searched google which tells me the Popen object has its own SIGCHLD process function.</span>
