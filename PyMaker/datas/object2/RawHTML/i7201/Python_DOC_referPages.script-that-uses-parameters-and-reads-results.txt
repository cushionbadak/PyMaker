<div class="post-text" itemprop="text">
<p>I am trying to write a script that takes in a URL with certain parameters, reads from the resulting web page a list of new URLs, and downloads them locally. I am very new to programming and have never used Python 3, so I am a little lost. </p>
<p>Here is example code to explain further: </p>
<pre><code>param1 = 
param2 = 
param3 = 

requestURL = "http://examplewebpage.com/live2/?target=param1&amp;query=param2&amp;other=param3"

html_content = urllib2.urlopen(requestURL).read()

#I don't know where to go from here
#Something that can find when a URL appears on the page and append it to a list 
#Then download everything from that list

#this can download something from a link:
#file = urllib.URLopener()
#file.retrieve(url, newfilelocation)
</code></pre>
<p>The output from the request-URL is a very long page that can be in XML or JSON and has a lot of information not necessarily needed, so some form of searching is needed to find the URLs that need to be downloaded from later. The URLs found on the page lead directly to the needed files (They end in .jpg, .cat, etc). </p>
<p>Please let me know if you need any other information! My apologies if this is confusing. </p>
<p>Also, ideally I would have the downloaded files all go to a new folder (sub-dir) created for them with the filename as the current date and time, but I think I can figure this part out myself.</p>
</div>
<div class="post-text" itemprop="text">
<p>It looks like you are trying to build something similar to a web crawler, unless you want to render the content. You should explore the source code from <a href="https://github.com/scrapy/scrapy" rel="nofollow noreferrer">scrapy</a> this will help in understanding how others wrote the similar logic. I would suggest using <a href="http://docs.python-requests.org/en/master/" rel="nofollow noreferrer">requests</a> library instead of urllib since it's easier. python library has builtin <a href="https://docs.python.org/3/library/html.parser.html" rel="nofollow noreferrer">html</a>, <a href="https://docs.python.org/3/library/json.html?highlight=json#module-json" rel="nofollow noreferrer">Json</a> and <a href="https://docs.python.org/3/library/xml.html?highlight=xml#module-xml" rel="nofollow noreferrer">XML</a> parsers. </p>
<p>You should inspect the content-type header to understand what kind of content you are trying to download if the page type is unknown. There can be alternative strategies, scrapy should give you more ideas. </p>
<p>Hope this helps. </p>
</div>
<div class="post-text" itemprop="text">
<p>I would recommend checking out <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="nofollow noreferrer">BeautifulSoup</a> for parsing the returned page.
With it, you can loop through the links and extract the link address fairly easy and append them to a list of the links.</p>
</div>
<span class="comment-copy">Are you trying to parse an HTML page for all links to other HTML pages, and download those other pages?</span>
<span class="comment-copy">Sorry, should have explained this, but the links are directly to the files that need to be downloaded. (They end in .jpg, .cat, etc). Editing question now.</span>
<span class="comment-copy">So wait do you mean use scrapy? Or just as inspiration? My lack of coding experience makes it tough to understand some of that.</span>
<span class="comment-copy">I understand you are new to python however, if you wish to build things you can get ideas from what people have done in the same area. RTFS (Read the friendly source) @moon17 is  one of the best ways.. If Scrapy seems complicated, there are others like Spider <a href="https://github.com/buckyroberts/Spider" rel="nofollow noreferrer">github.com/buckyroberts/Spider</a></span>
<span class="comment-copy">I looked into that, but isnt BeautifulSoup just for XML and HTML? It has to work on JSON too</span>
<span class="comment-copy">Yeah sorry, I don't think BeautifulSoup does the JSON.</span>
