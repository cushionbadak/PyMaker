<div class="post-text" itemprop="text">
<p>I'm trying to run a Python worker (PySpark app) which is using too much memory and my app is getting killed my YARN because of exceeding memory limits (I'm trying to lower memory usage in order to being able to spawn more workers).</p>
<p>I come from Java/Scala, so Python GC works similar than JVM in my head...</p>
<p>Is there a way to tell Python what's the amount of "available memory" it has? I mean, Java GCs when your heap size is almost-full. I want to perform the same operation on Python, so yarn doesn't kill my application because of using too much memory when that memory is garbage (I'm on Python3.3 and there are memory references @ my machine).</p>
<p>I've seen resource hard and soft limits, but no documentation say if GCs trigger on them or not. AFAIK nothing triggers GCs by memory usage, does any1 know a way to do so?</p>
<p>Thanks,</p>
</div>
<div class="post-text" itemprop="text">
<p>CPython (I assume this is the one you use) is significantly different compared to Java. The main garbage collecting method is <a href="https://en.wikipedia.org/wiki/Reference_counting" rel="nofollow noreferrer">reference counting</a>. Unless you deal with circular references (IMHO it is not common in normal PySpark workflows) you won't need full GC sweeps at all (data related objects should be collected once data is spilled / pickled).</p>
<p>Spark is also known to kill idle Python workers, even if you enable reuse option, so quite often it skips GC completely.</p>
<p>You can control CPython garbage collecting behavior using <a href="https://docs.python.org/3/library/gc.html#gc.set_threshold" rel="nofollow noreferrer"><code>set_threshold</code></a> method:</p>
<pre><code>gc.set_threshold(threshold0[, threshold1[, threshold2]]
</code></pre>
<p>or trigger GC sweep manually with <a href="https://docs.python.org/3/library/gc.html#gc.collect" rel="nofollow noreferrer"><code>collect</code></a>:</p>
<pre><code>gc.collect(generation=2)
</code></pre>
<p>but in my experience most of the GC problems in PySpark come from JVM part, not Python.</p>
</div>
<span class="comment-copy">Thanks (yet again).Yes, I'm using Cython. I saw those threshold options, but AFAIK they don't trigger on the amount of memory used, I've performed manual collects after some copies (I have some HUGE pandas memory copys, not doing normal PySpark workflow). I'm not having JVM problems because I changed the serializer to be CompressedSerializer(PickleSerializer()) as default for everything (the one in SparkContext constructor). My CPUs are vstrong/running light compared with my other concerns (mem basically), so Compression works great here (I'm even faster than without it).</span>
<span class="comment-copy">*Adding to my prev comment, so I guess there's no option to do it based on memory size. I've read that default GC triggers each X time/allocations (I don't want to wait for those, as YARN for the GCs to happen), so for now I'm going to invoke it manually as you said.   What puzzles me is that no1 mentions CompressedSerializer for PySpark, they use it as default for broadcast, but I found nothing about it in Google (I found it by lurking into the code). At least for me it works wonders.</span>
