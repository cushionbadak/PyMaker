<div class="post-text" itemprop="text">
<p>I am working on a data mining homework and I would like to apply some ensembling learning with voting. Therefore, I hoped I can get multiple copies of SVM models with a slight difference by creating them one by one because I can do the same thing like that on RNN model.
<a href="https://i.stack.imgur.com/t4rnY.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/t4rnY.png"/></a></p>
<p>However, I found that I got, for example, 30 same models after fitting my SVM 30 times, while I could get 30 slightly different models of RNN after I fit the RNN model.</p>
<p><a href="https://i.stack.imgur.com/6Xh2h.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/6Xh2h.png"/></a></p>
<p>Can you suggest any method to do the same way in SVM? Thank you very much!</p>
</div>
<div class="post-text" itemprop="text">
<h1>SVM: Max margin classifier</h1>
<p>The reason you get the same SVM model every time is because SVMs are max margin classifiers or in other words they maximize the margin separating the +ve and -ve classes. So everything you run it irrespective of the random state you start in, it always ends up finding the hyperplain whose margin to the +ve class and -ve class is maximum. </p>
<p>Other non max margin classifiers for example like a simple perceptron tries to minimize the loss where you can think of simple loss as number of data point which are wrongly classified. We normally use other kinds (differentiable) loss functions which corresponding to how confidently the model is predicting. </p>
<h1>Example</h1>
<h2>Perceptron</h2>
<pre><code>X = np.r_[np.random.randn(10, 2) - [2, 2], np.random.randn(10, 2) + [2, 2]]
y = [0] * 10 + [1] * 10

def plot_it(clf, X):     
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])

    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=.8)    
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
    plt.xticks([])
    plt.yticks([])


plt.close('all')
plt.figure()
seeds = [0,10,20,30,40,50]
for i in range(1,7):
    plt.subplot(2,3,i)    
    clf = Perceptron(random_state=seeds[i-1])
    clf.fit(X,y)    
    plot_it(clf, X)    
plt.tight_layout()
plt.show()
</code></pre>
<p><a href="https://i.stack.imgur.com/k8DZm.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/k8DZm.png"/></a></p>
<p>The above figure shows the decision boundaries identified by the perceptron with different seeds (initilizations). As you can see all the model correctly classify the data points but which model is the best ? Of course which generalizes on unseen data which will be the one which has sufficient margins around the decision boundary to cover unseen data.  This is where SVM's come to over rescue.</p>
<h2>SVM</h2>
<pre><code>plt.close('all')
plt.figure()
seeds = [0,10,20,30,40,50]
for i in range(1,7):
    plt.subplot(2,3,i)    
    clf = LinearSVC(random_state=seeds[i-1])
    clf.fit(X,y)    
    plot_it(clf, X)    
plt.tight_layout()  
plt.show()
</code></pre>
<p><a href="https://i.stack.imgur.com/8Eyyh.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/8Eyyh.png"/></a></p>
<p>As you can see irrespective of the random seed the SVM always return the same decision boundary the one which maximizes the margin. </p>
<p>With RNN you get a different model every time because RNN is not a max margin classifier. Moreover RNN convergence criteria is manual i.e we decide when to stop the training process and if we decide to run it for fixed number of epochs then depending on the weight initializations, the final weight of model will vary.  </p>
<h2>LSTM</h2>
<pre><code>import torch
from torch import nn
from torch import optim

def plot_rnn(lstm, X):     
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    p = np.c_[xx.ravel(), yy.ravel()]
    xt = torch.FloatTensor(p.reshape(-1,1,2).transpose(1, 0, 2))

    s = nn.Sigmoid()
    Z,_ = lstm(xt)
    Z = s(Z.view(len(p)))

    Z = Z.detach().numpy().reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=.8)    
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
    plt.xticks([])
    plt.yticks([])

def train(X, y):
    batch_size = 20
    input_size = 2
    time_steps = 1
    output_size = 1

    xt = torch.FloatTensor(X.reshape(batch_size,time_steps,input_size).transpose(1, 0, 2))
    yt = torch.FloatTensor(y)

    lstm = nn.LSTM(input_size, output_size, 1)
    s = nn.Sigmoid()
    loss_function = nn.BCELoss()
    optimizer = optim.SGD(lstm.parameters(), lr=0.05)

    for i in range(1000):
        lstm.zero_grad()
        y_hat,_ = lstm(xt)
        y_hat = y_hat.view(20)
        y_hat = s(y_hat)
        loss = loss_function(y_hat, yt)
        loss.backward()
        optimizer.step()
        #print (loss.data)
    return lstm

plt.close('all')
plt.figure()
for i in range(1,7):
    plt.subplot(2,3,i)    
    clf = train(X,y)    
    plot_rnn(clf, X)    

plt.tight_layout()
plt.show()
</code></pre>
<p><a href="https://i.stack.imgur.com/h6qzQ.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/h6qzQ.png"/></a></p>
</div>
<span class="comment-copy">Check out <code>random_state</code> param for <code>LinearSVC()</code></span>
<span class="comment-copy">You should read about some basics first. So why is there so much variance in one model but not the other. The answer is related to non-convex vs. Convex optimization and local vs. Global optima. Of course one could use stochastic opt for svms, but this approach is questionable in this context</span>
<span class="comment-copy">Please do <b>not</b> post code as images! -<a href="http://idownvotedbecau.se/imageofcode" rel="nofollow noreferrer">idownvotedbecau.se/imageofcode</a></span>
