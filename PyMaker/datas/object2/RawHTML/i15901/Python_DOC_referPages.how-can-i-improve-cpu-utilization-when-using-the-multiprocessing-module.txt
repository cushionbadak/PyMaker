<div class="post-text" itemprop="text">
<p>I am working in Python 3.4, performing a naive search against partitioned data in memory, and am attempting to fork processes to take advantage of all available processing power.  I say naive, because I am certain there are other additional things that can be done to improve performance, but those potentials are out of scope for the question at hand.  </p>
<p>The system I am testing on is a Windows 7 x64 environment.</p>
<p>What I would like to achieve is a relatively even, simultaneous distribution across <code>cpu_count() - 1</code> cores (reading suggests that distributing against all cores rather than n-1 cores does not show any additional improvement due to baseline os system processes). So 75% pegged cpu Usage for a 4 core machine. </p>
<p>What I am seeing (using windows task manager 'performance tab' and the 'process tab') is that <strong>I never achieve greater than 25% system dedicated cpu utilization</strong> and that the process view shows computation occurring one core at a time, switching every few seconds between the forked processes.</p>
<p>I haven't instrumented the code for timing, but I am pretty sure that my subjective observations are correct in that I am not gaining the performance increase I expected (3x on an i5 3320m).</p>
<p>I haven't tested on Linux.</p>
<p>Based on the code presented:
- How can I achieve 75% CPU utilization?</p>
<pre><code>#pseudo code
def search_method(search_term, partition):
    &lt;perform fuzzy search&gt;
    return results

partitions = [&lt;list of lists&gt;]
search_terms = [&lt;list of search terms&gt;]

#real code
import multiprocessing as mp

pool = mp.Pool(processes=mp.cpu_count() - 1)

for search_term in search_terms:
    results = []
    results = [pool.apply(search_method, args=(search_term, partitions[x])) for x in range(len(partitions))]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You're actually not doing anything concurrently here, because you're using <code>pool.apply</code>, which will block until the task you pass to it is complete. So, for every item in <code>partitions</code>, you're running <code>search_method</code> in some process inside of <code>pool</code>, waiting for it to complete, and then moving on to the next item. That perfectly coincides with what you're seeing in the Windows process manager. You want <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.apply_async" rel="nofollow"><code>pool.apply_async</code></a> instead:</p>
<pre><code>for search_term in search_terms:
    results = []
    results = [pool.apply_async(search_method, args=(search_term, partitions[x])) for x in range(len(partitions))]

    # Get the actual results from the AsyncResult objects returned.
    results = [r.get() for r in results]
</code></pre>
<p>Or better yet, use <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map" rel="nofollow"><code>pool.map</code></a> (along with <a href="https://docs.python.org/3/library/functools.html#functools.partial" rel="nofollow"><code>functools.partial</code></a> to enable passing multiple arguments to our worker function):</p>
<pre><code>from functools import partial
...

for search_term in search_terms:
    func = partial(search_method, search_term)
    results = pool.map(func, partitions)
</code></pre>
</div>
<span class="comment-copy">You may want to know, that some <code>scikit-learn</code> functions have in-built options for going on multiple local-host cores. The point is, whether the solver computational strategy allows for non-intervening parallelised processing or not. The <code>multiprocessing</code> module has no clue whether it is possible to split the problem into more non-intervening parallel code-execution streams ( not speaking about data-access mechanics )</span>
<span class="comment-copy">If your problem allows, there may be a more powerfull approach, to use a cloud of workers ( all based on python, distributed on multi-host, multi-CPU/multi-core infrastructure ), that can perform your <code>&lt;_search_method_&gt;</code> for a <code>&lt;_search_term_&gt;</code> on a given <code>&lt;_list_of_lists_&gt;</code>. Thus you may harness 10x, 100x, 1000x more CPU/core-s into such a privateCloud/Grid-engine tasking.</span>
<span class="comment-copy">this answers the specific question at hand. Thank you! I've been looking for an excuse to jump into cloud/grid computation and this may be the jumping off place...</span>
