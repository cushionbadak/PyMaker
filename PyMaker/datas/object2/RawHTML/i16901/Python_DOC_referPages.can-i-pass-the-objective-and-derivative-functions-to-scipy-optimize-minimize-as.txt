<div class="post-text" itemprop="text">
<p>I'm trying to use <code>scipy.optimize.minimize</code> to minimize a complicated function. I noticed in hindsight that the <code>minimize</code> function takes the objective and derivative functions as separate arguments. Unfortunately, I've already defined a function which returns the objective function value and first-derivative values together -- because the two are computed simultaneously in a <code>for</code> loop. I don't think there is a good way to separate my function into two without the program essentially running the same <code>for</code> loop twice.</p>
<p>Is there a way to pass this combined function to <code>minimize</code>?</p>
<p>(FYI, I'm writing an artificial neural network backpropagation algorithm, so the <code>for</code> loop is used to loop over training data. The objective and derivatives are accumulated concurrently.)</p>
</div>
<div class="post-text" itemprop="text">
<p>Something that might work is: you can memoize the function, meaning that if it gets called with the same inputs a second time, it will simply return the same outputs corresponding to those inputs without doing any actual work the second time.  What is happening behind the scenes is that the results are getting cached.  In the context of a nonlinear program, there could be thousands of calls which implies a large cache.  Often with memoizers(?), you can specify a cache limit and the population will be managed FIFO.  IOW you still benefit fully for your particular case because the inputs will be the same only when you are needing to return function value and derivative around the same point in time.  So what I'm getting at is that a small cache should suffice.</p>
<p>You don't say whether you are using py2 or py3.  In Py 3.2+, you can use <a href="http://docs.python.org/3/library/functools.html" rel="nofollow">functools.lru_cache</a> as a decorator to provide this memoization.  Then, you write your code like this:</p>
<pre><code>@functools.lru_cache
def original_fn(x):
   blah
   return fnvalue, fnderiv

def new_fn_value(x):
   fnvalue, fnderiv = original_fn(x)
   return fnvalue

def new_fn_deriv(x):
   fnvalue, fnderiv = original_fn(x)
   return fnderiv
</code></pre>
<p>Then you pass each of the new functions to <code>minimize</code>.  You still have a penalty because of the second call, but it will do no work if <code>x</code> is unchanged.  You <strong>will</strong> need to research what <em>unchanged</em> means in the context of floating point numbers, particularly since the change in <code>x</code> will fall away as the minimization begins to converge.</p>
<p>There are lots of recipes for memoization in py2.x if you look around a bit. </p>
<p>Did I make any sense at all?</p>
</div>
<div class="post-text" itemprop="text">
<p>Yes, you can pass them in a single function:</p>
<pre><code>import numpy as np
from scipy.optimize import minimize

def f(x):
    return np.sin(x) + x**2, np.cos(x) + 2*x

sol = minimize(f, [0], jac=True, method='L-BFGS-B')
</code></pre>
</div>
<span class="comment-copy">Why not use fmin_l_bfgs_b?</span>
<span class="comment-copy">Thanks, I didn't know this existed. I'll try it and the memoization suggestion below.</span>
<span class="comment-copy">Neither are necessary, as minimize() can do this for you.</span>
<span class="comment-copy">Thanks, never knew about memoization. It seems to make sense. I'll have to play around with it a bit. Btw, I'm running py2.7.</span>
