<div class="post-text" itemprop="text">
<div class="question-status question-originals-of-duplicate">
<p>This question already has an answer here:</p>
<ul>
<li>
<a dir="ltr" href="/questions/3346430/what-is-the-most-efficient-way-to-get-first-and-last-line-of-a-text-file">What is the most efficient way to get first and last line of a text file?</a>
<span class="question-originals-answer-count">
                    12 answers
                </span>
</li>
<li>
<a dir="ltr" href="/questions/2301789/read-a-file-in-reverse-order-using-python">Read a file in reverse order using python</a>
<span class="question-originals-answer-count">
                    18 answers
                </span>
</li>
</ul>
</div>
<p>I'm trying to iterate over a very large, ever-changing file (typically around 1.5M lines) and perform operations on each line. It's a log file, so new lines are appended at the end of the file. My program will allow users to specify parameters each line must match and return the most recent matches. As a result, I'd like to start at the end of the file and work up to make the program efficient (instead of making a list of lines and reversing it).</p>
<p>Here is an example situation:</p>
<pre><code>2016-01-01 01:00 apple

2016-01-02 05:00 banana

2016-01-03 03:00 apple

2016-01-04 00:00 apple

2016-01-05 12:00 banana
</code></pre>
<p>If a user requested 1 line that matched "apple," I'd like to return "2016-01-04 00:00 apple," the line closest to the end of the file. This is not difficult when there are only five lines, but performance suffers when there are millions. I've tried using <code>tail -n [file size]</code> to start at the end of the file, but this method does not scale well; I cannot use an iteration to improve performance (if the result is the last line in the file, I don't want to iterate through 1,500,000 lines).</p>
<p>Another method I've tried is breaking the file into "chunks":</p>
<pre><code>|
| Remaining lines
|

...

|
| Second group of n lines
|

|
| First group of n lines
|
</code></pre>
<p>I would then use GNU <code>sed</code> to stream only the lines in each chunk. I found, however, that the performance of the program had hardly improved (and actually suffered when <em>n</em> was smaller).</p>
<p>Is there a better way of doing this (minimizing run-time while iterating over a file)? I've been using other programs from the Linux command line (through "subprocess"), but it may be nice to use something built into Python. I would much appreciate any information that would lead me in the right direction.</p>
<p>I am using Linux with access to Python 2.7.3, 2.7.10, 2.7.11-c7, 3.3.6, and 3.5.1.</p>
</div>
<div class="post-text" itemprop="text">
<p>After you open a file, you can use the file handle's <code>seek(bytes, start_point)</code> method to skip to an arbitrary location in the file, denoted by a number of bytes. For example:</p>
<pre><code>with open(my_file) as f:
    f.seek(1024, 0)
    for line in f:
        print(line)
</code></pre>
<p>This will print every line in the file, except for the first kilobyte. If you supply a negative number, it'll go backwards, and supplying a value of <code>2</code> to the second argument will make it count from the end of the file. Hence, a call of <code>f.seek(-1024, 2)</code> would have caused the above to only print the last kilobyte of the file.</p>
<p>May need some security measures to prevent it from dying when the file is smaller than your chunk size, but that's how I'd do it. (And if it turns out that you need to go back further, that's also quite trivial: simply call <code>seek</code> again.)</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use:</p>
<pre><code>for line in reversed(open("filename").readlines()):
    print line.rstrip()
</code></pre>
<p>And in Python 3:</p>
<pre><code>for line in reversed(list(open("filename"))):
    print(line.rstrip())
</code></pre>
<p>This was already answered here: <a href="https://stackoverflow.com/questions/2301789/read-a-file-in-reverse-order-using-python">Read a file in reverse order using python</a></p>
</div>
<span class="comment-copy">This question gets asked a lot - like, <i>a lot</i>. Have you googled yet?</span>
<span class="comment-copy">Possible dublicate: <a href="http://stackoverflow.com/questions/3346430/what-is-the-most-efficient-way-to-get-first-and-last-line-of-a-text-file" title="what is the most efficient way to get first and last line of a text file">stackoverflow.com/questions/3346430/â€¦</a></span>
<span class="comment-copy">Yes, of course. The issue is limiting the number of lines that are viewed (i.e. if a user wants 5 results and they are found in the first 10 lines opened, I don't want to read the rest of the file) and working backward through the file. Is simply iterating through the file (<code>for line in reversed(open(file).readlines())</code>) my best option?</span>
<span class="comment-copy">@robben , I suggest that  <a href="http://stackoverflow.com/a/23646049/8747">stackoverflow.com/a/23646049/8747</a> is a better option for you than <code>reversed(lines)</code>.</span>
<span class="comment-copy">Thanks, Rob! The answer you linked works perfectly. On average, I can read 115,000 lines per second per core, which is far better performance than I had hoped for.</span>
<span class="comment-copy">But, heed <a href="https://docs.python.org/3/tutorial/inputoutput.html#methods-of-file-objects" rel="nofollow noreferrer">this warning</a>: "In text files (those opened without a <code>b</code> in the mode string), only seeks relative to the beginning of the file are allowed (the exception being seeking to the very file end with <code>seek(0, 2)</code>) and the only valid offset values are those returned from the <code>f.tell()</code>, or zero. Any other offset value produces undefined behaviour."</span>
<span class="comment-copy">Probably because <code>tell</code> and <code>seek</code> use byte offsets. If you pass an arbitrary value you may end up half-way through a multibyte character. Very good point, especially as this bug will be hard to find.</span>
<span class="comment-copy">This will be very very slow for large files.</span>
