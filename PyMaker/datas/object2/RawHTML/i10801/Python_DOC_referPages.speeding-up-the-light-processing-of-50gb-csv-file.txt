<div class="post-text" itemprop="text">
<p>I have a ~50GB csv file with which I have to</p>
<ul>
<li>Take several subsets of the columns of the CSV</li>
<li>Apply a different format string specification to each subset of columns of the CSV. </li>
<li>Output a new CSV for each subset with its own format specification.  </li>
</ul>
<p>I opted to use Pandas, and have a general approach of iterating over chunks of a handy chunk-size (of just over half a million lines) to produce a DataFrame, and appending the chunk to each output CSV. So something like this:</p>
<pre><code>_chunk_size = 630100

column_mapping = {
    'first_output_specification' : ['Scen', 'MS', 'Time', 'CCF2', 'ESW10'],
    # ..... similar mappings for rest of output specifications
}
union_of_used_cols = ['Scen', 'MS', 'Time', 'CCF1', 'CCF2', 'VS', 'ESW 0.00397', 'ESW0.08',
                    'ESW0.25', 'ESW1', 'ESW 2', 'ESW3', 'ESW 5', 'ESW7', 'ESW 10', 'ESW12',
                    'ESW 15', 'ESW18', 'ESW 20', 'ESW22', 'ESW 25', 'ESW30', 'ESW 35', 
                    'ESW40']

chnk_iter = pd.read_csv('my_big_csv.csv', header=0, index_col=False,
                        iterator=True, na_filter=False, usecols=union_of_used_cols)

cnt = 0
while cnt &lt; 100:
    chnk = chnk_iter.get_chunk(_chunk_size)
    chnk.to_csv('first_output_specification', float_format='%.8f',
                columns=column_mapping['first_output_specification'],
                mode='a',
                header=True,
                index=False)
    # ..... do the same thing for the rest of the output specifications

    cnt += 1
</code></pre>
<p><strong>My problem</strong> is that this is <em>really</em> slow. Each chunk takes about a minute to generate append to the CSV files for, and thus I'm looking at almost 2 hours for the task to complete. </p>
<p>I have tried to place a few optimizations by only using the union of the column subsets when reading in the CSV, as well as setting <code>na_filter=False</code>, but it still isn't acceptable. </p>
<p>I was wondering if there is a faster way to do this light processing of a CSV file in Python, either by means of an optimization or correction to my approach or perhaps simply there is a better tool suited for this kind of job then Pandas... to me (<em>an inexperienced Pandas user</em>) this looks like it is as fast as it could get with Pandas, but I may very well be mistaken. </p>
</div>
<div class="post-text" itemprop="text">
<p>I don't think you're getting any advantage from a Panda's dataframe, so it is just adding overhead.  Instead, you can use python's own <a href="https://docs.python.org/3/library/csv.html" rel="nofollow">CSV module</a> that is easy to use and nicely optimized in C.</p>
<p>Consider reading much larger chunks into memory (perhaps 10MB at a time), then writing-out each of the reformatted column subsets before advancing to the next chunk.  That way, the input file only gets read and parsed once.</p>
<p>One other approach you could try is to preprocess the data with the Unix <a href="http://linux.die.net/man/1/cut" rel="nofollow"><em>cut</em></a> command to extract only the relevant columns (so that Python doesn't have to create objects and allocate memory for data in the unused columns): <code>cut -d, -f1,3,5 somedata.csv</code></p>
<p>Lastly, try running the code under <a href="http://pypy.org/" rel="nofollow">PyPy</a> so that the CPU bound portion of your script gets optimized through their tracing JIT.</p>
</div>
<div class="post-text" itemprop="text">
<p>I would try using the python csv module and generators.</p>
<p>I've found generators much faster than other approaches for parsing huge server logs and such.</p>
<pre><code>import csv

def reader(csv_filename):
    with open(csv_filename, 'r') as f:
        csvreader = csv.reader(f, delimiter=',', quotechar="'")
        for line in csvreader:
            yield line  # line is a tuple

def formatter(lines):
    for line in lines:
        # format line according to specs
        yield formatted_line

def write(lines, csv_filename):
    with open(csv_filename, 'w') as f:
        writer = csv.writer(f)
        for line in lines:
            writer.writerow(line)

 lines = reader('myfile.in.csv')
 formatted_lines = formatter(lines)
 write(formatted_lines, 'myfile.out.csv')
</code></pre>
<p>This is just for reading a transforming a single input csv into a single output csv, but you could write the formatter and writer to output several files.</p>
<p>(I now see that this question is a month old - not sure if you've solved your problem already -  if not and if you want more detailed explanations/examples let me know.)</p>
</div>
<div class="post-text" itemprop="text">
<p>CPU is faster than disk access. One trick is to gzip your file and read from that.</p>
<pre><code>import gzip

with gzip.open('input.gz','r') as fin:
    for line in fin:
        print('got line', line)
</code></pre>
</div>
<span class="comment-copy">Is it possible to switch to a database approach ? This is one big csv file !</span>
<span class="comment-copy">@Jylo I really wish that were the case, but no.</span>
<span class="comment-copy">Does <code>chunksize</code> contain the number of rows you want to have in each chunk?</span>
<span class="comment-copy">Related thread with interesting info: <a href="http://softwarerecs.stackexchange.com/questions/7463/fastest-python-library-to-read-a-csv-file" title="fastest python library to read a csv file">softwarerecs.stackexchange.com/questions/7463/â€¦</a></span>
<span class="comment-copy">@albert It definitely looks like the bottleneck is writing to csv with <code>to_csv</code> : simply chunking the input into dataframes and doing nothing with them is extremely quick,. I've now found. So I'm not sure if the PowerShell file splitting would make any difference.</span>
