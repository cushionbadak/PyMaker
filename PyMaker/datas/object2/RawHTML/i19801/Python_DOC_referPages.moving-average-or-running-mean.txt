<div class="post-text" itemprop="text">
<p>Is there a scipy function or numpy function or module for python that calculates the running mean of a 1D array given a specific window?</p>
</div>
<div class="post-text" itemprop="text">
<p>For a short, fast solution that does the whole thing in one loop, without dependencies, the code below works great.</p>
<pre><code>mylist = [1, 2, 3, 4, 5, 6, 7]
N = 3
cumsum, moving_aves = [0], []

for i, x in enumerate(mylist, 1):
    cumsum.append(cumsum[i-1] + x)
    if i&gt;=N:
        moving_ave = (cumsum[i] - cumsum[i-N])/N
        #can do stuff with moving_ave here
        moving_aves.append(moving_ave)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>UPD:</strong> more efficient solutions have been proposed by <a href="https://stackoverflow.com/a/27681394/675674">Alleo</a> and <a href="https://stackoverflow.com/a/30141358/675674">jasaarim</a>.</p>
<hr/>
<p>You can use <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html" rel="noreferrer"><code>np.convolve</code></a> for that:  </p>
<pre><code>np.convolve(x, np.ones((N,))/N, mode='valid')
</code></pre>
<h1>Explanation</h1>
<p>The running mean is a case of the mathematical operation of <a href="https://en.wikipedia.org/wiki/Convolution" rel="noreferrer">convolution</a>. For the running mean, you slide a window along the input and compute the mean of the window's contents. For discrete 1D signals, convolution is the same thing, except instead of the mean you compute an arbitrary linear combination, i.e. multiply each element by a corresponding coefficient and add up the results. Those coefficients, one for each position in the window, are sometimes called the convolution <em>kernel</em>. Now, the arithmetic mean of N values is <code>(x_1 + x_2 + ... + x_N) / N</code>, so the corresponding kernel is <code>(1/N, 1/N, ..., 1/N)</code>, and that's exactly what we get by using <code>np.ones((N,))/N</code>.</p>
<h1>Edges</h1>
<p>The <code>mode</code> argument of <code>np.convolve</code> specifies how to handle the edges. I chose the <code>valid</code> mode here because I think that's how most people expect the running mean to work, but you may have other priorities. Here is a plot that illustrates the difference between the modes:</p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
modes = ['full', 'same', 'valid']
for m in modes:
    plt.plot(np.convolve(np.ones((200,)), np.ones((50,))/50, mode=m));
plt.axis([-10, 251, -.1, 1.1]);
plt.legend(modes, loc='lower center');
plt.show()
</code></pre>
<p><img alt="Running mean convolve modes" src="https://i.stack.imgur.com/IMt8g.png"/></p>
</div>
<div class="post-text" itemprop="text">
<h2>Efficient solution</h2>
<p>Convolution is much better than straightforward approach, but (I guess) it uses FFT and thus quite slow. However specially for computing the running mean the following approach works fine</p>
<pre><code>def running_mean(x, N):
    cumsum = numpy.cumsum(numpy.insert(x, 0, 0)) 
    return (cumsum[N:] - cumsum[:-N]) / float(N)
</code></pre>
<p>The code to check</p>
<pre><code>In[3]: x = numpy.random.random(100000)
In[4]: N = 1000
In[5]: %timeit result1 = numpy.convolve(x, numpy.ones((N,))/N, mode='valid')
10 loops, best of 3: 41.4 ms per loop
In[6]: %timeit result2 = running_mean(x, N)
1000 loops, best of 3: 1.04 ms per loop
</code></pre>
<p>Note that <code>numpy.allclose(result1, result2)</code> is <code>True</code>, two methods are equivalent.
The greater N, the greater difference in time.</p>
</div>
<div class="post-text" itemprop="text">
<p><strong>Update:</strong> The example below shows the old <code>pandas.rolling_mean</code> function which has been removed in recent versions of pandas. A modern equivalent of the function call below would be</p>
<pre class="lang-py prettyprint-override"><code>In [8]: pd.Series(x).rolling(window=N).mean().iloc[N-1:].values
Out[8]: 
array([ 0.49815397,  0.49844183,  0.49840518, ...,  0.49488191,
        0.49456679,  0.49427121])
</code></pre>
<hr/>
<p><a href="http://pandas.pydata.org/" rel="nofollow noreferrer">pandas</a> is more suitable for this than NumPy or SciPy.  Its function <a href="http://pandas.pydata.org/pandas-docs/stable/computation.html#moving-rolling-statistics-moments" rel="nofollow noreferrer">rolling_mean</a> does the job conveniently.  It also returns a NumPy array when the input is an array.</p>
<p>It is difficult to beat <code>rolling_mean</code> in performance with any custom pure Python implementation.  Here is an example performance against two of the proposed solutions:  </p>
<pre class="lang-py prettyprint-override"><code>In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: def running_mean(x, N):
   ...:     cumsum = np.cumsum(np.insert(x, 0, 0)) 
   ...:     return (cumsum[N:] - cumsum[:-N]) / N
   ...:

In [4]: x = np.random.random(100000)

In [5]: N = 1000

In [6]: %timeit np.convolve(x, np.ones((N,))/N, mode='valid')
10 loops, best of 3: 172 ms per loop

In [7]: %timeit running_mean(x, N)
100 loops, best of 3: 6.72 ms per loop

In [8]: %timeit pd.rolling_mean(x, N)[N-1:]
100 loops, best of 3: 4.74 ms per loop

In [9]: np.allclose(pd.rolling_mean(x, N)[N-1:], running_mean(x, N))
Out[9]: True
</code></pre>
<p>There are also nice options as to how to deal with the edge values.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can calculate a running mean with:</p>
<pre><code>import numpy as np

def runningMean(x, N):
    y = np.zeros((len(x),))
    for ctr in range(len(x)):
         y[ctr] = np.sum(x[ctr:(ctr+N)])
    return y/N
</code></pre>
<p>But it's slow.</p>
<p>Fortunately, numpy includes a <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html" rel="noreferrer">convolve</a> function which we can use to speed things up. The running mean is equivalent to convolving <code>x</code> with a vector that is <code>N</code> long, with all members equal to <code>1/N</code>. The numpy implementation of convolve includes the starting transient, so you have to remove the first N-1 points:</p>
<pre><code>def runningMeanFast(x, N):
    return np.convolve(x, np.ones((N,))/N)[(N-1):]
</code></pre>
<p>On my machine, the fast version is 20-30 times faster, depending on the length of the input vector and size of the averaging window.</p>
<p>Note that convolve does include a <code>'same'</code> mode which seems like it should address the starting transient issue,  but it splits it between the beginning and end.</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>or module for python that calculates</p>
</blockquote>
<p>in my tests at Tradewave.net TA-lib always wins:</p>
<pre><code>import talib as ta
import numpy as np
import pandas as pd
import scipy
from scipy import signal
import time as t

PAIR = info.primary_pair
PERIOD = 30

def initialize():
    storage.reset()
    storage.elapsed = storage.get('elapsed', [0,0,0,0,0,0])

def cumsum_sma(array, period):
    ret = np.cumsum(array, dtype=float)
    ret[period:] = ret[period:] - ret[:-period]
    return ret[period - 1:] / period

def pandas_sma(array, period):
    return pd.rolling_mean(array, period)

def api_sma(array, period):
    # this method is native to Tradewave and does NOT return an array
    return (data[PAIR].ma(PERIOD))

def talib_sma(array, period):
    return ta.MA(array, period)

def convolve_sma(array, period):
    return np.convolve(array, np.ones((period,))/period, mode='valid')

def fftconvolve_sma(array, period):    
    return scipy.signal.fftconvolve(
        array, np.ones((period,))/period, mode='valid')    

def tick():

    close = data[PAIR].warmup_period('close')

    t1 = t.time()
    sma_api = api_sma(close, PERIOD)
    t2 = t.time()
    sma_cumsum = cumsum_sma(close, PERIOD)
    t3 = t.time()
    sma_pandas = pandas_sma(close, PERIOD)
    t4 = t.time()
    sma_talib = talib_sma(close, PERIOD)
    t5 = t.time()
    sma_convolve = convolve_sma(close, PERIOD)
    t6 = t.time()
    sma_fftconvolve = fftconvolve_sma(close, PERIOD)
    t7 = t.time()

    storage.elapsed[-1] = storage.elapsed[-1] + t2-t1
    storage.elapsed[-2] = storage.elapsed[-2] + t3-t2
    storage.elapsed[-3] = storage.elapsed[-3] + t4-t3
    storage.elapsed[-4] = storage.elapsed[-4] + t5-t4
    storage.elapsed[-5] = storage.elapsed[-5] + t6-t5    
    storage.elapsed[-6] = storage.elapsed[-6] + t7-t6        

    plot('sma_api', sma_api)  
    plot('sma_cumsum', sma_cumsum[-5])
    plot('sma_pandas', sma_pandas[-10])
    plot('sma_talib', sma_talib[-15])
    plot('sma_convolve', sma_convolve[-20])    
    plot('sma_fftconvolve', sma_fftconvolve[-25])

def stop():

    log('ticks....: %s' % info.max_ticks)

    log('api......: %.5f' % storage.elapsed[-1])
    log('cumsum...: %.5f' % storage.elapsed[-2])
    log('pandas...: %.5f' % storage.elapsed[-3])
    log('talib....: %.5f' % storage.elapsed[-4])
    log('convolve.: %.5f' % storage.elapsed[-5])    
    log('fft......: %.5f' % storage.elapsed[-6])
</code></pre>
<p>results:</p>
<pre><code>[2015-01-31 23:00:00] ticks....: 744
[2015-01-31 23:00:00] api......: 0.16445
[2015-01-31 23:00:00] cumsum...: 0.03189
[2015-01-31 23:00:00] pandas...: 0.03677
[2015-01-31 23:00:00] talib....: 0.00700  # &lt;&lt;&lt; Winner!
[2015-01-31 23:00:00] convolve.: 0.04871
[2015-01-31 23:00:00] fft......: 0.22306
</code></pre>
<p><a href="https://i.stack.imgur.com/PkKQq.png"><img alt="enter image description here" src="https://i.stack.imgur.com/PkKQq.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>For a ready-to-use solution, see <a href="https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html" rel="nofollow noreferrer">https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html</a>.
It provides running average with the <code>flat</code> window type. Note that this is a bit more sophisticated than the simple do-it-yourself convolve-method, since it tries to handle the problems at the beginning and the end of the data by reflecting it (which may or may not work in your case...).</p>
<p>To start with, you could try:</p>
<pre><code>a = np.random.random(100)
plt.plot(a)
b = smooth(a, window='flat')
plt.plot(b)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I know this is an old question, but here is a solution that doesn't use any extra data structures or libraries. It is linear in the number of elements of the input list and I cannot think of any other way to make it more efficient (actually if anyone knows of a better way to allocate the result, please let me know).</p>
<p><strong>NOTE:</strong> this would be much faster using a numpy array instead of a list, but I wanted to eliminate all dependencies. It would also be possible to improve performance by multi-threaded execution</p>
<p>The function assumes that the input list is one dimensional, so be careful.</p>
<pre><code>### Running mean/Moving average
def running_mean(l, N):
    sum = 0
    result = list( 0 for x in l)

    for i in range( 0, N ):
        sum = sum + l[i]
        result[i] = sum / (i+1)

    for i in range( N, len(l) ):
        sum = sum - l[i-N] + l[i]
        result[i] = sum / N

    return result
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If it is important to keep the dimensions of the input (instead of restricting the output to the <code>'valid'</code> area of a convolution), you can use <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.filters.uniform_filter1d.html" rel="noreferrer">scipy.ndimage.filters.uniform_filter1d</a>:</p>
<pre><code>import numpy as np
from scipy.ndimage.filters import uniform_filter1d
N = 1000
x = np.random.random(100000)
y = uniform_filter1d(x, size=N)

y.shape == x.shape
&gt;&gt;&gt; True
</code></pre>
<p><code>uniform_filter1d</code> allows multiple ways to handle the border where <code>'reflect'</code> is the default, but in my case, I rather wanted <code>'nearest'</code>.</p>
<p>It is also rather quick (nearly 50 times faster than <code>np.convolve</code>):</p>
<pre><code>%timeit y1 = np.convolve(x, np.ones((N,))/N, mode='same')
100 loops, best of 3: 9.28 ms per loop

%timeit y2 = uniform_filter1d(x, size=N)
10000 loops, best of 3: 191 µs per loop
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I haven't yet checked how fast this is, but you could try:</p>
<pre><code>from collections import deque

cache = deque() # keep track of seen values
n = 10          # window size
A = xrange(100) # some dummy iterable
cum_sum = 0     # initialize cumulative sum

for t, val in enumerate(A, 1):
    cache.append(val)
    cum_sum += val
    if t &lt; n:
        avg = cum_sum / float(t)
    else:                           # if window is saturated,
        cum_sum -= cache.popleft()  # subtract oldest value
        avg = cum_sum / float(n)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>A bit late to the party, but I've made my own little function that does NOT wrap around the ends or pads with zeroes that are then used to find the average as well. As a further treat is, that it also re-samples the signal at linearly spaced points. Customize the code at will to get other features.</p>
<p>The method is a simple matrix multiplication with a normalized Gaussian kernel.</p>
<pre><code>def running_mean(y_in, x_in, N_out=101, sigma=1):
    '''
    Returns running mean as a Bell-curve weighted average at evenly spaced
    points. Does NOT wrap signal around, or pad with zeros.

    Arguments:
    y_in -- y values, the values to be smoothed and re-sampled
    x_in -- x values for array

    Keyword arguments:
    N_out -- NoOf elements in resampled array.
    sigma -- 'Width' of Bell-curve in units of param x .
    '''
    N_in = size(y_in)

    # Gaussian kernel
    x_out = np.linspace(np.min(x_in), np.max(x_in), N_out)
    x_in_mesh, x_out_mesh = np.meshgrid(x_in, x_out)
    gauss_kernel = np.exp(-np.square(x_in_mesh - x_out_mesh) / (2 * sigma**2))
    # Normalize kernel, such that the sum is one along axis 1
    normalization = np.tile(np.reshape(sum(gauss_kernel, axis=1), (N_out, 1)), (1, N_in))
    gauss_kernel_normalized = gauss_kernel / normalization
    # Perform running average as a linear operation
    y_out = gauss_kernel_normalized @ y_in

    return y_out, x_out
</code></pre>
<p>A simple usage on a sinusoidal signal with added normal distributed noise:
<a href="https://i.stack.imgur.com/9jgMM.png" rel="nofollow"><img alt="enter image description here" src="https://i.stack.imgur.com/9jgMM.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>Instead of numpy or scipy, I would recommend pandas to do this more swiftly:</p>
<pre><code>df['data'].rolling(3).mean()
</code></pre>
<p>This takes the moving average (MA) of 3 periods of the column "data". You can also calculate the shifted versions, for example the one that excludes the current cell (shifted one back) can be calculated easily as:</p>
<pre><code>df['data'].shift(periods=1).rolling(3).mean()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>Another</strong> approach to find moving average <em>without</em> using numpy, panda</p>
<pre><code>import itertools
sample = [2, 6, 10, 8, 11, 10]
list(itertools.starmap(lambda a,b: b/a, 
               enumerate(itertools.accumulate(sample), 1)))
</code></pre>
<p>will print [2.0, 4.0, 6.0, 6.5, 7.4, 7.833333333333333]</p>
</div>
<div class="post-text" itemprop="text">
<p>This question is now <em>even older</em> than when NeXuS wrote about it last month, BUT I like how his code deals with edge cases. However, because it is a "simple moving average," its results lag behind the data they apply to. I thought that dealing with edge cases in a more satisfying way than NumPy's modes <code>valid</code>, <code>same</code>, and <code>full</code> could be achieved by applying a similar approach to a <code>convolution()</code> based method.</p>
<p>My contribution uses a central running average to align its results with their data. When there are too few points available for the full-sized window to be used, running averages are computed from successively smaller windows at the edges of the array. [Actually, from successively larger windows, but that's an implementation detail.]</p>
<pre><code>import numpy as np

def running_mean(l, N):
    # Also works for the(strictly invalid) cases when N is even.
    if (N//2)*2 == N:
        N = N - 1
    front = np.zeros(N//2)
    back = np.zeros(N//2)

    for i in range(1, (N//2)*2, 2):
        front[i//2] = np.convolve(l[:i], np.ones((i,))/i, mode = 'valid')
    for i in range(1, (N//2)*2, 2):
        back[i//2] = np.convolve(l[-i:], np.ones((i,))/i, mode = 'valid')
    return np.concatenate([front, np.convolve(l, np.ones((N,))/N, mode = 'valid'), back[::-1]])
</code></pre>
<p>It's relatively slow because it uses <code>convolve()</code>, and could likely be spruced up quite a lot by a true Pythonista, however, I believe that the idea stands.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are many answers above about calculating a running mean. My answer adds two extra features:</p>
<ol>
<li>ignores nan values </li>
<li>calculates the mean for the N neighboring values NOT including the value of interest itself</li>
</ol>
<p>This second feature is particularly useful for determining which values differ from the general trend by a certain amount.</p>
<p>I use numpy.cumsum since it is the most time-efficient method (<a href="https://stackoverflow.com/a/27681394/8915665">see Alleo's answer above</a>). </p>
<pre><code>N=10 # number of points to test on each side of point of interest, best if even
padded_x = np.insert(np.insert( np.insert(x, len(x), np.empty(int(N/2))*np.nan), 0, np.empty(int(N/2))*np.nan ),0,0)
n_nan = np.cumsum(np.isnan(padded_x))
cumsum = np.nancumsum(padded_x) 
window_sum = cumsum[N+1:] - cumsum[:-(N+1)] - x # subtract value of interest from sum of all values within window
window_n_nan = n_nan[N+1:] - n_nan[:-(N+1)] - np.isnan(x)
window_n_values = (N - window_n_nan)
movavg = (window_sum) / (window_n_values)
</code></pre>
<p>This code works for even Ns only. It can be adjusted for odd numbers by changing the np.insert of padded_x and n_nan.</p>
<p>Example output (raw in black, movavg in blue):
<a href="https://i.stack.imgur.com/6Tqr9.png" rel="nofollow noreferrer"><img alt="raw data (black) and moving average (blue) of 10 points around each value, not including that value. nan values are ignored." src="https://i.stack.imgur.com/6Tqr9.png"/></a></p>
<p>This code can be easily adapted to remove all moving average values calculated from fewer than cutoff = 3 non-nan values.</p>
<pre><code>window_n_values = (N - window_n_nan).astype(float) # dtype must be float to set some values to nan
cutoff = 3
window_n_values[window_n_values&lt;cutoff] = np.nan
movavg = (window_sum) / (window_n_values)
</code></pre>
<p><a href="https://i.stack.imgur.com/wfEnA.png" rel="nofollow noreferrer"><img alt="raw data (black) and moving average (blue) while ignoring any window with fewer than 3 non-nan values" src="https://i.stack.imgur.com/wfEnA.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>This answer contains solutions using the Python <strong>standard library</strong> for three different scenarios.</p>
<p><br/></p>
<h3>Running average with <a href="https://docs.python.org/3.5/library/itertools.html#itertools.accumulate" rel="nofollow noreferrer"><code>itertools.accumulate</code></a></h3>
<p>This is a memory efficient Python 3.2+ solution computing the running average over an iterable of values by leveraging <code>itertools.accumulate</code>.</p>
<pre><code>&gt;&gt;&gt; from itertools import accumulate
&gt;&gt;&gt; values = range(100)
</code></pre>
<p>Note that <code>values</code> can be any iterable, including generators or any other object that produces values on the fly.</p>
<p>First, lazily construct the cumulative sum of the values.</p>
<pre><code>&gt;&gt;&gt; cumu_sum = accumulate(value_stream)
</code></pre>
<p>Next, <code>enumerate</code> the cumulative sum (starting at 1) and construct a generator that yields the fraction of accumulated values and the current enumeration index.</p>
<pre><code>&gt;&gt;&gt; rolling_avg = (accu/i for i, accu in enumerate(cumu_sum, 1))
</code></pre>
<p>You can issue <code>means = list(rolling_avg)</code> if you need all the values in memory at once or call <code>next</code> incrementally.<br/>
(Of course, you can also iterate over <code>rolling_avg</code> with a <code>for</code> loop, which will call <code>next</code> implicitly.)</p>
<pre><code>&gt;&gt;&gt; next(rolling_avg) # 0/1
&gt;&gt;&gt; 0.0
&gt;&gt;&gt; next(rolling_avg) # (0 + 1)/2
&gt;&gt;&gt; 0.5
&gt;&gt;&gt; next(rolling_avg) # (0 + 1 + 2)/3
&gt;&gt;&gt; 1.0
</code></pre>
<p>This solution can be written as a function as follows.</p>
<pre><code>from itertools import accumulate

def rolling_avg(iterable):
    cumu_sum = accumulate(iterable)
    yield from (accu/i for i, accu in enumerate(cumu_sum, 1))
</code></pre>
<h3>A <a href="http://dabeaz.com/coroutines/" rel="nofollow noreferrer">coroutine</a> to which you can send values at any time</h3>
<p>This coroutine consumes values you send it and keeps a running average of the values seen so far.</p>
<p>It is useful when you don’t have an iterable of values but aquire the values to be averaged one by one at different times throughout your program’s life.</p>
<pre><code>def rolling_avg_coro():
    i = 0
    total = 0.0
    avg = None

    while True:
        next_value = yield avg
        i += 1
        total += next_value
        avg = total/i
</code></pre>
<p>The coroutine works like this:</p>
<pre><code>&gt;&gt;&gt; averager = rolling_avg_coro() # instantiate coroutine
&gt;&gt;&gt; next(averager) # get coroutine going (this is called priming)
&gt;&gt;&gt;
&gt;&gt;&gt; averager.send(5) # 5/1
&gt;&gt;&gt; 5.0
&gt;&gt;&gt; averager.send(3) # (5 + 3)/2
&gt;&gt;&gt; 4.0
&gt;&gt;&gt; print('doing something else...')
doing something else...
&gt;&gt;&gt; averager.send(13) # (5 + 3 + 13)/3
&gt;&gt;&gt; 7.0
</code></pre>
<h3>Computing the average over a sliding window of size <code>N</code></h3>
<p>This generator-function takes an iterable and a window size <code>N</code>  and yields the average over the current values inside the window.  It uses a <a href="https://docs.python.org/3.7/library/collections.html#collections.deque" rel="nofollow noreferrer"><code>deque</code></a>, which is a datastructure similar to a list, but optimized for fast modifications (<code>pop</code>, <code>append</code>) <em>at both endpoints</em>.</p>
<pre><code>from collections import deque
from itertools import islice

def sliding_avg(iterable, N):        
    it = iter(iterable)
    window = deque(islice(it, N))        
    num_vals = len(window)

    if num_vals &lt; N:
        msg = 'window size {} exceeds total number of values {}'
        raise ValueError(msg.format(N, num_vals))

    N = float(N) # force floating point division if using Python 2
    s = sum(window)

    while True:
        yield s/N
        try:
            nxt = next(it)
        except StopIteration:
            break
        s = s - window.popleft() + nxt
        window.append(nxt)
</code></pre>
<p>Here is the function in action:</p>
<pre><code>&gt;&gt;&gt; values = range(100)
&gt;&gt;&gt; N = 5
&gt;&gt;&gt; window_avg = sliding_avg(values, N)
&gt;&gt;&gt; 
&gt;&gt;&gt; next(window_avg) # (0 + 1 + 2 + 3 + 4)/5
&gt;&gt;&gt; 2.0
&gt;&gt;&gt; next(window_avg) # (1 + 2 + 3 + 4 + 5)/5
&gt;&gt;&gt; 3.0
&gt;&gt;&gt; next(window_avg) # (2 + 3 + 4 + 5 + 6)/5
&gt;&gt;&gt; 4.0
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Although there are solutions for this question here, please take a look at my solution. It is very simple and working well. </p>
<pre><code>import numpy as np
dataset = np.asarray([1, 2, 3, 4, 5, 6, 7])
ma = list()
window = 3
for t in range(0, len(dataset)):
    if t+window &lt;= len(dataset):
        indices = range(t, t+window)
        ma.append(np.average(np.take(dataset, indices)))
else:
    ma = np.asarray(ma)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p><strong>Use Only Python Stadnard Library (Memory Efficient)</strong></p>
<p>Just give another version of using the standard library <code>deque</code> only. It's quite surprise to me that most of the answers are using <code>pandas</code> or <code>numpy</code>.</p>
<pre><code>def moving_average(iterable, n=3):
    d = deque(maxlen=n)
    for i in iterable:
        d.append(i)
        if len(d) == n:
            yield sum(d)/n

r = moving_average([40, 30, 50, 46, 39, 44])
assert list(r) == [40.0, 42.0, 45.0, 43.0]
</code></pre>
<p>Actaully I found another <a href="https://docs.python.org/3/library/collections.html#deque-recipes" rel="nofollow noreferrer">implementation in python docs</a></p>
<pre><code>def moving_average(iterable, n=3):
    # moving_average([40, 30, 50, 46, 39, 44]) --&gt; 40.0 42.0 45.0 43.0
    # http://en.wikipedia.org/wiki/Moving_average
    it = iter(iterable)
    d = deque(itertools.islice(it, n-1))
    d.appendleft(0)
    s = sum(d)
    for elem in it:
        s += elem - d.popleft()
        d.append(elem)
        yield s / n
</code></pre>
<p>However the implementation seems to me is a bit more complex than it should be. But it must be in the standard python docs for a reason, could someone comment on the implementation of mine and the standard doc?</p>
</div>
<div class="post-text" itemprop="text">
<p>From reading the other answers I don't think this is what the question asked for, but I got here with the need of keeping a running average of a list of values that was growing in size.</p>
<p>So if you want to keep a list of values that you are acquiring from somewhere (a site, a measuring device, etc.) and the average of the last <code>n</code> values updated, you can use the code bellow, that minimizes the effort of adding new elements:</p>
<pre><code>class Running_Average(object):
    def __init__(self, buffer_size=10):
        """
        Create a new Running_Average object.

        This object allows the efficient calculation of the average of the last
        `buffer_size` numbers added to it.

        Examples
        --------
        &gt;&gt;&gt; a = Running_Average(2)
        &gt;&gt;&gt; a.add(1)
        &gt;&gt;&gt; a.get()
        1.0
        &gt;&gt;&gt; a.add(1)  # there are two 1 in buffer
        &gt;&gt;&gt; a.get()
        1.0
        &gt;&gt;&gt; a.add(2)  # there's a 1 and a 2 in the buffer
        &gt;&gt;&gt; a.get()
        1.5
        &gt;&gt;&gt; a.add(2)
        &gt;&gt;&gt; a.get()  # now there's only two 2 in the buffer
        2.0
        """
        self._buffer_size = int(buffer_size)  # make sure it's an int
        self.reset()

    def add(self, new):
        """
        Add a new number to the buffer, or replaces the oldest one there.
        """
        new = float(new)  # make sure it's a float
        n = len(self._buffer)
        if n &lt; self.buffer_size:  # still have to had numbers to the buffer.
            self._buffer.append(new)
            if self._average != self._average:  # ~ if isNaN().
                self._average = new  # no previous numbers, so it's new.
            else:
                self._average *= n  # so it's only the sum of numbers.
                self._average += new  # add new number.
                self._average /= (n+1)  # divide by new number of numbers.
        else:  # buffer full, replace oldest value.
            old = self._buffer[self._index]  # the previous oldest number.
            self._buffer[self._index] = new  # replace with new one.
            self._index += 1  # update the index and make sure it's...
            self._index %= self.buffer_size  # ... smaller than buffer_size.
            self._average -= old/self.buffer_size  # remove old one...
            self._average += new/self.buffer_size  # ...and add new one...
            # ... weighted by the number of elements.

    def __call__(self):
        """
        Return the moving average value, for the lazy ones who don't want
        to write .get .
        """
        return self._average

    def get(self):
        """
        Return the moving average value.
        """
        return self()

    def reset(self):
        """
        Reset the moving average.

        If for some reason you don't want to just create a new one.
        """
        self._buffer = []  # could use np.empty(self.buffer_size)...
        self._index = 0  # and use this to keep track of how many numbers.
        self._average = float('nan')  # could use np.NaN .

    def get_buffer_size(self):
        """
        Return current buffer_size.
        """
        return self._buffer_size

    def set_buffer_size(self, buffer_size):
        """
        &gt;&gt;&gt; a = Running_Average(10)
        &gt;&gt;&gt; for i in range(15):
        ...     a.add(i)
        ...
        &gt;&gt;&gt; a()
        9.5
        &gt;&gt;&gt; a._buffer  # should not access this!!
        [10.0, 11.0, 12.0, 13.0, 14.0, 5.0, 6.0, 7.0, 8.0, 9.0]

        Decreasing buffer size:
        &gt;&gt;&gt; a.buffer_size = 6
        &gt;&gt;&gt; a._buffer  # should not access this!!
        [9.0, 10.0, 11.0, 12.0, 13.0, 14.0]
        &gt;&gt;&gt; a.buffer_size = 2
        &gt;&gt;&gt; a._buffer
        [13.0, 14.0]

        Increasing buffer size:
        &gt;&gt;&gt; a.buffer_size = 5
        Warning: no older data available!
        &gt;&gt;&gt; a._buffer
        [13.0, 14.0]

        Keeping buffer size:
        &gt;&gt;&gt; a = Running_Average(10)
        &gt;&gt;&gt; for i in range(15):
        ...     a.add(i)
        ...
        &gt;&gt;&gt; a()
        9.5
        &gt;&gt;&gt; a._buffer  # should not access this!!
        [10.0, 11.0, 12.0, 13.0, 14.0, 5.0, 6.0, 7.0, 8.0, 9.0]
        &gt;&gt;&gt; a.buffer_size = 10  # reorders buffer!
        &gt;&gt;&gt; a._buffer
        [5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]
        """
        buffer_size = int(buffer_size)
        # order the buffer so index is zero again:
        new_buffer = self._buffer[self._index:]
        new_buffer.extend(self._buffer[:self._index])
        self._index = 0
        if self._buffer_size &lt; buffer_size:
            print('Warning: no older data available!')  # should use Warnings!
        else:
            diff = self._buffer_size - buffer_size
            print(diff)
            new_buffer = new_buffer[diff:]
        self._buffer_size = buffer_size
        self._buffer = new_buffer

    buffer_size = property(get_buffer_size, set_buffer_size)
</code></pre>
<p>And you can test it with, for example:</p>
<pre><code>def graph_test(N=200):
    import matplotlib.pyplot as plt
    values = list(range(N))
    values_average_calculator = Running_Average(N/2)
    values_averages = []
    for value in values:
        values_average_calculator.add(value)
        values_averages.append(values_average_calculator())
    fig, ax = plt.subplots(1, 1)
    ax.plot(values, label='values')
    ax.plot(values_averages, label='averages')
    ax.grid()
    ax.set_xlim(0, N)
    ax.set_ylim(0, N)
    fig.show()
</code></pre>
<p>Which gives:</p>
<p><a href="https://i.stack.imgur.com/KbE6n.png" rel="nofollow noreferrer"><img alt="Values and their average as a function of values #" src="https://i.stack.imgur.com/KbE6n.png"/></a></p>
</div>
<div class="post-text" itemprop="text">
<p>There is a comment by <a href="https://stackoverflow.com/users/3703716/mab">mab</a> buried in one of the <a href="https://stackoverflow.com/a/30141358/4549682">answers</a> above which has this method.  <a href="https://github.com/kwgoodman/bottleneck" rel="nofollow noreferrer"><code>bottleneck</code></a> has <code>move_mean</code> which is a simple moving average:</p>
<pre><code>import numpy as np
import bottleneck as bn

a = np.arange(10) + np.random.random(10)

mva = bn.move_mean(a, window=2, min_count=1)
</code></pre>
<p><code>min_count</code> is a handy parameter that will basically take the moving average up to that point in your array.  If you don't set <code>min_count</code>, it will equal <code>window</code>, and everything up to <code>window</code> points will be <code>nan</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>I feel this can be elegantly solved using <a href="https://github.com/kwgoodman/bottleneck" rel="nofollow noreferrer">bottleneck</a></p>
<p>See basic sample below:</p>
<pre><code>import numpy as np
import bottleneck as bn

a = np.random.randint(4, 1000, size=100)
mm = bn.move_mean(a, window=5, min_count=1)
</code></pre>
<ul>
<li><p>"mm" is the moving mean for "a". </p></li>
<li><p>"window" is the max number of entries to consider for moving mean. </p></li>
<li><p>"min_count" is min number of entries to consider for moving mean (e.g. for first few elements or if the array has nan values).</p></li>
</ul>
<p>The good part is Bottleneck helps to deal with nan values and it's also very efficient.</p>
</div>
<div class="post-text" itemprop="text">
<p>How about <strong>a moving average filter</strong>? It is also a one-liner and has the advantage, that you can easily manipulate the window type if you need something else than the rectangle, ie. a N-long simple moving average of an array a:</p>
<pre><code>lfilter(np.ones(N)/N, [1], a)[N:]
</code></pre>
<p>And with the triangular window applied:</p>
<pre><code>lfilter(np.ones(N)*scipy.signal.triang(N)/N, [1], a)[N:]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Another solution just using a standard library and deque:</p>
<pre><code>from collections import deque
import itertools

def moving_average(iterable, n=3):
    # http://en.wikipedia.org/wiki/Moving_average
    it = iter(iterable) 
    # create an iterable object from input argument
    d = deque(itertools.islice(it, n-1))  
    # create deque object by slicing iterable
    d.appendleft(0)
    s = sum(d)
    for elem in it:
        s += elem - d.popleft()
        d.append(elem)
        yield s / n

# example on how to use it
for i in  moving_average([40, 30, 50, 46, 39, 44]):
    print(i)

# 40.0
# 42.0
# 45.0
# 43.0
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>For educational purposes, let me add two more Numpy solutions (which are slower than the cumsum solution):</p>
<pre><code>import numpy as np
from numpy.lib.stride_tricks import as_strided

def ra_strides(arr, window):
    ''' Running average using as_strided'''
    n = arr.shape[0] - window + 1
    arr_strided = as_strided(arr, shape=[n, window], strides=2*arr.strides)
    return arr_strided.mean(axis=1)

def ra_add(arr, window):
    ''' Running average using add.reduceat'''
    n = arr.shape[0] - window + 1
    indices = np.array([0, window]*n) + np.repeat(np.arange(n), 2)
    arr = np.append(arr, 0)
    return np.add.reduceat(arr, indices )[::2]/window
</code></pre>
<p>Functions used: <a href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.lib.stride_tricks.as_strided.html" rel="nofollow noreferrer">as_strided</a>, <a href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ufunc.reduceat.html" rel="nofollow noreferrer">add.reduceat</a></p>
</div>
<div class="post-text" itemprop="text">
<p>If you do choose to roll your own, rather than use an existing library, please be conscious of floating point error and try to minimize its effects:</p>
<pre><code>class SumAccumulator:
    def __init__(self):
        self.values = [0]
        self.count = 0

    def add( self, val ):
        self.values.append( val )
        self.count = self.count + 1
        i = self.count
        while i &amp; 0x01:
            i = i &gt;&gt; 1
            v0 = self.values.pop()
            v1 = self.values.pop()
            self.values.append( v0 + v1 )

    def get_total(self):
        return sum( reversed(self.values) )

    def get_size( self ):
        return self.count
</code></pre>
<p>If all your values are roughly the same order of magnitude, then this will help to preserve precision by always adding values of roughly similar magnitudes.</p>
</div>
<span class="comment-copy">How exactly does this work? I would love to use</span>
<span class="comment-copy">Fast?! This solution is orders of magnitude slower than the solutions with Numpy.</span>
<span class="comment-copy">Although this native solution is cool, the OP asked for a numpy/scipy function - presumably those will be considerably faster.</span>
<span class="comment-copy">I like this solution because it is clean (one line) and <i>relatively</i> efficient (work done inside numpy).  But Alleo's "Efficient solution" using <code>numpy.cumsum</code> has better complexity.</span>
<span class="comment-copy">no explanation how convolve works?</span>
<span class="comment-copy">@denfromufa, I believe the documentation covers the implementation well enough, and it also links to Wikipedia which explains the maths. Considering the focus of the question, do you think this answer needs to copy those?</span>
<span class="comment-copy">@lapis the usage of convolve for moving average is quite unusual and non-obvious. Here is the best visual explanation I have found: <a href="http://matlabtricks.com/post-11/moving-average-by-convolution" rel="nofollow noreferrer">matlabtricks.com/post-11/moving-average-by-convolution</a></span>
<span class="comment-copy">@denfromufa, I've added an explanation of the link between the running mean and convolution. Please feel free to edit it if you can think of an improvement.</span>
<span class="comment-copy">Nice solution!  My hunch is <code>numpy.convolve</code> is O(mn); its <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html" rel="nofollow noreferrer">docs</a> mention that <code>scipy.signal.fftconvolve</code> uses FFT.</span>
<span class="comment-copy">This method does not deal with the edges of the array, does it ?</span>
<span class="comment-copy">Nice solution, but note that it might suffer from numerical errors for large arrays, since towards the end of the array, you might be subtracting two large numbers to obtain a small result.</span>
<span class="comment-copy">This uses integer division instead of float division: <code>running_mean([1,2,3], 2)</code> gives <code>array([1, 2])</code>. Replacing <code>x</code> by <code>[float(value) for value in x]</code> does the trick.</span>
<span class="comment-copy">The numerical stability of this solution can become a problem if <code>x</code> contains floats. Example: <code>running_mean(np.arange(int(1e7))[::-1] + 0.2, 1)[-1] - 0.2</code> returns <code>0.003125</code> while one expects <code>0.0</code>. More information: <a href="https://en.wikipedia.org/wiki/Loss_of_significance" rel="nofollow noreferrer">en.wikipedia.org/wiki/Loss_of_significance</a></span>
<span class="comment-copy">The Pandas rolling_mean is a nice tool for the job but has been deprecated for ndarrays. In future Pandas releases it will only function on Pandas series. Where do we turn now for non-Pandas array data?</span>
<span class="comment-copy">@Mike rolling_mean() is deprecated, but now you can use rolling and mean separately: <code>df.rolling(windowsize).mean()</code> now works instead (very quickly I might add). for 6,000 row series <code>%timeit test1.rolling(20).mean()</code> returned <i>1000 loops, best of 3: 1.16 ms per loop</i></span>
<span class="comment-copy">@Vlox <code>df.rolling()</code> works well enough, the problem is that even this form will not support ndarrays in the future. To use it we will have to load our data into a Pandas Dataframe first. I would love to see this function added to either <code>numpy</code> or <code>scipy.signal</code>.</span>
<span class="comment-copy">@Mike totally agree. I am struggling in particular to match pandas .ewm().mean() speed for my own arrays (instead of having to load them into a df first). I mean, it's great that it's fast, but just feels a bit clunky moving in and out of dataframes too often.</span>
<span class="comment-copy"><a href="https://github.com/kwgoodman/bottleneck" rel="nofollow noreferrer"><code>%timeit bottleneck.move_mean(x, N)</code></a> is 3 to 15 times faster than the cumsum and pandas methods on my pc. Take a look at their benchmark in the repo's <a href="https://github.com/kwgoodman/bottleneck/blob/master/README.rst" rel="nofollow noreferrer">README</a>.</span>
<span class="comment-copy">Note that removing the first N-1 points still leaves a boundary effect in the last points. An easier way to solve the issue is to use <code>mode='valid'</code> in <code>convolve</code> which doesn't require any post-processing.</span>
<span class="comment-copy">@Psycho - <code>mode='valid'</code> removes the transient from both ends, right? If <code>len(x)=10</code> and <code>N=4</code>, for a running mean I would want 10 results but <code>valid</code> returns 7.</span>
<span class="comment-copy">It removes the transient from the end, and the beginning doesn't have one. Well, I guess it's a matter of priorities, I don't need the same number of results on the expense of getting a slope towards zero that isn't there in the data. BTW, here is a command to show the difference between the modes: <code>modes = ('full', 'same', 'valid'); [plot(convolve(ones((200,)), ones((50,))/50, mode=m)) for m in modes]; axis([-10, 251, -.1, 1.1]); legend(modes, loc='lower center')</code> (with pyplot and numpy imported).</span>
<span class="comment-copy">@Psycho - very nice. You should make this an answer, I would certainly give it a vote.</span>
<span class="comment-copy"><code>runningMean</code> Have I side effect of averaging with zeros, when you go out of array with <code>x[ctr:(ctr+N)]</code> for right side of array.</span>
<span class="comment-copy"><code>NameError: name 'info' is not defined</code> . I am getting this error, Sir.</span>
<span class="comment-copy">Looks like you time series are shifted after smoothing, is it desired effect?</span>
<span class="comment-copy">@mrgloom yes, for visualization purposes; else they would appear as one line on the chart ; Md. Rezwanul Haque you could remove all references to PAIR and info; those were internal sandboxed methods for the now defunct tradewave.net</span>
<span class="comment-copy">This method relies on <code>numpy.convolve</code>, the difference only in altering the sequence.</span>
<span class="comment-copy">I'm always annoyed by signal processing function that return output signals of different shape than the input signals when both inputs and outputs are of the same nature (e.g., both temporal signals). It breaks the correspondence with related independent variable (e.g., time, frequency) making plotting or comparison not a direct matter... anyway, if you share the feeling, you might want to change the last lines of the proposed function as      y=np.convolve(w/w.sum(),s,mode='same');      return y[window_len-1:-(window_len-1)]</span>
<span class="comment-copy">@ChristianO'Reilly, you should post that as a separate answer- that's exactly what I was looking for, as I indeed have two other arrays that have to match the lengths of the smoothed data, for plotting etc.  I'd like to know exactly how you did that - is <code>w</code> the window size, and <code>s</code> the data?</span>
<span class="comment-copy">@Demis Glad the comment helped. More info on the numpy convolve function here <a href="https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.convolve.html" rel="nofollow noreferrer">docs.scipy.org/doc/numpy-1.15.0/reference/generated/…</a> A convolution function (<a href="https://en.wikipedia.org/wiki/Convolution" rel="nofollow noreferrer">en.wikipedia.org/wiki/Convolution</a>) convolves two signals with one another. In this case, it convolves your signal (s) with a normalized (i.e. unitary area) window (w/w.sum()).</span>
<span class="comment-copy">This is the only answer that seems to take into account the border issues (rather important, particularly when plotting). Thank you!</span>
<span class="comment-copy">This is what I was going to do.  Can anyone please critique why this is a bad way to go?</span>
<span class="comment-copy">This simple python solution worked well for me without requiring numpy. I ended up rolling it into a class for re-use.</span>
<span class="comment-copy">This does not work for me (python 3.6).  <b>1</b> There is no function named <code>sum</code>, using <code>np.sum</code> instead <b>2</b> The <code>@</code> operator (no idea what that is) throws an error.  I may look into it later but I'm lacking the time right now</span>
<span class="comment-copy">The <code>@</code> is the matrix multiplication operator which implements <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html" rel="nofollow noreferrer">np.matmul</a>. Check if your <code>y_in</code> array is a numpy array, that might be the problem.</span>
<span class="comment-copy">How is this different from <a href="https://stackoverflow.com/a/30141358/8881141">the solution proposed in 2016</a>?</span>
<span class="comment-copy">The solution proposed in 2016 uses <code>pandas.rolling_mean</code> while mine uses <code>pandas.DataFrame.rolling</code>. You can also calculate moving <code>min(), max(), sum()</code> etc. as well as <code>mean()</code> with this method easily.</span>
<span class="comment-copy">In the former you need to use a different method like <code>pandas.rolling_min, pandas.rolling_max</code> etc. They are similar yet different.</span>
<span class="comment-copy">itertools.accumulate does not exist in python 2.7, but does in python 3.4</span>
<span class="comment-copy">This is a terribly unclear answer, at least some comment in the code or explanation of why this helps floating point error would be nice.</span>
<span class="comment-copy">In my last sentence I was trying to indicate why it helps floating point error.  If two values are approximately the same order of magnitude, then adding them loses less precision than if you added a very large number to a very small one.  The code combines "adjacent" values in a manner that even intermediate sums should always be reasonably close in magnitude, to minimize the floating point error.  Nothing is fool proof but this method has saved a couple very poorly implemented projects in production.</span>
<span class="comment-copy">1. being applied to original problem, this would be terribly slow (computing average), so this is just irrelevant 2. to suffer from the problem of precision of 64-bit numbers, one has to sum up &gt;&gt; 2^30 of nearly equal numbers.</span>
<span class="comment-copy">@Alleo: Instead of doing one addition per value, you'll be doing two.  The proof is the same as the bit-flipping problem.  However, the point of this answer is not necessarily performance, but precision.  Memory usage for averaging 64-bit values would not exceed 64 elements in the cache, so it's friendly in memory usage as well.</span>
<span class="comment-copy">Yes, you're right that this takes 2x more operations than simple sum, but the original problem is compute <b>running mean</b>, not just sum. Which can be done in O(n), but your answer requires O(mn), where m is size of window.</span>
