<div class="post-text" itemprop="text">
<p>I want to calculate the running time O(n, x) = Theta(n, x) for a given algorithm depending on n and x by a big amount (&gt; 100) of examples (how long the algorithm will take for n and x).
Is there actually a way to do this?</p>
<p>I know that the running time increases as n and x (!) increase, but I think the coherences are too complex to figure out O(n, x) by "hand", since n or x mac increase like n ^ x, or even worse.</p>
<p>btw. my most favored languages for solving this problem are Python or PHP.</p>
</div>
<div class="post-text" itemprop="text">
<p>There's a free tool called <a href="http://creativemachines.cornell.edu/eureqa" rel="nofollow">Eureqa</a> that might interest you. You can give it data, and it will find candidate equations that fit your data. For example, you run the algo on varying input sizes and record the execution time of each, and then give Eureqa this data. It will then give you math equations that fit your data.</p>
<p>Many algorithms have running times that are highly dependant on the specific values in the input data. Because of this, this wont always be a great method to use to do asymptotic analysis, because your just don't know if you data is pushing the algorithm to its bounds. </p>
<p>But, we use asymptotic analysis as a means to an end - we often want to choose an algorithm that <em>probably</em> works well in the real world on real world data. And, this is like benchmarking, but you get awesome additional mathematical insight. Also, keep in mind  asymptotic analysis itself is kinda a concession to the fact that we need to simplify and lower our expectations in order to get some answer thats simple enough to be useful.</p>
<p>Watch their youtube vid <a href="http://www.youtube.com/watch?v=NhC1Qb-PQ5Q" rel="nofollow">http://www.youtube.com/watch?v=NhC1Qb-PQ5Q</a></p>
</div>
<div class="post-text" itemprop="text">
<p>The best way is to have a very close look at the algorithm and analyze each step to calculate average and worst-case runtime class.</p>
<p>If that's not feasible, you can run the algorithm with relatively small numbers, and compare them to each other. If the runtime is exponential in order of any of the parameters, it should be blatantly obvious even with a difference of 10 or 20. Simply plotting the runtimes for, say</p>
<ul>
<li>x = 10 and y in range(50)</li>
<li>y = 10 and x in range(50)</li>
<li>x in range(50), y=x</li>
</ul>
<p>should give you a rough idea. You can abort early  when runtime grows to large, say larger than 10000 times the runtime of <code>(1,1)</code>.</p>
<p>This should give you a rough estimate, but you should be well aware that it's neither precise (your test data may inadvertently follow certain patterns and hit a good case) nor sufficient (The factors involved may be very small - you won't correctly identify, say, <code>x + 0.0001 * 1.05^y</code>). Fortunately, in many cases, the bases in exponential algorithms are significantly larger than 1.</p>
<p>In Python, you can use the <a href="http://docs.python.org/3/library/timeit.html" rel="nofollow"><code>timeit</code></a> module to correctly measure the runtime.</p>
</div>
<span class="comment-copy">I don't think there's much hope of doing this empirically rather than analytically.</span>
<span class="comment-copy">Big-O notation is not necessarily related to the actual running time. And running time doesn't necessarily increase with n. You should be able to determine the Big-O efficiency of your algorithm by anaylzing it; you shouldn't try to calculate it based on actual numerical data.</span>
<span class="comment-copy">The same way you determine anything experimentally -- Measure the timings for a constant x (or a few) at many different <code>n</code>'s.  Look at the graph and fit it to something reasonable to see if it increases as <code>n</code> or as <code>n**2</code>, etc.  Do the same for <code>x</code>.  Hope you got it right.  Re-evaluate as necessary.  Of course, you haven't ruled out the possibility of cross terms (<code>O(n*x)</code>).  There's not really a whole lot you can do about that...</span>
<span class="comment-copy">What is <code>O(n, x)</code>? I'm familiar with the common definition of Big-Oh, but I have never seen this notation before.</span>
<span class="comment-copy">@phant0m -- I think this is just Big-Oh but in 2 variables where the functional dependence is unknown.  e.g. <code>O(n*m)</code> =&gt; <code>Theta(n,m)</code> ... (but I'm just guessing here)</span>
<span class="comment-copy">I'm definitely going to try this out, thanks a lot!</span>
