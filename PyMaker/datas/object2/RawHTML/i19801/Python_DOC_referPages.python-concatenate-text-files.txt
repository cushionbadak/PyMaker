<div class="post-text" itemprop="text">
<p>I have a list of 20 file names, like <code>['file1.txt', 'file2.txt', ...]</code>. I want to write a Python script to concatenate these files into a new file. I could open each file by <code>f = open(...)</code>, read line by line by calling <code>f.readline()</code>, and write each line into that new file. It doesn't seem very "elegant" to me, especially the part where I have to read//write line by line. </p>
<p>Is there a more "elegant" way to do this in Python?</p>
</div>
<div class="post-text" itemprop="text">
<p>This should do it</p>
<p><strong>For large files:</strong></p>
<pre><code>filenames = ['file1.txt', 'file2.txt', ...]
with open('path/to/output/file', 'w') as outfile:
    for fname in filenames:
        with open(fname) as infile:
            for line in infile:
                outfile.write(line)
</code></pre>
<p><strong>For small files:</strong></p>
<pre><code>filenames = ['file1.txt', 'file2.txt', ...]
with open('path/to/output/file', 'w') as outfile:
    for fname in filenames:
        with open(fname) as infile:
            outfile.write(infile.read())
</code></pre>
<p><strong>â€¦ and another interesting one that I thought of</strong>:</p>
<pre><code>filenames = ['file1.txt', 'file2.txt', ...]
with open('path/to/output/file', 'w') as outfile:
    for line in itertools.chain.from_iterable(itertools.imap(open, filnames)):
        outfile.write(line)
</code></pre>
<p>Sadly, this last method leaves a few open file descriptors, which the GC should take care of anyway. I just thought it was interesting</p>
</div>
<div class="post-text" itemprop="text">
<p>Use <code>shutil.copyfileobj</code>.</p>
<p>It automatically reads the input files chunk by chunk for you, which is more more efficient and reading the input files in and will work even if some of the input files are too large to fit into memory:</p>
<pre><code>with open('output_file.txt','wb') as wfd:
    for f in ['seg1.txt','seg2.txt','seg3.txt']:
        with open(f,'rb') as fd:
            shutil.copyfileobj(fd, wfd)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>That's exactly what <a href="http://docs.python.org/3/library/fileinput.html" rel="noreferrer">fileinput</a> is for:</p>
<pre><code>import fileinput
with open(outfilename, 'w') as fout, fileinput.input(filenames) as fin:
    for line in fin:
        fout.write(line)
</code></pre>
<p>For this use case, it's really not much simpler than just iterating over the files manually, but in other cases, having a single iterator that iterates over all of the files as if they were a single file is very handy. (Also, the fact that <code>fileinput</code> closes each file as soon as it's done means there's no need to <code>with</code> or <code>close</code> each one, but that's just a one-line savings, not that big of a deal.)</p>
<p>There are some other nifty features in <code>fileinput</code>, like the ability to do in-place modifications of files just by filtering each line.</p>
<hr/>
<p>As noted in the comments, and discussed in another <a href="https://stackoverflow.com/questions/30835090/attributeerror-fileinput-instance-has-no-attribute-exit">post</a>, <code>fileinput</code> for Python 2.7 will not work as indicated. Here slight modification to make the code Python 2.7 compliant</p>
<pre><code>with open('outfilename', 'w') as fout:
    fin = fileinput.input(filenames)
    for line in fin:
        fout.write(line)
    fin.close()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>I don't know about elegance, but this works:</p>
<pre><code>    import glob
    import os
    for f in glob.glob("file*.txt"):
         os.system("cat "+f+" &gt;&gt; OutFile.txt")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>What's wrong with UNIX commands ? (given you're not working on Windows) : </p>
<p><code>ls | xargs cat | tee output.txt</code> does the job ( you can call it from python with subprocess if you want)</p>
</div>
<div class="post-text" itemprop="text">
<p>An alternative to @inspectorG4dget answer (best answer to date 29-03-2016). I tested with 3 files of 436MB. </p>
<p>@inspectorG4dget solution: 162 seconds</p>
<p>The following solution : 125 seconds</p>
<pre><code>from subprocess import Popen
filenames = ['file1.txt', 'file2.txt', 'file3.txt']
fbatch = open('batch.bat','w')
str ="type "
for f in filenames:
    str+= f + " "
fbatch.write(str + " &gt; file4results.txt")
fbatch.close()
p = Popen("batch.bat", cwd=r"Drive:\Path\to\folder")
stdout, stderr = p.communicate()
</code></pre>
<p>The idea is to create a batch file and execute it, taking advantage of "old good technology". Its semi-python but works faster. Works for windows. </p>
</div>
<div class="post-text" itemprop="text">
<pre><code>outfile.write(infile.read()) 2.1085190773010254s
shutil.copyfileobj(fd, wfd, 1024*1024*10) 0.60599684715271s
</code></pre>
<p>A simple benchmark shows that the shutil performs better.</p>
</div>
<div class="post-text" itemprop="text">
<p>Check out the .read() method of the File object:</p>
<p><a href="http://docs.python.org/2/tutorial/inputoutput.html#methods-of-file-objects" rel="nofollow">http://docs.python.org/2/tutorial/inputoutput.html#methods-of-file-objects</a></p>
<p>You could do something like:</p>
<pre><code>concat = ""
for file in files:
    concat += open(file).read()
</code></pre>
<p>or a more 'elegant' python-way:</p>
<pre><code>concat = ''.join([open(f).read() for f in files])
</code></pre>
<p>which, according to this article: <a href="http://www.skymind.com/~ocrow/python_string/" rel="nofollow">http://www.skymind.com/~ocrow/python_string/</a> would also be the fastest.</p>
</div>
<div class="post-text" itemprop="text">
<p>If the files are not gigantic:</p>
<pre><code>with open('newfile.txt','wb') as newf:
    for filename in list_of_files:
        with open(filename,'rb') as hf:
            newf.write(hf.read())
            # newf.write('\n\n\n')   if you want to introduce
            # some blank lines between the contents of the copied files
</code></pre>
<p>If the files are too big to be entirely read and held in RAM, the algorithm must be a little different to read each file to be copied in a loop by chunks of fixed length, using <code>read(10000)</code> for example.</p>
</div>
<div class="post-text" itemprop="text">
<p>If you have a lot of files in the directory then <code>glob2</code> might be a better option to generate a list of filenames rather than writing them by hand.</p>
<pre><code>import glob2

filenames = glob2.glob('*.txt')  # list of all .txt files in the directory

with open('outfile.txt', 'w') as f:
    for file in filenames:
        with open(file) as infile:
            f.write(infile.read()+'\n')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<pre><code>def concatFiles():
    path = 'input/'
    files = os.listdir(path)
    for idx, infile in enumerate(files):
        print ("File #" + str(idx) + "  " + infile)
    concat = ''.join([open(path + f).read() for f in files])
    with open("output_concatFile.txt", "w") as fo:
        fo.write(path + concat)

if __name__ == "__main__":
    concatFiles()
</code></pre>
</div>
<span class="comment-copy">Its not python, but in shell scripting you could do something like <code>cat file1.txt file2.txt file3.txt ... &gt; output.txt</code>.  In python, if you don't like <code>readline()</code>, there is always <code>readlines()</code> or simply <code>read()</code>.</span>
<span class="comment-copy">@jedwards simply run the <code>cat file1.txt file2.txt file3.txt</code> command using <code>subprocess</code> module and you're done. But I am not sure if <code>cat</code> works in windows.</span>
<span class="comment-copy">As a note, the way you describe is a terrible way to read a file. Use the <code>with</code> statement to ensure your files are closed properly, and iterate over the file to get lines, rather than using <code>f.readline()</code>.</span>
<span class="comment-copy">@jedwards cat doesn't work when the text file is unicode.</span>
<span class="comment-copy">Actual analysis <a href="https://waymoot.org/home/python_string/" rel="nofollow noreferrer">waymoot.org/home/python_string</a></span>
<span class="comment-copy">This will, for large files, be very memory inefficient.</span>
<span class="comment-copy">@Lattyware: updated for memory efficiency</span>
<span class="comment-copy">@inspectorG4dget: I wasn't asking you, I was asking eyquem, who complained that your solution wasn't going to be efficient. I'm willing to bet it's more than efficient enough for the OP's use case, and for whatever use case eyquem has in mind. If he thinks it isn't, it's his responsibility to prove that before demanding that you optimize it.</span>
<span class="comment-copy">@dee: a file so large that it's contents don't fit into main memory</span>
<span class="comment-copy">Just to reiterate: this is the wrong answer, shutil.copyfileobj is the right answer.</span>
<span class="comment-copy">this is the best solution. however i wonder why you specified the chunk size yourself: the docs say that <a href="https://docs.python.org/3/library/shutil.html#shutil.copyfileobj" rel="nofollow noreferrer"><code>copyfileobj</code></a> uses chunks by default.</span>
<span class="comment-copy"><code>for i in glob.glob(r'c:/Users/Desktop/folder/putty/*.txt'):</code> well i replaced the for statement to include all the files in directory but my <code>output_file</code> started growing really huge like in 100's of gb in very quick time.</span>
<span class="comment-copy">in my opinion this is a much better answer, since the question asked for elegance.</span>
<span class="comment-copy">Note, that is will merge last strings of each file with first strings of next file if there are no EOL characters. In my case I got totally corrupted result after using this code. I added wfd.write(b"\n") after copyfileobj to get normal result</span>
<span class="comment-copy">@flyingsheep I edited the answer to remove the explicit chunk size</span>
<span class="comment-copy">+1, I did not know <code>fileinput</code> existed, definitely the best way of doing this.</span>
<span class="comment-copy">@Lattyware: I think most people who learn about <code>fileinput</code> are told that it's a way to turn a simple <code>sys.argv</code> (or what's left as args after <code>optparse</code>/etc.) into a big virtual file for trivial scripts, and don't think to use it for anything else (i.e., when the list isn't command-line args). Or they do learn, but then forgetâ€”I keep re-discovering it every year or twoâ€¦</span>
<span class="comment-copy">@abament I think <code>for line in fileinput.input()</code> isn't the best way to choose in this particular case: the OP wants to concatenate files, not read them line by line which is a theoretically longer process to execute</span>
<span class="comment-copy">@eyquem: It's not a longer process to execute. As you yourself pointed out, line-based solutions don't read one character at a time; they read in chunks and pull lines out of a buffer. The I/O time will completely swamp the line-parsing time, so as long as the implementor didn't do something horribly stupid in the buffering, it will be just as fast (and possibly even faster than trying to guess at a good buffer size yourself, if you think 10000 is a good choice).</span>
<span class="comment-copy">Example code not quite valid for Python 2.7.10 and later: <a href="http://stackoverflow.com/questions/30835090/attributeerror-fileinput-instance-has-no-attribute-exit" title="attributeerror fileinput instance has no attribute exit">stackoverflow.com/questions/30835090/â€¦</a></span>
<span class="comment-copy">you can even avoid the loop: import os; os.system("cat file*.txt &gt;&gt; OutFile.txt")</span>
<span class="comment-copy">not crossplatform and will break for file names with spaces in them</span>
<span class="comment-copy">It did not work on Windows</span>
<span class="comment-copy">This is insecure; also, <code>cat</code> can take a list of files, so no need to repeatedly call it.  You can easily make it safe by calling <code>subprocess.check_call</code> instead of <code>os.system</code></span>
<span class="comment-copy">Coz these might not work in windows.</span>
<span class="comment-copy">because this is a question about python.</span>
<span class="comment-copy">Nothing wrong in general, but this answer is broken (don't pass the output of ls to xargs, just pass the list of files to cat directly: <code>cat * | tee output.txt</code>).</span>
<span class="comment-copy">If it can insert filename as well that would be great.</span>
<span class="comment-copy">This will produce a giant string, which, depending on the size of the files, could be larger than the available memory. As Python provides easy lazy access to files, it's a bad idea.</span>
<span class="comment-copy">Why read in length based chunks rather than line-by-line?</span>
<span class="comment-copy">@Lattyware Because I'm quite sure the execution is faster. By the way, in fact, even when the code orders to read a file line by line, the file is read by chunks, that are put in cache in which each line is then read one after the other. The better procedure would be to put the length of read chunk equal to the size of the cache. But I don't know how to determine this cache's size.</span>
<span class="comment-copy">That's the implementation in CPython, but none of that is guaranteed. Optimizing like that is a bad idea as while it may be effective on some systems, it may not on others.</span>
<span class="comment-copy">Yes, of course line-by-line reading is buffered. That's exactly why it's not that much slower. (In fact, in some cases, it may even be slightly faster, because whoever ported Python to your platform chose a much better chunk size than 10000.) If the performance of this really matters, you'll have to profile different implementations. But 99.99â€¦% of the time, either way is more than fast enough, or the actual disk I/O is the slow part and it doesn't matter what your code does.</span>
<span class="comment-copy">Also, if you really do need to manually optimize the buffering, you'll want to use <code>os.open</code> and <code>os.read</code>, because plain <code>open</code> uses Python's wrappers around C's stdio, which means either 1 or 2 extra buffers getting in your way.</span>
