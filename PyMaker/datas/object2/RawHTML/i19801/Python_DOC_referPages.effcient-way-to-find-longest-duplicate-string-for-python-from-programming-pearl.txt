<div class="post-text" itemprop="text">
<p>From Section 15.2 of Programming Pearls</p>
<p>The C codes can be viewed here: <a href="http://www.cs.bell-labs.com/cm/cs/pearls/longdup.c">http://www.cs.bell-labs.com/cm/cs/pearls/longdup.c</a></p>
<p>When I implement it in Python using suffix-array:</p>
<pre><code>example = open("iliad10.txt").read()
def comlen(p, q):
    i = 0
    for x in zip(p, q):
        if x[0] == x[1]:
            i += 1
        else:
            break
    return i

suffix_list = []
example_len = len(example)
idx = list(range(example_len))
idx.sort(cmp = lambda a, b: cmp(example[a:], example[b:]))  #VERY VERY SLOW

max_len = -1
for i in range(example_len - 1):
    this_len = comlen(example[idx[i]:], example[idx[i+1]:])
    print this_len
    if this_len &gt; max_len:
        max_len = this_len
        maxi = i
</code></pre>
<p>I found it very slow for the <code>idx.sort</code> step. I think it's slow because Python need to pass the substring by value instead of by pointer (as the C codes above).</p>
<p>The tested file can be downloaded from <a href="http://classics.mit.edu/Homer/iliad.mb.txt">here</a></p>
<p>The C codes need only 0.3 seconds to finish.</p>
<pre><code>time cat iliad10.txt |./longdup 
On this the rest of the Achaeans with one voice were for
respecting the priest and taking the ransom that he offered; but
not so Agamemnon, who spoke fiercely to him and sent him roughly
away. 

real    0m0.328s
user    0m0.291s
sys 0m0.006s
</code></pre>
<p>But for Python codes, it never ends on my computer (I waited for 10 minutes and killed it)</p>
<p>Does anyone have ideas how to make the codes efficient?  (For example, less than 10 seconds)</p>
</div>
<div class="post-text" itemprop="text">
<p>My solution is based on <em>Suffix array</em>. It is constructed by <em>Prefix doubling</em> of <em>Longest common prefix</em>. The worst-case complexity is O(n (log n)^2). The task "iliad.mb.txt" takes 4 seconds on my laptop. The code is good documented inside functions <code>suffix_array</code> and <code>longest_common_substring</code>. The latter function is short and can be easily modified e.g. for searching of 10 longest non overlapping substrings. This Python code is faster than the <a href="https://gist.github.com/hynekcer/7e8641998bb57b48c6146fd3640f4fd7" rel="nofollow noreferrer">original C code (copy here)</a>  from the question, if duplicate strings are longer than 10000 characters.</p>
<pre><code>from itertools import groupby
from operator import itemgetter

def longest_common_substring(text):
    """Get the longest common substrings and their positions.
    &gt;&gt;&gt; longest_common_substring('banana')
    {'ana': [1, 3]}
    &gt;&gt;&gt; text = "not so Agamemnon, who spoke fiercely to "
    &gt;&gt;&gt; sorted(longest_common_substring(text).items())
    [(' s', [3, 21]), ('no', [0, 13]), ('o ', [5, 20, 38])]

    This function can be easy modified for any criteria, e.g. for searching ten
    longest non overlapping repeated substrings.
    """
    sa, rsa, lcp = suffix_array(text)
    maxlen = max(lcp)
    result = {}
    for i in range(1, len(text)):
        if lcp[i] == maxlen:
            j1, j2, h = sa[i - 1], sa[i], lcp[i]
            assert text[j1:j1 + h] == text[j2:j2 + h]
            substring = text[j1:j1 + h]
            if not substring in result:
                result[substring] = [j1]
            result[substring].append(j2)
    return dict((k, sorted(v)) for k, v in result.items())

def suffix_array(text, _step=16):
    """Analyze all common strings in the text.

    Short substrings of the length _step a are first pre-sorted. The are the 
    results repeatedly merged so that the garanteed number of compared
    characters bytes is doubled in every iteration until all substrings are
    sorted exactly.

    Arguments:
        text:  The text to be analyzed.
        _step: Is only for optimization and testing. It is the optimal length
               of substrings used for initial pre-sorting. The bigger value is
               faster if there is enough memory. Memory requirements are
               approximately (estimate for 32 bit Python 3.3):
                   len(text) * (29 + (_size + 20 if _size &gt; 2 else 0)) + 1MB

    Return value:      (tuple)
      (sa, rsa, lcp)
        sa:  Suffix array                  for i in range(1, size):
               assert text[sa[i-1]:] &lt; text[sa[i]:]
        rsa: Reverse suffix array          for i in range(size):
               assert rsa[sa[i]] == i
        lcp: Longest common prefix         for i in range(1, size):
               assert text[sa[i-1]:sa[i-1]+lcp[i]] == text[sa[i]:sa[i]+lcp[i]]
               if sa[i-1] + lcp[i] &lt; len(text):
                   assert text[sa[i-1] + lcp[i]] &lt; text[sa[i] + lcp[i]]
    &gt;&gt;&gt; suffix_array(text='banana')
    ([5, 3, 1, 0, 4, 2], [3, 2, 5, 1, 4, 0], [0, 1, 3, 0, 0, 2])

    Explanation: 'a' &lt; 'ana' &lt; 'anana' &lt; 'banana' &lt; 'na' &lt; 'nana'
    The Longest Common String is 'ana': lcp[2] == 3 == len('ana')
    It is between  tx[sa[1]:] == 'ana' &lt; 'anana' == tx[sa[2]:]
    """
    tx = text
    size = len(tx)
    step = min(max(_step, 1), len(tx))
    sa = list(range(len(tx)))
    sa.sort(key=lambda i: tx[i:i + step])
    grpstart = size * [False] + [True]  # a boolean map for iteration speedup.
    # It helps to skip yet resolved values. The last value True is a sentinel.
    rsa = size * [None]
    stgrp, igrp = '', 0
    for i, pos in enumerate(sa):
        st = tx[pos:pos + step]
        if st != stgrp:
            grpstart[igrp] = (igrp &lt; i - 1)
            stgrp = st
            igrp = i
        rsa[pos] = igrp
        sa[i] = pos
    grpstart[igrp] = (igrp &lt; size - 1 or size == 0)
    while grpstart.index(True) &lt; size:
        # assert step &lt;= size
        nextgr = grpstart.index(True)
        while nextgr &lt; size:
            igrp = nextgr
            nextgr = grpstart.index(True, igrp + 1)
            glist = []
            for ig in range(igrp, nextgr):
                pos = sa[ig]
                if rsa[pos] != igrp:
                    break
                newgr = rsa[pos + step] if pos + step &lt; size else -1
                glist.append((newgr, pos))
            glist.sort()
            for ig, g in groupby(glist, key=itemgetter(0)):
                g = [x[1] for x in g]
                sa[igrp:igrp + len(g)] = g
                grpstart[igrp] = (len(g) &gt; 1)
                for pos in g:
                    rsa[pos] = igrp
                igrp += len(g)
        step *= 2
    del grpstart
    # create LCP array
    lcp = size * [None]
    h = 0
    for i in range(size):
        if rsa[i] &gt; 0:
            j = sa[rsa[i] - 1]
            while i != size - h and j != size - h and tx[i + h] == tx[j + h]:
                h += 1
            lcp[rsa[i]] = h
            if h &gt; 0:
                h -= 1
    if size &gt; 0:
        lcp[0] = 0
    return sa, rsa, lcp
</code></pre>
<p>I prefer this solution over <a href="http://webglimpse.net/pubs/suffix.pdf" rel="nofollow noreferrer">more complicated O(n log n)</a> because Python has a very fast list sorting (list.sort), probably faster than necessary linear time operations in the method from that article, that should be O(n) under very special presumptions of random strings together with a small alphabet (typical for DNA genom analyze). I read in <a href="http://www.siam.org/proceedings/alenex/2011/alx11_03_gogs.pdf" rel="nofollow noreferrer">Gog 2011</a> that worsest-case O(n log n) of my algorithm can be in practice faster than many O(n) algorithm, that can not use CPU memory cache.</p>
<p>The code in another answer based on <a href="https://stackoverflow.com/a/13562507/448474">grow_chains</a> is 19 times slower than the original example from the question, if the text contain a repeated string 8 kB long. Long repeated texts are not typical for classical literature, but they are frequent e.g. in "independent" school homeworks collections. The program should not freeze on it.</p>
<p>I wrote <a href="https://gist.github.com/hynekcer/fa340f3b63826168ffc0c4b33310ae9c" rel="nofollow noreferrer">an example and tests with the same code</a> for Python 2.7, 3.3 - 3.6.</p>
</div>
<div class="post-text" itemprop="text">
<p>The main problem seems to be that python does slicing by copy: <a href="https://stackoverflow.com/a/5722068/538551">https://stackoverflow.com/a/5722068/538551</a></p>
<p>You'll have to use a <a href="http://docs.python.org/3/library/stdtypes.html#memoryview" rel="nofollow noreferrer">memoryview</a> instead to get a reference instead of a copy. When I did this, the program hung <strong>after</strong> the <code>idx.sort</code> function (which was very fast).</p>
<p>I'm sure with a little work, you can get the rest working.</p>
<p><strong>Edit:</strong></p>
<p><strike>The above change will not work as a drop-in replacement because <code>cmp</code> does not work the same way as <code>strcmp</code>.  For example, try the following C code:</strike></p>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;string.h&gt;

int main() {
    char* test1 = "ovided by The Internet Classics Archive";
    char* test2 = "rovided by The Internet Classics Archive.";
    printf("%d\n", strcmp(test1, test2));
}
</code></pre>
<p>And compare the result to this python:</p>
<pre><code>test1 = "ovided by The Internet Classics Archive";
test2 = "rovided by The Internet Classics Archive."
print(cmp(test1, test2))
</code></pre>
<p>The C code prints <code>-3</code> on my machine while the python version prints <code>-1</code>. It looks like the example <code>C</code> code is abusing the return value of <code>strcmp</code> (it IS used in <code>qsort</code> after all). I couldn't find any documentation on when <code>strcmp</code> will return something other than <code>[-1, 0, 1]</code>, but adding a <code>printf</code> to <code>pstrcmp</code> in the original code showed a lot of values outside of that range (3, -31, 5 were the first 3 values).</p>
<p>To make sure that <code>-3</code> wasn't some error code, if we reverse test1 and test2, we'll get <code>3</code>.</p></div>
<div class="post-text" itemprop="text">
<p>The translation of the algorithm into Python:</p>
<pre class="lang-py prettyprint-override"><code>from itertools import imap, izip, starmap, tee
from os.path   import commonprefix

def pairwise(iterable): # itertools recipe
    a, b = tee(iterable)
    next(b, None)
    return izip(a, b)

def longest_duplicate_small(data):
    suffixes = sorted(data[i:] for i in xrange(len(data))) # O(n*n) in memory
    return max(imap(commonprefix, pairwise(suffixes)), key=len)
</code></pre>
<p><a href="https://stackoverflow.com/q/2282579/4279"><code>buffer()</code></a> allows to get a substring without copying:</p>
<pre class="lang-py prettyprint-override"><code>def longest_duplicate_buffer(data):
    n = len(data)
    sa = sorted(xrange(n), key=lambda i: buffer(data, i)) # suffix array
    def lcp_item(i, j):  # find longest common prefix array item
        start = i
        while i &lt; n and data[i] == data[i + j - start]:
            i += 1
        return i - start, start
    size, start = max(starmap(lcp_item, pairwise(sa)), key=lambda x: x[0])
    return data[start:start + size]
</code></pre>
<p>It takes 5 seconds on my machine for the <a href="http://classics.mit.edu/Homer/iliad.mb.txt" rel="nofollow noreferrer"><code>iliad.mb.txt</code></a>. </p>
<p>In principle it is possible to find the duplicate in O(n) time and O(n) memory using a <a href="http://en.wikipedia.org/wiki/Suffix_array" rel="nofollow noreferrer">suffix array</a> augmented with a <a href="http://en.wikipedia.org/wiki/LCP_array" rel="nofollow noreferrer">lcp array</a>.</p>
<hr/>
<p><sup>Note: <code>*_memoryview()</code> is deprecated by <code>*_buffer()</code> version</sup></p>
<p>More memory efficient version (compared to longest_duplicate_small()):</p>
<pre class="lang-py prettyprint-override"><code>def cmp_memoryview(a, b):
    for x, y in izip(a, b):
        if x &lt; y:
            return -1
        elif x &gt; y:
            return 1
    return cmp(len(a), len(b))

def common_prefix_memoryview((a, b)):
    for i, (x, y) in enumerate(izip(a, b)):
        if x != y:
            return a[:i]
    return a if len(a) &lt; len(b) else b

def longest_duplicate(data):
    mv = memoryview(data)
    suffixes = sorted((mv[i:] for i in xrange(len(mv))), cmp=cmp_memoryview)
    result = max(imap(common_prefix_memoryview, pairwise(suffixes)), key=len)
    return result.tobytes()
</code></pre>
<p>It takes 17 seconds on my machine for the <code>iliad.mb.txt</code>. The result is:</p>
<pre>
On this the rest of the Achaeans with one voice were for respecting
the priest and taking the ransom that he offered; but not so Agamemnon,
who spoke fiercely to him and sent him roughly away. 
</pre>
<p>I had to define custom functions to compare <code>memoryview</code> objects because <code>memoryview</code> comparison either raises an exception in Python 3 or produces wrong result in Python 2:</p>
<pre class="lang-py prettyprint-override"><code>&gt;&gt;&gt; s = b"abc"
&gt;&gt;&gt; memoryview(s[0:]) &gt; memoryview(s[1:])
True
&gt;&gt;&gt; memoryview(s[0:]) &lt; memoryview(s[1:])
True
</code></pre>
<p>Related questions:</p>
<p><a href="https://stackoverflow.com/questions/2172033/find-the-longest-repeating-string-and-the-number-of-times-it-repeats-in-a-given">Find the longest repeating string and the number of times it repeats in a given string</a></p>
<p><a href="https://stackoverflow.com/questions/398811/finding-long-repeated-substrings-in-a-massive-string">finding long repeated substrings in a massive string</a></p>
</div>
<div class="post-text" itemprop="text">
<p>This version takes about 17 secs on my circa-2007 desktop using totally different algorithm:</p>
<pre><code>#!/usr/bin/env python

ex = open("iliad.mb.txt").read()

chains = dict()

# populate initial chains dictionary
for (a,b) in enumerate(zip(ex,ex[1:])) :
    s = ''.join(b)
    if s not in chains :
        chains[s] = list()

    chains[s].append(a)

def grow_chains(chains) :
    new_chains = dict()
    for (string,pos) in chains :
        offset = len(string)
        for p in pos :
            if p + offset &gt;= len(ex) : break

            # add one more character
            s = string + ex[p + offset]

            if s not in new_chains :
                new_chains[s] = list()

            new_chains[s].append(p)
    return new_chains

# grow and filter, grow and filter
while len(chains) &gt; 1 :
    print 'length of chains', len(chains)

    # remove chains that appear only once
    chains = [(i,chains[i]) for i in chains if len(chains[i]) &gt; 1]

    print 'non-unique chains', len(chains)
    print [i[0] for i in chains[:3]]

    chains = grow_chains(chains)
</code></pre>
<p>The basic idea is to create a list of substrings and positions where they occure, thus eliminating the need to compare same strings again and again. The resulting list look like <code>[('ind him, but', [466548, 739011]), (' bulwark bot', [428251, 428924]), (' his armour,', [121559, 124919, 193285, 393566, 413634, 718953, 760088])]</code>. Unique strings are removed. Then every list member grows by 1 character and new list is created. Unique strings are removed again. And so on and so forth...</p>
</div>
<span class="comment-copy">How long does the C code take? How long does your code take?</span>
<span class="comment-copy">@tjameson C codes use 0.3 seconds. I don't know how long my codes takes as it never ends(at least 10 minutes)..</span>
<span class="comment-copy">The C code is slow because it fails to keep track of the "longest match so far" when sorting, and has to check everything a second time. The Python is slow for the same reason, plus because it's operating on strings and not pointers to strings, plus because it's Python.</span>
<span class="comment-copy"><code>example[a:]</code> copies a string each time (<code>O(N)</code>). So your sort is <code>O(N*N*logN)</code>. For iliad it is <code>~10**12</code> operation that is slow.</span>
<span class="comment-copy">Since Programming Swines, err, sorry Pearls, relies heavily on various forms of undefined, unspecified and imp.defined behavior, you cannot easily translate code from it to another language which doesn't have the same kind of non-specified behavior.</span>
<span class="comment-copy">the above link of the example with tests is broken. Could you please updated it?</span>
<span class="comment-copy">I fixed the links to my code and to the original C by paste my copies.</span>
<span class="comment-copy">Thanks, tjameson! But even using <code>memoryview</code>, you still need to pass the string to <code>cmp</code>, right? Then it still need to pass-by-value?</span>
<span class="comment-copy">This one doesn't work. As <code>cmp</code> can't be used for <code>memoryview</code> object</span>
<span class="comment-copy">Bentley's code does <i>not</i> abuse <code>strcmp</code>. It just uses it to compare strings in <code>qsort</code>, which in turn never relies on anything but the <i>sign</i> of the return value.</span>
<span class="comment-copy">@larsmans - As mentioned in my comment, I realized this about 5 minutes after posting. Right about the time I stopped staring at the code... Revising answer.</span>
<span class="comment-copy">memoryview comparison doesn't work. See example in <a href="http://stackoverflow.com/a/13574862/4279">my answer</a></span>
<span class="comment-copy">since your code requires python 3.+ and i don't have access to that version at the moment, could you please provide running time for my version of code in your environment as well?</span>
<span class="comment-copy">@lenik: The code works on Python 2.7. What could make you think that it were for Python 3?</span>
<span class="comment-copy">could you please stop arguing about unrelated things and just provide the running time?</span>
<span class="comment-copy">@lenik: if you can't run both Python 2.7 and 3. Here's the running time: 12 seconds.</span>
<span class="comment-copy">Side-note: The reason it produces an incorrect result on Python 2 (and an exception on Py3) is that <code>memoryview</code> only defines the equivalent of <code>__eq__</code> and <code>__ne__</code>, not the rest of the rich comparison operators; on Py2, this means it goes to the comparison of last resort (which ends up comparing the objects' memory addresses, totally useless), while Python 3 informs you that the comparison isn't supported. There is <a href="https://bugs.python.org/issue20399" rel="nofollow noreferrer">a bug open to fix this</a>, but it's seen no action in the last five years.</span>
<span class="comment-copy">If more than one repeated substrings have the same maximal length nothing is returned. Example: <code>ex = 'ABCxABCyDEFzDEF'</code></span>
<span class="comment-copy">@hynekcer the last set always empty (that's the loop stopping condition), but the one before that contains: <code>['ABC', 'DEF']</code> -- i don't see why this is wrong? there are obvious limitations in my code -- only 3 first chains are printed, if there are more -- you have to modify the code or something, pretty printing was never my goal.</span>
<span class="comment-copy">I expect that the result will be finally in the chain variable but they are lost. Debug printing is not important for an algorithm.</span>
<span class="comment-copy">@hynekcer debug printing helps to understand how it works. if you need the answer only -- save the result of filtering in the temporary variable and when it's empty -- print whatever you have in <code>chains</code> -- that should work just fine for any number of substrings of any length.</span>
<span class="comment-copy">The biggest problem is that your algorithm can require more than <code>N * N / 4</code> bytes of memory where N is the length of input string. Example: <code>ex = ' '.join('%03s' % i for i in range(500))</code> I print <code>sum(len(string) for string in chains)</code> and I see that the biggest value is 1001000. Required time is proportional to <code>N * N * N</code>.</span>
