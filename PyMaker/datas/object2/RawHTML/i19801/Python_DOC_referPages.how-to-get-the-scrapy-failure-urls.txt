<div class="post-text" itemprop="text">
<p>I'm a newbie of scrapy and it's amazing crawler framework i have known! </p>
<p>In my project, I sent more than 90, 000 requests, but there are some of them failed. 
I set the log level to be INFO, and i just can see some statistics but no details. </p>
<pre><code>2012-12-05 21:03:04+0800 [pd_spider] INFO: Dumping spider stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.ConnectionDone': 1,
 'downloader/request_bytes': 46282582,
 'downloader/request_count': 92383,
 'downloader/request_method_count/GET': 92383,
 'downloader/response_bytes': 123766459,
 'downloader/response_count': 92382,
 'downloader/response_status_count/200': 92382,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2012, 12, 5, 13, 3, 4, 836000),
 'item_scraped_count': 46191,
 'request_depth_max': 1,
 'scheduler/memory_enqueued': 92383,
 'start_time': datetime.datetime(2012, 12, 5, 12, 23, 25, 427000)}
</code></pre>
<p>Is there any way to get more detail report? For example, show those failed URLs. Thanks!</p>
</div>
<div class="post-text" itemprop="text">
<p>Yes, this is possible. </p>
<p>I added a failed_urls list to my spider class and appended urls to it if the response's status was 404 (this will need to be extended to cover other error statuses). </p>
<p>Then I added a handle that joins the list into a single string and add it to the stats when the spider is closed.</p>
<p>Based on your comments, it's possible to track Twisted errors.</p>
<pre><code>from scrapy.spider import BaseSpider
from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals

class MySpider(BaseSpider):
    handle_httpstatus_list = [404] 
    name = "myspider"
    allowed_domains = ["example.com"]
    start_urls = [
        'http://www.example.com/thisurlexists.html',
        'http://www.example.com/thisurldoesnotexist.html',
        'http://www.example.com/neitherdoesthisone.html'
    ]

    def __init__(self, category=None):
        self.failed_urls = []

    def parse(self, response):
        if response.status == 404:
            self.crawler.stats.inc_value('failed_url_count')
            self.failed_urls.append(response.url)

    def handle_spider_closed(spider, reason):
        self.crawler.stats.set_value('failed_urls', ','.join(spider.failed_urls))

    def process_exception(self, response, exception, spider):
        ex_class = "%s.%s" % (exception.__class__.__module__, exception.__class__.__name__)
        self.crawler.stats.inc_value('downloader/exception_count', spider=spider)
        self.crawler.stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)

    dispatcher.connect(handle_spider_closed, signals.spider_closed)
</code></pre>
<p>Output (the downloader/exception_count* stats will only appear if exceptions are actually thrown - I simulated them by trying to run the spider after I'd turned off my wireless adapter):</p>
<pre><code>2012-12-10 11:15:26+0000 [myspider] INFO: Dumping Scrapy stats:
    {'downloader/exception_count': 15,
     'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 15,
     'downloader/request_bytes': 717,
     'downloader/request_count': 3,
     'downloader/request_method_count/GET': 3,
     'downloader/response_bytes': 15209,
     'downloader/response_count': 3,
     'downloader/response_status_count/200': 1,
     'downloader/response_status_count/404': 2,
     'failed_url_count': 2,
     'failed_urls': 'http://www.example.com/thisurldoesnotexist.html, http://www.example.com/neitherdoesthisone.html'
     'finish_reason': 'finished',
     'finish_time': datetime.datetime(2012, 12, 10, 11, 15, 26, 874000),
     'log_count/DEBUG': 9,
     'log_count/ERROR': 2,
     'log_count/INFO': 4,
     'response_received_count': 3,
     'scheduler/dequeued': 3,
     'scheduler/dequeued/memory': 3,
     'scheduler/enqueued': 3,
     'scheduler/enqueued/memory': 3,
     'spider_exceptions/NameError': 2,
     'start_time': datetime.datetime(2012, 12, 10, 11, 15, 26, 560000)}
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's another example how to handle and collect 404 errors (checking github help pages):</p>
<pre><code>from scrapy.selector import HtmlXPathSelector
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.item import Item, Field


class GitHubLinkItem(Item):
    url = Field()
    referer = Field()
    status = Field()


class GithubHelpSpider(CrawlSpider):
    name = "github_help"
    allowed_domains = ["help.github.com"]
    start_urls = ["https://help.github.com", ]
    handle_httpstatus_list = [404]
    rules = (Rule(SgmlLinkExtractor(), callback='parse_item', follow=True),)

    def parse_item(self, response):
        if response.status == 404:
            item = GitHubLinkItem()
            item['url'] = response.url
            item['referer'] = response.request.headers.get('Referer')
            item['status'] = response.status

            return item
</code></pre>
<p>Just run <code>scrapy runspider</code> with <code>-o output.json</code> and see list of items in the <code>output.json</code> file.</p>
</div>
<div class="post-text" itemprop="text">
<p>The answers from @Talvalin and @alecxe helped me a great deal, but they do not seem to capture downloader events that do not generate a response object (for instance, <code>twisted.internet.error.TimeoutError</code> and <code>twisted.web.http.PotentialDataLoss</code>). These errors show up in the stats dump at the end of the run, but without any meta info. </p>
<p>As I found out <a href="https://groups.google.com/forum/#!forum/scrapy-users" rel="nofollow noreferrer">here</a>, the errors are tracked by the <a href="https://github.com/scrapy/scrapy/blob/master/scrapy/downloadermiddlewares/stats.py" rel="nofollow noreferrer">stats.py</a> middleware, captured in the <code>DownloaderStats</code> class' <code>process_exception</code> method, and specifically in the <code>ex_class</code> variable, which increments each error type as necessary, and then dumps the counts at the end of the run.  </p>
<p>To match such errors with information from the corresponding request object, you can add a unique id to each request (via <code>request.meta</code>), then pull it into the <code>process_exception</code> method of <code>stats.py</code>:</p>
<pre><code>self.stats.set_value('downloader/my_errs/{0}'.format(request.meta), ex_class)
</code></pre>
<p>That will generate a unique string for each downloader-based error not accompanied by a response. You can then save the altered <code>stats.py</code> as something else (e.g. <code>my_stats.py</code>), add it to the downloadermiddlewares (with the right precedence), and disable the stock <code>stats.py</code>:</p>
<pre><code>DOWNLOADER_MIDDLEWARES = {
    'myproject.my_stats.MyDownloaderStats': 850,
    'scrapy.downloadermiddleware.stats.DownloaderStats': None,
    }
</code></pre>
<p>The output at the end of the run looks like this (here using meta info where each request url is mapped to a group_id and member_id separated by a slash, like <code>'0/14'</code>):</p>
<pre><code>{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web.http.PotentialDataLoss': 3,
 'downloader/my_errs/0/1': 'twisted.web.http.PotentialDataLoss',
 'downloader/my_errs/0/38': 'twisted.web.http.PotentialDataLoss',
 'downloader/my_errs/0/86': 'twisted.web.http.PotentialDataLoss',
 'downloader/request_bytes': 47583,
 'downloader/request_count': 133,
 'downloader/request_method_count/GET': 133,
 'downloader/response_bytes': 3416996,
 'downloader/response_count': 130,
 'downloader/response_status_count/200': 95,
 'downloader/response_status_count/301': 24,
 'downloader/response_status_count/302': 8,
 'downloader/response_status_count/500': 3,
 'finish_reason': 'finished'....}
</code></pre>
<p><a href="https://stackoverflow.com/a/11071745/1599229">This answer</a> deals with non-downloader-based errors.</p>
</div>
<div class="post-text" itemprop="text">
<p>Scrapy ignore 404 by default and do not parse. To handle 404 error do this.
This is very easy , if you are getting error code 404 in response ,you can handle this is with very easy way.....
in settings write </p>
<pre><code>HTTPERROR_ALLOWED_CODES = [404,403]
</code></pre>
<p>and then handle the response status code in your parse function.</p>
<pre><code> def parse(self,response):
     if response.status == 404:
         #your action on error
</code></pre>
<p>in settings and get response in parse function</p>
</div>
<div class="post-text" itemprop="text">
<p>As of scrapy 0.24.6, the method suggested by <a href="https://stackoverflow.com/a/14593509/1906307">alecxe</a> won't catch errors with the start URLs. To record errors with the start URLs you need to override <code>parse_start_urls</code>. Adapting alexce's answer for this purpose, you'd get:</p>
<pre><code>from scrapy.selector import HtmlXPathSelector
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.item import Item, Field

class GitHubLinkItem(Item):
    url = Field()
    referer = Field()
    status = Field()

class GithubHelpSpider(CrawlSpider):
    name = "github_help"
    allowed_domains = ["help.github.com"]
    start_urls = ["https://help.github.com", ]
    handle_httpstatus_list = [404]
    rules = (Rule(SgmlLinkExtractor(), callback='parse_item', follow=True),)

    def parse_start_url(self, response):
        return self.handle_response(response)

    def parse_item(self, response):
        return self.handle_response(response)

    def handle_response(self, response):
        if response.status == 404:
            item = GitHubLinkItem()
            item['url'] = response.url
            item['referer'] = response.request.headers.get('Referer')
            item['status'] = response.status

            return item
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>This is an update on this question. I ran in to a similar problem and needed to use the scrapy signals to call a function in my pipeline. I have edited @Talvalin's code, but wanted to make an answer just for some more clarity. </p>
<p>Basically, you should add in self as an argument for handle_spider_closed.
You should also call the dispatcher in init so that you can pass the spider instance (self) to the handleing method.  </p>
<pre><code>from scrapy.spider import Spider
from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals

class MySpider(Spider):
    handle_httpstatus_list = [404] 
    name = "myspider"
    allowed_domains = ["example.com"]
    start_urls = [
        'http://www.example.com/thisurlexists.html',
        'http://www.example.com/thisurldoesnotexist.html',
        'http://www.example.com/neitherdoesthisone.html'
    ]

    def __init__(self, category=None):
        self.failed_urls = []
        # the dispatcher is now called in init
        dispatcher.connect(self.handle_spider_closed,signals.spider_closed) 


    def parse(self, response):
        if response.status == 404:
            self.crawler.stats.inc_value('failed_url_count')
            self.failed_urls.append(response.url)

    def handle_spider_closed(self, spider, reason): # added self 
        self.crawler.stats.set_value('failed_urls',','.join(spider.failed_urls))

    def process_exception(self, response, exception, spider):
        ex_class = "%s.%s" % (exception.__class__.__module__,  exception.__class__.__name__)
        self.crawler.stats.inc_value('downloader/exception_count', spider=spider)
        self.crawler.stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)
</code></pre>
<p>I hope this helps anyone with the same problem in the future.</p>
</div>
<div class="post-text" itemprop="text">
<p>In addition to some of these answers, if you want to track Twisted errors, I would take a look at using the Request object's <code>errback</code> parameter, on which you can set a callback function to be called with the <a href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html" rel="nofollow noreferrer">Twisted Failure</a> on a request failure.
In addition to the url, this method can allow you to track the type of failure.</p>
<p>You can then log the urls by using: <code>failure.request.url</code> (where <code>failure</code> is the Twisted <code>Failure</code> object passed into <code>errback</code>).</p>
<pre><code># these would be in a Spider
def start_requests(self):
    for url in self.start_urls:
        yield scrapy.Request(url, callback=self.parse,
                                  errback=self.handle_error)

def handle_error(self, failure):
    url = failure.request.url
    logging.error('Failure type: %s, URL: %s', failure.type,
                                               url)
</code></pre>
<p>The Scrapy docs give a full example of how this can be done, except that the calls to the Scrapy logger are now <a href="https://doc.scrapy.org/en/latest/topics/logging.html" rel="nofollow noreferrer">depreciated</a>, so I've adapted my example to use Python's built in <a href="https://docs.python.org/3/library/logging.html" rel="nofollow noreferrer">logging</a>):</p>
<p><a href="https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks" rel="nofollow noreferrer">https://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks</a></p>
</div>
<div class="post-text" itemprop="text">
<p>You can capture failed urls in two ways.</p>
<ol>
<li><p>Define scrapy request with errback</p>
<pre><code>class TestSpider(scrapy.Spider):
    def start_requests(self):
        yield scrapy.Request(url, callback=self.parse, errback=self.errback)

    def errback(self, failure):
        '''handle failed url (failure.request.url)'''
        pass
</code></pre></li>
<li><p>Use signals.item_dropped</p>
<pre><code>class TestSpider(scrapy.Spider):
    def __init__(self):
        crawler.signals.connect(self.request_dropped, signal=signals.request_dropped)

    def request_dropped(self, request, spider):
        '''handle failed url (request.url)'''
        pass
</code></pre></li>
</ol>
<p><strong>[!Notice]</strong> Scrapy request with errback can not catch some auto retry failure, like connection error, <a href="https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#retry-http-codes" rel="nofollow noreferrer">RETRY_HTTP_CODES</a> in settings.</p>
</div>
<div class="post-text" itemprop="text">
<p>Basically Scrapy Ignores 404 Error by Default, It was defined in httperror middleware.</p>
<p>So, Add <strong>HTTPERROR_ALLOW_ALL = True</strong> to your settings file.</p>
<p>After this you can access <strong>response.status</strong> through your <strong>parse function</strong>.</p>
<p>You can handle it like this.</p>
<pre><code>def parse(self,response):
    if response.status==404:
        print(response.status)
    else:
        do something
</code></pre>
</div>
<span class="comment-copy">This no longer works. <code>exceptions.NameError: global name 'self' is not defined</code> error occurs. <code>BaseSpider</code> is now just <code>Spider</code> <a href="http://doc.scrapy.org/en/0.24/news.html?highlight=basespider#id2" rel="nofollow noreferrer">doc.scrapy.org/en/0.24/news.html?highlight=basespider#id2</a> <a href="https://github.com/scrapy/dirbot/blob/master/dirbot/spiders/dmoz.py" rel="nofollow noreferrer">github.com/scrapy/dirbot/blob/master/dirbot/spiders/dmoz.py</a> but I can't find the fix to get your code working yet @Talvalin.</span>
<span class="comment-copy">Exactly what I'm looking for. I think Scrapy should add this feature to provide convenient access to failure info like URL.</span>
<span class="comment-copy">Use <code>scrapy.downloadermiddlewares.stats</code>  instead of deprecated on latest  (1.0.5) version <code>scrapy.contrib.downloadermiddleware.stats</code></span>
<span class="comment-copy">@ElRuso thanks - have updated the answer</span>
<span class="comment-copy">what a nice answer,but may I asked,if there are some other type of errors,not just only http status code error or downloader error, is these are the all error we should deal with about net error?</span>
<span class="comment-copy">How would you gracefully close the spider in those circumstances?</span>
<span class="comment-copy">@not2qubit What's circumstances?</span>
<span class="comment-copy">There seem to be some funny stuff going on with <i>Twisted</i> so that I keep getting <a href="https://stackoverflow.com/questions/52757819/how-to-handle-connection-or-download-error-in-scrapy">this error</a> even though I have already ordered the spider to shut down. So perhaps there is a better method to shut down the spider, before it retries, or even before that.</span>
<span class="comment-copy">@not2qubit  Check <code>self.crawler.crawling</code> in errback and request_dropped. If your shut down the spider, <code>self.crawler.crawling</code> will be <code>False</code>.</span>
