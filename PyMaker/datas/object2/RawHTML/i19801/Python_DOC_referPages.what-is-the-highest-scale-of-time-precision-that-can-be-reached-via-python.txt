<div class="post-text" itemprop="text">
<p>Consider a very simple timer;</p>
<pre><code>start = time.time()
end = time.time() - start 
while(end&lt;5):
    end = time.time() - start
    print end
</code></pre>
<p>how precise is this timer ? I mean compared to real-time clock, how synchronized and real-time is this one ?</p>
<p>Now for the real question ;</p>
<p>What is the smallest scale of time that can be measured precisely with Python ? </p>
</div>
<div class="post-text" itemprop="text">
<p>This is entirely platform dependent. Use the <a href="http://docs.python.org/2/library/timeit.html#timeit.default_timer" rel="noreferrer"><code>timeit.default_timer()</code> function</a>, it'll return the most precise timer <em>for your platform</em>.</p>
<p>From the documentation:</p>
<blockquote>
<p>Define a default timer, in a platform-specific manner. On Windows, <code>time.clock()</code> has microsecond granularity, but <code>time.time()</code>‘s granularity is 1/60th of a second. On Unix, <code>time.clock()</code> has 1/100th of a second granularity, and <code>time.time()</code> is much more precise.</p>
</blockquote>
<p>So, on Windows, you get microseconds, on Unix, you'll get whatever precision the platform can provide, which is <em>usually</em> (much) better than 1/100th of a second.</p>
</div>
<div class="post-text" itemprop="text">
<p>This entirely depends on the system you are running it on - there is no guarantee Python has any way of tracking time at all.</p>
<p>That said, it's pretty safe to assume you are going to get millisecond accuracy on modern systems, beyond that, it really is highly dependent on the system. To quote <a href="http://docs.python.org/3/library/time.html" rel="nofollow">the docs</a>:</p>
<blockquote>
<p>Although this module is always available, not all functions are
  available on all platforms. Most of the functions defined in this
  module call platform C library functions with the same name. It may
  sometimes be helpful to consult the platform documentation, because
  the semantics of these functions varies among platforms.</p>
</blockquote>
<p>And:</p>
<blockquote>
<p>The precision of the various real-time functions may be less than
  suggested by the units in which their value or argument is expressed.
  E.g. on most Unix systems, the clock “ticks” only 50 or 100 times a
  second.</p>
</blockquote>
</div>
<span class="comment-copy">+1, Useful information.</span>
<span class="comment-copy">And on an external device that has more "accurate" time keeping - a complete article in itself! +1</span>
<span class="comment-copy">@MartijnPieters One thing I don't understand. If in every computer there is a circuit dedicated to the real-time clock (here it is said so <a href="http://stackoverflow.com/questions/25785243/understanding-time-perf-counter-and-time-process-time" title="understanding time perf counter and time process time">stackoverflow.com/questions/25785243/…</a>) how can the timer precision differ from platform (i.e from unix or win) should that be always the same? I mean if the circuit refresh its counter every 1us shouldn't the precision be 1us regardless of the platform?</span>
