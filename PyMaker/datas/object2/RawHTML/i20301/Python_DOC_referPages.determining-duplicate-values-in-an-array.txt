<div class="post-text" itemprop="text">
<p>Suppose I have an array</p>
<pre><code>a = np.array([1, 2, 1, 3, 3, 3, 0])
</code></pre>
<p>How can I (efficiently, Pythonically) find which elements of <code>a</code> are duplicates (i.e., non-unique values)?  In this case the result would be <code>array([1, 3, 3])</code> or possibly <code>array([1, 3])</code> if efficient.</p>
<p>I've come up with a few methods that appear to work:</p>
<h3>Masking</h3>
<pre><code>m = np.zeros_like(a, dtype=bool)
m[np.unique(a, return_index=True)[1]] = True
a[~m]
</code></pre>
<h3>Set operations</h3>
<pre><code>a[~np.in1d(np.arange(len(a)), np.unique(a, return_index=True)[1], assume_unique=True)]
</code></pre>
<p>This one is cute but probably illegal (as <code>a</code> isn't actually unique):</p>
<pre><code>np.setxor1d(a, np.unique(a), assume_unique=True)
</code></pre>
<h3>Histograms</h3>
<pre><code>u, i = np.unique(a, return_inverse=True)
u[np.bincount(i) &gt; 1]
</code></pre>
<h3>Sorting</h3>
<pre><code>s = np.sort(a, axis=None)
s[:-1][s[1:] == s[:-1]]
</code></pre>
<h3>Pandas</h3>
<pre><code>s = pd.Series(a)
s[s.duplicated()]
</code></pre>
<p>Is there anything I've missed?  I'm not necessarily looking for a numpy-only solution, but it has to work with numpy data types and be efficient on medium-sized data sets (up to 10 million in size).</p>
<hr/>
<h2>Conclusions</h2>
<p>Testing with a 10 million size data set (on a 2.8GHz Xeon):</p>
<pre><code>a = np.random.randint(10**7, size=10**7)
</code></pre>
<p>The fastest is sorting, at 1.1s.  The dubious <code>xor1d</code> is second at 2.6s, followed by masking and Pandas <code>Series.duplicated</code> at 3.1s, <code>bincount</code> at 5.6s, and <code>in1d</code> and senderle's <code>setdiff1d</code> both at 7.3s.  Steven's <code>Counter</code> is only a little slower, at 10.5s; trailing behind are Burhan's <code>Counter.most_common</code> at 110s and DSM's <code>Counter</code> subtraction at 360s.</p>
<p>I'm going to use sorting for performance, but I'm accepting Steven's answer because the performance is acceptable and it <em>feels</em> clearer and more Pythonic.</p>
<p>Edit: discovered the Pandas solution.  If Pandas is available it's clear and performs well.</p>
</div>
<div class="post-text" itemprop="text">
<p>I think this is most clear done outside of <code>numpy</code>.  You'll have to time it against your <code>numpy</code> solutions if you are concerned with speed.</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; a = np.array([1, 2, 1, 3, 3, 3, 0])
&gt;&gt;&gt; [item for item, count in Counter(a).iteritems() if count &gt; 1]
[1, 3]
</code></pre>
<p><em>note:</em>  This is similar to Burhan Khalid's answer, but the use of <code>iteritems</code> without subscripting in the condition should be faster.</p>
</div>
<div class="post-text" itemprop="text">
<p>As of numpy version 1.9.0, <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html" rel="noreferrer"><code>np.unique</code></a> has an argument <code>return_counts</code> which greatly simplifies your task:</p>
<pre><code>u, c = np.unique(a, return_counts=True)
dup = u[c &gt; 1]
</code></pre>
<p>This is similar to using <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="noreferrer"><code>Counter</code></a>, except you get a pair of arrays instead of a mapping. I'd be curious to see how they perform relative to each other.</p>
</div>
<div class="post-text" itemprop="text">
<p>People have already suggested <code>Counter</code> variants, but here's one which doesn't use a listcomp:</p>
<pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; a = [1, 2, 1, 3, 3, 3, 0]
&gt;&gt;&gt; (Counter(a) - Counter(set(a))).keys()
[1, 3]
</code></pre>
<p>[Posted not because it's efficient -- it's not -- but because I think it's cute that you can subtract <code>Counter</code> instances.]</p>
</div>
<div class="post-text" itemprop="text">
<p>For Python 2.7+</p>
<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; n = numpy.array([1,1,2,3,3,3,0])
&gt;&gt;&gt; [x[1] for x in Counter(n).most_common() if x[0] &gt; 1]
[3, 1]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's another approach using set operations that I think is a bit more straightforward than the ones you offer:</p>
<pre><code>&gt;&gt;&gt; indices = np.setdiff1d(np.arange(len(a)), np.unique(a, return_index=True)[1])
&gt;&gt;&gt; a[indices]
array([1, 3, 3])
</code></pre>
<p>I suppose you're asking for <code>numpy</code>-only solutions, since if that's not the case, it's very difficult to argue with just using a <code>Counter</code> instead. I think you should make that requirement explicit though.</p>
</div>
<div class="post-text" itemprop="text">
<p>If <code>a</code> is made up of small integers you can use numpy.bincount directly:</p>
<pre><code>import numpy as np

a = np.array([3, 2, 2, 0, 4, 3])
counts = np.bincount(a)
print np.where(counts &gt; 1)[0]
# array([2, 3])
</code></pre>
<p>This is very similar your "histogram" method, which is the one I would use if <code>a</code> was not made up of small integers.</p>
</div>
<div class="post-text" itemprop="text">
<p>I'm adding my solution to the pile for this 3 year old question because none of the solutions fit what I wanted or used libs besides numpy. This method finds both the indices of duplicates and values for <em>distinct</em> sets of duplicates.</p>
<pre><code>import numpy as np

A = np.array([1,2,3,4,4,4,5,6,6,7,8])

# Record the indices where each unique element occurs.
list_of_dup_inds = [np.where(a == A)[0] for a in np.unique(A)]

# Filter out non-duplicates.
list_of_dup_inds = filter(lambda inds: len(inds) &gt; 1, list_of_dup_inds)

for inds in list_of_dup_inds: print inds, A[inds]
# &gt;&gt; [3 4 5] [4 4 4]
# &gt;&gt; [7 8] [6 6]
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>If the array is a sorted numpy array, then just do:</p>
<pre><code>a = np.array([1, 2, 2, 3, 4, 5, 5, 6])
rep_el = a[np.diff(a) == 0]
</code></pre>
</div>
<span class="comment-copy">Could you explain why the sorting solution works? I tried it out but for some reason I don't really get it.</span>
<span class="comment-copy">@Markus if you sort an array, any duplicate values are adjacent.  You then use a boolean mask to take only those items that are equal to the previous item.</span>
<span class="comment-copy">Shouldn't it be <code>s[:-1][ s[1:] == s[:-1] ]</code>? I get an <code>IndexError</code> otherwise, the boolean mask being one element shorter than the <code>s</code>-array....</span>
<span class="comment-copy">@snake_charmer I think earlier versions of numpy were more forgiving in this regard. I'll fix it, thanks.</span>
<span class="comment-copy">Note: Counter(a).items()  has to be used in python 3</span>
<span class="comment-copy">This is a really good one! easy! efficient!</span>
<span class="comment-copy">shouldn't x[0] &gt; 1 be x[1] &gt; 1? the latter x represents the frequency.</span>
<span class="comment-copy">I see it as a wart on this approach is that the <code>3</code> is repeated while the <code>1</code> is not.  It would be nice to have it one way or the other.  (This is not a criticism of your answer so much as of the original approach by the OP.)</span>
<span class="comment-copy">@StevenRumbalski, yeah, I see what you mean. My sense is that the repeated <code>3</code> makes sense if what's really needed is a mask rather than a list of items; if what's needed is a list of items, then I agree that not having repeated items is better.</span>
<span class="comment-copy">I'm not opposed to using <code>Counter</code>, but I am concerned about efficiency and compatibility.</span>
<span class="comment-copy">Three years later still, and you can use the <code>return_counts</code> argument to <code>unique</code> for this too. See my answer.</span>
<span class="comment-copy"><code>a[1:][np.diff(a) == 0]</code>, no?</span>
