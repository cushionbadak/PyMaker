<div class="post-text" itemprop="text">
<p>I'm having much trouble trying to understand just how the multiprocessing queue works on python and how to implement it. Lets say I have two python modules that access data from a shared file, let's call these two modules a writer and a reader. My plan is to have both the reader and writer put requests into two separate multiprocessing queues, and then have a third process pop these requests in a loop and execute as such.</p>
<p>My main problem is that I really don't know how to implement multiprocessing.queue correctly, you cannot really instantiate the object for each process since they will be separate queues, how do you make sure that all processes relate to a shared queue (or in this case, queues)</p>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>My main problem is that I really don't know how to implement multiprocessing.queue correctly, you cannot really instantiate the object for each process since they will be separate queues, how do you make sure that all processes relate to a shared queue (or in this case, queues)</p>
</blockquote>
<p>This is a simple example of a reader and writer sharing a single queue...  The writer sends a bunch of integers to the reader; when the writer runs out of numbers, it sends 'DONE', which lets the reader know to break out of the read loop.</p>
<pre><code>from multiprocessing import Process, Queue
import time
import sys

def reader_proc(queue):
    ## Read from the queue; this will be spawned as a separate Process
    while True:
        msg = queue.get()         # Read from the queue and do nothing
        if (msg == 'DONE'):
            break

def writer(count, queue):
    ## Write to the queue
    for ii in range(0, count):
        queue.put(ii)             # Write 'count' numbers into the queue
    queue.put('DONE')

if __name__=='__main__':
    pqueue = Queue() # writer() writes to pqueue from _this_ process
    for count in [10**4, 10**5, 10**6]:             
        ### reader_proc() reads from pqueue as a separate process
        reader_p = Process(target=reader_proc, args=((pqueue),))
        reader_p.daemon = True
        reader_p.start()        # Launch reader_proc() as a separate python process

        _start = time.time()
        writer(count, pqueue)    # Send a lot of stuff to reader()
        reader_p.join()         # Wait for the reader to finish
        print("Sending {0} numbers to Queue() took {1} seconds".format(count, 
            (time.time() - _start)))
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>in "<code>from queue import Queue</code>" there is no module called <code>queue</code>, instead <code>multiprocessing</code> should be used. Therefore, it should look like "<code>from multiprocessing import Queue</code>"</p>
</div>
<span class="comment-copy">pass the Queues to each process class as a parameter when you instantiate them in the parent process.</span>
<span class="comment-copy">Great example. Just as an additional bit of information to address the OP's confusion... This example shows that a shared queue needs to originate from the master process, which is then passed to all of its subprocesses. In order for two completely unrelated processes to share data, they must communicate over some central or associated network device (sockets for example). Something has to coordinate the information.</span>
<span class="comment-copy">nice example.. i'm also new to this topic.. if i have multiple processes running the same target function (with different arguments), how to make sure that they dont clash while putting the data into the queue.. is lock necessary?</span>
<span class="comment-copy">Explicit stop conditions are better than implicit stop conditions</span>
<span class="comment-copy">For Python 3 users, it took me quite a while to figure this out: Import <code>Queue</code> from <code>multiprocessing</code>, not from <code>queue</code>!</span>
<span class="comment-copy">Qsize can go to zero if the queue readers exceed the rate of the queue writer</span>
<span class="comment-copy"><a href="https://docs.python.org/3/library/queue.html" rel="nofollow noreferrer">docs.python.org/3/library/queue.html</a></span>
<span class="comment-copy">While years late, using <code>multiprocessing.Queue</code> is correct. The normal <code>Queue.Queue</code> is used for python <i>threads</i>. When you try to use <code>Queue.Queue</code> with multiprocessing, copies of the Queue object will be created in each child process and the child processes will never be updated. Basically, <code>Queue.Queue</code> works by using a global shared object, and <code>multiprocessing.Queue</code> works using IPC. See: <a href="https://stackoverflow.com/questions/925100/python-queue-multiprocessing-queue-how-they-behave" title="python queue multiprocessing queue how they behave">stackoverflow.com/questions/925100/â€¦</a></span>
