<div class="post-text" itemprop="text">
<p>I'm trying to apply two different masking methods to an input tensor, one is a half normal distribution filter and the other is a simple step function.</p>
<p>While the half Gauss filter works fine, when trying to apply a step function filter, the variable (i.e that defines the point where the step occurs) doesn't seems to learn at all.</p>
<p>This is the filters code:</p>
<pre><code>def per_kernel_step_filter(input,weight_param=20,trainable=True):
    input_shape = input.get_shape().as_list()

    weight_param_v = tf.Variable(np.full((input_shape[-1]),weight_param), dtype=tf.float32, trainable=trainable)
    weight_param_v_c = tf.clip_by_value(weight_param_v, 0, input_shape[-2])
    kernel_filter = tf.transpose(tf.sequence_mask(weight_param_v_c, input_shape[-2], dtype=tf.float32))
    kernel_filter = tf.reshape(kernel_filter,tf.concat([(1,1),kernel_filter.get_shape()],0))

    output = input * kernel_filter
    tf.summary.histogram("weight_param histogram", weight_param_v)

    return output
</code></pre>
<p>And from tensorboard it seems like it doesn't even attached to the Adam optimizer at the end.<a href="https://i.stack.imgur.com/xcdwPb.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/xcdwPb.png"/></a></p>
<p>and <code>weight_param_v</code> is flat on <code>weight_param</code>.</p>
<p>Is it possible that because other operations, e.g <code>sequence_mask</code> the variable becomes non-trainable?</p>
</div>
<div class="post-text" itemprop="text">
<p>The problem in this case is that <a href="https://www.tensorflow.org/api_docs/python/tf/sequence_mask" rel="nofollow noreferrer"><code>tf.sequence_mask</code></a> is not differentiable, that is, there is no analytical function that tells you how much the output (or the loss) changes if you apply a small change to <code>weight_param_v</code>. A possible workaround is to use instead some <a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="nofollow noreferrer">sigmoid</a> or <a href="https://en.wikipedia.org/wiki/Smoothstep" rel="nofollow noreferrer">smoothstep</a> function instead. For example, you could use the <a href="https://en.wikipedia.org/wiki/Logistic_function" rel="nofollow noreferrer">logistic function</a> (<a href="https://www.tensorflow.org/api_docs/python/tf/math/sigmoid" rel="nofollow noreferrer"><code>tf.math.sigmoid</code></a>), shifted so it is centered around the step point, and you can manipulate the points where it is evaluated to control how "steep" it is (note this will affect the gradients and in turn the ability of the variable to learn).</p>
<p>In general, you can use <a href="https://www.tensorflow.org/api_docs/python/tf/gradients" rel="nofollow noreferrer"><code>tf.gradients</code></a> to check if something is differentiable or not. For example, if you have a function <code>my_function</code>, you can take an input <code>x</code> and define <code>y = my_function(x)</code>, then check the output of <code>tf.gradients(y, x)</code>; if it is <code>[None]</code>, then the function is not differentiable.</p>
<pre><code>import tensorflow as tf

x = tf.placeholder(tf.float32, [None])

# Squaring is differentiable
print(tf.gradients(tf.square(x), x))
# [&lt;tf.Tensor 'gradients/Square_grad/Mul_1:0' shape=(?,) dtype=float32&gt;]

# Flooring is not differentiable
print(tf.gradients(tf.floor(x), x))
# [None]

# Sequence mask is not differentiable
print(tf.gradients(tf.sequence_mask(x, dtype=tf.float32), x))
# [None]

# Gather is differentiable for the parameters but not for the indices
x2 = tf.placeholder(tf.int32, [None])
print(tf.gradients(tf.gather(x, x2), [x, x2]))
# [&lt;tensorflow.python.framework.ops.IndexedSlices object at 0x000001F6EDD09160&gt;, None]
</code></pre>
<p>A tricky thing, which I think is what was happening to you in this case, is that the training may work even if there are some <code>None</code> gradients. As long as there is some valid gradient, TensorFlow (or, more specifically, <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer" rel="nofollow noreferrer"><code>tf.train.Optimizer</code></a> and its subclasses) assumes that <code>None</code> gradients are irrelevant. One possible check you could do is, instead of calling <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize" rel="nofollow noreferrer"><code>minimize</code></a> directly, call <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#compute_gradients" rel="nofollow noreferrer"><code>compute_gradients</code></a> and check there are no <code>None</code> gradients before calling <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#apply_gradients" rel="nofollow noreferrer"><code>apply_gradients</code></a>.</p>
</div>
<span class="comment-copy">Yes, I think the problem is that <a href="https://www.tensorflow.org/api_docs/python/tf/sequence_mask" rel="nofollow noreferrer"><code>tf.sequence_mask</code></a> is not differentiable, that is, there is no analytical function that tells you how much the output (or the loss) changes if you apply a small change to <code>weight_param_v</code> (you can check that with <a href="https://www.tensorflow.org/api_docs/python/tf/gradients" rel="nofollow noreferrer"><code>tf.gradients</code></a>).</span>
<span class="comment-copy">One (of several) possible differentiable approximation to the step function is the <a href="https://en.wikipedia.org/wiki/Logistic_function" rel="nofollow noreferrer">logistic function</a> (<a href="https://www.tensorflow.org/api_docs/python/tf/math/sigmoid" rel="nofollow noreferrer">tf.math.sigmoid</a>), shifted so it is centered around the step point. You can manipulate the points where it is evaluated to control how "steep" it is, although that will affect the gradients and in turn the ability of the variable to learn.</span>
<span class="comment-copy">jdehesa, fur future reference, can you please elaborate on how one can test if the gradients "move" past a certain point? (and not only using common logic that it's not differentiable)</span>
<span class="comment-copy">Let's say you want to know if <code>my_function</code> is differentiable. You take an input <code>x</code> (it can be whatever, e.g. a <code>tf.placeholder</code> or made with <code>tf.zeros</code>) and do <code>y = my_function(x)</code> and then check the value returned by <code>tf.gradients(y, x)</code>. If it is <code>[None]</code>, then the function is not differentiable. If you have several inputs, you can pass the list as the second parameter of <code>tf.gradients</code>; in that case, any <code>None</code> in the returned list will mean that the output cannot be differentiated with respect to the corresponding input.</span>
<span class="comment-copy">A tricky thing, which I think is what is happening to you in this case, is that the training may work even if there are some <code>None</code> gradients. As long as there is some valid gradient, TensorFlow assumes that <code>None</code> gradients are irrelevant.</span>
