<div class="post-text" itemprop="text">
<p>I've wrote this script here to read data from a <code>txt</code> file and process it. But it seems that if I give it a big file and a high number of threads, the more it reads from the list, the slower the script gets.</p>
<p>Is there a way to avoid waiting for all the threads to finish and start a new one whenever a thread is done with the work?</p>
<p>Also it seems that when it finishes processing, the script doesn't exit.</p>
<pre><code>import threading, Queue, time

class Work(threading.Thread):

    def __init__(self, jobs):
        threading.Thread.__init__(self)
        self.Lock = threading.Lock()
        self.jobs = jobs

    def myFunction(self):
        #simulate work
        self.Lock.acquire()
        print("Firstname: "+ self.firstname + " Lastname: "+ self.lastname)
        self.Lock.release()
        time.sleep(3)

    def run(self):
        while True:
            self.item = self.jobs.get().rstrip()
            self.firstname = self.item.split(":")[0]
            self.lastname = self.item.split(":")[1]
            self.myFunction()
            self.jobs.task_done()

def main(file):
    jobs = Queue.Queue()
    myList = open(file, "r").readlines()
    MAX_THREADS = 10
    pool = [Work(jobs) for i in range(MAX_THREADS)]
    for thread in pool:
        thread.start()
    for item in myList:
        jobs.put(item)
    for thread in pool:
        thread.join()

if __name__ == '__main__':
    main('list.txt')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The script probably seems to take longer on larger inputs because there's a 3 second pause between each batch of printing.</p>
<p>The issue with the script not finishing is, since you are using <code>Queue</code>, you need to call <code>join()</code> on the <code>Queue</code>, not on the individual threads.  To make sure that the script returns when the jobs have stopped running, you should also set <code>daemon = True</code>.</p>
<p>The <code>Lock</code> will also not work in the current code because <code>threading.Lock()</code> produces a new lock each time.  You need to have all the jobs share the same lock.</p>
<p>If you want to use this in Python 3 (which you should), the <code>Queue</code> module has been renamed to <code>queue</code>.</p>
<pre><code>import threading, Queue, time

lock = threading.Lock()  # One lock

class Work(threading.Thread):

    def __init__(self, jobs):
        threading.Thread.__init__(self)
        self.daemon = True  # set daemon
        self.jobs = jobs

    def myFunction(self):
        #simulate work
        lock.acquire()  # All jobs share the one lock
        print("Firstname: "+ self.firstname + " Lastname: "+ self.lastname)
        self.Lock.release()
        time.sleep(3)

    def run(self):
        while True:
            self.item = self.jobs.get().rstrip()
            self.firstname = self.item.split(":")[0]
            self.lastname = self.item.split(":")[1]
            self.myFunction()
            self.jobs.task_done()


def main(file):
    jobs = Queue.Queue()
    with open(file, 'r') as fp:  # Close the file when we're done
        myList = fp.readlines()
    MAX_THREADS = 10
    pool = [Work(jobs) for i in range(MAX_THREADS)]
    for thread in pool:
        thread.start()
    for item in myList:
        jobs.put(item)
    jobs.join()    # Join the Queue


if __name__ == '__main__':
    main('list.txt')
</code></pre>
<p>Simpler example (based on an example from the <a href="https://docs.python.org/3/library/queue.html" rel="nofollow noreferrer">Python docs</a>)</p>
<pre><code>import threading
import time
from Queue import Queue # Py2
# from queue import Queue # Py3

lock = threading.Lock()

def worker():
    while True:
        item = jobs.get()
        if item is None:
            break
        firstname, lastname = item.split(':')
        lock.acquire()
        print("Firstname: " + firstname + " Lastname: " + lastname)
        lock.release()
        time.sleep(3)
        jobs.task_done()

jobs = Queue()
pool = []
MAX_THREADS = 10
for i in range(MAX_THREADS):
    thread = threading.Thread(target=worker)
    thread.start()
    pool.append(thread)

with open('list.txt') as fp:
    for line in fp:
        jobs.put(line.rstrip())

# block until all tasks are done
jobs.join()

# stop workers
for i in range(MAX_THREADS):
    jobs.put(None)
for thread in pool:
    thread.join()
</code></pre>
</div>
<span class="comment-copy">You are aware of the <a href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="nofollow noreferrer"><b>dreaded GIL</b></a> and why your code won't be executing faster (apart from I/O operations) than running serially, right?</span>
<span class="comment-copy">Just a style note, the Python convention is to name classes in UpperCamelCase (so Work) and functions in lowercase_separated_by_underscores (so main).</span>
<span class="comment-copy">@zwer I'm, it explains the first of my question, but it doesn't explain why it is gradually slowing down.</span>
<span class="comment-copy">@jpyams I will take that into account from now on.</span>
<span class="comment-copy">Can I avoid the dreaded GIL with multiprocessing instead of threading? And if so, why does calling multiprocessing instead of threading in your code doesn't help that much in terms of speed?</span>
<span class="comment-copy">Based on <a href="https://stackoverflow.com/a/3044626/5031373">this answer</a>, it appears you can get around GIL with the <code>multiprocessing</code> module, but it comes with a bit of overhead, and since this code is doing trivial printing, the ability to run concurrent threads doesn't make much of a difference</span>
