<div class="post-text" itemprop="text">
<p>I am a bit baffled by the following profiling results, and would like to hear some explanation in order to make sense of them. I thought I would take the inner-product as a simple function to compare different possible implementations:</p>
<pre><code>import numpy as np

def iterprod(a,b):
    for x,y in zip(a,b):
        yield x*y

def dot1(a,b):
    return sum([ x*y for x,y in zip(a,b) ])

def dot2(a,b):
    return sum(iterprod(a,b))

def dot3(a,b):
    return np.dot(a,b)
</code></pre>
<p>The first implementation <code>dot1</code> is a "naive" one, where we first create a new list of pairwise products, and then sum its elements. I thought the second implementation <code>dot2</code> would be a bit smarter, because it eliminates the need to create a new list. And the third implementation <code>dot3</code> uses Numpy's <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" rel="nofollow noreferrer">dot</a> function.</p>
<p>In order to profile these functions, I am using the following:</p>
<pre><code>import timeit

def showtime( fun, a, b, rep=500 ):
    def wrapped():
        return fun(a,b)
    t = timeit.Timer( wrapped )
    print( t.timeit(rep) )
</code></pre>
<hr/>
<h2>Scenario 1: Python lists</h2>
<pre><code>import random
n = 100000

a = [ random.random() for k in range(n) ]
b = [ random.random() for k in range(n) ]

showtime( dot1, a, b )
showtime( dot2, a, b )
showtime( dot3, a, b )
</code></pre>
<p>output:</p>
<pre><code>3.883254656990175
3.9970695309893927
2.5059548830031417
</code></pre>
<p>So the "smarter" implementation <code>dot2</code> actually performs worse than the naive one <code>dot1</code>, and Numpy is much faster than both. But then..</p>
<h2>Scenario 2: Python arrays</h2>
<p>I thought maybe using a numeric container, like <a href="https://docs.python.org/3/library/array.html" rel="nofollow noreferrer">array</a>, would perhaps enable certain optimisations under the hood.</p>
<pre><code>import array
a = array.array( 'd', a )
b = array.array( 'd', b )

showtime( dot1, a, b )
showtime( dot2, a, b )
showtime( dot3, a, b )
</code></pre>
<p>output:</p>
<pre><code>4.048957359002088
5.460344396007713
0.005460165994009003
</code></pre>
<p>Nope. If anything, it made things <em>worse</em> for the pure-Python implementations, accentuating the difference between the "naive" and "smart" versions, and now Numpy is <strong>3 orders of magnitude</strong> faster!</p>
<hr/>
<h2>Questions</h2>
<p>Q1. The only way I can make sense of these results is if Numpy actually copies the data before processing it in Scenario 1, whereas it only "points" to it in scenario 2, does that sound reasonable?</p>
<p>Q2. Why is my "smart" implementation performing systematically slower than the "naive" one? If my hunch for Q1 is correct, it's entirely possible that it is faster to create a new array if <code>sum</code> does something smart under the hood. Is that it?</p>
<p>Q3. 3 orders of magnitude! How is that possible? Are my implementations really dumb, or are there some magical processor instructions to compute the dot product?</p>
</div>
<div class="post-text" itemprop="text">
<h3>Q1</h3>
<p>Python lists contain pointers to python objects, while arrays contain those numbers directly. The underlying numpy code however expects it to be a contiguous array. So when passed a list, it has to read the value out of the float into a new array for every element of the list.</p>
<p>As noted in the comments, using numpy's built-in arrays is even better.</p>
<h3>Q2</h3>
<p>Getting a value from a generator is (from memory) slighly less expensive as a python function call. This is a lot more expensive than the list comprehension, where everything but the <code>x*y</code> is handled inside of the interpreter. A list has to be produced, which is expensive, but this cost appears to be quickly ammortized.</p>
<h3>Q3</h3>
<p>Numpy is three orders of magnitued faster because it is built on very highly optimized low-level libraries. Depending on the backend used, it might even use multiple threads. Python has to deal with a lot of overhead to make things easier for you the programmer at every step of the way, so it really isn't a fair race.</p>
<h3>Bonus</h3>
<p>I made my generator suggestion because usually constructing the list is significant overhead. However, in this case it is moot as it appears that sum() over a list is faster than over an iterator.</p>
</div>
<div class="post-text" itemprop="text">
<p>The generators / yield mechanics does cost some CPU cycles. What it saves you is <em>memory</em> when you don't want the whole sequence at once, or helps when you want to interleave several dependent computations to lower your latency aka time to the <em>first</em> item in the sequence.</p>
<p>Using a <code>numpy</code> function on an array just lets it run regular C code over a contiguous chunk of memory, without dereferencing <code>float</code> objects from pointers in a list. So it becomes insanely fast (which is the whole point of <code>numpy</code>).</p>
</div>
<span class="comment-copy">you can likely make version 1 faster by removing the <code>[ ]</code> around the list comprehension to turn it into a generator instead</span>
<span class="comment-copy">@Azsgy It basically becomes <code>dot2</code> then; I just tested what you said, and it gives similar runtimes. It actually performs <i>worse</i> than with <code>[]</code>. :)</span>
<span class="comment-copy">Interesting, that doesn't make any sense with my understanding. It has to create the extra list after all</span>
<span class="comment-copy">Take a look at <a href="https://stackoverflow.com/questions/993984/why-numpy-instead-of-python-lists" title="why numpy instead of python lists">stackoverflow.com/questions/993984/â€¦</a> as well</span>
<span class="comment-copy">It also worth mentioning that using Numpy's native arrays is the best choice for large datasets.</span>
