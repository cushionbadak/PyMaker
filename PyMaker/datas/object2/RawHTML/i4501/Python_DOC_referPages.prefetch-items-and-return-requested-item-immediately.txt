<div class="post-text" itemprop="text">
<p>I need to load a lot of large image data from a network-share for processing (which is not very fast). The images are named following a sequence (e.g. 1.png, 2.png, 3.png, etc.).</p>
<p>In most cases, loading will happen in this sequence (loading n+1.png after n.png). I would like to have n+1.png in memory before the actual request.</p>
<p>I would like to keep a cache (as well), such that going 1 image back does not require disk access.</p>
<p>I envision something like this:</p>
<ol>
<li>Request image with index n</li>
<li>Check if n.png is in cache, if the image is not in cache:
a. load the image from disk
b. put the image in cache</li>
<li>Perform steps 1&amp;2 for image with index n+1</li>
<li>Do <strong>not</strong> wait for step 3 to finish, but take the image from cache and return that image</li>
</ol>
<p>Nice to have feature: clean the cache in the backgound such that it only contains the last requested 10 items, or that it removes the first requested items until it contains a max. of 10 items (I can imagine the latter option is easier to implement while being good enough for my case).</p>
<p>I am using Python 3.5. I am using PyQt5, but I prefer the function to not rely on PyQt5 functionality (but if this makes the implementation much more clean/easy/readable I will use it).</p>
</div>
<div class="post-text" itemprop="text">
<p>The simple answer (assuming you're not using coroutines or the like, which you probably aren't given that you're using PyQt5) is to spawn a daemon background thread to load image n+1 into the cache. Like this:</p>
<pre><code>def load(self, n):
    with self._cache_lock:
        try:
            return self._cache[n]
        except KeyError:
            self._cache[n] = self._load_image(n)
            return self._cache[n]
def process_image(self, n):
    img = self.load(n)
    threading.Thread(target=partial(self.load, n+1), daemon=True).start()
    self.step1(img)
    self.step2(img)
</code></pre>
<p>The problem with this design is that you're holding a lock around the entire <code>_load</code> operation. If <code>step1</code> and <code>step2</code> take significantly longer than <code>_load_image</code>, it may be cheaper to avoid that lock by allowing rare duplicate work:</p>
<pre><code>def cacheget(self, n):
    with self._cache_lock:
        return self._cache.get(n)
def preload(self, n):
    img = self._load_image(n)
    with self._cache_lock:
        self._cache[n] = img
    return img
def process_image(self, n):
    img = self.cacheget(n)
    if img is None:
        img = self.preload(n)
    threading.Thread(target=partial(self.preload, n+1), daemon=True).start()
    self.step1(img)
    self.step2(img)
</code></pre>
<p>If you're expecting to do lots of processing in parallel, you may want to use a <code>ThreadPoolExecutor</code> to queue up all of your preloads, instead of a daemon thread for each one.</p>
<p>If you want to clean old cache values, see <a href="https://docs.python.org/3/library/functools.html#functools.lru_cache" rel="nofollow noreferrer"><code>lru_cache</code></a> and <a href="https://github.com/python/cpython/blob/3.6/Lib/functools.py#L399" rel="nofollow noreferrer">its implementation</a>. There are a lot of tuning decisions to make (like: do you actually want background cache garbage collection, or can you just push the oldest item out whenever you add a 10th item the way <code>lru_cache</code> does?), but none of the options are particularly hard to build once you decide what you want.</p>
</div>
<span class="comment-copy">Please enlighten me; why the -1?</span>
