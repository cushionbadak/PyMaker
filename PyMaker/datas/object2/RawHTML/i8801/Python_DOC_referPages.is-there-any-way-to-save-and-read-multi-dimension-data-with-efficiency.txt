<div class="post-text" itemprop="text">
<h3>Introduction</h3>
<ul>
<li><p>I have a bunch of data series with 1000 stations and each station all have 4 features (e.g Temperature, Wind, CO2 concentration, solar radiation).  </p></li>
<li><p>All the features are in time-series with hourly resolution.  </p></li>
</ul>
<p>I read this data in <strong>.csv</strong> files with the support of Pandas.   </p>
<p>Now I need to save and organize them together for better re-use.  </p>
<h3>My solution</h3>
<p>I creat columns entitled by 'sample_x, feature_y'. And each column contain the time series data of feature_y for sample_x.  </p>
<p>This method is doable but not show efficiency. Because I had to creat like 4000 columns  with long column name. </p>
<h3>My question</h3>
<p>Is there any better way to save multi-demensions data in Python. I want a simple solution that can help me assessing and handling with specific data directly.  </p>
<p>Any advices or solution is appreciated!</p>
</div>
<div class="post-text" itemprop="text">
<p>I think you can use <a href="http://pandas.pydata.org/pandas-docs/stable/advanced.html" rel="nofollow noreferrer"><code>MultiIndex</code></a> or <a href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel" rel="nofollow noreferrer"><code>Panel</code></a> and then if necessary save data to <a href="http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5" rel="nofollow noreferrer"><code>hdf5</code></a>.</p>
<p>Also function <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html" rel="nofollow noreferrer"><code>concat</code></a> have parameter <code>keys</code> which create <code>MultiIndex</code> from <code>list of DataFrames</code>.</p>
<p>Sample:</p>
<pre><code>df1 = pd.DataFrame({'A':[1,2,3],
                   'B':[4,5,6],
                   'C':[7,8,9],
                   'D':[1,3,5]})

print (df1)
   A  B  C  D
0  1  4  7  1
1  2  5  8  3
2  3  6  9  5

df2 = df1 * 10

dfs = [df1, df2]

df3 = pd.concat(dfs, keys=['a','b'])
print (df3)
      A   B   C   D
a 0   1   4   7   1
  1   2   5   8   3
  2   3   6   9   5
b 0  10  40  70  10
  1  20  50  80  30
  2  30  60  90  50

print (df3.index)
MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
           labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]])
</code></pre>
<hr/>
<pre><code>wp = pd.Panel({'a' : df1, 'b' : df2})
print (wp)
&lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: 2 (items) x 3 (major_axis) x 4 (minor_axis)
Items axis: a to b
Major_axis axis: 0 to 2
Minor_axis axis: A to D
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You may want to use <a href="http://www.h5py.org/" rel="nofollow noreferrer">HDF</a>, which has been specifically designed to handle huge arrays of multidimensional data.</p>
</div>
<div class="post-text" itemprop="text">
<p>The simplest answer may be just to create a <a href="https://docs.python.org/3/library/sqlite3.html?highlight=sqlite#module-sqlite3" rel="nofollow noreferrer"><code>sqlite3</code></a> database.</p>
<p>It sounds like you have 6 pieces of data per hour (station, timestamp, feature1..feature4) times 1000 stations, times however-many hours.</p>
<p>So that's 6000 data items (at, say, 4 bytes each = 24k), times 24 hours/day times 365 days/year (* 8760), or about 200mb, per year. Depending on how far back you're going, that's not too bad for a db file. (If you're going to do more than 10 years, then yeah, go to something bigger, or maybe compress the data or break it up by year or something...)</p>
</div>
<span class="comment-copy"><a href="http://xarray.pydata.org/en/stable/" rel="nofollow noreferrer">xarray.pydata.org/en/stable</a> is designed precisely to handle storage and manipulation of weather data ; it is built on top of pandas</span>
<span class="comment-copy">I have learned to use the HDF files. And it works pretty well.</span>
