<div class="post-text" itemprop="text">
<p>I have 2 matrix 100kx200 and 200x100k
if they were small matrix I would just use numpy dot product </p>
<pre><code>sum(a.dot(b), axis = 0)
</code></pre>
<p>however the matrix is too big, and also I can't use loops is there a smart way for doing this?</p>
</div>
<div class="post-text" itemprop="text">
<p>A possible optimization is </p>
<pre><code>&gt;&gt;&gt; numpy.sum(a @ b, axis=0)
array([  1.83633615,  18.71643672,  15.26981078, -46.33670382,  13.30276476])
&gt;&gt;&gt; numpy.sum(a, axis=0) @ b
array([  1.83633615,  18.71643672,  15.26981078, -46.33670382,  13.30276476])
</code></pre>
<p>Computing <code>a @ b</code> requires 10k×200×10k operations, while summing the rows first will reduce the multiplication to 1×200×10k operations, giving a 10k× improvement.</p>
<p>This is mainly due to recognizing</p>
<pre><code>   numpy.sum(x, axis=0) == [1, 1, ..., 1] @ x
=&gt; numpy.sum(a @ b, axis=0) == [1, 1, ..., 1] @ (a @ b)
                            == ([1, 1, ..., 1] @ a) @ b
                            == numpy.sum(a, axis=0) @ b
</code></pre>
<p>Similar for the other axis.</p>
<pre><code>&gt;&gt;&gt; numpy.sum(a @ b, axis=1)
array([  2.8794171 ,   9.12128399,  14.52009991,  -8.70177811, -15.0303783 ])
&gt;&gt;&gt; a @ numpy.sum(b, axis=1)
array([  2.8794171 ,   9.12128399,  14.52009991,  -8.70177811, -15.0303783 ])
</code></pre>
<p>(Note: <a href="https://stackoverflow.com/questions/27385633/what-is-the-symbol-for-in-python"><code>x @ y</code></a> is equivalent to <code>x.dot(y)</code> for <a href="https://stackoverflow.com/questions/34142485/difference-between-numpy-dot-and-python-3-5-matrix-multiplication">2D matrixes and 1D vectors</a> on Python 3.5+ with <a href="https://github.com/numpy/numpy/blob/master/doc/release/1.10.0-notes.rst#support-for-the--operator-in-python-35" rel="nofollow noreferrer">numpy 1.10.0+</a>)</p>
<hr/>
<pre><code>$ INITIALIZATION='import numpy;numpy.random.seed(0);a=numpy.random.randn(1000,200);b=numpy.random.rand(200,1000)'

$ python3 -m timeit -s "$INITIALIZATION" 'numpy.einsum("ij,jk-&gt;k", a, b)'
10 loops, best of 3: 87.2 msec per loop

$ python3 -m timeit -s "$INITIALIZATION" 'numpy.sum(a@b, axis=0)'
100 loops, best of 3: 12.8 msec per loop

$ python3 -m timeit -s "$INITIALIZATION" 'numpy.sum(a, axis=0)@b'
1000 loops, best of 3: 300 usec per loop
</code></pre>
<hr/>
<p><strong>Illustration:</strong></p>
<pre><code>In [235]: a = np.random.rand(3,3)
array([[ 0.465,  0.758,  0.641],
       [ 0.897,  0.673,  0.742],
       [ 0.763,  0.274,  0.485]])

In [237]: b = np.random.rand(3,2)
array([[ 0.303,  0.378],
       [ 0.039,  0.095],
       [ 0.192,  0.668]])
</code></pre>
<p>Now, if we simply do <code>a @ b</code>, we would need <em>18 multiply and 6 addition ops</em>. On the other hand, if we do <code>np.sum(a, axis=0) @ b</code> we would only need <em>6 multiply and 2 addition ops</em>. An improvement of 3x because we had 3 rows in <code>a</code>. As for OP's case, this should give 10k times improvement over simple <code>a @ b</code> computation since he has 10k rows in <code>a</code>.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are two <code>sum-reductions</code> happening - One from the marix-multilication with <code>np.dot</code>, and then with the explicit <code>sum</code>.</p>
<p>We could use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html" rel="nofollow noreferrer"><code>np.einsum</code></a> to do both of those in one go, like so -</p>
<pre><code>np.einsum('ij,jk-&gt;k',a,b)
</code></pre>
<p>Sample run -</p>
<pre><code>In [27]: a = np.random.rand(3,4)

In [28]: b = np.random.rand(4,3)

In [29]: np.sum(a.dot(b), axis = 0)
Out[29]: array([ 2.70084316,  3.07448582,  3.28690401])

In [30]: np.einsum('ij,jk-&gt;k',a,b)
Out[30]: array([ 2.70084316,  3.07448582,  3.28690401])
</code></pre>
<p>Runtime test -</p>
<pre><code>In [45]: a = np.random.rand(1000,200)

In [46]: b = np.random.rand(200,1000)

In [47]: %timeit np.sum(a.dot(b), axis = 0)
100 loops, best of 3: 5.5 ms per loop

In [48]: %timeit np.einsum('ij,jk-&gt;k',a,b)
10 loops, best of 3: 71.8 ms per loop
</code></pre>
<p>Sadly, doesn't look like we are doing any better with <code>np.einsum</code>.</p>
<hr/>
<p>For changing to <code>np.sum(a.dot(b), axis = 1)</code>, just swap the output string notation there - <code>np.einsum('ij,jk-&gt;i',a,b)</code>, like so -</p>
<pre><code>In [42]: np.sum(a.dot(b), axis = 1)
Out[42]: array([ 3.97805141,  3.2249661 ,  1.85921549])

In [43]: np.einsum('ij,jk-&gt;i',a,b)
Out[43]: array([ 3.97805141,  3.2249661 ,  1.85921549])
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Some quick time tests using the idea I added to Divakar's answer:</p>
<pre><code>In [162]: a = np.random.rand(1000,200)
In [163]: b = np.random.rand(200,1000)

In [174]: timeit c1=np.sum(a.dot(b), axis=0)
10 loops, best of 3: 27.7 ms per loop

In [175]: timeit c2=np.sum(a,axis=0).dot(b)
1000 loops, best of 3: 432 µs per loop

In [176]: timeit c3=np.einsum('ij,jk-&gt;k',a,b)
10 loops, best of 3: 170 ms per loop

In [177]: timeit c4=np.einsum('j,jk-&gt;k', np.einsum('ij-&gt;j', a), b)
1000 loops, best of 3: 353 µs per loop

In [178]: timeit np.einsum('ij-&gt;j', a) @b
1000 loops, best of 3: 304 µs per loop
</code></pre>
<p><code>einsum</code> is actually faster than <code>np.sum</code>!</p>
<pre><code>In [180]: timeit np.einsum('ij-&gt;j', a)
1000 loops, best of 3: 173 µs per loop
In [181]: timeit np.sum(a,0)
1000 loops, best of 3: 312 µs per loop
</code></pre>
<p>For larger arrays the <code>einsum</code> advantage decreases</p>
<pre><code>In [183]: a = np.random.rand(100000,200)
In [184]: b = np.random.rand(200,100000)
In [185]: timeit np.einsum('ij-&gt;j', a) @b
10 loops, best of 3: 51.5 ms per loop
In [186]: timeit c2=np.sum(a,axis=0).dot(b)
10 loops, best of 3: 59.5 ms per loop
</code></pre>
</div>
<span class="comment-copy">Thank you so much it takes about 0.026 seconds to run</span>
<span class="comment-copy">Just a minor comment regarding your note: the <code>@</code> operator is not completely a python 3 feature as it was only introduced in <a href="https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465" rel="nofollow noreferrer">3.5 release</a>.</span>
<span class="comment-copy">@mgc Thanks. Updated.</span>
<span class="comment-copy">Also, one more thing to note: Your 2nd approach is only possible if we (finally) want to do <code>sum</code> along <code>axis=0</code></span>
<span class="comment-copy">Thank you if I wanted to make it for axis=1 what would I change?</span>
<span class="comment-copy">I tried it it has been running for over 5 mins now I guess its slow when it comes to big matrix</span>
<span class="comment-copy">@AyaAbdelsalam Yup! That's what I found out.</span>
<span class="comment-copy"><code>ij,jk-&gt;k</code> proves the winning solution.  We can sum on <code>i</code> first, then do the <code>j</code> dot.  I wonder how <code>np.einsum('j,jk-&gt;k', np.einsum('ij-&gt;j', a), b)</code> would do.</span>
<span class="comment-copy">@hpaulj I think <code>np.einsum('j,jk-&gt;k',</code> part is better off as a matrix multiplication with <code>dot</code> as done in the other solution.</span>
<span class="comment-copy">For your first set, I am getting with <code>np.einsum('ij-&gt;j', a)</code> : <code>96.8 µs</code> and with <code>np.sum(a,0)</code> : <code>74.8 µs</code>. I am on <code>NumPy 1.12.0</code>. I think I remember seeing <code>einsum</code> being faster than <code>sum</code> but on previous versions of NumPy, if I remember correctly.</span>
<span class="comment-copy">Same numpy version here.</span>
