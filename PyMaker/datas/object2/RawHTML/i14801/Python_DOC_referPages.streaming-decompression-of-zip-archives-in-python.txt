<div class="post-text" itemprop="text">
<p>Is there a way to do streaming decompression of single-file zip archives?</p>
<p>I currently have arbitrarily large zipped archives (single file per archive) in s3. I would like to be able to process the files by iterating over them without having to actually download the files to disk or into memory. </p>
<p>A simple example:</p>
<pre><code>import boto

def count_newlines(bucket_name, key_name):
    conn = boto.connect_s3()
    b = conn.get_bucket(bucket_name)
    # key is a .zip file
    key = b.get_key(key_name)

    count = 0
    for chunk in key:
        # How should decompress happen?
        count += decompress(chunk).count('\n')

    return count
</code></pre>
<p><a href="https://stackoverflow.com/a/12572031/888970">This answer</a> demonstrates a method of doing the same thing with gzip'd files. Unfortunately, I haven't been able to get the same technique to work using the <code>zipfile</code> module, as it seems to require random access to the entire file being unzipped.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use <a href="https://pypi.python.org/pypi/tubing" rel="nofollow">https://pypi.python.org/pypi/tubing</a>, it even has built in s3 source support using boto3.</p>
<pre><code>from tubing.ext import s3
from tubing import pipes, sinks
output = s3.S3Source(bucket, key) \
    | pipes.Gunzip() \
    | pipes.Split(on=b'\n') \
    | sinks.Objects()
print len(output)
</code></pre>
<p>If you didn't want to store the entire output in the returned sink, you could make your own sink that just counts. The impl would look like:</p>
<pre><code>class CountWriter(object):
    def __init__(self):
        self.count = 0
    def write(self, chunk):
        self.count += len(chunk)
Counter = sinks.MakeSink(CountWriter)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The zip header is at the end of the file, which is why it needs random access. See <a href="https://en.wikipedia.org/wiki/Zip_(file_format)#Structure" rel="nofollow">https://en.wikipedia.org/wiki/Zip_(file_format)#Structure</a>.</p>
<p>You could parse the local file header which <em>should</em> be at the start of the file for a simple zip, and decompress the bytes with <code>zlib</code> (see <a href="https://hg.python.org/cpython/file/3.4/Lib/zipfile.py" rel="nofollow">zipfile.py</a>). This is not a valid way to read a zip file, and while it might work for your specific scenario, it could also fail on a lot of valid zips. Reading the central directory file header is the only right way to read a zip.</p>
</div>
<div class="post-text" itemprop="text">
<p>Yes, but you'll likely have to write your own code to do it if it has to be in Python.  You can look at <a href="http://zlib.net/sunzip034.c.gz" rel="nofollow">sunzip</a> for an example in C for how to unzip a zip file from a stream.  sunzip creates temporary files as it decompresses the zip entries, and then moves those files and sets their attributes appropriately upon reading the central directory at the end.  Claims that you must be able to seek to the central directory in order to properly unzip a zip file are incorrect.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can do it in Python 3.4.3 using ZipFile as follows:</p>
<pre><code>with ZipFile('spam.zip') as myzip:
    with myzip.open('eggs.txt') as myfile:
        print(myfile.read())
</code></pre>
<p><a href="https://docs.python.org/3/library/zipfile.html" rel="nofollow">Python Docs</a></p>
</div>
<span class="comment-copy">Have you tried adapting that code to use <a href="https://docs.python.org/2/library/zipfile.html" rel="nofollow noreferrer"><code>zipfile</code></a> instead of <code>zlib</code>?</span>
<span class="comment-copy">Yep! ZipFile expects random access to the file it's unzipping, so I don't think it'll really work with the s3 iterator..</span>
<span class="comment-copy">See also <a href="https://stackoverflow.com/questions/10405210/create-and-stream-a-large-archive-without-storing-it-in-memory-or-on-disk" title="create and stream a large archive without storing it in memory or on disk">stackoverflow.com/questions/10405210/â€¦</a></span>
<span class="comment-copy">Both tar and gzip were designed to work with data streams. Zip, however, was not. So the best answer to this question would be to simply not use that format.</span>
<span class="comment-copy">Lacks a mention of 'streaming'.</span>
