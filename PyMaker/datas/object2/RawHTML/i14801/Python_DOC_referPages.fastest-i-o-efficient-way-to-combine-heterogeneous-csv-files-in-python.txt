<div class="post-text" itemprop="text">
<p>Given ten 1MB csv files, each with slightly different layouts, I need to combine them into a normalized single file with the same header. Empty string is fine for nulls.</p>
<p>Examples of columns: </p>
<pre><code>1. FIELD1, FIELD2, FIELD3
2. FIELD2, FIELD1, FIELD3
3. FIELD1, FIELD3, FIELD4
4. FIELD3, FIELD4, FIELD5, FIELD6
5. FIELD2
</code></pre>
<p>The output would look like (although order not important, my code puts them in order discovered): </p>
<pre><code>FIELD1, FIELD2, FIELD3, FIELD4, FIELD5, FIELD6
</code></pre>
<p>So basically the fields can come in any order, fields may be missing, or new fields not seen before.  All must be included in the output file.  No joining required, in the end the count of data rows in the parts must equal the count of rows in the output.</p>
<p>Reading all 10MB into memory is OK. Somehow using 100MB to do it would not be.  You can open all files at once if needed as well.  Lots of file hands, memory available, but it will be running against a NAS so it needs to be efficient for that (not too many NAS ops). </p>
<p>The method I have right now is to read each file into columns lists, build new columns lists as I discover new columns then write it all out to a single file. I'm hoping someone has something a bit more clever, though, as I'm bottlenecking on this process so any relief is helpful.</p>
<p>I have samples files <a href="https://www.dropbox.com/s/bdk7d2j44t8pqup/files.tar.gz?dl=0" rel="nofollow">here</a> if anyone wants to try.  I'll post my current code as a possible answer.  Looking for the fastest time when I run it on my server (lots of cores, lots of memory) using local disk.</p>
</div>
<div class="post-text" itemprop="text">
<p>Use a two-pass approach with <a href="https://docs.python.org/2/library/csv.html#csv.DictReader" rel="nofollow"><code>csv.DictReader()</code></a> and <a href="https://docs.python.org/2/library/csv.html#csv.DictWriter" rel="nofollow"><code>csv.DictWriter()</code> objects</a>. Pass one collects the set of headers used across all the files, and pass two then copies across data based on the headers.</p>
<p>Collecting headers is as simple as just accessing the <code>fieldnames</code> attribute on the reader objects is enough:</p>
<pre><code>import csv
import glob

files = []
readers = []
fields = set()

try:
    for filename in glob.glob('in*.csv'):
        try:
            fileobj = open(filename, 'rb')
        except IOError:
            print "Failed to open {}".format(filename)
            continue
        files.append(fileobj)  # for later closing

        reader = csv.DictReader(fileobj)
        fields.update(reader.fieldnames)  # reads the first row
        readers.append(reader)

    with open('result.csv', 'wb') as outf:
        writer = csv.DictWriter(outf, fieldnames=sorted(fields))
        writer.writeheader()
        for reader in readers:
            # copy across rows; missing fields will be left blank
            for row in reader:
                writer.writerow(row)
finally:
    # close out open file objects
    for fileobj in files:
        fileobj.close()
</code></pre>
<p>Each reader produces a dictionary with a subset of all fields, but <code>DictWriter</code> will use the value of the <code>restval</code> argument (defaulting to <code>''</code> when omitted like I did here) to fill in the value of each missing key.</p>
<p>I assumed Python 2 here; if this is Python 3 you could use an <a href="https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack" rel="nofollow"><code>ExitStack()</code></a> to manage the open files for the readers; omit the <code>b</code> from the file modes and add a <code>newline=''</code> argument to all open calls to leave newline handling to the CSV module.</p>
<p>The above code only ever uses a buffer to read and write rows; rows are mostly moved from one open reader to the writer one row at a time at a time.</p>
<p>Unfortunately, we cannot use <code>writer.writerows(reader)</code> as the <a href="https://hg.python.org/cpython/file/6986ccbcf41e/Lib/csv.py#l154" rel="nofollow"><code>DictWriter.writerows()</code> implementation</a> first converts everything in <code>reader</code> to a list of lists before passing it on to the underlying <code>csv.writer.writerows()</code> method, see <a href="http://bugs.python.org/issue23495" rel="nofollow">issue 23495</a> in the Python bug tracker.</p>
</div>
<div class="post-text" itemprop="text">
<p>Using the <a href="http://pandas.pydata.org" rel="nofollow">pandas</a> library and the <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html?highlight=concat#pandas.concat" rel="nofollow"><code>concat</code></a> function</p>
<pre><code>import pandas
import glob
df = pandas.concat([pandas.read_csv(x) for x in glob.glob("in*.csv")])
df.to_csv("output.csv")
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's a simple solution using standard library modules.  This is Python 3.  Use the alternate commented <code>with</code> lines for Python 2:</p>
<pre><code>import csv
import glob

rows = []
fields = set()

for filename in glob.glob('in*.csv'):
    #with open(filename,'rb') as f:
    with open(filename,newline='') as f:
        r = csv.DictReader(f)
        rows.extend(row for row in r)
        fields.update(r.fieldnames)

#with open('result.csv','wb') as f:
with open('result.csv','w',newline='') as f:
    w = csv.DictWriter(f,fieldnames=fields)
    w.writeheader()
    w.writerows(rows)
</code></pre>
<h3>Edit</h3>
<p>Per comment, adding file name and line number:</p>
<pre><code>import csv
import glob

rows = []
fields = set(['filename','lineno'])

for filename in glob.glob('in*.csv'):
    with open(filename,newline='') as f:
        r = csv.DictReader(f)
        for lineno,row in enumerate(r,1):
            row.update({'filename':filename,'lineno':lineno})
            rows.append(row)
        fields.update(r.fieldnames)

with open('result.csv','w',newline='') as f:
    w = csv.DictWriter(f,fieldnames=fields)
    w.writeheader()
    w.writerows(rows)
</code></pre>
<p>Original on my system took 8.8s.  This update took 10.6s.</p>
<p>Also note that if you order <code>fields</code> before passing to <code>DictWriter</code> you can put the columns in the order you want.</p>
</div>
<div class="post-text" itemprop="text">
<p>It's not super short or anything, but basically I'm reading these into column stores then writing them all out. I'm hoping for something faster, or same speed, same i/o and less memory is good too... but faster is most important.</p>
<pre><code>import csv
from os.path import join
from collections import OrderedDict


# Accumulators
#columnstore = OrderedDict of tuples ( Data List, Starting rowcount)
columnstore = OrderedDict()
total_rowcount = 0

def flush_to_merged_csv(merged_filename,delimiter):

    with open(merged_filename,'w') as f:
        writer = csv.writer(f, delimiter=bytes(delimiter) )

        # Write the header first for all columns
        writer.writerow(columnstore.keys())

        # Write each row
        for rowidx in range(0,total_rowcount):

            # Assemble row from columnstore
            row = []
            for col in columnstore.keys():
                if columnstore[col][1] &lt;= rowidx:
                    row.append(columnstore[col][0][rowidx - columnstore[col][1]])
                else:
                    row.append('')

            writer.writerow(row)


def combine(location, files, mergefile, delimiter):
    global total_rowcount

    for filename in files:

        with open(join(location,filename),'rb') as f:
            file_rowcount = 0
            reader = csv.reader( f, delimiter=bytes(delimiter) )

            # Get the column names.
            # Normalize the names (all upper, strip)
            columns = [ x.strip().upper() for x in reader.next() ]


            # Columnstore maintenance. Add new columns to columnstore
            for col in columns:
                if not columnstore.has_key(col):
                    columnstore[col] = ( [], total_rowcount )


            # Loop throught the remaining file, adding each cell to the proper columnstore
            for row in reader:
                field_count = len(row)
                total_rowcount += 1

                # Add the columns that exist to the columnstore.
                for columnidx in range(0,len(columns)):
                    # Handle missing trailing fields as empty
                    if columnidx &gt;= field_count:
                        columnstore[columns[columnidx]][0].append('')
                    else:
                        columnstore[columns[columnidx]][0].append(row[columnidx])

                # Add emptry strings to any columnstores that don't exist in this file to keep them all in sync
                for colname in set(columnstore.keys()) - set(columns):
                    columnstore[colname][0].append('')

    flush_to_merged_csv(join(location,mergefile),delimiter)

combine( './', ['in1.csv','in2.csv','in3.csv','in4.csv','in5.csv','in6.csv','in7.csv','in8.csv','in9.csv','in10.csv'],'output.csv',',')
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>@MartijnPieter's answer is very helpful, but due to keeping the files open after reading the headers to re-use when reading the content, it crashes at ~255 files (I found).  I needed to combine ~32,000 files, so rewrote his code slightly to not crash.  I also chose to split it into two functions, so that I could analyse the column headers, in between.</p>
<pre><code>def collectColumnNamesInCSVs():
    fields = set()

    for filename in glob.glob('//path//to//files/*.csv'):
        try:
            fileobj = open(filename, 'rb')
        except IOError:
            print "Failed to open {}".format(filename)
            continue

        reader = csv.DictReader(fileobj)
        fields.update(reader.fieldnames)  # reads the first row
        fileobj.close()

    return fields


def combineCSVs(fields):
    with open('result.csv', 'wb') as outf:
        writer = csv.DictWriter(outf, fieldnames=sorted(fields))
        writer.writeheader()

        for filename in glob.glob('//path//to//files/*.csv'):
            try:
                fileobj = open(filename, 'rb')
            except IOError:
                print "Failed to open {}".format(filename)
                continue

            reader = csv.DictReader(fileobj)

            for row in reader:
                writer.writerow(row)

            fileobj.close()

    outf.close()
</code></pre>
<p>When opening a very motley assortment of CSVs (&lt;1k - 700k; 20-60 mixed columns each; ~130 headers in the total set) the second stage is taking ~1 minute per 1000 files on a 1.4GHz MacBook Air.  Not bad, and several orders of magnitude faster than Pandas.</p>
</div>
<span class="comment-copy">" I'll post my current code as a possible answer. " Please provide your code in question. And if code works, maybe code review is better than SO.</span>
<span class="comment-copy">@Marcin Just posted it. I was going to offer a bounty for fun but that will have to wait 2 days apparently.</span>
<span class="comment-copy">Is the column count consistent <i>within</i> a file? Or does the number of columns vary even within a file?</span>
<span class="comment-copy">@MartijnPieters Each row should have the same number of columns as the header, but generally I don't trust the input entirely, and for my use, it's ok to just blank out missing info or drop extra columns. Also, I didn't mention it, but technically there might be enclosed fields with quotes.  <code>1,2,"3,3,3",4</code>.</span>
<span class="comment-copy">18.624s average time after 3 attempts.</span>
<span class="comment-copy">@woot: thanks for timing and the code corrections! How large are the files?</span>
<span class="comment-copy">About 1MB-1.4MB each, 10 files total.  Looks really great. It's on a solaris box... it's great at parallel work, but slower on individual tasks it seems.</span>
<span class="comment-copy">It's my lucky day for me, @MartijnPieters found my question. ;-)  Always learning from you, thanks!</span>
<span class="comment-copy">As it turns out, my "using lots of memory is OK" is turning out not to be true as my files are larger than expected.  For reasons I can't understand, this method is using memory that is scaling linearly with the size of files being combined, I thought it would iterate the reader but now I'm not too sure.  I'm using Python 2.7 on Solaris (which isn't helping me since I have trouble getting profilers working).  Just thought I would reach out in case you had any thoughts.</span>
<span class="comment-copy">I definitely like the simplicity of the answer. I'm working on a Solaris environment, but i'll try to get numpy and pandas set up for the test.</span>
<span class="comment-copy">20.8997s on my system, average of 3 attempts.</span>
<span class="comment-copy">If you wanted to add in the original file name and original line number columns to the file, is there a clever way of doing that?  This is nice and fast compared to mine, and lots simpler.</span>
<span class="comment-copy">@woot, updated to add the extra information.</span>
<span class="comment-copy">Mine is 28.9667.</span>
<span class="comment-copy">Interesting.  What was the error message when it crashed?  Also, what is your ulimit set to for number of files?</span>
<span class="comment-copy">Also, in the end, I ended up manually mapping the columns.  It was far faster than using DictReader.  This was a long time ago, though.</span>
<span class="comment-copy">@woot Lazy, but I never checked - your code caught the standard error message!</span>
<span class="comment-copy">@woot combined ~30,000 files most recently.  How did you 'manually map' the columns?</span>
<span class="comment-copy">@woot I no longer have the code but from memory... I read the headers into memory, created a dict to map positions to header names.  Then for each row, looped that and wrote out the appropriate position.  Using the tuples out of csv.reader was lots faster even with this seemingly extra work. Sorry I don't have the code... if I have time I'll try to recreate it but it'll be a few weeks before I get that kind of time.</span>
