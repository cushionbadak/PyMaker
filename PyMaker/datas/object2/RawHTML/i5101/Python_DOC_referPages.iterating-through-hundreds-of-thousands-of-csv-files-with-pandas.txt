<div class="post-text" itemprop="text">
<p>I am currently using concurrent.futures.ProcessPoolExectutor to iterate through a ton of CSV files like below: </p>
<pre><code>def readcsv(file):
    df = pd.read_csv(file, delimiter="\s+", names=[headers], comment="#")
    #DOING SOME OTHER STUFF TO IT 
    full.append(df) 

if __name__ == "__main__":
    full = []
    files = "glob2 path to files" 
    with concurrent.futures.ProcessPoolExecutor(max_workers=45) as proc:
        proc.map(readcsv,files)
    full = pd.concat(full)
</code></pre>
<p>This does not currently work in this fashion, as it returns a ValueError "No Objects to concatenate" on the last line. How can I iterate through the files and append them to a list and then concat them or just place them directly into a dataframe as fast as possible? Resources available are 64gb ram and 46 cores in a virtual machine. </p>
</div>
<div class="post-text" itemprop="text">
<p>The <code>map</code> function actually <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.map" rel="nofollow noreferrer">returns an iterable</a> with the results from the functions. So you just need to return the <code>df</code>:</p>
<pre><code>def readcsv(file):
    df = pd.read_csv(file, delimiter="\s+", names=[headers], comment="#")
    #DOING SOME OTHER STUFF TO IT 
    return df

if __name__ == "__main__":
    files = "glob2 path to files" 
    with concurrent.futures.ProcessPoolExecutor(max_workers=45) as proc:
        full = pd.concat(proc.map(readcsv,files))
</code></pre>
</div>
<span class="comment-copy">Have you looked at <a href="http://dask.pydata.org/en/latest/" rel="nofollow noreferrer">dask</a> - it'll do this for you... <code>df = dask.dataframe.read_csv('*.csv').compute()</code>.... If you take off the compute, you can also do operations on it while reading and have it piece them together if you don't require <i>all</i> the data in memory at once and just want to sum a column etc...</span>
