<div class="post-text" itemprop="text">
<p>I read <a href="https://docs.python.org/2/howto/unicode.html" rel="nofollow">Python2 Unicode HOWTO</a> and <a href="http://farmdev.com/talks/unicode/" rel="nofollow">Unicode In Python, Completely Demystified</a> to understand Python's Unicode system, and I met some code like this:</p>
<pre><code>f = open('test.txt','w')
f.write(uni.encode('utf-8'))
f.close()
</code></pre>
<p>Why does a unicode <code>str</code> needed to be encoded before it was written into a file?</p>
<p>I know the default encoding is ASCII, so there will be an error because out of range.</p>
<p>But as I write it to file, Isn't it just copying the bits of <code>uni</code> in RAM to file, why does the program need to care about the encoding?</p>
</div>
<div class="post-text" itemprop="text">
<p>Unicode characters are abstract entities called code points and have several encodings such as UTF32, UTF16 and UTF8. It would take 6 bytes per character to express all of the characters in a single entity (and even then, unicode has non-spacing characters so one could argue that the size is even bigger). To keep things confusing, many systems use "code pages" that existed before Unicode was standardized which are different mappings between bits and the characters they display.</p>
<p>Python's unicode characters are UTF16 in RAM. So right away we see an issue. If you want to write as UTF8, the in-memory string isn't going to work. Python needs to read the in-memory UTF16 string and write a UTF8 string. </p>
<p>There is another subtle issue which is that Intel based processors are "little endian" but Unicode multi-byte encodings are "big endian" (meaning bytes within a word are ordered differently). Even if you want to write UTF-16, changes must be made. Because of this little/big problem, it is common to write a BOM (Byte order Mark) at the front of strings so that encoders can guess the format.</p>
<p>Characters can be expressed in many ways (encodings), so what should the default be? its a historical thing really. Since ACSII was the way it was done thoughout history (well, unix history at least), its still the default.</p>
<p>When writing non-binary data, we always have to pass through some sort of codec. That's the price we pay for the time it took multi-lingual computing to mature and for computing systems to become powerful enough to deal with it. No way would my Commodore 64 have been able to deal with <a href="http://en.wikipedia.org/wiki/List_of_Unicode_characters#Phoenician" rel="nofollow">Phoenician</a>.</p>
</div>
<span class="comment-copy">Because only the encoding specifies which bits (or rather bytes) make up the unicode string.</span>
<span class="comment-copy">You can't even state anything definitive about the internal RAM format of Unicode strings, since it changed in <a href="https://docs.python.org/3/whatsnew/3.3.html" rel="nofollow noreferrer">Python 3.3</a>.</span>
<span class="comment-copy">@MarkRansom - I didn't know that. I knew that python could compile to UTF16 or UTF32 (at least back in the day), but that seemed like it would only confuse the issue.</span>
