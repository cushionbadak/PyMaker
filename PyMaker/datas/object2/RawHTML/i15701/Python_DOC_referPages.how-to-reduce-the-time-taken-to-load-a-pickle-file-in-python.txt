<div class="post-text" itemprop="text">
<p>I have created a dictionary in python and dumped into pickle. Its size went to 300MB. Now, I want to load the same pickle.</p>
<pre><code>output = open('myfile.pkl', 'rb')
mydict = pickle.load(output)
</code></pre>
<p>Loading this pickle takes around <strong>15 seconds</strong>. <strong>How can I reduce this time?</strong></p>
<p>Hardware Specification: Ubuntu 14.04, 4GB RAM</p>
<p>The code bellow shows how much time takes to dump or load a file using json, pickle, cPickle.</p>
<p>After dumping, file size would be around 300MB. </p>
<pre><code>import json, pickle, cPickle
import os, timeit
import json

mydict= {all values to be added}

def dump_json():    
    output = open('myfile1.json', 'wb')
    json.dump(mydict, output)
    output.close()    

def dump_pickle():    
    output = open('myfile2.pkl', 'wb')
    pickle.dump(mydict, output,protocol=cPickle.HIGHEST_PROTOCOL)
    output.close()

def dump_cpickle():    
    output = open('myfile3.pkl', 'wb')
    cPickle.dump(mydict, output,protocol=cPickle.HIGHEST_PROTOCOL)
    output.close()

def load_json():
    output = open('myfile1.json', 'rb')
    mydict = json.load(output)
    output.close()

def load_pickle():
    output = open('myfile2.pkl', 'rb')
    mydict = pickle.load(output)
    output.close()

def load_cpickle():
    output = open('myfile3.pkl', 'rb')
    mydict = pickle.load(output)
    output.close()


if __name__ == '__main__':
    print "Json dump: "
    t = timeit.Timer(stmt="pickle_wr.dump_json()", setup="import pickle_wr")  
    print t.timeit(1),'\n'

    print "Pickle dump: "
    t = timeit.Timer(stmt="pickle_wr.dump_pickle()", setup="import pickle_wr")  
    print t.timeit(1),'\n'

    print "cPickle dump: "
    t = timeit.Timer(stmt="pickle_wr.dump_cpickle()", setup="import pickle_wr")  
    print t.timeit(1),'\n'

    print "Json load: "
    t = timeit.Timer(stmt="pickle_wr.load_json()", setup="import pickle_wr")  
    print t.timeit(1),'\n'

    print "pickle load: "
    t = timeit.Timer(stmt="pickle_wr.load_pickle()", setup="import pickle_wr")  
    print t.timeit(1),'\n'

    print "cPickle load: "
    t = timeit.Timer(stmt="pickle_wr.load_cpickle()", setup="import pickle_wr")  
    print t.timeit(1),'\n'
</code></pre>
<p><strong>Output :</strong></p>
<pre><code>Json dump: 
42.5809804916 

Pickle dump: 
52.87407804489 

cPickle dump: 
1.1903790187836 

Json load: 
12.240660209656 

pickle load: 
24.48748306274 

cPickle load: 
24.4888298893
</code></pre>
<p>I have seen that cPickle takes less time to dump and load but <strong>loading a file still takes a long time</strong>. </p>
</div>
<div class="post-text" itemprop="text">
<p>Try using the <a href="https://docs.python.org/library/json.html" rel="noreferrer"><code>json</code> library</a> instead of <code>pickle</code>. This should be an option in your case because you're dealing with a dictionary which is a relatively simple object.</p>
<p>According to <a href="http://kovshenin.com/2010/pickle-vs-json-which-is-faster/" rel="noreferrer">this website</a>,</p>
<blockquote>
<p>JSON is 25 times faster in reading (loads) and 15 times faster in
  writing (dumps).</p>
</blockquote>
<p>Also see this question: <a href="https://stackoverflow.com/questions/18517949/what-is-faster-loading-a-pickled-dictionary-object-or-loading-a-json-file-to">What is faster - Loading a pickled dictionary object or Loading a JSON file - to a dictionary?</a></p>
<p>Upgrading Python or using <a href="https://docs.python.org/library/marshal.html" rel="noreferrer">the <code>marshal</code> module</a> with a fixed Python version also helps boost speed (<a href="https://gist.github.com/marians/f1314446b8bf4d34e782" rel="noreferrer">code adapted from here</a>):</p>
<pre><code>try: import cPickle
except: import pickle as cPickle
import pickle
import json, marshal, random
from time import time
from hashlib import md5

test_runs = 1000

if __name__ == "__main__":
    payload = {
        "float": [(random.randrange(0, 99) + random.random()) for i in range(1000)],
        "int": [random.randrange(0, 9999) for i in range(1000)],
        "str": [md5(str(random.random()).encode('utf8')).hexdigest() for i in range(1000)]
    }
    modules = [json, pickle, cPickle, marshal]

    for payload_type in payload:
        data = payload[payload_type]
        for module in modules:
            start = time()
            if module.__name__ in ['pickle', 'cPickle']:
                for i in range(test_runs): serialized = module.dumps(data, protocol=-1)
            else:
                for i in range(test_runs): serialized = module.dumps(data)
            w = time() - start
            start = time()
            for i in range(test_runs):
                unserialized = module.loads(serialized)
            r = time() - start
            print("%s %s W %.3f R %.3f" % (module.__name__, payload_type, w, r))
</code></pre>
<p>Results:</p>
<pre><code>C:\Python27\python.exe -u "serialization_benchmark.py"
json int W 0.125 R 0.156
pickle int W 2.808 R 1.139
cPickle int W 0.047 R 0.046
marshal int W 0.016 R 0.031
json float W 1.981 R 0.624
pickle float W 2.607 R 1.092
cPickle float W 0.063 R 0.062
marshal float W 0.047 R 0.031
json str W 0.172 R 0.437
pickle str W 5.149 R 2.309
cPickle str W 0.281 R 0.156
marshal str W 0.109 R 0.047

C:\pypy-1.6\pypy-c -u "serialization_benchmark.py"
json int W 0.515 R 0.452
pickle int W 0.546 R 0.219
cPickle int W 0.577 R 0.171
marshal int W 0.032 R 0.031
json float W 2.390 R 1.341
pickle float W 0.656 R 0.436
cPickle float W 0.593 R 0.406
marshal float W 0.327 R 0.203
json str W 1.141 R 1.186
pickle str W 0.702 R 0.546
cPickle str W 0.828 R 0.562
marshal str W 0.265 R 0.078

c:\Python34\python -u "serialization_benchmark.py"
json int W 0.203 R 0.140
pickle int W 0.047 R 0.062
pickle int W 0.031 R 0.062
marshal int W 0.031 R 0.047
json float W 1.935 R 0.749
pickle float W 0.047 R 0.062
pickle float W 0.047 R 0.062
marshal float W 0.047 R 0.047
json str W 0.281 R 0.187
pickle str W 0.125 R 0.140
pickle str W 0.125 R 0.140
marshal str W 0.094 R 0.078
</code></pre>
<p><a href="https://docs.python.org/3/library/pickle.html#data-stream-format" rel="noreferrer">Python 3.4 uses pickle protocol 3 as default</a>, which gave no difference compared to protocol 4. Python 2 has protocol 2 as highest pickle protocol (selected if negative value is provided to dump), which is twice as slow as protocol 3.</p>
</div>
<div class="post-text" itemprop="text">
<p>I've had nice results in reading huge files (e.g: ~750 MB igraph object - a binary pickle file) using cPickle itself. This was achieved by simply wrapping up the pickle load call as mentioned <a href="https://stackoverflow.com/a/9270029/2385420">here</a></p>
<p>Example snippet in your case would be something like:</p>
<pre><code>import timeit
import cPickle as pickle
import gc


def load_cpickle_gc():
    output = open('myfile3.pkl', 'rb')

    # disable garbage collector
    gc.disable()

    mydict = pickle.load(output)

    # enable garbage collector again
    gc.enable()
    output.close()


if __name__ == '__main__':
    print "cPickle load (with gc workaround): "
    t = timeit.Timer(stmt="pickle_wr.load_cpickle_gc()", setup="import pickle_wr")
    print t.timeit(1),'\n'
</code></pre>
<p>Surely, there might be more apt ways to get the task done, however, this workaround does reduce the time required drastically.
(For me, it reduced from 843.04s to 41.28s, around 20x)</p>
</div>
<div class="post-text" itemprop="text">
<p>If you are trying to store the dictionary to a single file, it's the load time for the large file that is slowing you down.  One of the easiest things you can do is to <strong>write the dictionary to a directory</strong> on disk, with <strong>each dictionary entry being an individual file</strong>.  Then you can have the files pickled and unpickled in multiple threads (or using multiprocessing).  For a very large dictionary, this should be much faster than reading to and from a single file, regardless of the serializer you choose.  There are some packages like <code>klepto</code> and <code>joblib</code> that already do much (if not all of the above) for you.  I'd check those packages out.  (Note: I am the <code>klepto</code> author. See <a href="https://github.com/uqfoundation/klepto" rel="nofollow">https://github.com/uqfoundation/klepto</a>).</p>
</div>
<span class="comment-copy">Are you using <a href="https://docs.python.org/2/library/pickle.html#module-cPickle" rel="nofollow noreferrer"><code>cPickle</code></a>? If not, please try it. You can just use it as a drop-in replacement.</span>
<span class="comment-copy">@Carsten : thanks. I heard about cPickle which is faster than Pickle but it doesn't reduce that much of time, i needed.</span>
<span class="comment-copy">Can you add the code to create the dictionary as well.</span>
<span class="comment-copy">When you dump the dictionary, try adding an optional protocol argument of <a href="https://docs.python.org/2/library/pickle.html#usage" rel="nofollow noreferrer"><code>pickle.HIGHEST_PROTOCOL</code></a> (or <code>-1</code>). This will use a more compact binary-mode data format than the default ASCII-based one.</span>
<span class="comment-copy">@iNikkz by the way if the answer is helpful, please accept the answer by clicking on the green checkbox.</span>
<span class="comment-copy">@twasbrillig I don't wish to downgrade, but when running <code>pip</code> from the <code>Scripts</code> dir, i run into <a href="http://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat" title="error unable to find vcvarsall bat">stackoverflow.com/questions/2817869/â€¦</a>  @Nikkz The relative time should be the same. For 10 million 30-byte plaintext strings, <a href="http://stackoverflow.com/a/18475192/819417">use compression</a> to offload the processing burden from the slow storage device to the fast CPU.</span>
<span class="comment-copy">@CeesTimmerman I tried installing <code>ujson</code> with <code>pip</code> and got that error too. But there are Windows binaries here <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#ujson" rel="nofollow noreferrer">lfd.uci.edu/~gohlke/pythonlibs/#ujson</a> and I installed the 64-bit versions for Python 2.7 and 3.4 and both worked for me!</span>
<span class="comment-copy">@twasbrillig Thanks. In 32-bit Python 3.4 on my 64-bit machine, <code>marshal</code> is 2 to 3 times faster than <code>ujson</code>, and produces up to 50% smaller output.</span>
<span class="comment-copy">I tested <code>zlib</code> and <code>bz2</code> compression <a href="https://gist.github.com/CTimmerman/1f328f02ac2740f4c90d" rel="nofollow noreferrer">here</a>. <code>zlib</code> default level 6 is roughly twice as small but 5 times as slow to load, though i only used RAM.</span>
<span class="comment-copy">JSON will not work if you have any values of bytes in your dictionary, so this post makes a huge assumption. Not everything is json serializable!</span>
<span class="comment-copy">If i try this, i get the error : TypeError: expected str, bytes or os.PathLike object, not _io.BufferedReader. The pickle was written with "wb" mode</span>
<span class="comment-copy">Can you provide the snippet / try following for pickling the obj?:  <code>with open(filename, 'wb') as output:         pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)</code></span>
<span class="comment-copy">cPickle is sooooo much faster</span>
<span class="comment-copy">Thanks very much, this is a very convenient way to speed up my script :)</span>
