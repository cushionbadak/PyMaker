<div class="post-text" itemprop="text">
<p>I could be missing something fundamental, but consider this interpreter session<sup>1</sup>:</p>
<pre><code>&gt;&gt;&gt; -0.0 is 0.0
False
&gt;&gt;&gt; 0.0 is 0.0
True
&gt;&gt;&gt; -0.0  # The sign is even retained in the output.  Why?
-0.0
&gt;&gt;&gt;
</code></pre>
<p>You would think that the Python interpreter would realize that <code>-0.0</code> and <code>0.0</code> are the same number.  In fact, it compares them as being equal:</p>
<pre><code>&gt;&gt;&gt; -0.0 == 0.0
True
&gt;&gt;&gt;
</code></pre>
<p>So why is Python differentiating between the two and generating a whole new object for <code>-0.0</code>?  It doesn't do this with integers:</p>
<pre><code>&gt;&gt;&gt; -0 is 0
True
&gt;&gt;&gt; -0  # Sign is not retained
0
&gt;&gt;&gt;
</code></pre>
<p>Now, I realize that floating point numbers are a huge source of problems with computers, but those problems are always with regard to their accuracy.  For example:</p>
<pre><code>&gt;&gt;&gt; 1.3 + 0.1
1.4000000000000001
&gt;&gt;&gt;
</code></pre>
<p>But this isn't an accuracy problem, is it?  I mean, we are talking about the sign of the number here, not its decimal places.</p>
<hr/>
<p><sup>1</sup><sub>I can reproduce this behavior in both Python 2.7 and Python 3.4, so this is not a version-specific question.</sub></p>
</div>
<div class="post-text" itemprop="text">
<p>In IEEE754, the format of floating point numbers, the sign is a separate bit. So -0.0 and 0.0 are different by this bit.
Integers use the two's complement, to represent negative numbers; that's why there is only one <code>0</code>.</p>
<p>Use <code>is</code> only of you really want to compare instances of objects. Otherwise, especially for numbers, use <code>==</code>:</p>
<pre><code>&gt;&gt;&gt; 1999+1 is 2000
False

&gt;&gt;&gt; 0.0 == -0.0
True
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>The IEEE Standard for Floating-Point Arithmetic (<a href="http://en.wikipedia.org/wiki/IEEE_floating_point" rel="nofollow">IEEE 754</a>) defines the inclusion of <a href="http://en.wikipedia.org/wiki/Signed_zero" rel="nofollow">signed zeroes</a>. In theory they allow you to distinguish between negative number underflow and positive number <a href="http://en.wikipedia.org/wiki/Arithmetic_underflow" rel="nofollow">underflow</a>.</p>
<p>As far as python specifically is concerned, use <code>==</code> rather than <code>is</code> to compare numbers.</p>
</div>
<div class="post-text" itemprop="text">
<p>Because the binary representation of those two numbers is different.  In 0.0 the 32nd bit is 0 and in -0.0 the 32nd bit is 1.</p>
</div>
<span class="comment-copy">I believe this is a feature of IEEE 754 floating point representation, which would mean it's not specific to Python either.</span>
<span class="comment-copy">Integer overflow is probably a more serious "source of problems with computers."</span>
<span class="comment-copy">Why are you using <code>is</code> for numeric comparisons?  Much of your question has little to do with signed zeros: try <code>x = 2.3</code>, <code>y = 2.3</code>, followed by <code>x is y</code>.  And then, just for fun, try <code>x = 2.3; y = 2.3</code> (all on one line), followed by <code>x is y</code>.</span>
<span class="comment-copy">I accept your answer because you explained both the float case as well as the integer one.  However, can you think of any real-world use cases for <code>-0.0</code>?  I know there are a few <i>theoretical</i> uses for it, but it really seems like it just gets in the way.</span>
<span class="comment-copy">@iCodez: Signed zero is useful for the implementation of complex math functions. The canonical paper is: William Kahan, Branch Cuts for Complex Elementary Functions, or Much Ado About Nothing's Sign Bit. In: The State of the Art in Numerical Analysis, Clarendon Press, Oxford, 1987. Online copies are easily located via Google Scholar.</span>
<span class="comment-copy">Note that Python doesn't specifically mandate IEEE-754. The <a href="https://docs.python.org/3/reference/datamodel.html#the-standard-type-hierarchy" rel="nofollow noreferrer">reference</a> says "These represent machine-level double precision floating point numbers. You are at the mercy of the underlying machine architecture (and C or Java implementation) for the accepted range and handling of overflow." But on most platforms and implementations, that means IEEE-754 doubles, or something very close to it.</span>
<span class="comment-copy">@njuffa: Also on Kahan's website.</span>
<span class="comment-copy">Funny python lore: note that 10+1 is 11 returns True. The first 256 values (and some negative ones) are internalized, so you always get the same instance.</span>
<span class="comment-copy">floating point numbers in python are 64bit.</span>
<span class="comment-copy">Ok.  The answer still remains mostly the same.  The last bit in a float is the sign bit. 0 for positive 1 for negative.</span>
<span class="comment-copy">Actually, in 2.x, floating-point numbers in Python are unspecified in general, and specified as "whatever <code>double</code> is in the C compiler used to build the interpreter" in CPython. In 3.x, they cleaned it up a bit and specified them as "a machine <code>double</code>" across implementations, but note that "You are at the mercy of the underlying machine architecture (and C or Java implementation)".</span>
