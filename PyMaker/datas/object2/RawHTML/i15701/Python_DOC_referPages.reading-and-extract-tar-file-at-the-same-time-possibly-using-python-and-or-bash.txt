<div class="post-text" itemprop="text">
<p>There is a very large tar file on the web (1.2 TB) which contains many many hi res images(possibly in subtared files) and some text files. I need all the images but only in a lower resolution, also I need the text files. But I don't have enough space to download the whole thing. Also the large tar file supports download resume.</p>
<p>So I want to do a script that downloads just one part of the file, extracts the contained file and process it. Then do the next part and so on. Possibly python should be the easiest way, no? or maybe a bash script? how can I do this?</p>
</div>
<div class="post-text" itemprop="text">
<p>You can do this in python, but it's not straightforward. At all.</p>
<p>You can use <a href="https://docs.python.org/2/library/tarfile.html#tarfile.open" rel="nofollow"><code>tarfile.open</code></a> and supply the <code>fileobj</code> argument.</p>
<p>You technically <em>could</em> supply it something straight out of <a href="https://docs.python.org/2/library/urllib.html#urllib.urlopen" rel="nofollow"><code>urllib.urlopen</code></a>. The main problem is that, since you're processing over a terabyte, the transfer <em>will</em> fail. </p>
<p>As you said, you'll need to retry the transfer on demand. Your best bet is to to craft a file-like object that resiliently reads from a URL, handling disconnections and timeouts.
Apparently, <a href="https://stackoverflow.com/questions/15431044/can-i-set-max-retries-for-requests-request"><code>urllib3</code> will do this automatically</a>, so you don't need to reinvent the wheel.</p>
<p>Another problem is that (normal) <code>tar</code> files have no index. You can't really list the files inside without processing the whole <code>tar</code> first - so you'll need to extract them as they appear. There doesn't seem to be a built-in way to do this <em>and</em> regain flow control after each file is extracted (i.e.:a callback), so you'll have to write it yourself. Take a look into <code>TarFile.extractall</code>'s source code to see how it's done (<code>print inspect.getsource(tarfile.TarFile.extractall)</code>)</p>
</div>
<div class="post-text" itemprop="text">
<p>My own partial answer, so as to kick start Ideas, unfortunately it seems I am not proficient enough in python or bash to know the most elegant and straight forward way, but here is what I have found:</p>
<p>Python has this tarmodule:
<a href="https://docs.python.org/3/library/tarfile.html" rel="nofollow">https://docs.python.org/3/library/tarfile.html</a>, </p>
<p>and also there is this file resuming download script:
<a href="http://code.activestate.com/recipes/83208-resuming-download-of-a-file/" rel="nofollow">http://code.activestate.com/recipes/83208-resuming-download-of-a-file/</a></p>
<p>But I don't know how to stick them together. </p>
<p>Also I can download and untar at the same time using bash, but how can I then do this recursively (remember there may be other tar files that we need to go into or text files that we have to process accordingly), also is this resumable?
<a href="http://www.howtogeek.com/howto/uncategorized/linux-quicktip-downloading-and-un-tarring-in-one-step/" rel="nofollow">http://www.howtogeek.com/howto/uncategorized/linux-quicktip-downloading-and-un-tarring-in-one-step/</a></p>
<p>One Idea is to use a Frankenstein of bash and python. That is to use curl and untar to get the files individually and then pass the file to my own script to process, then I can have all the checks in the script:</p>
<pre><code>curl http://wordpress.org/latest.tar.gz | tar xz | python myScript
</code></pre>
<p>curl can support resume:
<a href="http://www.cyberciti.biz/faq/curl-command-resume-broken-download/" rel="nofollow">http://www.cyberciti.biz/faq/curl-command-resume-broken-download/</a></p>
<p>But then we'd have the problem of: is tar resumable!</p>
</div>
<span class="comment-copy">I am contemplating this. It seems to be much cleaner than the piped tar approach.</span>
<span class="comment-copy">I have locked at the extractAll code and this seems to be the "easiest" approach, so I am going to accept it as the answer.</span>
<span class="comment-copy">bash is a good idea, and could save you a lot of implementation complexity. <code>wget</code> supposedly will retry automatically: <code>Wget has been designed for robustness over slow or unstable network connections; if a download fails due to a network problem, it will keep retrying until the whole file has been retrieved.  If the server supports regetting, it will instruct the server to continue the download from where it left off.</code></span>
<span class="comment-copy">(cont) The main problem is, as you said, knowing when to process the files inside. If you're on linux, you can use <code>inotify</code> to be notified of new files as they appear, and process them accordingly</span>
<span class="comment-copy">Also, check the <code>--retry</code> option on <code>curl</code></span>
<span class="comment-copy">@goncalopp The wget method writes to a file. so I'd still need the large drive. curl doesn't support resume does it?</span>
<span class="comment-copy">you can get <code>wget</code> writing to the <code>stdout</code> using <code>-O -</code> Check <a href="http://linux.die.net/man/1/wget" rel="nofollow noreferrer">the manpage</a></span>
