<div class="post-text" itemprop="text">
<p><strong>Problem :</strong> </p>
<p>Iam working on a data analysis project which require me to compare the substrings of an unknown word against a corpus of good and bad words.</p>
<p>I initially generated 4 lists and stored them in pickle format in disk.</p>
<pre><code>-rw-rw-r-- 1 malware_corpus malware_corpus 189M May  4 13:11 clean_a.pkl
-rw-rw-r-- 1 malware_corpus malware_corpus 183M May  4 13:12 clean_b.pkl
-rw-rw-r-- 1 malware_corpus malware_corpus 1.7M Apr 30 11:12 data_backup.csv
-rw-rw-r-- 1 malware_corpus malware_corpus 2.9M May  4 13:13 data.csv
-rw-rw-r-- 1 malware_corpus malware_corpus 231M May  4 13:12 mal_a.pkl
-rw-rw-r-- 1 malware_corpus malware_corpus 232M May  4 13:13 mal_b.pkl
</code></pre>
<p>So in my code whenever a new string arises i will take these 4 lists and compare the sub-strings into these 4 lists and calculate the score.Due to all these 4 lists stored in memory,program is slow</p>
<p>Also each list has millions of words and if i want to do a search iam taking much longer time as it is taking O(n) time.</p>
<p><strong>Solution Required:</strong></p>
<ul>
<li>Any effiecient way to store the 4 lists so that they wont bulk up my memory.</li>
<li>Any better way to search the string in the 4 lists.</li>
<li>How to access large lists in python.</li>
</ul>
<p>Code Part:</p>
<pre><code>    def create_corpus(self):
    """corpus

    :param domain: Doamin passed will be split and words are stored in
    corpus.
    """
    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)),'utils/x.txt'),'r') as f:
        for line in f:
            line = line.rstrip()

            self.line_x = self.calculate_xs(line)
            for i in self.line_x:
                self.clean_xs.append(i)
            self.line_y = self.calculate_ys(line)
            for i in self.line_y:
                self.clean_ys.append(i)
    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)),'utils/y.txt'),'r') as f:
        for line in f:
            line = line.rstrip()
            self.line_x = self.calculate_x(line)
            for i in self.line_x:
                self.mal_xs.append(i)
            self.line_y = self.calculate_y(line)
            for i in self.line_y:
                self.mal_ys.append(i)

    # Store the Datasets in pickle Formats
    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)),\
                           'utils/clean_x.pkl'),'wb') as f:
        pickle.dump(self.clean_xs , f)

    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)),\
                           'utils/clean_ys.pkl'),'wb') as f:
        pickle.dump(self.clean_ys , f)
    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)),\
                           'utils/mal_xs.pkl'),'wb') as f:
        pickle.dump(self.mal_xs , f)
    with open(os.path.join(os.path.dirname(os.path.realpath(__file__)),\
                           'utils/mal_ys.pkl'),'wb') as f:
        pickle.dump(self.mal_ys, f)
    return 1


def compare_score_function(self,domain):
    self.domain = domain
    self.g_freq = {}
    self.b_score = 0.0
    from collections import Counter
    for x in self.substrings_of_domain:
        self.g_freq[x] = {}
        self.g_freq[x]['occur'] = self.clean_x.count(x)
        self.g_freq[x]['freq']  = self.clean_x.count(x)/len(self.clean_x)
    for key,value in self.g_freq.iteritems():
        self.b_score += value['freq']
    return self.b_score

def calculate_x(self,domain):
    self.domain = self.clean_url(domain)
    self.bgrm = list(ngrams(self.domain,2))
    self.bgrm = [''.join(a) for a in self.bgrm ]
    return self.bgrm

def calculate_y(self,domain):
    self.domain = self.clean_url(domain)
    self.tgrm = list(ngrams(self.domain,3))
    self.tgrm = [''.join(a) for a in self.tgrm]
    return self.tgrm
</code></pre>
<p><strong>Example Explaination</strong> </p>
<ul>
<li>clean_x_list = ['ap','pp','pl','le','bo','xl','ap'] </li>
<li>clean_y_list = ['apa','ppa','fpl','lef','bfo','xdl','mpd'] </li>
<li>bad_x_list = ['ti','qw','zx','qa','qa','qa','uy'] </li>
<li>bad_y_list =  ['zzx','zxx','qww','qww','qww','uyx','uyx']</li>
</ul>
<p>Here suppose these are my 4 Lists:</p>
<p>My new string came -- suppose apple
 - Now i will calculate x words for apple =&gt; ['ap','pp','pl','le']
 - Now i will calculate y words for apple =&gt; ['app','ppl','ple','lea']</p>
<ul>
<li>Now i will search each x-word of apple i.e ['ap','pp','pl','le'] in both clean_x_list and bad_x_list</li>
<li><p>then i will calculate frequency and occurency count</p></li>
<li><p>occurence of ap in clean_x_list = 2</p></li>
<li>frequence of ap in clean_x_list = 2 /7</li>
<li>occurence of ap in bad_x_list   = 0</li>
<li>occurence of ap in bad_x_list   = 0 /7</li>
</ul>
<p>similarly i compute the other words occurence and frequency and finally sum it</p>
</div>
<div class="post-text" itemprop="text">
<p>Consider sorting your lists, and using <a href="https://docs.python.org/3.0/library/bisect.html" rel="nofollow noreferrer"><code>bisect</code></a> for searching your lists. Worst case lookup time is O(log n) in this case.</p>
</div>
<div class="post-text" itemprop="text">
<p>There are basically three options: A linear scan of the list in O(n), ...</p>
<pre><code>&gt;&gt;&gt; lst = random.sample(range(1, 1000000), 100000)
&gt;&gt;&gt; x = lst[50000]
&gt;&gt;&gt; %timeit x in lst
100 loops, best of 3: 2.12 ms per loop
</code></pre>
<p>... a binary search in the sorted list in O(logn), using the <a href="https://docs.python.org/3/library/bisect.html" rel="nofollow noreferrer"><code>bisect</code></a> module, ...</p>
<pre><code>&gt;&gt;&gt; srt = sorted(lst)
&gt;&gt;&gt; srt[bisect.bisect_left(srt, x)] == x
True
&gt;&gt;&gt; %timeit srt[bisect.bisect_left(srt, x)] == x
1000000 loops, best of 3: 444 ns per loop
</code></pre>
<p>... and a lookup in a hash <code>set</code> in O(1): </p>
<pre><code>&gt;&gt;&gt; st = set(lst)
&gt;&gt;&gt; %timeit x in st
10000000 loops, best of 3: 38.3 ns per loop
</code></pre>
<p>Obviously, the <code>set</code> is by far the fastest, but it also takes up a bit more memory than the <code>list</code> based approaches. The <code>bisect</code> approach might be a good compromise, being already 5000 times faster then the linear scan in this example and only requiring to sort the list.</p>
<pre><code>&gt;&gt;&gt; sys.getsizeof(lst)
800064
&gt;&gt;&gt; sys.getsizeof(srt)
900112
&gt;&gt;&gt; sys.getsizeof(st)
4194528
</code></pre>
<p>However, unless your computer is very limited on memory, this should not be a problem. In particular, it will not make the code slower. Either it all fits into memory, and all is well, or it does not, and your program comes to a grinding halt.</p>
<hr/>
<p>If your good/bad word lists can contain duplicates, then <code>set</code> is not an option, and <code>bisect</code> will not work well, either. In this case, create a <a href="https://docs.python.org/3/library/collections.html#collections.Counter" rel="nofollow noreferrer"><code>Counter</code></a> for each of those lists. Then you can get the occurrence count and frequencies for each of the substrings in your text. Being a kind of hash map / dictionary, lookup in the <code>Counter</code> will also be O(1).</p>
<pre><code>&gt;&gt;&gt; clean_x_list = ['ap','pp','pl','le','bo','xl','ap']
&gt;&gt;&gt; w = "apple"
&gt;&gt;&gt; wx = [w[i:i+2] for i in range(len(w)-1)]
&gt;&gt;&gt; ccx = collections.Counter(clean_x_list)

&gt;&gt;&gt; occ_wx = {x: ccx[x] for x in wx}
&gt;&gt;&gt; occ_wx
{'ap': 2, 'pp': 1, 'pl': 1, 'le': 1}

&gt;&gt;&gt; freq_wx = {x: ccx[x] / len(clean_x_list) for x in wx}
&gt;&gt;&gt; freq_wx
{'ap': 0.2857142857142857,
 'pp': 0.14285714285714285,
 'pl': 0.14285714285714285,
 'le': 0.14285714285714285}
</code></pre>
<p>And analogously for <code>clean_y_list</code>, <code>bad_x_list</code>, and so on.</p>
</div>
<div class="post-text" itemprop="text">
<p>One option to save space is to store the word files in a compressed way, but also not read the entire word file into memory.  For this an easy option is <a href="https://docs.python.org/3/library/gzip.html#gzip.GzipFile" rel="nofollow noreferrer">gzip.GzipFile</a> which allows you to operate on a gzip archive like a regular file:</p>
<pre><code>import gzip

with gzip.open('input.gz','rt') as text_f:
    for line in text_f:
        line = line.strip()
        print(line)
</code></pre>
<p>This way you can treat each line in the file as an item in a list, and process them accordingly.</p>
<p>Note the <code>rt</code> (or <code>wt</code>) open method, which causes it to be processed as text, not binary - this will depend on if you're just storing plain text/json, or using a binary format for your data (like pickle).</p>
</div>
<span class="comment-copy">The code you posted seems to be part of a class, but a lot of member function are missing (for example <code>calculate_xs</code>). So it is difficult to know what you are doing. Have you tried to store the list as dictionary for faster search ?</span>
<span class="comment-copy">Not sure about the taking-up-less-memory part, but you could just store them in a <code>set</code>. The set would provide O(1) lookup, but take up more memory than the list.</span>
<span class="comment-copy">Would using a trie tree helped?</span>
<span class="comment-copy">"Due to all these 4 lists stored in memory,program is slow" Storing the lists in memory would not make the program slow, unless you have so few memory that this will cause your program to swap memory to the hard drive, in which case it will be <i>very</i> slow. But if that's not the case (and it should not on most modern computers) the memory consumption should not be an issue, and you can just use sets for fast lookup.</span>
<span class="comment-copy">One issue with your Explaination : If you see my code along with the search,iam even counting the no of occurences of a sub-string.</span>
<span class="comment-copy">@BackdoorCipher To be honest, I could not really make much sense of your code, and how it is connected to the actual question. What exactly are you counting, how often a "good" or "bad" word appear in the text? In this case, do it the other way around and create a <code>Counter</code> from the text, then iterate the good/bad words and look up the counts in the <code>Counter</code>. Or if you just need the accumulated counts, use the <code>set</code> and a <code>sum(x in bad_words_set for x in text)</code></span>
<span class="comment-copy">Iam counting the no of occurences of each substring in both clean and malicious string list.</span>
<span class="comment-copy">@BackdoorCipher So what exactly is a "substring" in the first place? I'd expect the good/bad lists to contain each word at most once, so how can there be counts greater than 1 then? Or are you actually counting parts of words, i.e. prefixes or suffixes?</span>
<span class="comment-copy">good and bad lists is a list of all substrings of my corpus of words,it can have repetition.</span>
