<div class="post-text" itemprop="text">
<p>I'm attempting to use <a href="https://esa.github.io/pagmo2/" rel="nofollow noreferrer">pygmo 2.5</a> optimization libraries installed via Anaconda3 with some existing code which wraps the asynchronous and distributed evaluation of parameter vectors via an executable which does trajectory optimization (it's <a href="https://post2.larc.nasa.gov/" rel="nofollow noreferrer">POST2</a> if anyone is interested). To facilitate this I use multiprocessing.SyncManager and multiprocessing.Queues across the network to pass inputs and receive output and logging messages. So in this context, pygmo would be selecting vectors to try, and the supporting code would pass this into an input queue which some distributed worker would grab, evaluate via the executable, and pass the result back, which would ultimately be handed back to whatever pygmo.algorithm is in use for evaluation</p>
<p>My issue is that when pygmo initializes a problem it does a deep copy of the supplied class, which in my case and in the supplied example code below, contains several queues. Upon performing the deepcopy I get the error</p>
<pre><code>  File "pygmo_testing.py", line 121, in &lt;module&gt;
    main()
  File "pygmo_testing.py", line 108, in main
    prob = pg.problem(my_prob)
  File "C:\Anaconda3\lib\copy.py", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "C:\Anaconda3\lib\copy.py", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File "C:\Anaconda3\lib\copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "C:\Anaconda3\lib\copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "C:\Anaconda3\lib\copy.py", line 169, in deepcopy
    rv = reductor(4)
  File "C:\Anaconda3\lib\multiprocessing\queues.py", line 58, in __getstate__
    context.assert_spawning(self)
  File "C:\Anaconda3\lib\multiprocessing\context.py", line 356, in assert_spawning
    ' through inheritance' % type(obj).__name__
RuntimeError: Queue objects should only be shared between processes through inheritance
</code></pre>
<p>Is there a way around this? I need to keep the execution style asynchronous and distributed for the other methods this code employs. I've also tried queue.Queue and a multiprocessing.Manager.Queue (both of which wouldn't work with other existing code) for completeness but it always comes down to the deepcopy.</p>
<p>Thanks everyone!</p>
<hr/>
<pre><code>"""
****************************** Import Statements ******************************
"""
import pygmo as pg
from multiprocessing import Pool, Queue

"""
****************************** Utility Functions ******************************
"""  
def sphere_fitness(x):
    return sum(x*x)

def worker(inp_q, out_q):

    while True:

        x = inp_q.get()
        print("got {}".format(x))
        if x == False:
            break
        else:
            fit = sphere_fitness(x)
            print("x: {} f: {}".format(x, fit))
            out_q.put_nowait(fit)
            print("submitted {}".format(x))        
"""
********************************** Class(es) ************************************
"""
class distributed_submit(object):
    """ Class for pygmo Problem"""

    def __init__(self, dim, inp_q, out_q):
        self.dim = dim
        self._inp_q = inp_q
        self._out_q = out_q

    def _submit(self, inp_q, x):
        self._inp_q.put_nowait(x)
        print("x delivered")

    def _receive(self, out_q):
        return self._out_q.get()

    def fitness(self, x):
        self._submit(x)
        print("put in {}".format(x))
        fit = self._receive()
        print("got {}".format(fit))
        return [fit]

    def get_bounds(self):
        return ([-1]*self.dim, [1]*self.dim)

    def get_name(self):
        return "Sphere Function"

    def get_extra_info(self):
        return "\tDimensions: {}".format(self.dim)


"""
******************************* Main Function ********************************
"""
def main():

    # Queues from multiprocessing
    _inp_q   = Queue()
    _out_q   = Queue()

    _workers = Pool(initializer=worker,
                    initargs=(_inp_q, _out_q))
    _workers.close()

    my_prob = distributed_submit(3, _inp_q, _out_q)

    prob = pg.problem(my_prob)

    algo = pg.algorithm(pg.bee_colony(gen=20, limit=20))

    pop = pg.population(prob, 10)
    print(pop)

    pop = algo.evolve(pop)

    print(pop.champion_f)


if __name__ == "__main__":
    main()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>For anyone who finds this and needs an answer - collections.deque supports deepcopy</p>
<p>Per <a href="https://docs.python.org/3/library/collections.html#collections.deque" rel="nofollow noreferrer">https://docs.python.org/3/library/collections.html#collections.deque</a></p>
<blockquote>
<p>In addition to the above, deques support ... copy.copy(d), copy.deepcopy(d),</p>
</blockquote>
<p>However collections.deque is not process-aware, so it cannot be used for distribution </p>
</div>
<span class="comment-copy">Everything I'm reading says Queue is for synchronization across threads or processes (collections.Queue and multiprocessing.Queue respectively) and that they cannot be copied.  Like a generator, to copy it would be to exhaust it. If pygmo must copy what it is given, then you'll need to find a way to work with copyable data structures I think.  I'll have to look at it some more...</span>
