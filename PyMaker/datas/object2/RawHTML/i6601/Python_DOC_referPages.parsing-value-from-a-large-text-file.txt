<div class="post-text" itemprop="text">
<p>I need to parse a huge text file (100s of GB) and do some analysis. I though of splitting the text file and parse through that block and append values in dictionary similar to below</p>
<pre><code>import collections
import re
d = collections.defaultdict(lambda: [0,0,0])

bufsize = 65536
with open(path) as infile:
    while True:
        lines = infile.readlines(bufsize)
        if not lines:
            break
        for line in lines:
            temp  = line.split(' ')
            d[temp[0]][0]+=1

            if re.match(r"[2,3].{2}", temp[1]):
                d[temp[0]][1]+=1
            else:
                d[temp[0]][2]+=1
</code></pre>
<p>I use if condition inside for loop. Will it affect the performance.Is there any other efficient way to do this with less time </p>
</div>
<div class="post-text" itemprop="text">
<p>According to <a href="https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files" rel="nofollow noreferrer">Python docs</a> all you need to do is use context manager:</p>
<pre><code>with open(path) as myfile:
    for line in myfile:
        do_something_with(line)
</code></pre>
<p>The <code>with</code>
 construct is suggested because is handling all the <code>open/close</code> file operations even if there is an exception, so ti avoids you to use a <code>try catch</code> block. </p>
<p>Moreover the <code>for line in myfile</code> is also suggested in the same doc page for memory efficency. I quote from documentation:</p>
<blockquote>
<p>For reading lines from a file, you can loop over the file object. This
  is memory efficient, fast, and leads to simple code: <code>for line in f:  print(line, end='')</code></p>
</blockquote>
</div>
<div class="post-text" itemprop="text">
<p>The best way is to read the file line by line and process each line accordingly (as @rakwaht suggested)</p>
<p>But, cpu performance is going to be high when you read the file line by line. It reaches <code>&gt;90%</code> whenever the program is running.</p>
<p>Now, if you want to avoid that cpu performance hike, introduce a sleep counter (this doesn't need to be in seconds just <code>0.1 seconds</code> would do)</p>
<pre><code>with open(path) as fp:
    for line in fp:
        foo(line)
        sleep(0.1)
</code></pre>
<p>Experiment with the sleep timer. For me, when I used <code>0.1 seconds</code>, the cpu performance fell down to <code>6%-7%</code>. If you need much below than that, you can increase it to <code>0.3 seconds</code>. Just try to keep it below <code>1 second</code>, other wise your program will be slow as well.</p>
<p><strong>Note 1</strong>: Always try to reduce the cpu performance. Because, when you are trying to read a 1GB file, it takes quite a bit of time. You don't want the cpu hike for the whole time. Other processes wouldn't work. Now imagine if you have a 4GB file or more than that. CPU is going to be stacked up - by the time you complete your program's execution you will hear that high revolving speed of the system cooling fan - this makes you to not even open another application.</p>
<p><strong>Note 2</strong>: Also, never try to store anything in memory when you are processing such huge files. Memory usage exponentially increases in these kind of programs. Try to use more of generators</p>
</div>
<span class="comment-copy">I'm not sure why you're using a buffer here. Can't you directly do <code>for line in lines</code> instead of wrapping it all in an infinite loop?</span>
<span class="comment-copy">how looks like the data in your file ?</span>
<span class="comment-copy">@AdamSmith Because the file is so huge. If I take all the data for file, it might exhaust the memory</span>
<span class="comment-copy"><code>for line in lines</code> iterates by line, it will consume less memory than this approach (unless your lines are over 65536 bytes in size)</span>
<span class="comment-copy">Post the input file fragment and expected result so that you code and logic could be further optimized</span>
