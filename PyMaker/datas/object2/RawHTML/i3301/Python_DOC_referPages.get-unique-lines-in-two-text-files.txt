<div class="post-text" itemprop="text">
<p>I have two unsorted text files (between 150MB and 1GB in size).</p>
<p>I want to find all the lines that are present in <code>a.txt</code> and <strong>not</strong> in <code>b.txt</code>.</p>
<p><code>a.txt</code> contains--&gt;</p>
<pre><code>qwe
asd
zxc
rty
</code></pre>
<p><code>b.txt</code> contains--&gt;</p>
<pre><code>qwe
zxc
</code></pre>
<p>If I combine <code>a.txt</code> and 'b.txt<code>in</code>c.txt` I get:</p>
<pre><code>qwe
asd
zxc
rty
qwe
zxc
</code></pre>
<p>I sort them alphabetically and get:</p>
<pre><code>asd
qwe
qwe
rty
zxc
zxc
</code></pre>
<p>Then I use regx mode to search for (.*)\n(\1)\n and replace them all with null and then I replace all \n\n multiple times with \n to get the "difference" between two files.</p>
<p>Now I am unable to do so in python. I am able to do it till the sorting part but regular expressions doesn't seems to be working in multi-lines.
Here is my python code</p>
<pre><code>f = open("output.txt", 'w')
s = open(outputfile,'r+')
for line in s.readlines():
    s = line.replace('(.*)\n(\1)\n', '')
    f.write(s)

f.close() 
</code></pre>
</div>
<div class="post-text" itemprop="text">
<blockquote>
<p>I am able to do it till the sorting part but regular expressions doesn't seems to be working in multi-lines.</p>
</blockquote>
<p>Your regex is fine. You <em>don't have multi-lines</em>. You have <strong>single</strong> lines:</p>
<pre><code>for line in s.readlines():
</code></pre>
<p><code>file.readlines()</code> reads all of a file into memory as a list of lines. You then the iterates over <em>each of those single lines</em>, so <code>line</code> will be <code>'asd\n'</code> or <code>'qwe\n'</code>, and <em>never</em> <code>'qwe\nqwe\n'</code>.</p>
<p>Given that you are reading all of your merged file into memory, I'm going to presume that your files are not that big. In that case, it'd be <em>much easier</em> to just read one of those files into a set object, then just test each line of the other file to find the differences:</p>
<pre><code>with open('a.txt', 'r') as file_a:
    lines = set(file_a)  # all lines, as a set, with newlines

new_in_b = []
with open('b.txt', 'r') as file_b:
    for line in file_b:
        if line in lines:
            # present in both files, remove from `lines` to find extra lines in a
            lines.remove(line)
        else:
            # extra line in b
            new_in_b.append(line)

print('Lines in a missing from b')
for line in sorted(lines):
    print(line.rstrip())  # remove the newline when printing.
print()

print('Lines in b missing from a')
for line in new_in_b:
    print(line.rstrip())  # remove the newline when printing.
print()
</code></pre>
<p>If you wanted to write those all out to a file, you could just combine the two sequences and write out the sorted list:</p>
<pre><code>with open('c.txt', 'w') as file_c:
    file_c.writelines(sorted(list(lines) + new_in_b))
</code></pre>
<p>Your approach, sorting your lines first, putting them all in a file, and then matching paired lines, is possible too. All you need to do is remember the <em>preceding line</em>. Together with the current line, that's a pair. Note that you <em>don't need a regular expression for this</em>, just an equality test:</p>
<pre><code>with open('c.txt', 'r') as file_c, open('output.txt', 'w') as outfile:
    preceding = None
    skip = False
    for line in file_c:
        if preceding and preceding == line:
            # skip writing this line, but clear 'preceding' so we don't
            # check the next line against it
            preceding = None
        else:
            outfile.write(preceding)
            preceding = line
    # write out the last line
    if preceding:
        outfile.write(preceding)
</code></pre>
<p>Note that this never reads the whole file into memory! Iteration directly over the file gives you individual lines, where the file is read in chunks into a buffer. This is a very efficient method of processing lines.</p>
<p>You can also iterate over the file two lines at a time using the <a href="https://docs.python.org/3/library/itertools.html" rel="nofollow noreferrer"><code>itertools</code> library</a> to tee off the file object iterator:</p>
<pre><code>with open('c.txt', 'r') as file_c, open('output.txt', 'w') as outfile:
    iter1, iter2 = tee(file_c)  # two iterators with shared source
    line2 = next(iter2, None)  # move second iterator ahead a line
    # iterate over this and the next line, and add a counter
    for i, (line1, line2) in enumerate(zip(iter1, iter2)):
        if line1 != line2:
            outfile.write(line1)
        else:
            # clear the last line so we don't try to write it out
            # at the end
            line2 = None
    # write out the last line if it didn't match the preceding
    if line2:
        outfile.write(line2)
</code></pre>
<p>A third approach is to use <a href="https://docs.python.org/3/library/itertools.html#itertools.groupby" rel="nofollow noreferrer"><code>itertools.groupby()</code></a> to group lines that are equal together. You can then decide what to do with those groups:</p>
<pre><code>from itertools import groupby

with open('c.txt', 'r') as file_c, open('output.txt', 'w') as outfile:
    for line, group in groupby(file_c):
        # group is an iterator of all the lines in c that are equal
        # the same value is already in line, so all we need to do is
        # *count* how many such lines there are:
        count = sum(1 for line in group)  # get an efficient count
        if count == 1:
            # line is unique, write it out
            outfile.write(line)
</code></pre>
<p>I'm assuming that it doesn't matter if there are 2 or more copies of the same line. In other words, you don't want <em>pairing</em>, you want to only find the unique lines (those only present in a or b).</p>
<p>If your files are extremely large <em>but already sorted</em>, you can use a merge sort approach, <em>without</em> having to merge your two files into one manually. The <a href="https://docs.python.org/3/library/heapq.html#heapq.merge" rel="nofollow noreferrer"><code>heapq.merge()</code> function</a> gives you lines from multiple files in sorted order provided the <em>inputs</em> are sorted individually. Use this together with <code>groupby()</code>:</p>
<pre><code>import heapq
from itertools import groupby

# files a.txt and b.txt are assumed to be sorted already
with open('a.txt', 'r') as file_a, open('b.txt', 'r') as file_b,\
        open('output.txt', 'w') as outfile:
    for line, group in groupby(heapq.merge(file_a, file_b)):
        count = sum(1 for line in group)
        if count == 1:
            outfile.write(line)
</code></pre>
<p>Again, these approaches only read enough data from each file to fill a buffer. The <code>heapq.merge()</code> iterator only holds two lines in memory at a time, as does <code>groupby()</code>. This lets you process files of any size, regardless of your memory constraints.</p>
</div>
<span class="comment-copy">That's... a rather inefficient and round-about way of finding the difference. Why not use sets?</span>
<span class="comment-copy">And you are matching against <i>each individual line</i>, so no, you can't then use a regex that requires there to be at least <i>two</i> lines to work.</span>
<span class="comment-copy">It would be better to use a library than writing the diff logic on your own. <a href="https://stackoverflow.com/questions/977491/comparing-two-txt-files-using-difflib-in-python" title="comparing two txt files using difflib in python">stackoverflow.com/questions/977491/â€¦</a></span>
<span class="comment-copy">@MartijnPieters How do I cancel them using sets</span>
<span class="comment-copy">Can I store two lines in a single variable by using any means?</span>
<span class="comment-copy">My files are are in the range of 150 MBs to 1 GB, all containing image file name in different line like 201805002113_P.jpg</span>
<span class="comment-copy">@SanjayWadhwa: that's up to 50 million lines then, that does become a little.. large. Are your input files sorted at all?</span>
<span class="comment-copy">@SanjayWadhwa: your own approach can't handle that very well either, because you are reading <i>all lines of <code>c.txt</code> into memory</i>. With up to 100 million lines (two 1GB files of filenames) you <i>will</i> run out of memory.</span>
<span class="comment-copy">I know my approach will not work... And no... they aren't sorted at all</span>
<span class="comment-copy">@SanjayWadhwa: I'd get them sorted then; there are efficient command-line tools that can get you a sorted file output. Even if you only sort <i>subsets of each file</i>, you can use <code>heapq.merge()</code> to sort together any number of subsets if each subset is sorted.</span>
