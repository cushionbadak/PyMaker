<div class="post-text" itemprop="text">
<p>I have several files and I would like to read those files, filter some keywords and write them into different files. I use Process() and it turns out that it takes more time to process the readwrite function. 
Do I need to separate the read and write to two functions? How I can read multiple files at one time and write key words in different files to different csv?</p>
<p>Thank you very much.</p>
<pre><code>def readwritevalue():
    for file in gettxtpath():    ##gettxtpath will return a list of files
        file1=file+".csv"
        ##Identify some variable
##Read the file
        with open(file) as fp:
            for line in fp:
                #Process the data
                data1=xxx
                data2=xxx
                ....
         ##Write it to different files
        with open(file1,"w") as fp1
            print(data1,file=fp1 )
            w = csv.writer(fp1)
            writer.writerow(data2)
            ...
if __name__ == '__main__':
    p = Process(target=readwritevalue)
    t1 = time.time()
    p.start()
    p.join()
</code></pre>
<p>Want to edit my questions. I have more functions to modify the csv generated by the readwritevalue() functions. 
So, if Pool.map() is fine. Will it be ok to change all the remaining functions like this? However, it seems that it did not save much time for that.</p>
<pre><code>def getFormated(file):  ##Merge each csv with a well-defined formatted csv and generate a final report with writing all the csv to one output csv

   csvMerge('Format.csv',file,file1)
   getResult()

if __name__=="__main__":
    pool=Pool(2)
    pool.map(readwritevalue,[file for file in gettxtpath()])
    pool.map(GetFormated,[file for file in getcsvName()])
    pool.map(Otherfunction,file_list)
    t1=time.time()
    pool.close()
    pool.join()
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can extract the body of the <code>for</code> loop into its own function, create <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool" rel="nofollow noreferrer">a <code>multiprocessing.Pool</code> object</a>, then call <a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.map" rel="nofollow noreferrer"><code>pool.map()</code></a> like so (I’ve used more descriptive names):</p>
<pre><code>import csv
import multiprocessing

def read_and_write_single_file(stem):
    data = None

    with open(stem, "r") as f:
        # populate data somehow

    csv_file = stem + ".csv"

    with open(csv_file, "w", encoding="utf-8") as f:
        w = csv.writer(f)

        for row in data:
            w.writerow(data)

if __name__ == "__main__":
    pool = multiprocessing.Pool()
    result = pool.map(read_and_write_single_file, get_list_of_files())
</code></pre>
<p>See the linked documentation for how to control the number of workers, tasks per worker, etc.</p>
</div>
<div class="post-text" itemprop="text">
<p>I may have found an answer myself. Not so sure if it is indeed a good answer, but the time is 6 times shorter than before.</p>
<pre><code>def readwritevalue(file):
    with open(file, 'r', encoding='UTF-8') as fp:
        ##dataprocess
    file1=file+".csv"
    with open(file1,"w") as fp2:
        ##write data


if __name__=="__main__":
    pool=Pool(processes=int(mp.cpu_count()*0.7))
    pool.map(readwritevalue,[file for file in gettxtpath()])
    t1=time.time()
    pool.close()
    pool.join()
</code></pre>
</div>
<span class="comment-copy">You can only speed it up a little with concurrency like this. The bottleneck is much more likely to be reading the files than processing them.</span>
<span class="comment-copy">@Aankhen Will threading help? I once considered aiofiles but I have errors with using async with aiofile....Thx!</span>
<span class="comment-copy">Threading isn’t likely to help if the bottleneck is the I/O. Async I/O may give you some benefit. You’d have to try it out and see, since the impact varies depending on the specifics (including the machine that you run it on).</span>
<span class="comment-copy">@Aankhen Thank you very much for your patient answer... I will try to figure it out.</span>
<span class="comment-copy">Thank you for your answer!...I found an answer and I posted it. It seems a bit like yours. Would you please help to check my code to see if it is fine?</span>
<span class="comment-copy">This looks semantically correct, so you should be fine, except that <code>int(1 * 0.7)</code> is <code>0</code>, so if you have only a single CPU then it passes 0 as the number of processes. BTW, you don’t need the list comprehension when you call <code>map</code>. You can pass it <code>gettxtpath()</code> directly.</span>
<span class="comment-copy">@Aankhen, I deleted the Pool(processes) line and change to pool.map(readwritevalue, gettxtpath). The time is 2 times longer....Any idea please?</span>
