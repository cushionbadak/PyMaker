<div class="post-text" itemprop="text">
<p>What I'm aiming to get is only the raw link which I can then use to download the image. but I keep getting some extra characters along with the link.
from bs4 import BeautifulSoup
import requests</p>
<pre><code>from bs4 import BeautifulSoup
import requests

def getPages():
    x = 0
    url = 'https://readheroacademia.net/manga/boku-no-hero-academia-chapter-137/'
    req = requests.get(url)
    webpage = req.content
    soup = BeautifulSoup(webpage, 'html.parser')
    pages = soup.findAll('div', attrs={'class': 'acp_content'})
    for p in pages:
        y = p.findAll('img')
        print(y)
getPages()
</code></pre>
<p>What I end up getting looks like this:</p>
<p><code>[&lt;img src="https://2.bp.blogspot.com/-p72DilhF-_s/WRSF41vu50I/AAAAAAAAlsk/6BTxzQAzPkwteMgEHch2JFH0JKKpbKrZACHM/s16000/0137-001.png"/&gt;]</code></p>
<p>and I was hoping I could get something like this:</p>
<p><code>https://2.bp.blogspot.com/-p72DilhF-_s/WRSF41vu50I/AAAAAAAAlsk/6BTxzQAzPkwteMgEHch2JFH0JKKpbKrZACHM/s16000/0137-001.png</code></p>
</div>
<div class="post-text" itemprop="text">
<p>If you want to get only the <code>src</code>, you can do:</p>
<pre><code>for p in pages:
    y = [tag["src"] for tag in p.findAll("img")]
    print(y)
</code></pre>
<p>It gets the url out of each img tag instead of getting the whole tag.</p>
<p>Also, if you're using <code>bs4</code> or <code>BeautifulSoup4</code>, use <code>find_all</code> instead of <code>findAll</code>. <code>findAll</code> is <code>bs3</code>, the older version.</p>
</div>
<div class="post-text" itemprop="text">
<p>I think it will work:</p>
<pre><code>&gt;&gt;&gt; from bs4 import BeautifulSoup
&gt;&gt;&gt; data = """&lt;img src="https://2.bp.blogspot.com/-p72DilhF-_s/WRSF41vu50I/AAAAAAAAlsk/6BTxzQAzPkwteMgEHch2JFH0JKKpbKrZACHM/s16000/0137-001.png"/&gt;"""
&gt;&gt;&gt; soap = BeautifulSoup(data,"lxml")
&gt;&gt;&gt; for i in soap.find_all("img"):
        link = i.get("src")
        print(link)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>An alternative approach is to use XPath. I suggest using lxml here since there is no XPath support within Beautiful. This is actually a very simple solution:</p>
<pre><code>from lxml import html
import requests

page = requests.get('https://readheroacademia.net/manga/boku-no-hero-academia-chapter-137/')
tree = html.fromstring(page.content)
#This will create a list of img src attributes beneth the `&lt;div id="acp_content" class="acp_content"&gt;` tag:
srcs = tree.xpath('//div[@id="acp_content"]//img/@src')
</code></pre>
</div>
<span class="comment-copy">Obligatory warning that the builtin <code>lxml</code> is vulnerable to several attacks via maliciously constructed payloads: <a href="https://docs.python.org/3/library/xml.html#xml-vulnerabilities" rel="nofollow noreferrer">docs.python.org/3/library/xml.html#xml-vulnerabilities</a></span>
<span class="comment-copy">@BaileyParker Thanks for the warning. That's always something to look out for. But is this important here?</span>
<span class="comment-copy">Unless you fully trust everyone that could interfere with the contents of that page, yes. And even if you think you do, why take that risk?</span>
<span class="comment-copy">@BaileyParker I get it. However, using any software/library against arbitrary content arguably comes with a risk and lxml is not necessarily the worst offender or - if let's say xpath is required, what would you suggest?</span>
<span class="comment-copy">Citing from the <a href="https://lxml.de/FAQ.html" rel="nofollow noreferrer">lxml FAQ</a>: Is lxml vulnerable to XML bombs? "This has nothing to do with lxml itself, only with the parser of libxml2. Since libxml2 version 2.7, the parser imposes hard security limits on input documents to prevent DoS attacks with forged input data. Since lxml 2.2.1, you can disable these limits with the huge_tree parser option if you need to parse really large, trusted documents. All lxml versions will leave these restrictions enabled by default." It looks like using a current version of lxml should be fine for one-off jobs like this.</span>
