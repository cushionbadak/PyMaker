<div class="post-text" itemprop="text">
<p>I have a python script that reads a flat file and writes the records to a JSON file. Would it be faster to do a write all at once:</p>
<pre><code>dict_array = []
for record in records:
    dict_array.append(record)
# writes one big array to file
out_file.write(json.dumps(dict_array))
</code></pre>
<p>Or write to the file as the iterator yields each record?</p>
<pre><code>for record in records:
    out_file.write(json.dumps(record) + '\n')
</code></pre>
<p>The amount of records in <code>records</code> is around <code>81,000</code>.</p>
<p>Also, the format of JSON can be one big array of objects (case 1) or line-separated objects (case 2).</p>
</div>
<div class="post-text" itemprop="text">
<p>Your two solutions aren't doing the same thing. The first one writes a valid JSON object. The second writes a probably-valid (but you have to be careful) JSONlines (and probably also NDJSON/LDJSON and NDJ) file. So, the way you process the data later is going to be very different. And that's the most important thing here—do you want a JSON file, or a JSONlines file?</p>
<p>But since you asked about performance: It depends.</p>
<p>Python files are buffered by default, so doing a whole bunch of small writes is only a tiny bit slower than doing one big write. But it is a <em>tiny</em> bit slower, not <em>zero</em>.</p>
<p>On the other hand, building a huge list in memory means allocation, and copies, that are otherwise unneeded. This is almost certainly going to be more significant, unless your values are really tiny and your list is also really short.</p>
<p>Without seeing your data, I'd give about 10:1 odds that the iterative solution will turn out faster, but why rely on that barely-educated guess? If it matters, measure with your actual data with <a href="https://docs.python.org/3/library/timeit.html" rel="nofollow noreferrer"><code>timeit</code></a>. </p>
<p>On the third hand, 81,000 typical JSON records is basically nothing, so unless you're doing this zillions of times, it's probably not even worth measuring. If you spend an hour figuring out how to measure it, running the tests, and interpreting the results (not to mention the time you spent on SO) to save 23 milliseconds per day for about a week and then nothing ever again… well, to a programmer, that's always attractive, but still it's not always a good idea.</p>
</div>
<div class="post-text" itemprop="text">
<pre><code>import json

dict_array = []
records = range(10**5)

start = time.time()
for record in records:
    dict_array.append(record)
out_file.write(json.dumps(dict_array))
end = time.time()
print(end-start)
#0.07105851173400879

start = time.time()
for record in records:
    out_file.write(json.dumps(record) + '\n')
end = time.time()
print(end-start)
#1.1138122081756592

start = time.time()
out_file.write(json.dumps([record for record in records]))
end = time.time()
print(end-start)
#0.051038265228271484
</code></pre>
<p>I don't know what <code>records</code> is, but based on these tests, list comprehension is fastest, followed by constructing a list and writing it all at once, followed by writing one record at a time. Depending on what <code>records</code> is, just doing <code>out_file.write(json.dumps(records)))</code> may be even faster.</p>
</div>
<span class="comment-copy">You could wrap those sections of code in a <a href="https://docs.python.org/2/library/timeit.html" rel="nofollow noreferrer">timeit function</a> and compare the run times</span>
<span class="comment-copy">It probably doesn't make much difference, it presumably uses output buffering.</span>
<span class="comment-copy">Your statement at the end is incorrect. Line-separated objects is <i>not</i> valid JSON.</span>
<span class="comment-copy">@Barmar you are correct, what I meant to say was the output format could be JSON lines or standard JSON.</span>
<span class="comment-copy"><a href="http://c2.com/cgi/wiki?PrematureOptimization" rel="nofollow noreferrer">Premature optimization is the root of all evil</a></span>
<span class="comment-copy">Calling <code>json.dumps()</code> thousands of times instead of doing it just once might be expensive. It's really hard to estimate which parts of this will be the bottlenecks.</span>
<span class="comment-copy">@Barmar It <i>might</i> be, but I don't think it will be the determining factor here. I could be wrong. But as I said in the answer, why should the OP rely on either of our guesses? It probably won't matter—and, if it does, the right thing to do is to test it (with real data). (Which I realize is basically the same thing you said in your second sentence.)</span>
