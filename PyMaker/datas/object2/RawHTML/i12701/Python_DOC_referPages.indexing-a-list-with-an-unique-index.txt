<div class="post-text" itemprop="text">
<p>I have a list say <code>l = [10,10,20,15,10,20]</code>. I want to assign each unique value a certain "index" to get <code>[1,1,2,3,1,2]</code>.</p>
<p>This is my code:</p>
<pre><code>a = list(set(l))
res = [a.index(x) for x in l]
</code></pre>
<p>Which turns out to be very slow. </p>
<p><code>l</code> has 1M elements, and 100K unique elements. I have also tried map with lambda and sorting, which did not help. What is the ideal way to do this?</p>
</div>
<div class="post-text" itemprop="text">
<p>The slowness of your code arises because <code>a.index(x)</code> performs a linear search and you perform that linear search for each of the elements in <code>l</code>.  So for each of the 1M items you perform (up to) 100K comparisons.</p>
<p>The fastest way to transform one value to another is looking it up in a map.  You'll need to create the map and fill in the relationship between the original values and the values you want.  Then retrieve the value from the map when you encounter another of the same value in your list.</p>
<p>Here is an example that makes a single pass through <code>l</code>.  There may be room for further optimization to eliminate the need to repeatedly reallocate <code>res</code> when appending to it.</p>
<pre><code>res = []
conversion = {}
i = 0
for x in l:
    if x not in conversion:
        value = conversion[x] = i
        i += 1
    else:
        value = conversion[x]
    res.append(value)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You can do this in <code>O(N)</code> time using a <a href="https://docs.python.org/2/library/collections.html#collections.defaultdict" rel="noreferrer"><code>defaultdict</code></a> and a list comprehension:</p>
<pre><code>&gt;&gt;&gt; from itertools import count
&gt;&gt;&gt; from collections import defaultdict
&gt;&gt;&gt; lst = [10, 10, 20, 15, 10, 20]
&gt;&gt;&gt; d = defaultdict(count(1).next)
&gt;&gt;&gt; [d[k] for k in lst]
[1, 1, 2, 3, 1, 2]
</code></pre>
<p>In Python 3 use <a href="https://docs.python.org/3/library/stdtypes.html#iterator.__next__" rel="noreferrer"><code>__next__</code></a> instead of <a href="https://docs.python.org/2/library/stdtypes.html#iterator.next" rel="noreferrer"><code>next</code></a>.</p>
<hr/>
<p><strong>If you're wondering how it works?</strong></p>
<p>The <code>default_factory</code>(i.e <code>count(1).next</code> in this case) passed to <code>defaultdict</code> is called only when Python encounters a missing key, so for 10 the value is going to be 1, then for the next ten it is not a missing key anymore hence the previously calculated 1 is used, now 20 is again a missing key and Python will call the <code>default_factory</code> again to get its value and so on.</p>
<p><code>d</code> at the end will look like this:</p>
<pre><code>&gt;&gt;&gt; d
defaultdict(&lt;method-wrapper 'next' of itertools.count object at 0x1057c83b0&gt;,
            {10: 1, 20: 2, 15: 3})
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Well I guess it depends on if you want it to return the indexes in that specific order or not. If you want the example to return:</p>
<pre><code>    [1,1,2,3,1,2]
</code></pre>
<p>then you can look at the other answers submitted. However if you only care about getting a unique index for each unique number then I have a fast solution for you</p>
<pre><code>    import numpy as np
    l = [10,10,20,15,10,20]
    a = np.array(l)
    x,y = np.unique(a,return_inverse = True)
</code></pre>
<p>and for this example the output of y is:</p>
<pre><code>    y = [0,0,2,1,0,2]
</code></pre>
<p>I tested this for 1,000,000 entries and it was done essentially immediately.</p>
</div>
<div class="post-text" itemprop="text">
<p>Your solution is slow because its complexity is <code>O(nm)</code> with <code>m</code> being the number of unique elements in <code>l</code>: <code>a.index()</code> is <code>O(m)</code> and you call it for every element in <code>l</code>.</p>
<p>To make it <code>O(n)</code>, get rid of <code>index()</code> and store indexes in a dictionary:</p>
<pre><code>&gt;&gt;&gt; idx, indexes = 1, {}
&gt;&gt;&gt; for x in l:
...     if x not in indexes:
...         indexes[x] = idx
...         idx += 1
... 
&gt;&gt;&gt; [indexes[x] for x in l]
[1, 1, 2, 3, 1, 2]
</code></pre>
<p>If <code>l</code> contains only integers in a known range, you could also store indexes in a list instead of a dictionary for faster lookups.</p>
</div>
<div class="post-text" itemprop="text">
<p>You can use <code>collections.OrderedDict()</code> in order to preserve the unique items in order and, loop over the enumerate of this ordered unique items in order to get a dict of items and those indices (based on their order) then pass this dictionary with the main list to <code>operator.itemgetter()</code> to get the corresponding index for each item:</p>
<pre><code>&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt; from operator import itemgetter
&gt;&gt;&gt; itemgetter(*lst)({j:i for i,j in enumerate(OrderedDict.fromkeys(lst),1)})
(1, 1, 2, 3, 1, 2)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>For completness, you can also do it eagerly:  </p>
<blockquote>
<pre><code>from itertools import count

wordid = dict(zip(set(list_), count(1)))
</code></pre>
<p>This uses a set to obtain the unique words in <code>list_</code>, pairs
  each of those unique words with the next value from <code>count()</code> (which
  counts upwards), and constructs a dictionary from the results.</p>
</blockquote>
<p><a href="https://stackoverflow.com/a/33504153/3821804">Original answer</a>, written by nneonneo.</p>
</div>
<span class="comment-copy">Do you care about space complexity or only time complexity?</span>
<span class="comment-copy">Can you use NumPy?</span>
<span class="comment-copy">This is how I would do it. I believe this answer will be the easiest for OP to understand. Couple questions if I may, lets say we have 1b records, 1m unique, then the size of <code>conversion</code> would be 1m, is there a way for us to reduce that? also how would you optimize <code>res</code> append operation</span>
<span class="comment-copy"><code>for each of the 1M items you perform (up to) 100K comparisons</code> â€” why 100K? It should be 1M x 1M, I guess.</span>
<span class="comment-copy">Thank you for the answer. So with your code I am able to get a dictionary, in which neither keys and values have duplicated number. By using a inversed dictionary <code>inv_map = {v: k for k, v in conversion.items()}</code> I can get the original values with index values.</span>
<span class="comment-copy">@eugeney Since <code>a = list(set(l))</code>, <code>len(a)</code> will be 100K. The conversion to a set reduces the size to just the unique values.</span>
<span class="comment-copy">@dsh: thanks, I somehow missed the <code>set</code> part.</span>
<span class="comment-copy">It requires numpy, which is a pretty big dependency for such a task. And it will obviously be fast due to the fact that numpy implements its algorithms in C or Fortran.</span>
<span class="comment-copy">the question asked for the fastest way, but did not specify any dependency restrictions. As I sort of hinted at there are other fine answers available if this route is not appropriate</span>
<span class="comment-copy">I know, I don't think your answer is bad, but it wasn't made clear from your post that it required a huge third-party dependency.</span>
<span class="comment-copy">Hint for readers: this approach uses <code>OrderedDict</code> as set preserving order.</span>
<span class="comment-copy">Sets are unordered, so the indexes may not be assigned in the right order.</span>
