<div class="post-text" itemprop="text">
<p>there is a function in my code that should read the file .each file is about 8M,however the reading speed is too low,and to improve that i use the multiprocessing.sadly,it seems it got blocked.i wanna know is there any methods to help solve this and improve the reading speed?</p>
<p>my code is as follows:</p>
<pre><code>import multiprocessing as mp
import json
import os

def gainOneFile(filename):

    file_from = open(filename)
    json_str = file_from.read()
    temp = json.loads(json_str)
    print "load:",filename," len ",len(temp)
    file_from.close()
    return temp

def gainSortedArr(path):
    arr = []
    pool = mp.Pool(4)
    for i in xrange(1,40):
        abs_from_filename = os.path.join(path, "outputDict"+str(i))
        result = pool.apply_async(gainOneFile,(abs_from_filename,)) 
        arr.append(result.get())

    pool.close()
    pool.join()                                               
    arr = sorted(arr,key = lambda dic:len(dic))

    return arr
</code></pre>
<p>and the call function:</p>
<pre><code>whole_arr = gainSortedArr("sortKeyOut/")  
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>You have a few problems. First, you're not parallelizing. You do:</p>
<pre><code>result = pool.apply_async(gainOneFile,(abs_from_filename,)) 
arr.append(result.get())
</code></pre>
<p>over and over, dispatching a task, then immediately calling <code>.get()</code> which waits for it to complete before you dispatch any additional tasks; you never actually have more than one worker running at once. Store all the results without calling <code>.get()</code>, then call <code>.get()</code> later. Or just use <code>Pool.map</code> or related methods and save yourself some hassle from manual individual result management, e.g. (using <code>imap_unordered</code> to minimize overhead since you're just sorting anyway):</p>
<pre><code># Make generator of paths to load
paths = (os.path.join(path, "outputDict"+str(i)) for i in xrange(1, 40))
# Load them all in parallel, and sort the results by length (lambda is redundant)
arr = sorted(pool.imap_unordered(gainOneFile, paths), key=len)
</code></pre>
<p>Second, <code>multiprocessing</code> has to pickle and unpickle all arguments and return values sent between the main process and the workers, and it's all sent over pipes that incur system call overhead to boot. Since your file system isn't likely to gain substantial speed from parallelizing the reads, it's likely to be a net loss, not a gain.</p>
<p>You <em>might</em> be able to get a bit of a boost by switching to a thread based pool; change the <code>import</code> to <code>import multiprocessing.dummy as mp</code> and you'll get a version of <code>Pool</code> implemented in terms of threads; they don't work around the CPython GIL, but since this code is almost certainly I/O bound, that hardly matters, and it removes the pickling and unpickling as well as the IPC involved in worker communications.</p>
<p>Lastly, if you're using Python 3.3 or higher on a UNIX like system, you may be able to get the OS to help you out by having it pull files into the system cache more aggressively. If you can open the file, then use <a href="https://docs.python.org/3/library/os.html#os.posix_fadvise" rel="nofollow"><code>os.posix_fadvise</code></a> on the file descriptor (<code>.fileno()</code> on file objects) with either <code>WILLNEED</code> or <code>SEQUENTIAL</code> it <em>might</em> improve read performance when you read from the file at some later point by aggressively prefetching file data before you request it.</p>
</div>
<span class="comment-copy">It seems the problem is serialization/deserialization during collection of the data in the main process.</span>
<span class="comment-copy">You would have a modest gain by skipping the intermediate string... <code>temp = json.load(file_from)</code>.</span>
<span class="comment-copy">There is no mp benefit here. You decode the JSON in the child process but it has to be serialized and deserialized again when going back to the parent. Its likely slower than doing it all in one file. Here you have high cost of transfer with low parallel computation.</span>
<span class="comment-copy">multiprocessing <code>Value</code> or <code>Array</code> use shared memory... there may be some benefit there. But you may want to experiment with other faster (?) json parsers.</span>
<span class="comment-copy">but i do have to read the file and decoded it .is there any solutions?@tdelaney</span>
<span class="comment-copy">I'd skip the reading entirely and just map the file.</span>
<span class="comment-copy">@IgnacioVazquez-Abrams: In many cases, so would I, but that's a whole other can of worms I didn't want to open just yet. It's also not always the best approach; 32 bit systems would have issues with huge files, and in the case of files being parsed for JSON, on Python 3, <code>mmap</code> objects can only be used as <code>bytes</code>-like objects, not <code>str</code>, and <code>json</code> on Python 3 only loads from <code>str</code>; you'd still need to read and decode from the <code>mmap</code>, so you've gained very little from it. Even on Py2, if <code>mmap</code> worked with <code>loads</code>, I suspect it would end up getting decoded, so again, no real savings.</span>
<span class="comment-copy">Don't forget that unless you're reading from multiple disks, this is going to be IO-bound, so reading multiple files from the same disk, especially if it's spinning, is likely to <i>hurt</i> performance.</span>
<span class="comment-copy">@DavidEhrmann: For limited parallelism it might help (time spent parsing the data read in one worker is time for other workers to read), and if the OS schedules the reads well, it <i>could</i> reduce latency (of course, it could just as easily increase it if it's interleaving reads at either end of a physical disk). Striped or mirrored RAID arrays, or NFS (where network latency is the killer) could also make this sort of pattern make sense. I did mention that parallelizing the reads is unlikely to produce a gain, but that doesn't mean it couldn't.</span>
