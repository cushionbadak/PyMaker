<div class="post-text" itemprop="text">
<p>I need a specific answer on how to actually iterate and parse through multiple pages on a website , where the URL is known but just to an extent.
I have looked through many tutorials but none tells me actually how to get to the next page - maybe I need to use a regular expression.
I would like to know if anyone can just give me some advice or start on where to look for this:
As such I will give you an example - let us use the Python Website for instance - we know them all very well: <a href="https://docs.python.org/3/tutorial/" rel="nofollow">https://docs.python.org/3/tutorial/</a>
On this page you will see that there is the "next" button that continues the page to : <a href="https://docs.python.org/3/tutorial/appetite.html" rel="nofollow">https://docs.python.org/3/tutorial/appetite.html</a>
From there on if you click the "next" button only the last /*.html changes.</p>
<p>What I would like to have advice on is how to actually go through all the last /*.html iterations and catch those html pages.</p>
</div>
<div class="post-text" itemprop="text">
<p>Since the <code>href</code> values are all relative to the current URL, you cannot simply check if <code>href</code> attribute starts with <code>https://docs.python.org/3/tutorial/</code>. Note that these links have the <code>reference</code> and <code>internal</code> classes, let's use that:</p>
<pre><code>soup.find_all("a", class_=["reference", "internal"])
soup.select("a.reference.internal")  # CSS selector to check multiple classes
</code></pre>
<p>Here is an example working code that extracts the <code>href</code> values for the page:</p>
<pre><code>from urlparse import urljoin

import requests
from bs4 import BeautifulSoup


base_url = "https://docs.python.org/3/tutorial/"
response = requests.get(base_url)
soup = BeautifulSoup(response.content, "html.parser")

for link in soup.select("a.reference.internal"):
    url = link["href"]
    absolute_url = urljoin(base_url, url)

    print(url, absolute_url)
</code></pre>
<p>Note that we have to use <a href="https://docs.python.org/2/library/urlparse.html#urlparse.urljoin" rel="nofollow"><code>.urljoin()</code></a> to get the absolute URLs so that we can follow them.</p>
</div>
<div class="post-text" itemprop="text">
<p>alecxe's answer is good and was essentially going to be the second half to this answer, but it duplicates pages. For example, the urls <code>https://docs.python.org/3/tutorial/inputoutput.html</code> and <code>https://docs.python.org/3/tutorial/inputoutput.html#old-string-formatting</code> are actually the same page, the second is just an anchor on the page.</p>
<p>If you want to do this like you initially stated - find the value of the "next" link's href and then navigate there - you can do something like this:</p>
<p>Use regex to find the <code>div</code>s with "next" in them, and then use their parents to get the actual href. Use <a href="https://docs.python.org/2/library/urlparse.html#urlparse.urljoin" rel="nofollow"><code>urljoin()</code></a> to join together the base_url and the href together to get the absolute url of the next page.</p>
<pre><code>import re
import requests
from bs4 import BeautifulSoup
from urlparse import urljoin


BASE_URL = "https://docs.python.org/3/tutorial/"

def get_next_url(url):
    r = requests.get(url)
    soup = BeautifulSoup(r.text)
    selected = soup.select('div.related h3')
    nav = selected[-1] if selected else None# grab the last one with this css selector
    if nav:
        href = nav.parent.find('a', text=re.compile('next'))['href']
        new_url = urljoin(BASE_URL, href)
        return new_url
    else:
        return None

next = get_next_url(BASE_URL)
while next:
    old = next
    next = get_next_url(old)
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here's my version of the function that recursively searches for pages of Python tutorial. It's shorter and I think even more clear.</p>
<pre><code>from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup as bs

base_url = 'https://docs.python.org/3/tutorial/'

def find_pages(url):
    """Loop over all pages in online Python tutorial."""
    # try open url
    try:
        page = urlopen(url).read()
    # quit if there's no Next link
    except HTTPError:
        print("The end!")
        return

    # parse the page
    soup = bs(page, 'html.parser')

    # find all occurences of the links, that contain text 'next' and have no attributes
    next_url = soup.findAll('a', text = "next", attrs = {'accesskey' : ''})[0].get('href')

    # do something meaningful with the scrapped page here
    print(next_url)

    # recur with the newly obtained next page's url
    find_pages(base_url + next_url)

find_pages(base_url)
</code></pre>
<p>The program can be broken down into following parts:</p>
<ol>
<li>Obtaining html code of the page with <a href="https://docs.python.org/3/library/urllib.request.html" rel="nofollow">urllib</a> (it's worth learning urllib, if you're working with BeautifulSoup!)</li>
<li>Parsing the page with BS</li>
<li>Finding a link that encloses the word 'next'  (see more details in BS's <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="nofollow">docs</a>)</li>
<li>Doing something with the page if you need (I'm just printing a name of the link)</li>
<li>Doing all previous steps, but for the next page until no next pages left</li>
</ol>
<p>The code tested in Python 3. Happy hacking and learning!</p>
</div>
<div class="post-text" itemprop="text">
<p>You need to follow them one by one. Or you can grab the links from an index. For example the page: <a href="https://docs.python.org/3/tutorial/" rel="nofollow">https://docs.python.org/3/tutorial/</a> contains all the links you will go through if you follow the <code>next</code> button. So you can grab them all from this one place. </p>
<p>You have to decide how you can do this best. That usually needs analyzing the link structures and giving it some thoughts. </p>
</div>
<span class="comment-copy">Look at the page, the <code>href</code> attributes are all relative. They look like <code>appetite.html</code> not <code>https://docs.python.org/3/tutorial/appetite.html</code></span>
<span class="comment-copy">@wilbur good point, updated.</span>
<span class="comment-copy">@wilbur sure, updated with a complete working code.</span>
<span class="comment-copy">I just noticed that this grabs EVERY link - including links that are anchors for the same page. Take a peek at my answer and lemme know what you thinK?</span>
<span class="comment-copy">@wilbur I think your answer is much closer to what AP was actually planning to do. Thanks.</span>
