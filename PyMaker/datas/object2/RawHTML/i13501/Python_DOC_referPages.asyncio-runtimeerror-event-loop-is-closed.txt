<div class="post-text" itemprop="text">
<p>I'm trying to make a bunch of requests (~1000) using Asyncio and the aiohttp library, but I am running into a problem that I can't find much info on.</p>
<p>When I run this code with 10 urls, it runs just fine. When I run it with 100+ urls, it breaks and gives me <code>RuntimeError: Event loop is closed</code> error.</p>
<pre><code>import asyncio
import aiohttp


@asyncio.coroutine
def get_status(url):
    code = '000'
    try:
        res = yield from asyncio.wait_for(aiohttp.request('GET', url), 4)
        code = res.status
        res.close()
    except Exception as e:
        print(e)
    print(code)


if __name__ == "__main__":
    urls = ['https://google.com/'] * 100
    coros = [asyncio.Task(get_status(url)) for url in urls]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(coros))
    loop.close()
</code></pre>
<p>The stack trace can be found <a href="https://bpaste.net/show/3ee9e9640987">here</a>.</p>
<p>Any help or insight would be greatly appreciated as I've been banging my head over this for  a few hours now. Obviously this would suggest that an event loop has been closed that should still be open, but I don't see how that is possible.</p>
</div>
<div class="post-text" itemprop="text">
<p>You're right, <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.BaseEventLoop.getaddrinfo" rel="nofollow">loop.getaddrinfo</a> uses a <code>ThreadPoolExecutor</code> to run <code>socket.getaddrinfo</code> in a thread.</p>
<p>You're using <a href="https://docs.python.org/3/library/asyncio-task.html?highlight=wait_for#asyncio.wait_for" rel="nofollow">asyncio.wait_for</a> with a timeout, which means <code>res = yield from asyncio.wait_for...</code> will raise a <code>asyncio.TimeoutError</code> after 4 seconds. Then the <code>get_status</code> coroutines return <code>None</code> and the loop stops. If a job finishes after that, it will try to schedule a callback in the event loop and raises an exception since it is already closed.</p>
</div>
<div class="post-text" itemprop="text">
<p>The bug is filed as <a href="https://github.com/python/asyncio/issues/258">https://github.com/python/asyncio/issues/258</a> 
Stay tuned.</p>
<p>As quick workaround I suggest using custom executor, e.g.</p>
<pre><code>loop = asyncio.get_event_loop()
executor = concurrent.futures.ThreadPoolExecutor(5)
loop.set_default_executor(executor)
</code></pre>
<p>Before finishing your program please do</p>
<pre><code>executor.shutdown(wait=True)
loop.close()
</code></pre>
</div>
<span class="comment-copy">is not <code>Asyncio</code> error. Python recursive error, reached limit. need thread for all non class function...</span>
<span class="comment-copy">First, make sure you are using the latest aiohttp release. I assume you do. Technically aiohttp need one loop iteration after finishing request for closing underlying sockets.  So insert <code>loop.run_until_complete(asyncio.sleep(0))</code> before <code>loop.close()</code> call.</span>
<span class="comment-copy">Your traceback suggests that a job submitted to an <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor" rel="nofollow noreferrer">Executor</a> through <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.BaseEventLoop.run_in_executor" rel="nofollow noreferrer">run_in_executor</a> returned after the loop has been closed. Weirdly enough, <a href="https://github.com/KeepSafe/aiohttp/search?utf8=%E2%9C%93&amp;q=run_in_executor&amp;type=Code" rel="nofollow noreferrer">aiohttp</a> and <a href="https://github.com/python/asyncio/search?utf8=%E2%9C%93&amp;q=run_in_executor" rel="nofollow noreferrer">asyncio</a> don't use <code>run_in_executor</code>...</span>
<span class="comment-copy">@AndrewSvetlov, thanks for the reply - I tried sleeping before close, but still no dice... any other ideas?</span>
<span class="comment-copy">@Vincent technically they does, DNS resolving is performed by <code>run_in_executor</code> -- but it should be done before finishing <code>get_status</code> tasks.</span>
<span class="comment-copy">Ahh, that makes sense, but this is the only way I have found to implement request timeouts. Do you know of a way that I could timeout without closing the loop?</span>
<span class="comment-copy">@PatrickAllen You might want to increase the <a href="https://github.com/python/asyncio/blob/27f3499f968e8734fef91677eb339b5d32a6f675/asyncio/base_events.py#L44" rel="nofollow noreferrer">number of workers</a> that is 5 by default.</span>
<span class="comment-copy">@PatrickAllen Or use <code>loop._default_executor.shutdown(wait=True)</code> before closing the loop.</span>
<span class="comment-copy">I'll mark this as answered, because this seems to have fixed the original problem. Should I be limiting the max number of connections? It seems that requests are timing out for no apparent reason. Maybe I'm making too many requests too quickly?</span>
<span class="comment-copy">@PatrickAllen Well, 5 worker threads and a thousand of request means you're trying to run 200 <code>socket.getaddrinfo</code> in 4 seconds which seems reasonable to me, even though the number of workers can be increased. You can also give a custom <code>TcpConnector</code> to <code>request</code> in order to specify a connection timeout: <code>connector=aiohttp.TCPConnector(loop=loop, force_close=True, conn_timeout=1)</code></span>
<span class="comment-copy">Awesome Andrew, thanks for your help. I didn't realize I was talking to part of the team :). Following this on GH</span>
<span class="comment-copy"><code>Changed in version 3.5.3: BaseEventLoop.run_in_executor() no longer configures the max_workers of the thread pool executor it creates</code></span>
<span class="comment-copy">Andrew, can you suggest not "quick workaround" but some robust workaround for Python 3.5 ?</span>
