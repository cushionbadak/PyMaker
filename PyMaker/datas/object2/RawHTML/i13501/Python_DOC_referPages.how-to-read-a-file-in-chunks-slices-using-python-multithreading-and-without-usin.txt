<div class="post-text" itemprop="text">
<p>I am trying to read a file using multiple threads. I want to divide the file into chunks so that each thread could act separately on each chunk which eliminates the need of a lock as the data is not being shared among different threads. How could I possible do this slicing in memory using python ? To explain it further - </p>
<p>I would need to read the file beforehand to count the number of lines in the file so that I can decide the chunk size (say chunk size = total number of lines/no of threads). In this case, as soon as the main process reads the first chunk, I would want the threads to start processing those lines in the chunk simultaneously.</p>
<p>Could someone provide a sample example? </p>
</div>
<div class="post-text" itemprop="text">
<p>There is no way to count the lines in a file without reading it (you could <code>mmap</code> it to allow the virtual memory subsystem to page out data under memory pressure, but you still have to read the whole file in to find the newlines). If chunks are defined as lines, you're stuck; the file must be read in one way or another to do it.</p>
<p>If chunks can be fixed size blocks of bytes (which may begin and end in the middle of a line), it's easier, but you need to clarify.</p>
<p>Alternatively, if neighboring lines aren't important to one another, instead of chunking, round robin or use a producer/consumer approach (where threads pull new data as it becomes available, rather than distributing by fiat), so the work is naturally distributed evenly.</p>
<p><a href="https://docs.python.org/3/library/multiprocessing.html#using-a-pool-of-workers" rel="nofollow"><code>multiprocessing.Pool</code></a> (or <code>multiprocessing.dummy.Pool</code> if you must use threads instead of processes) makes this easy. For example:</p>
<pre><code>def somefunctionthatprocessesaline(line):
    ... do stuff with line ...
    return result_of_processing

with multiprocessing.Pool() as pool, open(filename) as f:
    results = pool.map(somefunctionthatprocessesaline, f)
... do stuff with results ...
</code></pre>
<p>will create a pool of worker processes matching the number of cores you have available, and have the main process feed queues that each worker pull lines from for processing, returning the results in a <code>list</code> for the main process to use. If you want to process the results from the workers as they become available (instead of waiting for all results to appear in a <code>list</code> like <code>Pool.map</code> does), you can use <code>Pool.imap</code> or <code>Pool.imap_unordered</code> (depending on whether the results of processing each line should be handled in the same order the lines appear) like so:</p>
<pre><code>with multiprocessing.Pool() as pool, open(filename) as f:
    for result in pool.imap_unordered(somefunctionthatprocessesaline, f):
        ... do stuff with one result ...
</code></pre>
</div>
<span class="comment-copy">If you want to gain a significant speed improvement on this in Python, you will probably need to use multiprocessing rather than multithreading.</span>
<span class="comment-copy">Could you please give some reasons why multiprocessing would be more faster in this case ? Is it because of thread context switch overhead ?</span>
<span class="comment-copy">If you are reading the file once why not make life easy for yourself by passing off the chunks to the different threads intead of making them read the file again</span>
<span class="comment-copy">possible duplicate of <a href="http://stackoverflow.com/questions/5128072/how-efficient-is-threading-in-python">How efficient is threading in Python?</a></span>
<span class="comment-copy">The issue is that, due to the global interpreter lock, Python, or at least CPython, cannot actually have multiple threads access memory simultaneously.  See the possible duplicate for details.  Multiprocessing removes that constraint, since different processes don't share the same memory space, so their memory space locks don't interfere.</span>
