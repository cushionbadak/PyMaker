<div class="post-text" itemprop="text">
<p>I'm doing a <code>kernel density estimation</code> of a dataset (a collection of points). </p>
<p>The <code>estimation process</code> is ok, the problem is that, when I'm trying to get the <code>density value</code> for each point, the speed is very slow:</p>
<pre><code>from sklearn.neighbors import KernelDensity
# this speed is ok
kde = KernelDensity(bandwidth=2.0,atol=0.0005,rtol=0.01).fit(sample) 
# this is very slow
kde_result = kde.score_samples(sample) 
</code></pre>
<p>The sample is consist of <code>300,000 (x,y) points</code>. </p>
<p>I'm wondering if it's possible to make it run parallely, so the speed would be quicker? </p>
<p>For example, maybe I can divide the <code>sample</code> in to smaller sets and run the <code>score_samples</code> for each set at the same time? Specifically:</p>
<ol>
<li>I'm not familliar with <code>parallel computing</code> at all. So I'm wondering if it's applicable in my case?</li>
<li>If this can really speed up the process, what should I do? I'm just running the script in <code>ipython notebook</code>, and have no prior expereince in this, is there any good and simple example for my case?</li>
</ol>
<p>I'm reading <a href="http://ipython.org/ipython-doc/dev/parallel/parallel_intro.html" rel="nofollow">http://ipython.org/ipython-doc/dev/parallel/parallel_intro.html</a> now.</p>
<p>UPDATE:</p>
<pre><code>import cProfile
cProfile.run('kde.score_samples(sample)')

        64 function calls in 8.653 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    8.653    8.653 &lt;string&gt;:1(&lt;module&gt;)
        2    0.000    0.000    0.000    0.000 _methods.py:31(_sum)
        2    0.000    0.000    0.000    0.000 base.py:870(isspmatrix)
        1    0.000    0.000    8.653    8.653 kde.py:133(score_samples)
        4    0.000    0.000    0.000    0.000 numeric.py:464(asanyarray)
        2    0.000    0.000    0.000    0.000 shape_base.py:60(atleast_2d)
        2    0.000    0.000    0.000    0.000 validation.py:105(_num_samples)
        2    0.000    0.000    0.000    0.000 validation.py:126(_shape_repr)
        6    0.000    0.000    0.000    0.000 validation.py:153(&lt;genexpr&gt;)
        2    0.000    0.000    0.000    0.000 validation.py:268(check_array)
        2    0.000    0.000    0.000    0.000 validation.py:43(_assert_all_finite)
        6    0.000    0.000    0.000    0.000 {hasattr}
        4    0.000    0.000    0.000    0.000 {isinstance}
       12    0.000    0.000    0.000    0.000 {len}
        2    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}
        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
        2    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}
        1    8.652    8.652    8.652    8.652 {method 'kernel_density' of 'sklearn.neighbors.kd_tree.BinaryTree' objects}
        2    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}
        2    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}
        6    0.000    0.000    0.000    0.000 {numpy.core.multiarray.array}
</code></pre>
</div>
<div class="post-text" itemprop="text">
<p>Here is a simple example of parallelization using <a href="https://docs.python.org/3.4/library/multiprocessing.html?highlight=process" rel="nofollow noreferrer">multiprocessing built-in module</a> :</p>
<pre><code>import numpy as np
import multiprocessing
from sklearn.neighbors import KernelDensity

def parrallel_score_samples(kde, samples, thread_count=int(0.875 * multiprocessing.cpu_count())):
    with multiprocessing.Pool(thread_count) as p:
        return np.concatenate(p.map(kde.score_samples, np.array_split(samples, thread_count)))

kde = KernelDensity(bandwidth=2.0,atol=0.0005,rtol=0.01).fit(sample) 
kde_result = parrallel_score_samples(kde, sample)
</code></pre>
<p>As you can see from code above, <code>multiprocessing.Pool</code> allows you to map a pool of worker processes executing <code>kde.score_samples</code> on a subset of your samples.<br/>
The speedup will be significant if your processor have enough cores.</p>
</div>
<span class="comment-copy">Have you tried using a different kernel? With so many points, the choice of kernel should have only a marginal effect, but 'linear' and 'tophat' maybe faster to calculate.</span>
<span class="comment-copy">@Rob, just tried <code>linear</code>, still very slow on <code>kde_result = kde.score_samples(sample)</code></span>
<span class="comment-copy">You stated that you want parallelism to make it quicker. This is not necessarily true. First, <a href="https://docs.python.org/3/library/profile.html#module-profile" rel="nofollow noreferrer">profile</a> your code to check the hotspots. Once you're sure what should be optimized, see what you <i>can</i> do to optimize it. We both have a gut feeling on what is taking long, but profiling will give you a much better insight (something explicit you can look up and maybe someone else already tried an optimization)</span>
<span class="comment-copy">@FelipeLema this is really what I should looking for, thanks a lot!</span>
<span class="comment-copy">Unfortunatelly the hard work is being done <a href="https://github.com/scikit-learn/scikit-learn/blob/2d559ba20a7102aee140ef7d2f49bf279b347b06/sklearn/neighbors/kd_tree.pyx" rel="nofollow noreferrer">cython</a> at <code>kernel_density</code>. I'm not aware of a parallel implementation of this, so it looks you're going to have to start from scratch. I found <a href="http://spartanideas.msu.edu/2014/06/20/an-introduction-to-parallel-programming-using-pythons-multiprocessing-module/#Kernel-density-estimation-as-benchmarking-function" rel="nofollow noreferrer">this post</a> that might help you get started and edited the question so a dev from scikit can give you a better insight.</span>
