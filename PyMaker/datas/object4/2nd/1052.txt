link:
library/codecs.html#encodings-and-unicode

docs:

Encodings and Unicode¶
Strings are stored internally as sequences of code points in
range –.  (See PEP 393 for
more details about the implementation.)
Once a string object is used outside of CPU and memory, endianness
and how these arrays are stored as bytes become an issue.  As with other
codecs, serialising a string into a sequence of bytes is known as encoding,
and recreating the string from the sequence of bytes is known as decoding.
There are a variety of different text serialisation codecs, which are
collectivity referred to as text encodings.
The simplest text encoding (called  or ) maps
the code points 0–255 to the bytes –, which means that a string
object that contains code points above  can’t be encoded with this
codec. Doing so will raise a  that looks
like the following (although the details of the error message may differ):
.
There’s another group of encodings (the so called charmap encodings) that choose
a different subset of all Unicode code points and how these code points are
mapped to the bytes –. To see how this is done simply open
e.g.  (which is an encoding that is used primarily on
Windows). There’s a string constant with 256 characters that shows you which
character is mapped to which byte value.
All of these encodings can only encode 256 of the 1114112 code points
defined in Unicode. A simple and straightforward way that can store each Unicode
code point, is to store each code point as four consecutive bytes. There are two
possibilities: store the bytes in big endian or in little endian order. These
two encodings are called  and  respectively. Their
disadvantage is that if e.g. you use  on a little endian machine you
will always have to swap bytes on encoding and decoding.  avoids this
problem: bytes will always be in natural endianness. When these bytes are read
by a CPU with a different endianness, then bytes have to be swapped though. To
be able to detect the endianness of a  or  byte sequence,
there’s the so called BOM (“Byte Order Mark”). This is the Unicode character
. This character can be prepended to every  or 
byte sequence. The byte swapped version of this character () is an
illegal character that may not appear in a Unicode text. So when the
first character in an  or  byte sequence
appears to be a  the bytes have to be swapped on decoding.
Unfortunately the character  had a second purpose as
a : a character that has no width and doesn’t allow
a word to be split. It can e.g. be used to give hints to a ligature algorithm.
With Unicode 4.0 using  as a  has been
deprecated (with  () assuming this role). Nevertheless
Unicode software still must be able to handle  in both roles: as a BOM
it’s a device to determine the storage layout of the encoded bytes, and vanishes
once the byte sequence has been decoded into a string; as a  it’s a normal character that will be decoded like any other.
There’s another encoding that is able to encoding the full range of Unicode
characters: UTF-8. UTF-8 is an 8-bit encoding, which means there are no issues
with byte order in UTF-8. Each byte in a UTF-8 byte sequence consists of two
parts: marker bits (the most significant bits) and payload bits. The marker bits
are a sequence of zero to four  bits followed by a  bit. Unicode characters are
encoded like this (with x being payload bits, which when concatenated give the
Unicode character):






Range
Encoding



 … 
0xxxxxxx

 … 
110xxxxx 10xxxxxx

 … 
1110xxxx 10xxxxxx 10xxxxxx

 … 
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx



The least significant bit of the Unicode character is the rightmost x bit.
As UTF-8 is an 8-bit encoding no BOM is required and any  character in
the decoded string (even if it’s the first character) is treated as a .
Without external information it’s impossible to reliably determine which
encoding was used for encoding a string. Each charmap encoding can
decode any random byte sequence. However that’s not possible with UTF-8, as
UTF-8 byte sequences have a structure that doesn’t allow arbitrary byte
sequences. To increase the reliability with which a UTF-8 encoding can be
detected, Microsoft invented a variant of UTF-8 (that Python 2.5 calls
) for its Notepad program: Before any of the Unicode characters
is written to the file, a UTF-8 encoded BOM (which looks like this as a byte
sequence: , , ) is written. As it’s rather improbable
that any charmap encoded file starts with these byte values (which would e.g.
map to


LATIN SMALL LETTER I WITH DIAERESIS
RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
INVERTED QUESTION MARK


in iso-8859-1), this increases the probability that a  encoding can be
correctly guessed from the byte sequence. So here the BOM is not used to be able
to determine the byte order used for generating the byte sequence, but as a
signature that helps in guessing the encoding. On encoding the utf-8-sig codec
will write , ,  as the first three bytes to the file. On
decoding  will skip those three bytes if they appear as the first
three bytes in the file.  In UTF-8, the use of the BOM is discouraged and
should generally be avoided.
