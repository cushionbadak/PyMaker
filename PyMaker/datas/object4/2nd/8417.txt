link:
library/unittest.html

docs:


 — Unit testing framework¶
Source code: Lib/unittest/__init__.py

(If you are already familiar with the basic concepts of testing, you might want
to skip to the list of assert methods.)
The  unit testing framework was originally inspired by JUnit
and has a similar flavor as major unit testing frameworks in other
languages.  It supports test automation, sharing of setup and shutdown code
for tests, aggregation of tests into collections, and independence of the
tests from the reporting framework.
To achieve this,  supports some important concepts in an
object-oriented way:

test fixture
A test fixture represents the preparation needed to perform one or more
tests, and any associate cleanup actions.  This may involve, for example,
creating temporary or proxy databases, directories, or starting a server
process.
test case
A test case is the individual unit of testing.  It checks for a specific
response to a particular set of inputs.   provides a base class,
, which may be used to create new test cases.
test suite
A test suite is a collection of test cases, test suites, or both.  It is
used to aggregate tests that should be executed together.
test runner
A test runner is a component which orchestrates the execution of tests
and provides the outcome to the user.  The runner may use a graphical interface,
a textual interface, or return a special value to indicate the results of
executing the tests.


See also

Module 
Another test-support module with a very different flavor.
Simple Smalltalk Testing: With Patterns
Kent Beck’s original paper on testing frameworks using the pattern shared
by .
Nose and pytest
Third-party unittest frameworks with a lighter-weight syntax for writing
tests.  For example, .
The Python Testing Tools Taxonomy
An extensive list of Python testing tools including functional testing
frameworks and mock object libraries.
Testing in Python Mailing List
A special-interest-group for discussion of testing, and testing tools,
in Python.

The script  in the Python source distribution is
a GUI tool for test discovery and execution.  This is intended largely for ease of use
for those new to unit testing.  For production environments it is
recommended that tests be driven by a continuous integration system such as
Buildbot, Jenkins
or  Hudson.


Basic example¶
The  module provides a rich set of tools for constructing and
running tests.  This section demonstrates that a small subset of the tools
suffice to meet the needs of most users.
Here is a short script to test three string methods:


A testcase is created by subclassing .  The three
individual tests are defined with methods whose names start with the letters
.  This naming convention informs the test runner about which methods
represent tests.
The crux of each test is a call to  to check for an
expected result;  or 
to verify a condition; or  to verify that a
specific exception gets raised.  These methods are used instead of the
 statement so the test runner can accumulate all test results
and produce a report.
The  and  methods allow you
to define instructions that will be executed before and after each test method.
They are covered in more detail in the section Organizing test code.
The final block shows a simple way to run the tests. 
provides a command-line interface to the test script.  When run from the command
line, the above script produces an output that looks like this:


Passing the  option to your test script will instruct 
to enable a higher level of verbosity, and produce the following output:


The above examples show the most commonly used  features which
are sufficient to meet many everyday testing needs.  The remainder of the
documentation explores the full feature set from first principles.


Command-Line Interface¶
The unittest module can be used from the command line to run tests from
modules, classes or even individual test methods:


You can pass in a list with any combination of module names, and fully
qualified class or method names.
Test modules can be specified by file path as well:


This allows you to use the shell filename completion to specify the test module.
The file specified must still be importable as a module. The path is converted
to a module name by removing the ‘.py’ and converting path separators into ‘.’.
If you want to execute a test file that isn’t importable as a module you should
execute the file directly instead.
You can run tests with more detail (higher verbosity) by passing in the -v flag:


When executed without arguments Test Discovery is started:


For a list of all the command-line options:



Changed in version 3.2: In earlier versions it was only possible to run individual test methods and
not modules or classes.


Command-line options¶
unittest supports these command-line options:


¶
The standard output and standard error streams are buffered during the test
run. Output during a passing test is discarded. Output is echoed normally
on test fail or error and is added to the failure messages.



¶
Control-C during the test run waits for the current test to end and then
reports all the results so far. A second Control-C raises the normal
 exception.
See Signal Handling for the functions that provide this functionality.



¶
Stop the test run on the first error or failure.



¶
Only run test methods and classes that match the pattern or substring.
This option may be used multiple times, in which case all test cases that
match of the given patterns are included.
Patterns that contain a wildcard character () are matched against the
test name using ; otherwise simple case-sensitive
substring matching is used.
Patterns are matched against the fully qualified test method name as
imported by the test loader.
For example,  matches ,
, but not .



¶
Show local variables in tracebacks.


New in version 3.2: The command-line options ,  and  were added.


New in version 3.5: The command-line option .


New in version 3.7: The command-line option .

The command line can also be used for test discovery, for running all of the
tests in a project or just a subset.



Test Discovery¶

New in version 3.2.

Unittest supports simple test discovery. In order to be compatible with test
discovery, all of the test files must be modules or
packages (including namespace packages) importable from the top-level directory of
the project (this means that their filenames must be valid identifiers).
Test discovery is implemented in , but can also be
used from the command line. The basic command-line usage is:



Note
As a shortcut,  is the equivalent of
. If you want to pass arguments to test
discovery the  sub-command must be used explicitly.

The  sub-command has the following options:


¶
Verbose output



¶
Directory to start discovery ( default)



¶
Pattern to match test files ( default)



¶
Top level directory of project (defaults to start directory)

The , , and  options can be passed in
as positional arguments in that order. The following two command lines
are equivalent:


As well as being a path it is possible to pass a package name, for example
, as the start directory. The package name you
supply will then be imported and its location on the filesystem will be used
as the start directory.

Caution
Test discovery loads tests by importing them. Once test discovery has found
all the test files from the start directory you specify it turns the paths
into package names to import. For example  will be
imported as .
If you have a package installed globally and attempt test discovery on
a different copy of the package then the import could happen from the
wrong place. If this happens test discovery will warn you and exit.
If you supply the start directory as a package name rather than a
path to a directory then discover assumes that whichever location it
imports from is the location you intended, so you will not get the
warning.

Test modules and packages can customize test loading and discovery by through
the load_tests protocol.

Changed in version 3.4: Test discovery supports namespace packages.



Organizing test code¶
The basic building blocks of unit testing are test cases — single
scenarios that must be set up and checked for correctness.  In ,
test cases are represented by  instances.
To make your own test cases you must write subclasses of
 or use .
The testing code of a  instance should be entirely self
contained, such that it can be run either in isolation or in arbitrary
combination with any number of other test cases.
The simplest  subclass will simply implement a test method
(i.e. a method whose name starts with ) in order to perform specific
testing code:


Note that in order to test something, we use one of the 
methods provided by the  base class.  If the test fails, an
exception will be raised with an explanatory message, and 
will identify the test case as a failure.  Any other exceptions will be
treated as errors.
Tests can be numerous, and their set-up can be repetitive.  Luckily, we
can factor out set-up code by implementing a method called
, which the testing framework will automatically
call for every single test we run:



Note
The order in which the various tests will be run is determined
by sorting the test method names with respect to the built-in
ordering for strings.

If the  method raises an exception while the test is
running, the framework will consider the test to have suffered an error, and
the test method will not be executed.
Similarly, we can provide a  method that tidies up
after the test method has been run:


If  succeeded,  will be
run whether the test method succeeded or not.
Such a working environment for the testing code is called a
test fixture.  A new TestCase instance is created as a unique
test fixture used to execute each individual test method.  Thus
, , and 
will be called once per test.
It is recommended that you use TestCase implementations to group tests together
according to the features they test.   provides a mechanism for
this: the test suite, represented by ’s
 class.  In most cases, calling  will do
the right thing and collect all the module’s test cases for you and execute
them.
However, should you want to customize the building of your test suite,
you can do it yourself:


You can place the definitions of test cases and test suites in the same modules
as the code they are to test (such as ), but there are several
advantages to placing the test code in a separate module, such as
:

The test module can be run standalone from the command line.
The test code can more easily be separated from shipped code.
There is less temptation to change test code to fit the code it tests without
a good reason.
Test code should be modified much less frequently than the code it tests.
Tested code can be refactored more easily.
Tests for modules written in C must be in separate modules anyway, so why not
be consistent?
If the testing strategy changes, there is no need to change the source code.



Re-using old test code¶
Some users will find that they have existing test code that they would like to
run from , without converting every old test function to a
 subclass.
For this reason,  provides a  class.
This subclass of  can be used to wrap an existing test
function.  Set-up and tear-down functions can also be provided.
Given the following test function:


one can create an equivalent test case instance as follows, with optional
set-up and tear-down methods:



Note
Even though  can be used to quickly convert an
existing test base over to a -based system, this approach is
not recommended.  Taking the time to set up proper 
subclasses will make future test refactorings infinitely easier.

In some cases, the existing tests may have been written using the 
module.  If so,  provides a  class that can
automatically build  instances from the existing
-based tests.


Skipping tests and expected failures¶

New in version 3.1.

Unittest supports skipping individual test methods and even whole classes of
tests.  In addition, it supports marking a test as an “expected failure,” a test
that is broken and will fail, but shouldn’t be counted as a failure on a
.
Skipping a test is simply a matter of using the  decorator
or one of its conditional variants.
Basic skipping looks like this:


This is the output of running the example above in verbose mode:


Classes can be skipped just like methods:


 can also skip the test.  This is useful when a resource
that needs to be set up is not available.
Expected failures use the  decorator.


It’s easy to roll your own skipping decorators by making a decorator that calls
 on the test when it wants it to be skipped.  This decorator skips
the test unless the passed object has a certain attribute:


The following decorators implement test skipping and expected failures:


(reason)¶
Unconditionally skip the decorated test.  reason should describe why the
test is being skipped.



(condition, reason)¶
Skip the decorated test if condition is true.



(condition, reason)¶
Skip the decorated test unless condition is true.



¶
Mark the test as an expected failure.  If the test fails it will be
considered a success.  If the test passes, it will be considered a failure.



exception (reason)¶
This exception is raised to skip a test.
Usually you can use  or one of the skipping
decorators instead of raising this directly.

Skipped tests will not have  or  run around them.
Skipped classes will not have  or  run.
Skipped modules will not have  or  run.


Distinguishing test iterations using subtests¶

New in version 3.4.

When there are very small differences among your tests, for
instance some parameters, unittest allows you to distinguish them inside
the body of a test method using the  context manager.
For example, the following test:


will produce the following output:


Without using a subtest, execution would stop after the first failure,
and the error would be less easy to diagnose because the value of 
wouldn’t be displayed:




Classes and functions¶
This section describes in depth the API of .

Test cases¶


class (methodName='runTest')¶
Instances of the  class represent the logical test units
in the  universe.  This class is intended to be used as a base
class, with specific tests being implemented by concrete subclasses.  This class
implements the interface needed by the test runner to allow it to drive the
tests, and methods that the test code can use to check for and report various
kinds of failure.
Each instance of  will run a single base method: the method
named methodName.
In most uses of , you will neither change
the methodName nor reimplement the default  method.

Changed in version 3.2:  can be instantiated successfully without providing a
methodName. This makes it easier to experiment with 
from the interactive interpreter.

 instances provide three groups of methods: one group used
to run the test, another used by the test implementation to check conditions
and report failures, and some inquiry methods allowing information about the
test itself to be gathered.
Methods in the first group (running the test) are:


()¶
Method called to prepare the test fixture.  This is called immediately
before calling the test method; other than  or ,
any exception raised by this method will be considered an error rather than
a test failure. The default implementation does nothing.



()¶
Method called immediately after the test method has been called and the
result recorded.  This is called even if the test method raised an
exception, so the implementation in subclasses may need to be particularly
careful about checking internal state.  Any exception, other than
 or , raised by this method will be
considered an additional error rather than a test failure (thus increasing
the total number of reported errors). This method will only be called if
the  succeeds, regardless of the outcome of the test method.
The default implementation does nothing.



()¶
A class method called before tests in an individual class are run.
 is called with the class as the only argument
and must be decorated as a :


See Class and Module Fixtures for more details.

New in version 3.2.




()¶
A class method called after tests in an individual class have run.
 is called with the class as the only argument
and must be decorated as a :


See Class and Module Fixtures for more details.

New in version 3.2.




(result=None)¶
Run the test, collecting the result into the  object
passed as result.  If result is omitted or , a temporary
result object is created (by calling the 
method) and used. The result object is returned to ’s
caller.
The same effect may be had by simply calling the 
instance.

Changed in version 3.3: Previous versions of  did not return the result. Neither did
calling an instance.




(reason)¶
Calling this during a test method or  skips the current
test.  See Skipping tests and expected failures for more information.

New in version 3.1.




(msg=None, **params)¶
Return a context manager which executes the enclosed code block as a
subtest.  msg and params are optional, arbitrary values which are
displayed whenever a subtest fails, allowing you to identify them
clearly.
A test case can contain any number of subtest declarations, and
they can be arbitrarily nested.
See Distinguishing test iterations using subtests for more information.

New in version 3.4.




()¶
Run the test without collecting the result.  This allows exceptions raised
by the test to be propagated to the caller, and can be used to support
running tests under a debugger.

The  class provides several assert methods to check for and
report failures.  The following table lists the most commonly used methods
(see the tables below for more assert methods):







Method
Checks that
New in





 



 



 



 



3.1



3.1



3.1



3.1



3.1



3.1



3.2



3.2



All the assert methods accept a msg argument that, if specified, is used
as the error message on failure (see also ).
Note that the msg keyword argument can be passed to ,
, , 
only when they are used as a context manager.


(first, second, msg=None)¶
Test that first and second are equal.  If the values do not
compare equal, the test will fail.
In addition, if first and second are the exact same type and one of
list, tuple, dict, set, frozenset or str or any type that a subclass
registers with  the type-specific equality
function will be called in order to generate a more useful default
error message (see also the list of type-specific methods).

Changed in version 3.1: Added the automatic calling of type-specific equality function.


Changed in version 3.2:  added as the default type equality
function for comparing strings.




(first, second, msg=None)¶
Test that first and second are not equal.  If the values do
compare equal, the test will fail.



(expr, msg=None)¶

(expr, msg=None)¶
Test that expr is true (or false).
Note that this is equivalent to  and not to  (use  for the latter).  This method
should also be avoided when more specific methods are available (e.g.
 instead of ), because they
provide a better error message in case of failure.



(first, second, msg=None)¶

(first, second, msg=None)¶
Test that first and second evaluate (or don’t evaluate) to the
same object.

New in version 3.1.




(expr, msg=None)¶

(expr, msg=None)¶
Test that expr is (or is not) .

New in version 3.1.




(first, second, msg=None)¶

(first, second, msg=None)¶
Test that first is (or is not) in second.

New in version 3.1.




(obj, cls, msg=None)¶

(obj, cls, msg=None)¶
Test that obj is (or is not) an instance of cls (which can be a
class or a tuple of classes, as supported by ).
To check for the exact type, use .

New in version 3.2.


It is also possible to check the production of exceptions, warnings, and
log messages using the following methods:







Method
Checks that
New in




 raises exc
 


 raises exc
and the message matches regex r
3.1


 raises warn
3.2


 raises warn
and the message matches regex r
3.2


The  block logs on logger
with minimum level
3.4





(exception, callable, *args, **kwds)¶

(exception, *, msg=None)
Test that an exception is raised when callable is called with any
positional or keyword arguments that are also passed to
.  The test passes if exception is raised, is an
error if another exception is raised, or fails if no exception is raised.
To catch any of a group of exceptions, a tuple containing the exception
classes may be passed as exception.
If only the exception and possibly the msg arguments are given,
return a context manager so that the code under test can be written
inline rather than as a function:


When used as a context manager,  accepts the
additional keyword argument msg.
The context manager will store the caught exception object in its
 attribute.  This can be useful if the intention
is to perform additional checks on the exception raised:



Changed in version 3.1: Added the ability to use  as a context manager.


Changed in version 3.2: Added the  attribute.


Changed in version 3.3: Added the msg keyword argument when used as a context manager.




(exception, regex, callable, *args, **kwds)¶

(exception, regex, *, msg=None)
Like  but also tests that regex matches
on the string representation of the raised exception.  regex may be
a regular expression object or a string containing a regular expression
suitable for use by .  Examples:


or:



New in version 3.1: Added under the name .


Changed in version 3.2: Renamed to .


Changed in version 3.3: Added the msg keyword argument when used as a context manager.




(warning, callable, *args, **kwds)¶

(warning, *, msg=None)
Test that a warning is triggered when callable is called with any
positional or keyword arguments that are also passed to
.  The test passes if warning is triggered and
fails if it isn’t.  Any exception is an error.
To catch any of a group of warnings, a tuple containing the warning
classes may be passed as warnings.
If only the warning and possibly the msg arguments are given,
return a context manager so that the code under test can be written
inline rather than as a function:


When used as a context manager,  accepts the
additional keyword argument msg.
The context manager will store the caught warning object in its
 attribute, and the source line which triggered the
warnings in the  and  attributes.
This can be useful if the intention is to perform additional checks
on the warning caught:


This method works regardless of the warning filters in place when it
is called.

New in version 3.2.


Changed in version 3.3: Added the msg keyword argument when used as a context manager.




(warning, regex, callable, *args, **kwds)¶

(warning, regex, *, msg=None)
Like  but also tests that regex matches on the
message of the triggered warning.  regex may be a regular expression
object or a string containing a regular expression suitable for use
by .  Example:


or:



New in version 3.2.


Changed in version 3.3: Added the msg keyword argument when used as a context manager.




(logger=None, level=None)¶
A context manager to test that at least one message is logged on
the logger or one of its children, with at least the given
level.
If given, logger should be a  object or a
 giving the name of a logger.  The default is the root
logger, which will catch all messages.
If given, level should be either a numeric logging level or
its string equivalent (for example either  or
).  The default is .
The test passes if at least one message emitted inside the 
block matches the logger and level conditions, otherwise it fails.
The object returned by the context manager is a recording helper
which keeps tracks of the matching log messages.  It has two
attributes:


¶
A list of  objects of the matching
log messages.



¶
A list of  objects with the formatted output of
matching messages.

Example:



New in version 3.4.


There are also other methods used to perform more specific checks, such as:







Method
Checks that
New in





 



 



3.1



3.1



3.1



3.1



3.1



3.2


a and b have the same
elements in the same number,
regardless of their order.
3.2





(first, second, places=7, msg=None, delta=None)¶

(first, second, places=7, msg=None, delta=None)¶
Test that first and second are approximately (or not approximately)
equal by computing the difference, rounding to the given number of
decimal places (default 7), and comparing to zero.  Note that these
methods round the values to the given number of decimal places (i.e.
like the  function) and not significant digits.
If delta is supplied instead of places then the difference
between first and second must be less or equal to (or greater than) delta.
Supplying both delta and places raises a .

Changed in version 3.2:  automatically considers almost equal objects
that compare equal.   automatically fails
if the objects compare equal.  Added the delta keyword argument.




(first, second, msg=None)¶

(first, second, msg=None)¶

(first, second, msg=None)¶

(first, second, msg=None)¶
Test that first is respectively >, >=, < or <= than second depending
on the method name.  If not, the test will fail:



New in version 3.1.




(text, regex, msg=None)¶

(text, regex, msg=None)¶
Test that a regex search matches (or does not match) text.  In case
of failure, the error message will include the pattern and the text (or
the pattern and the part of text that unexpectedly matched).  regex
may be a regular expression object or a string containing a regular
expression suitable for use by .

New in version 3.1: Added under the name .


Changed in version 3.2: The method  has been renamed to
.


New in version 3.2: .


New in version 3.5: The name  is a deprecated alias
for .




(first, second, msg=None)¶
Test that sequence first contains the same elements as second,
regardless of their order. When they don’t, an error message listing the
differences between the sequences will be generated.
Duplicate elements are not ignored when comparing first and
second. It verifies whether each element has the same count in both
sequences. Equivalent to:

but works with sequences of unhashable objects as well.

New in version 3.2.


The  method dispatches the equality check for objects of
the same type to different type-specific methods.  These methods are already
implemented for most of the built-in types, but it’s also possible to
register new methods using :


(typeobj, function)¶
Registers a type-specific method called by  to check
if two objects of exactly the same typeobj (not subclasses) compare
equal.  function must take two positional arguments and a third msg=None
keyword argument just as  does.  It must raise
 when inequality
between the first two parameters is detected – possibly providing useful
information and explaining the inequalities in details in the error
message.

New in version 3.1.


The list of type-specific methods automatically used by
 are summarized in the following table.  Note
that it’s usually not necessary to invoke these methods directly.







Method
Used to compare
New in




strings
3.1


sequences
3.1


lists
3.1


tuples
3.1


sets or frozensets
3.1


dicts
3.1





(first, second, msg=None)¶
Test that the multiline string first is equal to the string second.
When not equal a diff of the two strings highlighting the differences
will be included in the error message. This method is used by default
when comparing strings with .

New in version 3.1.




(first, second, msg=None, seq_type=None)¶
Tests that two sequences are equal.  If a seq_type is supplied, both
first and second must be instances of seq_type or a failure will
be raised.  If the sequences are different an error message is
constructed that shows the difference between the two.
This method is not called directly by , but
it’s used to implement  and
.

New in version 3.1.




(first, second, msg=None)¶

(first, second, msg=None)¶
Tests that two lists or tuples are equal.  If not, an error message is
constructed that shows only the differences between the two.  An error
is also raised if either of the parameters are of the wrong type.
These methods are used by default when comparing lists or tuples with
.

New in version 3.1.




(first, second, msg=None)¶
Tests that two sets are equal.  If not, an error message is constructed
that lists the differences between the sets.  This method is used by
default when comparing sets or frozensets with .
Fails if either of first or second does not have a 
method.

New in version 3.1.




(first, second, msg=None)¶
Test that two dictionaries are equal.  If not, an error message is
constructed that shows the differences in the dictionaries. This
method will be used by default to compare dictionaries in
calls to .

New in version 3.1.


Finally the  provides the following methods and attributes:


(msg=None)¶
Signals a test failure unconditionally, with msg or  for
the error message.



¶
This class attribute gives the exception raised by the test method.  If a
test framework needs to use a specialized exception, possibly to carry
additional information, it must subclass this exception in order to “play
fair” with the framework.  The initial value of this attribute is
.



¶
This class attribute determines what happens when a custom failure message
is passed as the msg argument to an assertXYY call that fails.
 is the default value. In this case, the custom message is appended
to the end of the standard failure message.
When set to , the custom message replaces the standard message.
The class setting can be overridden in individual test methods by assigning
an instance attribute, self.longMessage, to  or  before
calling the assert methods.
The class setting gets reset before each test call.

New in version 3.1.




¶
This attribute controls the maximum length of diffs output by assert
methods that report diffs on failure. It defaults to 80*8 characters.
Assert methods affected by this attribute are
 (including all the sequence comparison
methods that delegate to it),  and
.
Setting  to  means that there is no maximum length of
diffs.

New in version 3.2.


Testing frameworks can use the following methods to collect information on
the test:


()¶
Return the number of tests represented by this test object.  For
 instances, this will always be .



()¶
Return an instance of the test result class that should be used for this
test case class (if no other result instance is provided to the
 method).
For  instances, this will always be an instance of
; subclasses of  should override this
as necessary.



()¶
Return a string identifying the specific test case.  This is usually the
full name of the test method, including the module and class name.



()¶
Returns a description of the test, or  if no description
has been provided.  The default implementation of this method
returns the first line of the test method’s docstring, if available,
or .

Changed in version 3.1: In 3.1 this was changed to add the test name to the short description
even in the presence of a docstring.  This caused compatibility issues
with unittest extensions and adding the test name was moved to the
 in Python 3.2.




(function, *args, **kwargs)¶
Add a function to be called after  to cleanup resources
used during the test. Functions will be called in reverse order to the
order they are added (LIFO).  They
are called with any arguments and keyword arguments passed into
 when they are added.
If  fails, meaning that  is not called,
then any cleanup functions added will still be called.

New in version 3.1.




()¶
This method is called unconditionally after , or
after  if  raises an exception.
It is responsible for calling all the cleanup functions added by
. If you need cleanup functions to be called
prior to  then you can call 
yourself.
 pops methods off the stack of cleanup
functions one at a time, so it can be called at any time.

New in version 3.1.





class (testFunc, setUp=None, tearDown=None, description=None)¶
This class implements the portion of the  interface which
allows the test runner to drive the test, but does not provide the methods
which test code can use to check and report errors.  This is used to create
test cases using legacy test code, allowing it to be integrated into a
-based test framework.


Deprecated aliases¶
For historical reasons, some of the  methods had one or more
aliases that are now deprecated.  The following table lists the correct names
along with their deprecated aliases:








Method Name
Deprecated alias
Deprecated alias




failUnlessEqual
assertEquals


failIfEqual
assertNotEquals


failUnless
assert_


failIf
 


failUnlessRaises
 


failUnlessAlmostEqual
assertAlmostEquals


failIfAlmostEqual
assertNotAlmostEquals


 
assertRegexpMatches


 
assertNotRegexpMatches


 
assertRaisesRegexp




Deprecated since version 3.1: The fail* aliases listed in the second column have been deprecated.


Deprecated since version 3.2: The assert* aliases listed in the third column have been deprecated.


Deprecated since version 3.2:  and  have been renamed to
 and .


Deprecated since version 3.5: The  name is deprecated in favor of .





Grouping tests¶


class (tests=())¶
This class represents an aggregation of individual test cases and test suites.
The class presents the interface needed by the test runner to allow it to be run
as any other test case.  Running a  instance is the same as
iterating over the suite, running each test individually.
If tests is given, it must be an iterable of individual test cases or other
test suites that will be used to build the suite initially. Additional methods
are provided to add test cases and suites to the collection later on.
 objects behave much like  objects, except
they do not actually implement a test.  Instead, they are used to aggregate
tests into groups of tests that should be run together. Some additional
methods are available to add tests to  instances:


(test)¶
Add a  or  to the suite.



(tests)¶
Add all the tests from an iterable of  and 
instances to this test suite.
This is equivalent to iterating over tests, calling  for
each element.

 shares the following methods with :


(result)¶
Run the tests associated with this suite, collecting the result into the
test result object passed as result.  Note that unlike
,  requires the result object to
be passed in.



()¶
Run the tests associated with this suite without collecting the
result. This allows exceptions raised by the test to be propagated to the
caller and can be used to support running tests under a debugger.



()¶
Return the number of tests represented by this test object, including all
individual tests and sub-suites.



()¶
Tests grouped by a  are always accessed by iteration.
Subclasses can lazily provide tests by overriding . Note
that this method may be called several times on a single suite (for
example when counting tests or comparing for equality) so the tests
returned by repeated iterations before  must be the
same for each call iteration. After , callers should
not rely on the tests returned by this method unless the caller uses a
subclass that overrides  to preserve
test references.

Changed in version 3.2: In earlier versions the  accessed tests directly rather
than through iteration, so overriding  wasn’t sufficient
for providing tests.


Changed in version 3.4: In earlier versions the  held references to each
 after . Subclasses can restore
that behavior by overriding .


In the typical usage of a  object, the  method
is invoked by a  rather than by the end-user test harness.



Loading and running tests¶


class ¶
The  class is used to create test suites from classes and
modules.  Normally, there is no need to create an instance of this class; the
 module provides an instance that can be shared as
.  Using a subclass or instance, however,
allows customization of some configurable properties.
 objects have the following attributes:


¶
A list of the non-fatal errors encountered while loading tests. Not reset
by the loader at any point. Fatal errors are signalled by the relevant
a method raising an exception to the caller. Non-fatal errors are also
indicated by a synthetic test that will raise the original error when
run.

New in version 3.5.


 objects have the following methods:


(testCaseClass)¶
Return a suite of all test cases contained in the -derived
.
A test case instance is created for each method named by
. By default these are the method names
beginning with . If  returns no
methods, but the  method is implemented, a single test
case is created for that method instead.



(module, pattern=None)¶
Return a suite of all test cases contained in the given module. This
method searches module for classes derived from  and
creates an instance of the class for each test method defined for the
class.

Note
While using a hierarchy of -derived classes can be
convenient in sharing fixtures and helper functions, defining test
methods on base classes that are not intended to be instantiated
directly does not play well with this method.  Doing so, however, can
be useful when the fixtures are different and defined in subclasses.

If a module provides a  function it will be called to
load the tests. This allows modules to customize test loading.
This is the load_tests protocol.  The pattern argument is passed as
the third argument to .

Changed in version 3.2: Support for  added.


Changed in version 3.5: The undocumented and unofficial use_load_tests default argument is
deprecated and ignored, although it is still accepted for backward
compatibility.  The method also now accepts a keyword-only argument
pattern which is passed to  as the third argument.




(name, module=None)¶
Return a suite of all test cases given a string specifier.
The specifier name is a “dotted name” that may resolve either to a
module, a test case class, a test method within a test case class, a
 instance, or a callable object which returns a
 or  instance.  These checks are
applied in the order listed here; that is, a method on a possible test
case class will be picked up as “a test method within a test case class”,
rather than “a callable object”.
For example, if you have a module  containing a
-derived class  with three test
methods (, , and ), the
specifier  would cause this method to
return a suite which will run all three test methods. Using the specifier
 would cause it to return a test
suite which will run only the  test method. The specifier
can refer to modules and packages which have not been imported; they will
be imported as a side-effect.
The method optionally resolves name relative to the given module.

Changed in version 3.5: If an  or  occurs while traversing
name then a synthetic test that raises that error when run will be
returned. These errors are included in the errors accumulated by
self.errors.




(names, module=None)¶
Similar to , but takes a sequence of names rather
than a single name.  The return value is a test suite which supports all
the tests defined for each name.



(testCaseClass)¶
Return a sorted sequence of method names found within testCaseClass;
this should be a subclass of .



(start_dir, pattern='test*.py', top_level_dir=None)¶
Find all the test modules by recursing into subdirectories from the
specified start directory, and return a TestSuite object containing them.
Only test files that match pattern will be loaded. (Using shell style
pattern matching.) Only module names that are importable (i.e. are valid
Python identifiers) will be loaded.
All test modules must be importable from the top level of the project. If
the start directory is not the top level directory then the top level
directory must be specified separately.
If importing a module fails, for example due to a syntax error, then
this will be recorded as a single error and discovery will continue.  If
the import failure is due to  being raised, it will be
recorded as a skip instead of an error.
If a package (a directory containing a file named ) is
found, the package will be checked for a  function. If this
exists then it will be called
. Test discovery takes care
to ensure that a package is only checked for tests once during an
invocation, even if the load_tests function itself calls
.
If  exists then discovery does not recurse into the
package,  is responsible for loading all tests in the
package.
The pattern is deliberately not stored as a loader attribute so that
packages can continue discovery themselves. top_level_dir is stored so
 does not need to pass this argument in to
.
start_dir can be a dotted module name as well as a directory.

New in version 3.2.


Changed in version 3.4: Modules that raise  on import are recorded as skips,
  not errors.
Discovery works for namespace packages.
Paths are sorted before being imported so that execution order is
  the same even if the underlying file system’s ordering is not
  dependent on file name.


Changed in version 3.5: Found packages are now checked for  regardless of
whether their path matches pattern, because it is impossible for
a package name to match the default pattern.


The following attributes of a  can be configured either by
subclassing or assignment on an instance:


¶
String giving the prefix of method names which will be interpreted as test
methods.  The default value is .
This affects  and all the 
methods.



¶
Function to be used to compare method names when sorting them in
 and all the  methods.



¶
Callable object that constructs a test suite from a list of tests. No
methods on the resulting object are needed.  The default value is the
 class.
This affects all the  methods.



¶
List of Unix shell-style wildcard test name patterns that test methods
have to match to be included in test suites (see  option).
If this attribute is not  (the default), all test methods to be
included in test suites must match one of the patterns in this list.
Note that matches are always performed using ,
so unlike patterns passed to the  option, simple substring patterns
will have to be converted using  wildcards.
This affects all the  methods.

New in version 3.7.





class ¶
This class is used to compile information about which tests have succeeded
and which have failed.
A  object stores the results of a set of tests.  The
 and  classes ensure that results are
properly recorded; test authors do not need to worry about recording the
outcome of tests.
Testing frameworks built on top of  may want access to the
 object generated by running a set of tests for reporting
purposes; a  instance is returned by the
 method for this purpose.
 instances have the following attributes that will be of
interest when inspecting the results of running a set of tests:


¶
A list containing 2-tuples of  instances and strings
holding formatted tracebacks. Each tuple represents a test which raised an
unexpected exception.



¶
A list containing 2-tuples of  instances and strings
holding formatted tracebacks. Each tuple represents a test where a failure
was explicitly signalled using the  methods.



¶
A list containing 2-tuples of  instances and strings
holding the reason for skipping the test.

New in version 3.1.




¶
A list containing 2-tuples of  instances and strings
holding formatted tracebacks.  Each tuple represents an expected failure
of the test case.



¶
A list containing  instances that were marked as expected
failures, but succeeded.



¶
Set to  when the execution of tests should stop by .



¶
The total number of tests run so far.



¶
If set to true,  and  will be buffered in between
 and  being called. Collected output will
only be echoed onto the real  and  if the test
fails or errors. Any output is also attached to the failure / error message.

New in version 3.2.




¶
If set to true  will be called on the first failure or error,
halting the test run.

New in version 3.2.




¶
If set to true then local variables will be shown in tracebacks.

New in version 3.5.




()¶
Return  if all tests run so far have passed, otherwise returns
.

Changed in version 3.4: Returns  if there were any 
from tests marked with the  decorator.




()¶
This method can be called to signal that the set of tests being run should
be aborted by setting the  attribute to .
 objects should respect this flag and return without
running any additional tests.
For example, this feature is used by the  class to
stop the test framework when the user signals an interrupt from the
keyboard.  Interactive tools which provide 
implementations can use this in a similar manner.

The following methods of the  class are used to maintain
the internal data structures, and may be extended in subclasses to support
additional reporting requirements.  This is particularly useful in building
tools which support interactive reporting while tests are being run.


(test)¶
Called when the test case test is about to be run.



(test)¶
Called after the test case test has been executed, regardless of the
outcome.



()¶
Called once before any tests are executed.

New in version 3.1.




()¶
Called once after all tests are executed.

New in version 3.1.




(test, err)¶
Called when the test case test raises an unexpected exception. err is a
tuple of the form returned by : .
The default implementation appends a tuple  to
the instance’s  attribute, where formatted_err is a
formatted traceback derived from err.



(test, err)¶
Called when the test case test signals a failure. err is a tuple of
the form returned by : .
The default implementation appends a tuple  to
the instance’s  attribute, where formatted_err is a
formatted traceback derived from err.



(test)¶
Called when the test case test succeeds.
The default implementation does nothing.



(test, reason)¶
Called when the test case test is skipped.  reason is the reason the
test gave for skipping.
The default implementation appends a tuple  to the
instance’s  attribute.



(test, err)¶
Called when the test case test fails, but was marked with the
 decorator.
The default implementation appends a tuple  to
the instance’s  attribute, where formatted_err
is a formatted traceback derived from err.



(test)¶
Called when the test case test was marked with the
 decorator, but succeeded.
The default implementation appends the test to the instance’s
 attribute.



(test, subtest, outcome)¶
Called when a subtest finishes.  test is the test case
corresponding to the test method.  subtest is a custom
 instance describing the subtest.
If outcome is , the subtest succeeded.  Otherwise,
it failed with an exception where outcome is a tuple of the form
returned by : .
The default implementation does nothing when the outcome is a
success, and records subtest failures as normal failures.

New in version 3.4.





class (stream, descriptions, verbosity)¶
A concrete implementation of  used by the
.

New in version 3.2: This class was previously named . The old name still
exists as an alias but is deprecated.




¶
Instance of the  class intended to be shared.  If no
customization of the  is needed, this instance can be used
instead of repeatedly creating new instances.



class (stream=None, descriptions=True, verbosity=1, failfast=False, buffer=False, resultclass=None, warnings=None, *, tb_locals=False)¶
A basic test runner implementation that outputs results to a stream. If stream
is , the default,  is used as the output stream. This class
has a few configurable parameters, but is essentially very simple.  Graphical
applications which run test suites should provide alternate implementations. Such
implementations should accept  as the interface to construct runners
changes when features are added to unittest.
By default this runner shows ,
,  and
 even if they are ignored by default. Deprecation warnings caused by deprecated unittest
methods are also special-cased and, when the warning
filters are  or , they will appear only once
per-module, in order to avoid too many warning messages.  This behavior can
be overridden using Python’s  or  options
(see Warning control) and leaving
warnings to .

Changed in version 3.2: Added the  argument.


Changed in version 3.2: The default stream is set to  at instantiation time rather
than import time.


Changed in version 3.5: Added the tb_locals parameter.



()¶
This method returns the instance of  used by .
It is not intended to be called directly, but can be overridden in
subclasses to provide a custom .
 instantiates the class or callable passed in the
 constructor as the  argument. It
defaults to  if no  is provided.
The result class is instantiated with the following arguments:





(test)¶
This method is the main public interface to the . This
method takes a  or  instance. A
 is created by calling
 and the test(s) are run and the
results printed to stdout.




(module='__main__', defaultTest=None, argv=None, testRunner=None, testLoader=unittest.defaultTestLoader, exit=True, verbosity=1, failfast=None, catchbreak=None, buffer=None, warnings=None)¶
A command-line program that loads a set of tests from module and runs them;
this is primarily for making test modules conveniently executable.
The simplest use for this function is to include the following line at the
end of a test script:


You can run tests with more detailed information by passing in the verbosity
argument:


The defaultTest argument is either the name of a single test or an
iterable of test names to run if no test names are specified via argv.  If
not specified or  and no test names are provided via argv, all
tests found in module are run.
The argv argument can be a list of options passed to the program, with the
first element being the program name.  If not specified or ,
the values of  are used.
The testRunner argument can either be a test runner class or an already
created instance of it. By default  calls  with
an exit code indicating success or failure of the tests run.
The testLoader argument has to be a  instance,
and defaults to .
 supports being used from the interactive interpreter by passing in the
argument . This displays the result on standard output without
calling :


The failfast, catchbreak and buffer parameters have the same
effect as the same-name command-line options.
The warnings argument specifies the warning filter
that should be used while running the tests.  If it’s not specified, it will
remain  if a  option is passed to python
(see Warning control),
otherwise it will be set to .
Calling  actually returns an instance of the  class.
This stores the result of the tests run as the  attribute.

Changed in version 3.1: The exit parameter was added.


Changed in version 3.2: The verbosity, failfast, catchbreak, buffer
and warnings parameters were added.


Changed in version 3.4: The defaultTest parameter was changed to also accept an iterable of
test names.



load_tests Protocol¶

New in version 3.2.

Modules or packages can customize how tests are loaded from them during normal
test runs or test discovery by implementing a function called .
If a test module defines  it will be called by
 with the following arguments:


where pattern is passed straight through from .  It
defaults to .
It should return a .
loader is the instance of  doing the loading.
standard_tests are the tests that would be loaded by default from the
module. It is common for test modules to only want to add or remove tests
from the standard set of tests.
The third argument is used when loading packages as part of test discovery.
A typical  function that loads tests from a specific set of
 classes may look like:


If discovery is started in a directory containing a package, either from the
command line or by calling , then the package
 will be checked for .  If that function does
not exist, discovery will recurse into the package as though it were just
another directory.  Otherwise, discovery of the package’s tests will be left up
to  which is called with the following arguments:


This should return a  representing all the tests
from the package. ( will only contain tests
collected from .)
Because the pattern is passed into  the package is free to
continue (and potentially modify) test discovery. A ‘do nothing’
 function for a test package would look like:



Changed in version 3.5: Discovery no longer checks package names for matching pattern due to the
impossibility of package names matching the default pattern.





Class and Module Fixtures¶
Class and module level fixtures are implemented in . When
the test suite encounters a test from a new class then 
from the previous class (if there is one) is called, followed by
 from the new class.
Similarly if a test is from a different module from the previous test then
 from the previous module is run, followed by
 from the new module.
After all the tests have run the final  and
 are run.
Note that shared fixtures do not play well with [potential] features like test
parallelization and they break test isolation. They should be used with care.
The default ordering of tests created by the unittest test loaders is to group
all tests from the same modules and classes together. This will lead to
 /  (etc) being called exactly once per class and
module. If you randomize the order, so that tests from different modules and
classes are adjacent to each other, then these shared fixture functions may be
called multiple times in a single test run.
Shared fixtures are not intended to work with suites with non-standard
ordering. A  still exists for frameworks that don’t want to
support shared fixtures.
If there are any exceptions raised during one of the shared fixture functions
the test is reported as an error. Because there is no corresponding test
instance an  object (that has the same interface as a
) is created to represent the error. If you are just using
the standard unittest test runner then this detail doesn’t matter, but if you
are a framework author it may be relevant.

setUpClass and tearDownClass¶
These must be implemented as class methods:


If you want the  and  on base classes called
then you must call up to them yourself. The implementations in
 are empty.
If an exception is raised during a  then the tests in the class
are not run and the  is not run. Skipped classes will not
have  or  run. If the exception is a
 exception then the class will be reported as having been skipped
instead of as an error.


setUpModule and tearDownModule¶
These should be implemented as functions:


If an exception is raised in a  then none of the tests in the
module will be run and the  will not be run. If the exception is a
 exception then the module will be reported as having been skipped
instead of as an error.



Signal Handling¶

New in version 3.2.

The  command-line option to unittest,
along with the  parameter to , provide
more friendly handling of control-C during a test run. With catch break
behavior enabled control-C will allow the currently running test to complete,
and the test run will then end and report all the results so far. A second
control-c will raise a  in the usual way.
The control-c handling signal handler attempts to remain compatible with code or
tests that install their own  handler. If the 
handler is called but isn’t the installed  handler,
i.e. it has been replaced by the system under test and delegated to, then it
calls the default handler. This will normally be the expected behavior by code
that replaces an installed handler and delegates to it. For individual tests
that need  control-c handling disabled the 
decorator can be used.
There are a few utility functions for framework authors to enable control-c
handling functionality within test frameworks.


()¶
Install the control-c handler. When a  is received
(usually in response to the user pressing control-c) all registered results
have  called.



(result)¶
Register a  object for control-c handling. Registering a
result stores a weak reference to it, so it doesn’t prevent the result from
being garbage collected.
Registering a  object has no side-effects if control-c
handling is not enabled, so test frameworks can unconditionally register
all results they create independently of whether or not handling is enabled.



(result)¶
Remove a registered result. Once a result has been removed then
 will no longer be called on that result object in
response to a control-c.



(function=None)¶
When called without arguments this function removes the control-c handler
if it has been installed. This function can also be used as a test decorator
to temporarily remove the handler while the test is being executed:





