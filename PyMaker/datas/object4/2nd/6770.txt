link:
library/urllib.robotparser.html

docs:


 —  Parser for robots.txt¶
Source code: Lib/urllib/robotparser.py

This module provides a single class, , which answers
questions about whether or not a particular user agent can fetch a URL on the
Web site that published the  file.  For more details on the
structure of  files, see http://www.robotstxt.org/orig.html.


class (url='')¶
This class provides methods to read, parse and answer questions about the
 file at url.


(url)¶
Sets the URL referring to a  file.



()¶
Reads the  URL and feeds it to the parser.



(lines)¶
Parses the lines argument.



(useragent, url)¶
Returns  if the useragent is allowed to fetch the url
according to the rules contained in the parsed 
file.



()¶
Returns the time the  file was last fetched.  This is
useful for long-running web spiders that need to check for new
 files periodically.



()¶
Sets the time the  file was last fetched to the current
time.



(useragent)¶
Returns the value of the  parameter from 
for the useragent in question.  If there is no such parameter or it
doesn’t apply to the useragent specified or the  entry
for this parameter has invalid syntax, return .

New in version 3.6.




(useragent)¶
Returns the contents of the  parameter from
 as a named tuple .
If there is no such parameter or it doesn’t apply to the useragent
specified or the  entry for this parameter has invalid
syntax, return .

New in version 3.6.



The following example demonstrates basic use of the 
class:



