link:
library/re.html#writing-a-tokenizer

docs:

Writing a Tokenizer¶
A tokenizer or scanner
analyzes a string to categorize groups of characters.  This is a useful first
step in writing a compiler or interpreter.
The text categories are specified with regular expressions.  The technique is
to combine those into a single master regular expression and to loop over
successive matches:


The tokenizer produces the following output:





[Frie09]Friedl, Jeffrey. Mastering Regular Expressions. 3rd ed., O’Reilly
Media, 2009. The third edition of the book no longer covers Python at all,
but the first edition covered writing good regular expression patterns in
great detail.


