link:
library/doctest.html#soapbox

docs:

Soapbox¶
As mentioned in the introduction, doctest has grown to have three primary
uses:

Checking examples in docstrings.
Regression testing.
Executable documentation / literate testing.

These uses have different requirements, and it is important to distinguish them.
In particular, filling your docstrings with obscure test cases makes for bad
documentation.
When writing a docstring, choose docstring examples with care. There’s an art to
this that needs to be learned—it may not be natural at first.  Examples should
add genuine value to the documentation.  A good example can often be worth many
words. If done with care, the examples will be invaluable for your users, and
will pay back the time it takes to collect them many times over as the years go
by and things change.  I’m still amazed at how often one of my doctest
examples stops working after a “harmless” change.
Doctest also makes an excellent tool for regression testing, especially if you
don’t skimp on explanatory text.  By interleaving prose and examples, it becomes
much easier to keep track of what’s actually being tested, and why.  When a test
fails, good prose can make it much easier to figure out what the problem is, and
how it should be fixed.  It’s true that you could write extensive comments in
code-based testing, but few programmers do. Many have found that using doctest
approaches instead leads to much clearer tests.  Perhaps this is simply because
doctest makes writing prose a little easier than writing code, while writing
comments in code is a little harder.  I think it goes deeper than just that:
the natural attitude when writing a doctest-based test is that you want to
explain the fine points of your software, and illustrate them with examples.
This in turn naturally leads to test files that start with the simplest
features, and logically progress to complications and edge cases.  A coherent
narrative is the result, instead of a collection of isolated functions that test
isolated bits of functionality seemingly at random.  It’s a different attitude,
and produces different results, blurring the distinction between testing and
explaining.
Regression testing is best confined to dedicated objects or files.  There are
several options for organizing tests:

Write text files containing test cases as interactive examples, and test the
files using testfile() or DocFileSuite().  This is recommended,
although is easiest to do for new projects, designed from the start to use
doctest.
Define functions named _regrtest_topic that consist of single docstrings,
containing test cases for the named topics.  These functions can be included in
the same file as the module, or separated out into a separate test file.
Define a __test__ dictionary mapping from regression test topics to
docstrings containing test cases.

When you have placed your tests in a module, the module can itself be the test
runner.  When a test fails, you can arrange for your test runner to re-run only
the failing doctest while you debug the problem.  Here is a minimal example of
such a test runner:
if __name__ == '__main__':
    import doctest
    flags = doctest.REPORT_NDIFF|doctest.FAIL_FAST
    if len(sys.argv) > 1:
        name = sys.argv[1]
        if name in globals():
            obj = globals()[name]
        else:
            obj = __test__[name]
        doctest.run_docstring_examples(obj, globals(), name=name,
                                       optionflags=flags)
    else:
        fail, total = doctest.testmod(optionflags=flags)
        print("{} failures out of {} tests".format(fail, total))


Footnotes



[1]Examples containing both expected output and an exception are not supported.
Trying to guess where one ends and the other begins is too error-prone, and that
also makes for a confusing test.


