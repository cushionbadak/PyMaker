link:
library/codecs.html#encodings-and-unicode

docs:

Encodings and Unicode¶
Strings are stored internally as sequences of code points in
range 0x0–0x10FFFF.  (See PEP 393 for
more details about the implementation.)
Once a string object is used outside of CPU and memory, endianness
and how these arrays are stored as bytes become an issue.  As with other
codecs, serialising a string into a sequence of bytes is known as encoding,
and recreating the string from the sequence of bytes is known as decoding.
There are a variety of different text serialisation codecs, which are
collectivity referred to as text encodings.
The simplest text encoding (called 'latin-1' or 'iso-8859-1') maps
the code points 0–255 to the bytes 0x0–0xff, which means that a string
object that contains code points above U+00FF can’t be encoded with this
codec. Doing so will raise a UnicodeEncodeError that looks
like the following (although the details of the error message may differ):
UnicodeEncodeError: 'latin-1' codec can't encode character '\u1234' in
position 3: ordinal not in range(256).
There’s another group of encodings (the so called charmap encodings) that choose
a different subset of all Unicode code points and how these code points are
mapped to the bytes 0x0–0xff. To see how this is done simply open
e.g. encodings/cp1252.py (which is an encoding that is used primarily on
Windows). There’s a string constant with 256 characters that shows you which
character is mapped to which byte value.
All of these encodings can only encode 256 of the 1114112 code points
defined in Unicode. A simple and straightforward way that can store each Unicode
code point, is to store each code point as four consecutive bytes. There are two
possibilities: store the bytes in big endian or in little endian order. These
two encodings are called UTF-32-BE and UTF-32-LE respectively. Their
disadvantage is that if e.g. you use UTF-32-BE on a little endian machine you
will always have to swap bytes on encoding and decoding. UTF-32 avoids this
problem: bytes will always be in natural endianness. When these bytes are read
by a CPU with a different endianness, then bytes have to be swapped though. To
be able to detect the endianness of a UTF-16 or UTF-32 byte sequence,
there’s the so called BOM (“Byte Order Mark”). This is the Unicode character
U+FEFF. This character can be prepended to every UTF-16 or UTF-32
byte sequence. The byte swapped version of this character (0xFFFE) is an
illegal character that may not appear in a Unicode text. So when the
first character in an UTF-16 or UTF-32 byte sequence
appears to be a U+FFFE the bytes have to be swapped on decoding.
Unfortunately the character U+FEFF had a second purpose as
a ZERO WIDTH NO-BREAK SPACE: a character that has no width and doesn’t allow
a word to be split. It can e.g. be used to give hints to a ligature algorithm.
With Unicode 4.0 using U+FEFF as a ZERO WIDTH NO-BREAK SPACE has been
deprecated (with U+2060 (WORD JOINER) assuming this role). Nevertheless
Unicode software still must be able to handle U+FEFF in both roles: as a BOM
it’s a device to determine the storage layout of the encoded bytes, and vanishes
once the byte sequence has been decoded into a string; as a ZERO WIDTH
NO-BREAK SPACE it’s a normal character that will be decoded like any other.
There’s another encoding that is able to encoding the full range of Unicode
characters: UTF-8. UTF-8 is an 8-bit encoding, which means there are no issues
with byte order in UTF-8. Each byte in a UTF-8 byte sequence consists of two
parts: marker bits (the most significant bits) and payload bits. The marker bits
are a sequence of zero to four 1 bits followed by a 0 bit. Unicode characters are
encoded like this (with x being payload bits, which when concatenated give the
Unicode character):






Range
Encoding



U-00000000 … U-0000007F
0xxxxxxx

U-00000080 … U-000007FF
110xxxxx 10xxxxxx

U-00000800 … U-0000FFFF
1110xxxx 10xxxxxx 10xxxxxx

U-00010000 … U-0010FFFF
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx



The least significant bit of the Unicode character is the rightmost x bit.
As UTF-8 is an 8-bit encoding no BOM is required and any U+FEFF character in
the decoded string (even if it’s the first character) is treated as a ZERO
WIDTH NO-BREAK SPACE.
Without external information it’s impossible to reliably determine which
encoding was used for encoding a string. Each charmap encoding can
decode any random byte sequence. However that’s not possible with UTF-8, as
UTF-8 byte sequences have a structure that doesn’t allow arbitrary byte
sequences. To increase the reliability with which a UTF-8 encoding can be
detected, Microsoft invented a variant of UTF-8 (that Python 2.5 calls
"utf-8-sig") for its Notepad program: Before any of the Unicode characters
is written to the file, a UTF-8 encoded BOM (which looks like this as a byte
sequence: 0xef, 0xbb, 0xbf) is written. As it’s rather improbable
that any charmap encoded file starts with these byte values (which would e.g.
map to


LATIN SMALL LETTER I WITH DIAERESIS
RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
INVERTED QUESTION MARK


in iso-8859-1), this increases the probability that a utf-8-sig encoding can be
correctly guessed from the byte sequence. So here the BOM is not used to be able
to determine the byte order used for generating the byte sequence, but as a
signature that helps in guessing the encoding. On encoding the utf-8-sig codec
will write 0xef, 0xbb, 0xbf as the first three bytes to the file. On
decoding utf-8-sig will skip those three bytes if they appear as the first
three bytes in the file.  In UTF-8, the use of the BOM is discouraged and
should generally be avoided.
