link:
library/multiprocessing.html#module-multiprocessing.pool

docs:

Process Pools¶
One can create a pool of processes which will carry out tasks submitted to it
with the Pool class.


class multiprocessing.pool.Pool([processes[, initializer[, initargs[, maxtasksperchild[, context]]]]])¶
A process pool object which controls a pool of worker processes to which jobs
can be submitted.  It supports asynchronous results with timeouts and
callbacks and has a parallel map implementation.
processes is the number of worker processes to use.  If processes is
None then the number returned by os.cpu_count() is used.
If initializer is not None then each worker process will call
initializer(*initargs) when it starts.
maxtasksperchild is the number of tasks a worker process can complete
before it will exit and be replaced with a fresh worker process, to enable
unused resources to be freed. The default maxtasksperchild is None, which
means worker processes will live as long as the pool.
context can be used to specify the context used for starting
the worker processes.  Usually a pool is created using the
function multiprocessing.Pool() or the Pool() method
of a context object.  In both cases context is set
appropriately.
Note that the methods of the pool object should only be called by
the process which created the pool.

New in version 3.2: maxtasksperchild


New in version 3.4: context


Note
Worker processes within a Pool typically live for the complete
duration of the Pool’s work queue. A frequent pattern found in other
systems (such as Apache, mod_wsgi, etc) to free resources held by
workers is to allow a worker within a pool to complete only a set
amount of work before being exiting, being cleaned up and a new
process spawned to replace the old one. The maxtasksperchild
argument to the Pool exposes this ability to the end user.



apply(func[, args[, kwds]])¶
Call func with arguments args and keyword arguments kwds.  It blocks
until the result is ready. Given this blocks, apply_async() is
better suited for performing work in parallel. Additionally, func
is only executed in one of the workers of the pool.



apply_async(func[, args[, kwds[, callback[, error_callback]]]])¶
A variant of the apply() method which returns a result object.
If callback is specified then it should be a callable which accepts a
single argument.  When the result becomes ready callback is applied to
it, that is unless the call failed, in which case the error_callback
is applied instead.
If error_callback is specified then it should be a callable which
accepts a single argument.  If the target function fails, then
the error_callback is called with the exception instance.
Callbacks should complete immediately since otherwise the thread which
handles the results will get blocked.



map(func, iterable[, chunksize])¶
A parallel equivalent of the map() built-in function (it supports only
one iterable argument though).  It blocks until the result is ready.
This method chops the iterable into a number of chunks which it submits to
the process pool as separate tasks.  The (approximate) size of these
chunks can be specified by setting chunksize to a positive integer.
Note that it may cause high memory usage for very long iterables. Consider
using imap() or imap_unordered() with explicit chunksize
option for better efficiency.



map_async(func, iterable[, chunksize[, callback[, error_callback]]])¶
A variant of the map() method which returns a result object.
If callback is specified then it should be a callable which accepts a
single argument.  When the result becomes ready callback is applied to
it, that is unless the call failed, in which case the error_callback
is applied instead.
If error_callback is specified then it should be a callable which
accepts a single argument.  If the target function fails, then
the error_callback is called with the exception instance.
Callbacks should complete immediately since otherwise the thread which
handles the results will get blocked.



imap(func, iterable[, chunksize])¶
A lazier version of map().
The chunksize argument is the same as the one used by the map()
method.  For very long iterables using a large value for chunksize can
make the job complete much faster than using the default value of
1.
Also if chunksize is 1 then the next() method of the iterator
returned by the imap() method has an optional timeout parameter:
next(timeout) will raise multiprocessing.TimeoutError if the
result cannot be returned within timeout seconds.



imap_unordered(func, iterable[, chunksize])¶
The same as imap() except that the ordering of the results from the
returned iterator should be considered arbitrary.  (Only when there is
only one worker process is the order guaranteed to be “correct”.)



starmap(func, iterable[, chunksize])¶
Like map() except that the elements of the iterable are expected
to be iterables that are unpacked as arguments.
Hence an iterable of [(1,2), (3, 4)] results in [func(1,2),
func(3,4)].

New in version 3.3.




starmap_async(func, iterable[, chunksize[, callback[, error_callback]]])¶
A combination of starmap() and map_async() that iterates over
iterable of iterables and calls func with the iterables unpacked.
Returns a result object.

New in version 3.3.




close()¶
Prevents any more tasks from being submitted to the pool.  Once all the
tasks have been completed the worker processes will exit.



terminate()¶
Stops the worker processes immediately without completing outstanding
work.  When the pool object is garbage collected terminate() will be
called immediately.



join()¶
Wait for the worker processes to exit.  One must call close() or
terminate() before using join().


New in version 3.3: Pool objects now support the context management protocol – see
Context Manager Types.  __enter__() returns the
pool object, and __exit__() calls terminate().




class multiprocessing.pool.AsyncResult¶
The class of the result returned by Pool.apply_async() and
Pool.map_async().


get([timeout])¶
Return the result when it arrives.  If timeout is not None and the
result does not arrive within timeout seconds then
multiprocessing.TimeoutError is raised.  If the remote call raised
an exception then that exception will be reraised by get().



wait([timeout])¶
Wait until the result is available or until timeout seconds pass.



ready()¶
Return whether the call has completed.



successful()¶
Return whether the call completed without raising an exception.  Will
raise AssertionError if the result is not ready.


The following example demonstrates the use of a pool:
from multiprocessing import Pool
import time

def f(x):
    return x*x

if __name__ == '__main__':
    with Pool(processes=4) as pool:         # start 4 worker processes
        result = pool.apply_async(f, (10,)) # evaluate "f(10)" asynchronously in a single process
        print(result.get(timeout=1))        # prints "100" unless your computer is *very* slow

        print(pool.map(f, range(10)))       # prints "[0, 1, 4,..., 81]"

        it = pool.imap(f, range(10))
        print(next(it))                     # prints "0"
        print(next(it))                     # prints "1"
        print(it.next(timeout=1))           # prints "4" unless your computer is *very* slow

        result = pool.apply_async(time.sleep, (10,))
        print(result.get(timeout=1))        # raises multiprocessing.TimeoutError


